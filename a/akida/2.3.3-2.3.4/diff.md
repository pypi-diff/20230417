# Comparing `tmp/akida-2.3.3-cp39-cp39-win_amd64.whl.zip` & `tmp/akida-2.3.4-cp39-cp39-manylinux_2_24_aarch64.whl.zip`

## zipinfo {}

```diff
@@ -1,162 +1,191 @@
-Zip file size: 1256402 bytes, number of entries: 160
--rw-rw-rw-  2.0 fat     1389 b- defN 23-Apr-04 14:33 akida/__init__.py
--rw-rw-rw-  2.0 fat  1232896 b- defN 23-Apr-04 14:33 akida/akida.dll
--rw-rw-rw-  2.0 fat    35182 b- defN 23-Apr-04 14:33 akida/akida.lib
--rw-rw-rw-  2.0 fat     3022 b- defN 23-Apr-04 14:33 akida/cli.py
--rw-rw-rw-  2.0 fat  1174016 b- defN 23-Apr-04 14:33 akida/core.pyd
--rw-rw-rw-  2.0 fat     3971 b- defN 23-Apr-04 14:33 akida/layer.py
--rw-rw-rw-  2.0 fat   701440 b- defN 23-Apr-04 14:33 akida/libomp.dll
--rw-rw-rw-  2.0 fat    13080 b- defN 23-Apr-04 14:33 akida/model.py
--rw-rw-rw-  2.0 fat      634 b- defN 23-Apr-04 14:33 akida/np.py
--rw-rw-rw-  2.0 fat      474 b- defN 23-Apr-04 14:33 akida/sequence.py
--rw-rw-rw-  2.0 fat      961 b- defN 23-Apr-04 14:33 akida/sparsity.py
--rw-rw-rw-  2.0 fat     4196 b- defN 23-Apr-04 14:33 akida/statistics.py
--rw-rw-rw-  2.0 fat     6506 b- defN 23-Apr-04 14:33 akida/virtual_devices.py
--rw-rw-rw-  2.0 fat      439 b- defN 23-Apr-04 14:31 akida/api/akida/backend_type.h
--rw-rw-rw-  2.0 fat      645 b- defN 23-Apr-04 14:31 akida/api/akida/backends.h
--rw-rw-rw-  2.0 fat     1888 b- defN 23-Apr-04 14:31 akida/api/akida/device.h
--rw-rw-rw-  2.0 fat     3637 b- defN 23-Apr-04 14:31 akida/api/akida/layer.h
--rw-rw-rw-  2.0 fat     3448 b- defN 23-Apr-04 14:31 akida/api/akida/layer_params.h
--rw-rw-rw-  2.0 fat     2218 b- defN 23-Apr-04 14:31 akida/api/akida/learning_params.h
--rw-rw-rw-  2.0 fat      349 b- defN 23-Apr-04 14:31 akida/api/akida/mesh.h
--rw-rw-rw-  2.0 fat     1281 b- defN 23-Apr-04 14:31 akida/api/akida/mesh_mapper.h
--rw-rw-rw-  2.0 fat     7396 b- defN 23-Apr-04 14:31 akida/api/akida/model.h
--rw-rw-rw-  2.0 fat     1821 b- defN 23-Apr-04 14:31 akida/api/akida/np_mapping.h
--rw-rw-rw-  2.0 fat     2503 b- defN 23-Apr-04 14:31 akida/api/akida/sequence.h
--rw-rw-rw-  2.0 fat     1379 b- defN 23-Apr-04 14:31 akida/api/akida/variables.h
--rw-rw-rw-  2.0 fat     3168 b- defN 23-Apr-04 14:31 akida/api/host/circular_queue.h
--rw-rw-rw-  2.0 fat     1109 b- defN 23-Apr-04 14:31 akida/api/host/hardware_devices.h
--rw-rw-rw-  2.0 fat      345 b- defN 23-Apr-04 14:31 akida/api/host/hardware_drivers.h
--rw-rw-rw-  2.0 fat      876 b- defN 23-Apr-04 14:31 akida/api/host/host_device.h
--rw-rw-rw-  2.0 fat     1234 b- defN 23-Apr-04 14:31 akida/api/host/power_meter.h
--rw-rw-rw-  2.0 fat      542 b- defN 23-Apr-04 14:31 akida/api/host/soc_clock_mode.h
--rw-rw-rw-  2.0 fat      600 b- defN 23-Apr-04 14:31 akida/api/host/soc_driver.h
--rw-rw-rw-  2.0 fat      231 b- defN 23-Apr-04 14:31 akida/api/infra/exports.h
--rw-rw-rw-  2.0 fat     2139 b- defN 23-Apr-04 14:31 akida/api/infra/hardware_driver.h
--rw-rw-rw-  2.0 fat      610 b- defN 23-Apr-04 14:31 akida/api/infra/int_ops.h
--rw-rw-rw-  2.0 fat     1040 b- defN 23-Apr-04 14:31 akida/api/infra/registers_common.h
--rw-rw-rw-  2.0 fat      823 b- defN 23-Apr-04 14:31 akida/api/infra/system.h
--rw-rw-rw-  2.0 fat       27 b- defN 23-Apr-04 14:33 akida/compatibility/__init__.py
--rw-rw-rw-  2.0 fat     5989 b- defN 23-Apr-04 14:33 akida/compatibility/conversion.py
--rw-rw-rw-  2.0 fat      798 b- defN 23-Apr-04 14:33 akida/deploy/__init__.py
--rw-rw-rw-  2.0 fat     1651 b- defN 23-Apr-04 14:33 akida/deploy/engine.py
--rw-rw-rw-  2.0 fat      139 b- defN 23-Apr-04 14:33 akida/engine/CMakeLists.txt
--rw-rw-rw-  2.0 fat    12902 b- defN 23-Apr-04 14:33 akida/engine/README.md
--rw-rw-rw-  2.0 fat     1502 b- defN 23-Apr-04 14:33 akida/engine/api/akd1000/bare_metal_driver.h
--rw-rw-rw-  2.0 fat      835 b- defN 23-Apr-04 14:33 akida/engine/api/akd1000/memory_mapping.h
--rw-rw-rw-  2.0 fat     2604 b- defN 23-Apr-04 14:33 akida/engine/api/akd1000/registers_soc.h
--rw-rw-rw-  2.0 fat     6812 b- defN 23-Apr-04 14:33 akida/engine/api/akida/dense.h
--rw-rw-rw-  2.0 fat     8697 b- defN 23-Apr-04 14:33 akida/engine/api/akida/hardware_device.h
--rw-rw-rw-  2.0 fat     1083 b- defN 23-Apr-04 14:33 akida/engine/api/akida/hw_version.h
--rw-rw-rw-  2.0 fat      464 b- defN 23-Apr-04 14:33 akida/engine/api/akida/input_conversion.h
--rw-rw-rw-  2.0 fat     1125 b- defN 23-Apr-04 14:33 akida/engine/api/akida/np.h
--rw-rw-rw-  2.0 fat     1620 b- defN 23-Apr-04 14:33 akida/engine/api/akida/program_memory_info.h
--rw-rw-rw-  2.0 fat     2960 b- defN 23-Apr-04 14:33 akida/engine/api/akida/registers_top_level.h
--rw-rw-rw-  2.0 fat     4078 b- defN 23-Apr-04 14:33 akida/engine/api/akida/shape.h
--rw-rw-rw-  2.0 fat     2630 b- defN 23-Apr-04 14:33 akida/engine/api/akida/sparse.h
--rw-rw-rw-  2.0 fat     6493 b- defN 23-Apr-04 14:33 akida/engine/api/akida/tensor.h
--rw-rw-rw-  2.0 fat      138 b- defN 23-Apr-04 14:33 akida/engine/api/akida/version.h
--rw-rw-rw-  2.0 fat      231 b- defN 23-Apr-04 14:33 akida/engine/api/infra/exports.h
--rw-rw-rw-  2.0 fat     2139 b- defN 23-Apr-04 14:33 akida/engine/api/infra/hardware_driver.h
--rw-rw-rw-  2.0 fat      610 b- defN 23-Apr-04 14:33 akida/engine/api/infra/int_ops.h
--rw-rw-rw-  2.0 fat     1040 b- defN 23-Apr-04 14:33 akida/engine/api/infra/registers_common.h
--rw-rw-rw-  2.0 fat      823 b- defN 23-Apr-04 14:33 akida/engine/api/infra/system.h
--rw-rw-rw-  2.0 fat      965 b- defN 23-Apr-04 14:33 akida/engine/cmake/akida-engine.cmake
--rw-rw-rw-  2.0 fat     1303 b- defN 23-Apr-04 14:33 akida/engine/devices/akd1000/bare_metal_driver.cpp
--rw-rw-rw-  2.0 fat    32276 b- defN 23-Apr-04 14:33 akida/engine/inc/engine/akida_device_program_fb_generated.h
--rw-rw-rw-  2.0 fat      994 b- defN 23-Apr-04 14:33 akida/engine/inc/engine/dma.h
--rw-rw-rw-  2.0 fat      981 b- defN 23-Apr-04 14:33 akida/engine/inc/engine/dma_config_ops.h
--rw-rw-rw-  2.0 fat     1898 b- defN 23-Apr-04 14:33 akida/engine/inc/engine/int_conversion.h
--rw-rw-rw-  2.0 fat     8961 b- defN 23-Apr-04 14:33 akida/engine/src/dense.cpp
--rw-rw-rw-  2.0 fat      196 b- defN 23-Apr-04 14:33 akida/engine/src/device_memory.h
--rw-rw-rw-  2.0 fat     2965 b- defN 23-Apr-04 14:33 akida/engine/src/dma_cnp_events.h
--rw-rw-rw-  2.0 fat     1116 b- defN 23-Apr-04 14:33 akida/engine/src/dma_config_format.h
--rw-rw-rw-  2.0 fat     2303 b- defN 23-Apr-04 14:33 akida/engine/src/dma_config_ops.cpp
--rw-rw-rw-  2.0 fat     4399 b- defN 23-Apr-04 14:33 akida/engine/src/dma_desc_format.h
--rw-rw-rw-  2.0 fat     4474 b- defN 23-Apr-04 14:33 akida/engine/src/dma_desc_ops.cpp
--rw-rw-rw-  2.0 fat     1202 b- defN 23-Apr-04 14:33 akida/engine/src/dma_desc_ops.h
--rw-rw-rw-  2.0 fat    16815 b- defN 23-Apr-04 14:33 akida/engine/src/dma_engine.cpp
--rw-rw-rw-  2.0 fat      447 b- defN 23-Apr-04 14:33 akida/engine/src/dma_engine.h
--rw-rw-rw-  2.0 fat     5769 b- defN 23-Apr-04 14:33 akida/engine/src/dma_engine_ops.h
--rw-rw-rw-  2.0 fat     1396 b- defN 23-Apr-04 14:33 akida/engine/src/dma_events.h
--rw-rw-rw-  2.0 fat      786 b- defN 23-Apr-04 14:33 akida/engine/src/dma_events_format.h
--rw-rw-rw-  2.0 fat    14845 b- defN 23-Apr-04 14:33 akida/engine/src/dma_events_ops.cpp
--rw-rw-rw-  2.0 fat     1285 b- defN 23-Apr-04 14:33 akida/engine/src/dma_events_ops.h
--rw-rw-rw-  2.0 fat     2705 b- defN 23-Apr-04 14:33 akida/engine/src/dma_fnp_events.h
--rw-rw-rw-  2.0 fat     3189 b- defN 23-Apr-04 14:33 akida/engine/src/dma_hrc_events.h
--rw-rw-rw-  2.0 fat     2071 b- defN 23-Apr-04 14:33 akida/engine/src/dma_image_ops.cpp
--rw-rw-rw-  2.0 fat      471 b- defN 23-Apr-04 14:33 akida/engine/src/dma_image_ops.h
--rw-rw-rw-  2.0 fat     1752 b- defN 23-Apr-04 14:33 akida/engine/src/external_mem_mgr.cpp
--rw-rw-rw-  2.0 fat     1173 b- defN 23-Apr-04 14:33 akida/engine/src/external_mem_mgr.h
--rw-rw-rw-  2.0 fat      620 b- defN 23-Apr-04 14:33 akida/engine/src/fnp2_mem_conf_reg.h
--rw-rw-rw-  2.0 fat      267 b- defN 23-Apr-04 14:33 akida/engine/src/hardware_device.cpp
--rw-rw-rw-  2.0 fat    31796 b- defN 23-Apr-04 14:33 akida/engine/src/hardware_device_impl.cpp
--rw-rw-rw-  2.0 fat     4050 b- defN 23-Apr-04 14:33 akida/engine/src/hardware_device_impl.h
--rw-rw-rw-  2.0 fat     1099 b- defN 23-Apr-04 14:33 akida/engine/src/hw_version.cpp
--rw-rw-rw-  2.0 fat      941 b- defN 23-Apr-04 14:33 akida/engine/src/input_conversion.cpp
--rw-rw-rw-  2.0 fat     2658 b- defN 23-Apr-04 14:33 akida/engine/src/memory_mgr.cpp
--rw-rw-rw-  2.0 fat     1101 b- defN 23-Apr-04 14:33 akida/engine/src/memory_mgr.h
--rw-rw-rw-  2.0 fat      699 b- defN 23-Apr-04 14:33 akida/engine/src/memory_utils.cpp
--rw-rw-rw-  2.0 fat      405 b- defN 23-Apr-04 14:33 akida/engine/src/memory_utils.h
--rw-rw-rw-  2.0 fat     1219 b- defN 23-Apr-04 14:33 akida/engine/src/multipass_memory.cpp
--rw-rw-rw-  2.0 fat      803 b- defN 23-Apr-04 14:33 akida/engine/src/multipass_memory.h
--rw-rw-rw-  2.0 fat     2198 b- defN 23-Apr-04 14:33 akida/engine/src/pipeline_state.h
--rw-rw-rw-  2.0 fat     2990 b- defN 23-Apr-04 14:33 akida/engine/src/program_memory_info.cpp
--rw-rw-rw-  2.0 fat    28397 b- defN 23-Apr-04 14:33 akida/engine/src/program_play.cpp
--rw-rw-rw-  2.0 fat     2836 b- defN 23-Apr-04 14:33 akida/engine/src/program_play.h
--rw-rw-rw-  2.0 fat     8060 b- defN 23-Apr-04 14:33 akida/engine/src/registers_dma_engine.h
--rw-rw-rw-  2.0 fat      824 b- defN 23-Apr-04 14:33 akida/engine/src/registers_reset.h
--rw-rw-rw-  2.0 fat     1022 b- defN 23-Apr-04 14:33 akida/engine/src/reset_nps.cpp
--rw-rw-rw-  2.0 fat      300 b- defN 23-Apr-04 14:33 akida/engine/src/reset_nps.h
--rw-rw-rw-  2.0 fat      646 b- defN 23-Apr-04 14:33 akida/engine/src/sparse.cpp
--rw-rw-rw-  2.0 fat      835 b- defN 23-Apr-04 14:33 akida/engine/src/tensor.cpp
--rw-rw-rw-  2.0 fat      204 b- defN 23-Apr-04 14:33 akida/engine/src/version.cpp
--rw-rw-rw-  2.0 fat      974 b- defN 23-Apr-04 14:33 akida/generate/__init__.py
--rw-rw-rw-  2.0 fat     2440 b- defN 23-Apr-04 14:33 akida/generate/application_generator.py
--rw-rw-rw-  2.0 fat     5482 b- defN 23-Apr-04 14:33 akida/generate/array_to_cpp.py
--rw-rw-rw-  2.0 fat     1080 b- defN 23-Apr-04 14:33 akida/generate/model.py
--rw-rw-rw-  2.0 fat      396 b- defN 23-Apr-04 14:33 akida/generate/test/template.py
--rw-rw-rw-  2.0 fat     4459 b- defN 23-Apr-04 14:33 akida/generate/test/test_tools.py
--rw-rw-rw-  2.0 fat     1199 b- defN 23-Apr-04 14:28 akida/generate/test/cmake/akida-model.cmake
--rw-rw-rw-  2.0 fat     1350 b- defN 23-Apr-04 14:33 akida/generate/test/engine/test_generator.py
--rw-rw-rw-  2.0 fat     4458 b- defN 23-Apr-04 14:33 akida/generate/test/engine/app_templates/test.cpp
--rw-rw-rw-  2.0 fat     1360 b- defN 23-Apr-04 14:33 akida/generate/test/engine/app_templates/test.h
--rw-rw-rw-  2.0 fat      830 b- defN 23-Apr-04 14:33 akida/generate/test/engine/fixtures/simple_conv_evaluate_v2.py
--rw-rw-rw-  2.0 fat      829 b- defN 23-Apr-04 14:33 akida/generate/test/engine/fixtures/simple_conv_v2.py
--rw-rw-rw-  2.0 fat     1702 b- defN 23-Apr-04 14:33 akida/generate/test/engine/fixtures/simple_hrc_v2.py
--rw-rw-rw-  2.0 fat      894 b- defN 23-Apr-04 14:33 akida/generate/test/engine/fixtures/simple_sep_conv_v2.py
--rw-rw-rw-  2.0 fat      803 b- defN 23-Apr-04 14:33 akida/generate/test/engine/fixtures/test_fnp2.py
--rw-rw-rw-  2.0 fat      871 b- defN 23-Apr-04 14:33 akida/generate/test/engine/fixtures/test_multiple_inputs.py
--rw-rw-rw-  2.0 fat     1487 b- defN 23-Apr-04 14:33 akida/generate/test/model/test_generator.py
--rw-rw-rw-  2.0 fat     1102 b- defN 23-Apr-04 14:33 akida/generate/test/model/app_templates/test.cpp
--rw-rw-rw-  2.0 fat      576 b- defN 23-Apr-04 14:33 akida/generate/test/model/app_templates/test.h
--rw-rw-rw-  2.0 fat      835 b- defN 23-Apr-04 14:33 akida/layers/__init__.py
--rw-rw-rw-  2.0 fat     1878 b- defN 23-Apr-04 14:33 akida/layers/add.py
--rw-rw-rw-  2.0 fat     2207 b- defN 23-Apr-04 14:33 akida/layers/attention.py
--rw-rw-rw-  2.0 fat     1272 b- defN 23-Apr-04 14:33 akida/layers/batch_normalization.py
--rw-rw-rw-  2.0 fat     1059 b- defN 23-Apr-04 14:33 akida/layers/concatenate.py
--rw-rw-rw-  2.0 fat     3800 b- defN 23-Apr-04 14:33 akida/layers/conv2d.py
--rw-rw-rw-  2.0 fat     2687 b- defN 23-Apr-04 14:33 akida/layers/conv2d_transpose.py
--rw-rw-rw-  2.0 fat     4072 b- defN 23-Apr-04 14:33 akida/layers/convolutional.py
--rw-rw-rw-  2.0 fat     2245 b- defN 23-Apr-04 14:33 akida/layers/dense1d.py
--rw-rw-rw-  2.0 fat     2317 b- defN 23-Apr-04 14:33 akida/layers/dense2d.py
--rw-rw-rw-  2.0 fat     3471 b- defN 23-Apr-04 14:33 akida/layers/depthwise_conv2d.py
--rw-rw-rw-  2.0 fat     2729 b- defN 23-Apr-04 14:33 akida/layers/depthwise_conv2d_transpose.py
--rw-rw-rw-  2.0 fat      497 b- defN 23-Apr-04 14:33 akida/layers/dequantizer.py
--rw-rw-rw-  2.0 fat     1212 b- defN 23-Apr-04 14:33 akida/layers/extract_token.py
--rw-rw-rw-  2.0 fat     1837 b- defN 23-Apr-04 14:33 akida/layers/fully_connected.py
--rw-rw-rw-  2.0 fat     4359 b- defN 23-Apr-04 14:33 akida/layers/input_conv2d.py
--rw-rw-rw-  2.0 fat     4490 b- defN 23-Apr-04 14:33 akida/layers/input_convolutional.py
--rw-rw-rw-  2.0 fat     1643 b- defN 23-Apr-04 14:33 akida/layers/input_data.py
--rw-rw-rw-  2.0 fat     1365 b- defN 23-Apr-04 14:33 akida/layers/madnorm.py
--rw-rw-rw-  2.0 fat     4253 b- defN 23-Apr-04 14:33 akida/layers/separable_convolutional.py
--rw-rw-rw-  2.0 fat     1215 b- defN 23-Apr-04 14:33 akida/layers/shiftmax.py
--rw-rw-rw-  2.0 fat     3138 b- defN 23-Apr-04 14:33 akida/layers/stem.py
--rw-rw-rw-  2.0 fat     8974 b- defN 23-Apr-04 14:33 akida-2.3.3.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     1980 b- defN 23-Apr-04 14:33 akida-2.3.3.dist-info/LICENSE.3rdparty
--rw-rw-rw-  2.0 fat      624 b- defN 23-Apr-04 14:33 akida-2.3.3.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 23-Apr-04 14:33 akida-2.3.3.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       41 b- defN 23-Apr-04 14:33 akida-2.3.3.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat        6 b- defN 23-Apr-04 14:33 akida-2.3.3.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat    14131 b- defN 23-Apr-04 14:33 akida-2.3.3.dist-info/RECORD
-160 files, 3606314 bytes uncompressed, 1234006 bytes compressed:  65.8%
+Zip file size: 1027915 bytes, number of entries: 189
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida-2.3.4.dist-info/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida.libs/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/api/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/compatibility/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/deploy/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/engine/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/generate/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/layers/
+-rw-r--r--  2.0 unx     1368 b- defN 23-Apr-17 08:57 akida/__init__.py
+-rw-r--r--  2.0 unx     2944 b- defN 23-Apr-17 08:57 akida/cli.py
+-rwxr-xr-x  2.0 unx   679536 b- defN 23-Apr-17 08:57 akida/core.so
+-rw-r--r--  2.0 unx     3930 b- defN 23-Apr-17 08:57 akida/layer.py
+-rwxr-xr-x  2.0 unx  1822120 b- defN 23-Apr-17 08:57 akida/libakida.so.2
+-rw-r--r--  2.0 unx    13165 b- defN 23-Apr-17 08:57 akida/model.py
+-rw-r--r--  2.0 unx      613 b- defN 23-Apr-17 08:57 akida/np.py
+-rw-r--r--  2.0 unx      457 b- defN 23-Apr-17 08:57 akida/sequence.py
+-rw-r--r--  2.0 unx      932 b- defN 23-Apr-17 08:57 akida/sparsity.py
+-rw-r--r--  2.0 unx     4082 b- defN 23-Apr-17 08:57 akida/statistics.py
+-rw-r--r--  2.0 unx     6385 b- defN 23-Apr-17 08:57 akida/virtual_devices.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/api/akida/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/api/host/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/api/infra/
+-rw-r--r--  2.0 unx      417 b- defN 23-Apr-17 08:57 akida/api/akida/backend_type.h
+-rw-r--r--  2.0 unx      619 b- defN 23-Apr-17 08:57 akida/api/akida/backends.h
+-rw-r--r--  2.0 unx     1815 b- defN 23-Apr-17 08:57 akida/api/akida/device.h
+-rw-r--r--  2.0 unx     3494 b- defN 23-Apr-17 08:57 akida/api/akida/layer.h
+-rw-r--r--  2.0 unx     3304 b- defN 23-Apr-17 08:57 akida/api/akida/layer_params.h
+-rw-r--r--  2.0 unx     2133 b- defN 23-Apr-17 08:57 akida/api/akida/learning_params.h
+-rw-r--r--  2.0 unx      328 b- defN 23-Apr-17 08:57 akida/api/akida/mesh.h
+-rw-r--r--  2.0 unx     1227 b- defN 23-Apr-17 08:57 akida/api/akida/mesh_mapper.h
+-rw-r--r--  2.0 unx     7170 b- defN 23-Apr-17 08:57 akida/api/akida/model.h
+-rw-r--r--  2.0 unx     1757 b- defN 23-Apr-17 08:57 akida/api/akida/np_mapping.h
+-rw-r--r--  2.0 unx     2416 b- defN 23-Apr-17 08:57 akida/api/akida/sequence.h
+-rw-r--r--  2.0 unx      148 b- defN 23-Apr-17 08:57 akida/api/akida/variable_helpers.h
+-rw-r--r--  2.0 unx     1324 b- defN 23-Apr-17 08:57 akida/api/akida/variables.h
+-rw-r--r--  2.0 unx     3035 b- defN 23-Apr-17 08:57 akida/api/host/circular_queue.h
+-rw-r--r--  2.0 unx     1076 b- defN 23-Apr-17 08:57 akida/api/host/hardware_devices.h
+-rw-r--r--  2.0 unx      328 b- defN 23-Apr-17 08:57 akida/api/host/hardware_drivers.h
+-rw-r--r--  2.0 unx      841 b- defN 23-Apr-17 08:57 akida/api/host/host_device.h
+-rw-r--r--  2.0 unx     1186 b- defN 23-Apr-17 08:57 akida/api/host/power_meter.h
+-rw-r--r--  2.0 unx      521 b- defN 23-Apr-17 08:57 akida/api/host/soc_clock_mode.h
+-rw-r--r--  2.0 unx      578 b- defN 23-Apr-17 08:57 akida/api/host/soc_driver.h
+-rw-r--r--  2.0 unx      222 b- defN 23-Apr-17 08:57 akida/api/infra/exports.h
+-rw-r--r--  2.0 unx     2056 b- defN 23-Apr-17 08:57 akida/api/infra/hardware_driver.h
+-rw-r--r--  2.0 unx      584 b- defN 23-Apr-17 08:57 akida/api/infra/int_ops.h
+-rw-r--r--  2.0 unx     1002 b- defN 23-Apr-17 08:57 akida/api/infra/registers_common.h
+-rw-r--r--  2.0 unx      788 b- defN 23-Apr-17 08:57 akida/api/infra/system.h
+-rw-r--r--  2.0 unx       26 b- defN 23-Apr-17 08:57 akida/compatibility/__init__.py
+-rw-r--r--  2.0 unx     5827 b- defN 23-Apr-17 08:57 akida/compatibility/conversion.py
+-rw-r--r--  2.0 unx      782 b- defN 23-Apr-17 08:57 akida/deploy/__init__.py
+-rw-r--r--  2.0 unx     1611 b- defN 23-Apr-17 08:57 akida/deploy/engine.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/engine/api/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/engine/cmake/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/engine/devices/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/engine/inc/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/engine/src/
+-rw-r--r--  2.0 unx      132 b- defN 23-Apr-17 08:57 akida/engine/CMakeLists.txt
+-rw-r--r--  2.0 unx    12524 b- defN 23-Apr-17 08:57 akida/engine/README.md
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/engine/api/akd1000/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/engine/api/akida/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/engine/api/infra/
+-rw-r--r--  2.0 unx     1447 b- defN 23-Apr-17 08:57 akida/engine/api/akd1000/bare_metal_driver.h
+-rw-r--r--  2.0 unx      808 b- defN 23-Apr-17 08:57 akida/engine/api/akd1000/memory_mapping.h
+-rw-r--r--  2.0 unx     2547 b- defN 23-Apr-17 08:57 akida/engine/api/akd1000/registers_soc.h
+-rw-r--r--  2.0 unx     6605 b- defN 23-Apr-17 08:57 akida/engine/api/akida/dense.h
+-rw-r--r--  2.0 unx     8444 b- defN 23-Apr-17 08:57 akida/engine/api/akida/hardware_device.h
+-rw-r--r--  2.0 unx     1047 b- defN 23-Apr-17 08:57 akida/engine/api/akida/hw_version.h
+-rw-r--r--  2.0 unx      441 b- defN 23-Apr-17 08:57 akida/engine/api/akida/input_conversion.h
+-rw-r--r--  2.0 unx     1077 b- defN 23-Apr-17 08:57 akida/engine/api/akida/np.h
+-rw-r--r--  2.0 unx     1565 b- defN 23-Apr-17 08:57 akida/engine/api/akida/program_memory_info.h
+-rw-r--r--  2.0 unx     2895 b- defN 23-Apr-17 08:57 akida/engine/api/akida/registers_top_level.h
+-rw-r--r--  2.0 unx     3933 b- defN 23-Apr-17 08:57 akida/engine/api/akida/shape.h
+-rw-r--r--  2.0 unx     2524 b- defN 23-Apr-17 08:57 akida/engine/api/akida/sparse.h
+-rw-r--r--  2.0 unx     6227 b- defN 23-Apr-17 08:57 akida/engine/api/akida/tensor.h
+-rw-r--r--  2.0 unx      129 b- defN 23-Apr-17 08:57 akida/engine/api/akida/version.h
+-rw-r--r--  2.0 unx      222 b- defN 23-Apr-17 08:57 akida/engine/api/infra/exports.h
+-rw-r--r--  2.0 unx     2056 b- defN 23-Apr-17 08:57 akida/engine/api/infra/hardware_driver.h
+-rw-r--r--  2.0 unx      584 b- defN 23-Apr-17 08:57 akida/engine/api/infra/int_ops.h
+-rw-r--r--  2.0 unx     1002 b- defN 23-Apr-17 08:57 akida/engine/api/infra/registers_common.h
+-rw-r--r--  2.0 unx      788 b- defN 23-Apr-17 08:57 akida/engine/api/infra/system.h
+-rw-r--r--  2.0 unx      927 b- defN 23-Apr-17 08:57 akida/engine/cmake/akida-engine.cmake
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/engine/devices/akd1000/
+-rw-r--r--  2.0 unx     1263 b- defN 23-Apr-17 08:57 akida/engine/devices/akd1000/bare_metal_driver.cpp
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/engine/inc/engine/
+-rw-r--r--  2.0 unx    31379 b- defN 23-Apr-17 08:57 akida/engine/inc/engine/akida_device_program_fb_generated.h
+-rw-r--r--  2.0 unx      962 b- defN 23-Apr-17 08:57 akida/engine/inc/engine/dma.h
+-rw-r--r--  2.0 unx      940 b- defN 23-Apr-17 08:57 akida/engine/inc/engine/dma_config_ops.h
+-rw-r--r--  2.0 unx     1838 b- defN 23-Apr-17 08:57 akida/engine/inc/engine/int_conversion.h
+-rw-r--r--  2.0 unx     8687 b- defN 23-Apr-17 08:57 akida/engine/src/dense.cpp
+-rw-r--r--  2.0 unx      184 b- defN 23-Apr-17 08:57 akida/engine/src/device_memory.h
+-rw-r--r--  2.0 unx     2877 b- defN 23-Apr-17 08:57 akida/engine/src/dma_cnp_events.h
+-rw-r--r--  2.0 unx     1081 b- defN 23-Apr-17 08:57 akida/engine/src/dma_config_format.h
+-rw-r--r--  2.0 unx     2226 b- defN 23-Apr-17 08:57 akida/engine/src/dma_config_ops.cpp
+-rw-r--r--  2.0 unx     4262 b- defN 23-Apr-17 08:57 akida/engine/src/dma_desc_format.h
+-rw-r--r--  2.0 unx     4365 b- defN 23-Apr-17 08:57 akida/engine/src/dma_desc_ops.cpp
+-rw-r--r--  2.0 unx     1171 b- defN 23-Apr-17 08:57 akida/engine/src/dma_desc_ops.h
+-rw-r--r--  2.0 unx    16406 b- defN 23-Apr-17 08:57 akida/engine/src/dma_engine.cpp
+-rw-r--r--  2.0 unx      417 b- defN 23-Apr-17 08:57 akida/engine/src/dma_engine.h
+-rw-r--r--  2.0 unx     5635 b- defN 23-Apr-17 08:57 akida/engine/src/dma_engine_ops.h
+-rw-r--r--  2.0 unx     1342 b- defN 23-Apr-17 08:57 akida/engine/src/dma_events.h
+-rw-r--r--  2.0 unx      760 b- defN 23-Apr-17 08:57 akida/engine/src/dma_events_format.h
+-rw-r--r--  2.0 unx    14472 b- defN 23-Apr-17 08:57 akida/engine/src/dma_events_ops.cpp
+-rw-r--r--  2.0 unx     1247 b- defN 23-Apr-17 08:57 akida/engine/src/dma_events_ops.h
+-rw-r--r--  2.0 unx     2626 b- defN 23-Apr-17 08:57 akida/engine/src/dma_fnp_events.h
+-rw-r--r--  2.0 unx     3095 b- defN 23-Apr-17 08:57 akida/engine/src/dma_hrc_events.h
+-rw-r--r--  2.0 unx     2015 b- defN 23-Apr-17 08:57 akida/engine/src/dma_image_ops.cpp
+-rw-r--r--  2.0 unx      455 b- defN 23-Apr-17 08:57 akida/engine/src/dma_image_ops.h
+-rw-r--r--  2.0 unx     1684 b- defN 23-Apr-17 08:57 akida/engine/src/external_mem_mgr.cpp
+-rw-r--r--  2.0 unx     1130 b- defN 23-Apr-17 08:57 akida/engine/src/external_mem_mgr.h
+-rw-r--r--  2.0 unx      601 b- defN 23-Apr-17 08:57 akida/engine/src/fnp2_mem_conf_reg.h
+-rw-r--r--  2.0 unx      255 b- defN 23-Apr-17 08:57 akida/engine/src/hardware_device.cpp
+-rw-r--r--  2.0 unx    30960 b- defN 23-Apr-17 08:57 akida/engine/src/hardware_device_impl.cpp
+-rw-r--r--  2.0 unx     3905 b- defN 23-Apr-17 08:57 akida/engine/src/hardware_device_impl.h
+-rw-r--r--  2.0 unx     1067 b- defN 23-Apr-17 08:57 akida/engine/src/hw_version.cpp
+-rw-r--r--  2.0 unx      904 b- defN 23-Apr-17 08:57 akida/engine/src/input_conversion.cpp
+-rw-r--r--  2.0 unx     2581 b- defN 23-Apr-17 08:57 akida/engine/src/memory_mgr.cpp
+-rw-r--r--  2.0 unx     1054 b- defN 23-Apr-17 08:57 akida/engine/src/memory_mgr.h
+-rw-r--r--  2.0 unx      676 b- defN 23-Apr-17 08:57 akida/engine/src/memory_utils.cpp
+-rw-r--r--  2.0 unx      387 b- defN 23-Apr-17 08:57 akida/engine/src/memory_utils.h
+-rw-r--r--  2.0 unx     1185 b- defN 23-Apr-17 08:57 akida/engine/src/multipass_memory.cpp
+-rw-r--r--  2.0 unx      778 b- defN 23-Apr-17 08:57 akida/engine/src/multipass_memory.h
+-rw-r--r--  2.0 unx     2124 b- defN 23-Apr-17 08:57 akida/engine/src/pipeline_state.h
+-rw-r--r--  2.0 unx     2906 b- defN 23-Apr-17 08:57 akida/engine/src/program_memory_info.cpp
+-rw-r--r--  2.0 unx    27663 b- defN 23-Apr-17 08:57 akida/engine/src/program_play.cpp
+-rw-r--r--  2.0 unx     2771 b- defN 23-Apr-17 08:57 akida/engine/src/program_play.h
+-rw-r--r--  2.0 unx     7875 b- defN 23-Apr-17 08:57 akida/engine/src/registers_dma_engine.h
+-rw-r--r--  2.0 unx      800 b- defN 23-Apr-17 08:57 akida/engine/src/registers_reset.h
+-rw-r--r--  2.0 unx      994 b- defN 23-Apr-17 08:57 akida/engine/src/reset_nps.cpp
+-rw-r--r--  2.0 unx      286 b- defN 23-Apr-17 08:57 akida/engine/src/reset_nps.h
+-rw-r--r--  2.0 unx      622 b- defN 23-Apr-17 08:57 akida/engine/src/sparse.cpp
+-rw-r--r--  2.0 unx      801 b- defN 23-Apr-17 08:57 akida/engine/src/tensor.cpp
+-rw-r--r--  2.0 unx      194 b- defN 23-Apr-17 08:57 akida/engine/src/version.cpp
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/generate/test/
+-rw-r--r--  2.0 unx      955 b- defN 23-Apr-17 08:57 akida/generate/__init__.py
+-rw-r--r--  2.0 unx     2374 b- defN 23-Apr-17 08:57 akida/generate/application_generator.py
+-rw-r--r--  2.0 unx     5306 b- defN 23-Apr-17 08:57 akida/generate/array_to_cpp.py
+-rw-r--r--  2.0 unx     1054 b- defN 23-Apr-17 08:57 akida/generate/model.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/generate/test/cmake/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/generate/test/engine/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/generate/test/model/
+-rw-r--r--  2.0 unx      386 b- defN 23-Apr-17 08:57 akida/generate/test/template.py
+-rw-r--r--  2.0 unx     4365 b- defN 23-Apr-17 08:57 akida/generate/test/test_tools.py
+-rw-r--r--  2.0 unx     1169 b- defN 23-Apr-17 08:57 akida/generate/test/cmake/akida-model.cmake
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/generate/test/engine/app_templates/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/generate/test/engine/fixtures/
+-rw-r--r--  2.0 unx     1313 b- defN 23-Apr-17 08:57 akida/generate/test/engine/test_generator.py
+-rw-r--r--  2.0 unx     4337 b- defN 23-Apr-17 08:57 akida/generate/test/engine/app_templates/test.cpp
+-rw-r--r--  2.0 unx     1315 b- defN 23-Apr-17 08:57 akida/generate/test/engine/app_templates/test.h
+-rw-r--r--  2.0 unx      801 b- defN 23-Apr-17 08:57 akida/generate/test/engine/fixtures/simple_conv_evaluate_v2.py
+-rw-r--r--  2.0 unx      800 b- defN 23-Apr-17 08:57 akida/generate/test/engine/fixtures/simple_conv_v2.py
+-rw-r--r--  2.0 unx     1655 b- defN 23-Apr-17 08:57 akida/generate/test/engine/fixtures/simple_hrc_v2.py
+-rw-r--r--  2.0 unx      863 b- defN 23-Apr-17 08:57 akida/generate/test/engine/fixtures/simple_sep_conv_v2.py
+-rw-r--r--  2.0 unx      775 b- defN 23-Apr-17 08:57 akida/generate/test/engine/fixtures/test_fnp2.py
+-rw-r--r--  2.0 unx      842 b- defN 23-Apr-17 08:57 akida/generate/test/engine/fixtures/test_multiple_inputs.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-17 08:57 akida/generate/test/model/app_templates/
+-rw-r--r--  2.0 unx     1450 b- defN 23-Apr-17 08:57 akida/generate/test/model/test_generator.py
+-rw-r--r--  2.0 unx     1063 b- defN 23-Apr-17 08:57 akida/generate/test/model/app_templates/test.cpp
+-rw-r--r--  2.0 unx      557 b- defN 23-Apr-17 08:57 akida/generate/test/model/app_templates/test.h
+-rw-r--r--  2.0 unx      814 b- defN 23-Apr-17 08:57 akida/layers/__init__.py
+-rw-r--r--  2.0 unx     1830 b- defN 23-Apr-17 08:57 akida/layers/add.py
+-rw-r--r--  2.0 unx     2152 b- defN 23-Apr-17 08:57 akida/layers/attention.py
+-rw-r--r--  2.0 unx     1236 b- defN 23-Apr-17 08:57 akida/layers/batch_normalization.py
+-rw-r--r--  2.0 unx     1028 b- defN 23-Apr-17 08:57 akida/layers/concatenate.py
+-rw-r--r--  2.0 unx     3715 b- defN 23-Apr-17 08:57 akida/layers/conv2d.py
+-rw-r--r--  2.0 unx     2624 b- defN 23-Apr-17 08:57 akida/layers/conv2d_transpose.py
+-rw-r--r--  2.0 unx     3985 b- defN 23-Apr-17 08:57 akida/layers/convolutional.py
+-rw-r--r--  2.0 unx     2190 b- defN 23-Apr-17 08:57 akida/layers/dense1d.py
+-rw-r--r--  2.0 unx     2261 b- defN 23-Apr-17 08:57 akida/layers/dense2d.py
+-rw-r--r--  2.0 unx     3401 b- defN 23-Apr-17 08:57 akida/layers/depthwise_conv2d.py
+-rw-r--r--  2.0 unx     2668 b- defN 23-Apr-17 08:57 akida/layers/depthwise_conv2d_transpose.py
+-rw-r--r--  2.0 unx      479 b- defN 23-Apr-17 08:57 akida/layers/dequantizer.py
+-rw-r--r--  2.0 unx     1176 b- defN 23-Apr-17 08:57 akida/layers/extract_token.py
+-rw-r--r--  2.0 unx     1790 b- defN 23-Apr-17 08:57 akida/layers/fully_connected.py
+-rw-r--r--  2.0 unx     4263 b- defN 23-Apr-17 08:57 akida/layers/input_conv2d.py
+-rw-r--r--  2.0 unx     4395 b- defN 23-Apr-17 08:57 akida/layers/input_convolutional.py
+-rw-r--r--  2.0 unx     1603 b- defN 23-Apr-17 08:57 akida/layers/input_data.py
+-rw-r--r--  2.0 unx     1327 b- defN 23-Apr-17 08:57 akida/layers/madnorm.py
+-rw-r--r--  2.0 unx     4163 b- defN 23-Apr-17 08:57 akida/layers/separable_convolutional.py
+-rw-r--r--  2.0 unx     1175 b- defN 23-Apr-17 08:57 akida/layers/shiftmax.py
+-rw-r--r--  2.0 unx     3064 b- defN 23-Apr-17 08:57 akida/layers/stem.py
+-rw-r--r--  2.0 unx     8933 b- defN 23-Apr-17 08:57 akida-2.3.4.dist-info/LICENSE
+-rw-r--r--  2.0 unx     1942 b- defN 23-Apr-17 08:57 akida-2.3.4.dist-info/LICENSE.3rdparty
+-rw-r--r--  2.0 unx      604 b- defN 23-Apr-17 08:57 akida-2.3.4.dist-info/METADATA
+-rw-r--r--  2.0 unx      113 b- defN 23-Apr-17 08:57 akida-2.3.4.dist-info/WHEEL
+-rw-r--r--  2.0 unx       41 b- defN 23-Apr-17 08:57 akida-2.3.4.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        6 b- defN 23-Apr-17 08:57 akida-2.3.4.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    14329 b- defN 23-Apr-17 08:57 akida-2.3.4.dist-info/RECORD
+-rw-r--r--  2.0 unx   209120 b- defN 23-Apr-17 08:57 akida.libs/libgomp-01527a09.so.1.0.0
+189 files, 3161786 bytes uncompressed, 1002029 bytes compressed:  68.3%
```

## zipnote {}

```diff
@@ -1,26 +1,47 @@
-Filename: akida/__init__.py
+Filename: akida/
+Comment: 
+
+Filename: akida-2.3.4.dist-info/
+Comment: 
+
+Filename: akida.libs/
+Comment: 
+
+Filename: akida/api/
+Comment: 
+
+Filename: akida/compatibility/
+Comment: 
+
+Filename: akida/deploy/
+Comment: 
+
+Filename: akida/engine/
+Comment: 
+
+Filename: akida/generate/
 Comment: 
 
-Filename: akida/akida.dll
+Filename: akida/layers/
 Comment: 
 
-Filename: akida/akida.lib
+Filename: akida/__init__.py
 Comment: 
 
 Filename: akida/cli.py
 Comment: 
 
-Filename: akida/core.pyd
+Filename: akida/core.so
 Comment: 
 
 Filename: akida/layer.py
 Comment: 
 
-Filename: akida/libomp.dll
+Filename: akida/libakida.so.2
 Comment: 
 
 Filename: akida/model.py
 Comment: 
 
 Filename: akida/np.py
 Comment: 
@@ -33,14 +54,23 @@
 
 Filename: akida/statistics.py
 Comment: 
 
 Filename: akida/virtual_devices.py
 Comment: 
 
+Filename: akida/api/akida/
+Comment: 
+
+Filename: akida/api/host/
+Comment: 
+
+Filename: akida/api/infra/
+Comment: 
+
 Filename: akida/api/akida/backend_type.h
 Comment: 
 
 Filename: akida/api/akida/backends.h
 Comment: 
 
 Filename: akida/api/akida/device.h
@@ -66,14 +96,17 @@
 
 Filename: akida/api/akida/np_mapping.h
 Comment: 
 
 Filename: akida/api/akida/sequence.h
 Comment: 
 
+Filename: akida/api/akida/variable_helpers.h
+Comment: 
+
 Filename: akida/api/akida/variables.h
 Comment: 
 
 Filename: akida/api/host/circular_queue.h
 Comment: 
 
 Filename: akida/api/host/hardware_devices.h
@@ -117,20 +150,44 @@
 
 Filename: akida/deploy/__init__.py
 Comment: 
 
 Filename: akida/deploy/engine.py
 Comment: 
 
+Filename: akida/engine/api/
+Comment: 
+
+Filename: akida/engine/cmake/
+Comment: 
+
+Filename: akida/engine/devices/
+Comment: 
+
+Filename: akida/engine/inc/
+Comment: 
+
+Filename: akida/engine/src/
+Comment: 
+
 Filename: akida/engine/CMakeLists.txt
 Comment: 
 
 Filename: akida/engine/README.md
 Comment: 
 
+Filename: akida/engine/api/akd1000/
+Comment: 
+
+Filename: akida/engine/api/akida/
+Comment: 
+
+Filename: akida/engine/api/infra/
+Comment: 
+
 Filename: akida/engine/api/akd1000/bare_metal_driver.h
 Comment: 
 
 Filename: akida/engine/api/akd1000/memory_mapping.h
 Comment: 
 
 Filename: akida/engine/api/akd1000/registers_soc.h
@@ -183,17 +240,23 @@
 
 Filename: akida/engine/api/infra/system.h
 Comment: 
 
 Filename: akida/engine/cmake/akida-engine.cmake
 Comment: 
 
+Filename: akida/engine/devices/akd1000/
+Comment: 
+
 Filename: akida/engine/devices/akd1000/bare_metal_driver.cpp
 Comment: 
 
+Filename: akida/engine/inc/engine/
+Comment: 
+
 Filename: akida/engine/inc/engine/akida_device_program_fb_generated.h
 Comment: 
 
 Filename: akida/engine/inc/engine/dma.h
 Comment: 
 
 Filename: akida/engine/inc/engine/dma_config_ops.h
@@ -330,35 +393,53 @@
 
 Filename: akida/engine/src/tensor.cpp
 Comment: 
 
 Filename: akida/engine/src/version.cpp
 Comment: 
 
+Filename: akida/generate/test/
+Comment: 
+
 Filename: akida/generate/__init__.py
 Comment: 
 
 Filename: akida/generate/application_generator.py
 Comment: 
 
 Filename: akida/generate/array_to_cpp.py
 Comment: 
 
 Filename: akida/generate/model.py
 Comment: 
 
+Filename: akida/generate/test/cmake/
+Comment: 
+
+Filename: akida/generate/test/engine/
+Comment: 
+
+Filename: akida/generate/test/model/
+Comment: 
+
 Filename: akida/generate/test/template.py
 Comment: 
 
 Filename: akida/generate/test/test_tools.py
 Comment: 
 
 Filename: akida/generate/test/cmake/akida-model.cmake
 Comment: 
 
+Filename: akida/generate/test/engine/app_templates/
+Comment: 
+
+Filename: akida/generate/test/engine/fixtures/
+Comment: 
+
 Filename: akida/generate/test/engine/test_generator.py
 Comment: 
 
 Filename: akida/generate/test/engine/app_templates/test.cpp
 Comment: 
 
 Filename: akida/generate/test/engine/app_templates/test.h
@@ -378,14 +459,17 @@
 
 Filename: akida/generate/test/engine/fixtures/test_fnp2.py
 Comment: 
 
 Filename: akida/generate/test/engine/fixtures/test_multiple_inputs.py
 Comment: 
 
+Filename: akida/generate/test/model/app_templates/
+Comment: 
+
 Filename: akida/generate/test/model/test_generator.py
 Comment: 
 
 Filename: akida/generate/test/model/app_templates/test.cpp
 Comment: 
 
 Filename: akida/generate/test/model/app_templates/test.h
@@ -453,29 +537,32 @@
 
 Filename: akida/layers/shiftmax.py
 Comment: 
 
 Filename: akida/layers/stem.py
 Comment: 
 
-Filename: akida-2.3.3.dist-info/LICENSE
+Filename: akida-2.3.4.dist-info/LICENSE
+Comment: 
+
+Filename: akida-2.3.4.dist-info/LICENSE.3rdparty
 Comment: 
 
-Filename: akida-2.3.3.dist-info/LICENSE.3rdparty
+Filename: akida-2.3.4.dist-info/METADATA
 Comment: 
 
-Filename: akida-2.3.3.dist-info/METADATA
+Filename: akida-2.3.4.dist-info/WHEEL
 Comment: 
 
-Filename: akida-2.3.3.dist-info/WHEEL
+Filename: akida-2.3.4.dist-info/entry_points.txt
 Comment: 
 
-Filename: akida-2.3.3.dist-info/entry_points.txt
+Filename: akida-2.3.4.dist-info/top_level.txt
 Comment: 
 
-Filename: akida-2.3.3.dist-info/top_level.txt
+Filename: akida-2.3.4.dist-info/RECORD
 Comment: 
 
-Filename: akida-2.3.3.dist-info/RECORD
+Filename: akida.libs/libgomp-01527a09.so.1.0.0
 Comment: 
 
 Zip file comment:
```

## filetype from file(1)

```diff
@@ -1 +1 @@
-Zip archive data, at least v2.0 to extract, compression method=deflate
+Zip archive data, at least v2.0 to extract, compression method=store
```

## akida/__init__.py

```diff
@@ -1,40 +1,40 @@
-from .core import (BackendType, Padding, PoolType, LayerType, HwVersion, NP,
-                   MeshMapper, Model, Layer, AkidaUnsupervised, Device,
-                   HardwareDevice, devices, NSoC_v1, NSoC_v2, Latest,
-                   TwoNodesIP_v1, AKD500_v1, PowerMeter, PowerEvent, Sequence,
-                   Pass, soc, LayerParams, Optimizer, __version__,
-                   get_program_memory_infos, nn)
-
-from .layer import *
-from .model import *
-from .layers import *
-from .statistics import Statistics
-from .sparsity import evaluate_sparsity
-from .np import *
-from .sequence import *
-from .virtual_devices import *
-
-Model.__str__ = model_str
-Model.__repr__ = model_repr
-Model.statistics = statistics
-Model.summary = summary
-Model.predict_classes = predict_classes
-Model.to_dict = model_to_dict
-Model.from_dict = staticmethod(model_from_dict)
-Model.to_json = model_to_json
-Model.from_json = staticmethod(model_from_json)
-
-Layer.__str__ = layer_str
-Layer.__repr__ = layer_repr
-Layer.set_variable = set_variable
-Layer.get_variable = get_variable
-Layer.get_variable_names = get_variable_names
-Layer.get_learning_histogram = get_learning_histogram
-Layer.to_dict = layer_to_dict
-
-Sequence.__repr__ = sequence_repr
-Pass.__repr__ = pass_repr
-
-NP.Info.__repr__ = np_info_repr
-NP.Mesh.__repr__ = np_mesh_repr
-NP.Mapping.__repr__ = np_mapping_repr
+from .core import (BackendType, Padding, PoolType, LayerType, HwVersion, NP,
+                   MeshMapper, Model, Layer, AkidaUnsupervised, Device,
+                   HardwareDevice, devices, NSoC_v1, NSoC_v2, Latest,
+                   TwoNodesIP_v1, AKD500_v1, PowerMeter, PowerEvent, Sequence,
+                   Pass, soc, LayerParams, Optimizer, __version__,
+                   get_program_memory_infos, nn, evaluate_bitwidth)
+
+from .layer import *
+from .model import *
+from .layers import *
+from .statistics import Statistics
+from .sparsity import evaluate_sparsity
+from .np import *
+from .sequence import *
+from .virtual_devices import *
+
+Model.__str__ = model_str
+Model.__repr__ = model_repr
+Model.statistics = statistics
+Model.summary = summary
+Model.predict_classes = predict_classes
+Model.to_dict = model_to_dict
+Model.from_dict = staticmethod(model_from_dict)
+Model.to_json = model_to_json
+Model.from_json = staticmethod(model_from_json)
+
+Layer.__str__ = layer_str
+Layer.__repr__ = layer_repr
+Layer.set_variable = set_variable
+Layer.get_variable = get_variable
+Layer.get_variable_names = get_variable_names
+Layer.get_learning_histogram = get_learning_histogram
+Layer.to_dict = layer_to_dict
+
+Sequence.__repr__ = sequence_repr
+Pass.__repr__ = pass_repr
+
+NP.Info.__repr__ = np_info_repr
+NP.Mesh.__repr__ = np_mesh_repr
+NP.Mapping.__repr__ = np_mapping_repr
```

## akida/cli.py

 * *Ordering differences only*

```diff
@@ -1,78 +1,78 @@
-import argparse
-import os
-import glob
-
-from .core import devices, __version__
-from .deploy import deploy_engine
-from .generate.model import deploy_cmake
-from .generate.application_generator import generate_files
-
-
-def list_devices():
-    devices_list = devices()
-    if len(devices_list) == 0:
-        print("No devices detected")
-    else:
-        print("Available devices:")
-        for device in devices_list:
-            print(device.desc)
-
-
-def default_fixture_path():
-    current_dir = os.path.abspath(os.path.dirname(__file__))
-    fixture_path = current_dir + "/generate/test/engine/fixtures/*.py"
-    return glob.glob(fixture_path)
-
-
-def main():
-    parser = argparse.ArgumentParser()
-    sp = parser.add_subparsers(dest="action")
-    sp.add_parser("devices", help="List available devices")
-    sp.add_parser("version", help="Return akida version")
-    gen_parser = sp.add_parser("generate",
-                               help="Generate application(s) from fixture file(s).")
-    gen_parser.add_argument("--fixture-files",
-                            help="A list of python fixture files",
-                            nargs="+",
-                            type=str,
-                            required=True)
-    gen_parser.add_argument("--modules-paths",
-                            help="Path to additional modules required for fixtures",
-                            nargs="+",
-                            type=str,
-                            default=None)
-    gen_parser.add_argument("--dest-path",
-                            type=str,
-                            default=None,
-                            required=True,
-                            help="The destination path.")
-    engine_parser = sp.add_parser("engine", help="Deploy engine sources and applications")
-    # Create parent subparser for arguments shared between engine methods
-    engine_parent = argparse.ArgumentParser(add_help=False)
-    engine_parent.add_argument(
-        "--dest-path",
-        type=str,
-        default=None,
-        required=True,
-        help="The destination path.")
-    engine_action_parser = engine_parser.add_subparsers(
-        dest="engine_action",
-        help="Action: deploy or generate.")
-    engine_action_parser.add_parser(
-        "deploy",
-        help="Deploy the engine library.",
-        parents=[engine_parent])
-    args = parser.parse_args()
-    if args.action == "devices":
-        list_devices()
-    elif args.action == "version":
-        print(__version__)
-    elif args.action == "engine":
-        if args.engine_action == "deploy":
-            deploy_engine(args.dest_path)
-            fixture_files = default_fixture_path()
-            dest_path = os.path.join(args.dest_path, "engine/test/akd1000")
-            generate_files(fixture_files, dest_path, None)
-    elif args.action == "generate":
-        deploy_cmake(args.dest_path)
-        generate_files(args.fixture_files, args.dest_path, args.modules_paths)
+import argparse
+import os
+import glob
+
+from .core import devices, __version__
+from .deploy import deploy_engine
+from .generate.model import deploy_cmake
+from .generate.application_generator import generate_files
+
+
+def list_devices():
+    devices_list = devices()
+    if len(devices_list) == 0:
+        print("No devices detected")
+    else:
+        print("Available devices:")
+        for device in devices_list:
+            print(device.desc)
+
+
+def default_fixture_path():
+    current_dir = os.path.abspath(os.path.dirname(__file__))
+    fixture_path = current_dir + "/generate/test/engine/fixtures/*.py"
+    return glob.glob(fixture_path)
+
+
+def main():
+    parser = argparse.ArgumentParser()
+    sp = parser.add_subparsers(dest="action")
+    sp.add_parser("devices", help="List available devices")
+    sp.add_parser("version", help="Return akida version")
+    gen_parser = sp.add_parser("generate",
+                               help="Generate application(s) from fixture file(s).")
+    gen_parser.add_argument("--fixture-files",
+                            help="A list of python fixture files",
+                            nargs="+",
+                            type=str,
+                            required=True)
+    gen_parser.add_argument("--modules-paths",
+                            help="Path to additional modules required for fixtures",
+                            nargs="+",
+                            type=str,
+                            default=None)
+    gen_parser.add_argument("--dest-path",
+                            type=str,
+                            default=None,
+                            required=True,
+                            help="The destination path.")
+    engine_parser = sp.add_parser("engine", help="Deploy engine sources and applications")
+    # Create parent subparser for arguments shared between engine methods
+    engine_parent = argparse.ArgumentParser(add_help=False)
+    engine_parent.add_argument(
+        "--dest-path",
+        type=str,
+        default=None,
+        required=True,
+        help="The destination path.")
+    engine_action_parser = engine_parser.add_subparsers(
+        dest="engine_action",
+        help="Action: deploy or generate.")
+    engine_action_parser.add_parser(
+        "deploy",
+        help="Deploy the engine library.",
+        parents=[engine_parent])
+    args = parser.parse_args()
+    if args.action == "devices":
+        list_devices()
+    elif args.action == "version":
+        print(__version__)
+    elif args.action == "engine":
+        if args.engine_action == "deploy":
+            deploy_engine(args.dest_path)
+            fixture_files = default_fixture_path()
+            dest_path = os.path.join(args.dest_path, "engine/test/akd1000")
+            generate_files(fixture_files, dest_path, None)
+    elif args.action == "generate":
+        deploy_cmake(args.dest_path)
+        generate_files(args.fixture_files, args.dest_path, args.modules_paths)
```

## akida/layer.py

```diff
@@ -1,125 +1,127 @@
-import numpy as np
-
-
-def layer_str(self):
-    layer_type = str(self.parameters.layer_type).split('.')[-1]
-    layer_type = layer_type.replace("FullyConnected", "Fully.")
-    layer_type = layer_type.replace("Convolutional", "Conv.")
-    layer_type = layer_type.replace("Separable", "Sep.")
-    return self.name + " (" + layer_type + ")"
-
-
-def layer_repr(self):
-    data = "<akida.Layer, type=" + str(self.parameters.layer_type)
-    data += ", name=" + self.name
-    data += ", input_dims=" + str(self.input_dims)
-    data += ", output_dims=" + str(self.output_dims)
-    if self.mapping is not None:
-        data += ",nps=" + repr(self.mapping.nps)
-    data += ">"
-    return data
-
-
-def layer_to_dict(self):
-    """Provide a dict representation of the Layer
-
-    Returns:
-        dict: a Layer dictionary.
-    """
-    params = {name: getattr(self.parameters, name) for name in dir(self.parameters)}
-    params["layer_type"] = self.parameters.layer_type.name
-    variables = {}
-    for name in self.variables.names:
-        var = self.variables[name]
-        variables[name] = {
-            "shape": var.shape,
-            "dtype": str(var.dtype),
-            "data": var.tolist()
-        }
-    has_shapes = "input_channels" in dir(self.parameters) or len(self.inbounds) > 0
-    return {
-        "name": self.name,
-        "parameters": params,
-        "variables": variables,
-        "inbounds": [layer.name for layer in self.inbounds],
-        "input_shape": self.input_dims if has_shapes else None,
-        "output_shape": self.output_dims if has_shapes else None
-    }
-
-
-def set_variable(self, name, values):
-    """Set the value of a layer variable.
-
-    Layer variables are named entities representing the weights or
-    thresholds used during inference:
-
-    * Weights variables are typically integer arrays of shape:
-
-      (num_neurons, features/channels, y, x) col-major ordered ('F')
-
-    or equivalently:
-
-      (x, y, features/channels, num_neurons) row-major ('C').
-
-    * Threshold variables are typically integer or float arrays of shape:
-      (num_neurons).
-
-    Args:
-        name (str): the variable name.
-        values (:obj:`numpy.ndarray`): a numpy.ndarray containing the variable values.
-
-    """
-    self.variables[name] = np.ascontiguousarray(values)
-
-
-def get_variable(self, name):
-    """Get the value of a layer variable.
-
-    Layer variables are named entities representing the weights or
-    thresholds used during inference:
-
-    * Weights variables are typically integer arrays of shape:
-      (x, y, features/channels, num_neurons) row-major ('C').
-    * Threshold variables are typically integer or float arrays of shape:
-      (num_neurons).
-
-    Args:
-        name (str): the variable name.
-
-    Returns:
-        :obj:`numpy.ndarray`: an array containing the variable.
-
-    """
-    return self.variables[name]
-
-
-def get_variable_names(self):
-    """Get the list of variable names for this layer.
-
-    Returns:
-        a list of variable names.
-
-    """
-    return self.variables.names
-
-
-def get_learning_histogram(self):
-    """Returns an histogram of learning percentages.
-
-    Returns a list of learning percentages and the associated number of
-    neurons.
-
-    Returns:
-        :obj:`numpy.ndarray`: a (n,2) numpy.ndarray containing the learning
-        percentages and the number of neurons.
-
-    """
-    histogram = np.zeros((100, 2), dtype=np.uint32)
-    num_neurons = self.get_variable("weights").shape[3]
-    num_weights = np.count_nonzero(self.get_variable("weights")) / num_neurons
-    for i in range(num_neurons):
-        threshold_learn = self.get_variable("threshold_learning")[i]
-        learn_percentage = int(100 * threshold_learn / num_weights)
-        histogram[learn_percentage, 0] = learn_percentage
-        histogram[learn_percentage, 1] += 1
-    return histogram[histogram[:, 0] != 0, :]
+import numpy as np
+from .core import evaluate_bitwidth
+
+
+def layer_str(self):
+    layer_type = str(self.parameters.layer_type).split('.')[-1]
+    layer_type = layer_type.replace("FullyConnected", "Fully.")
+    layer_type = layer_type.replace("Convolutional", "Conv.")
+    layer_type = layer_type.replace("Separable", "Sep.")
+    return self.name + " (" + layer_type + ")"
+
+
+def layer_repr(self):
+    data = "<akida.Layer, type=" + str(self.parameters.layer_type)
+    data += ", name=" + self.name
+    data += ", input_dims=" + str(self.input_dims)
+    data += ", output_dims=" + str(self.output_dims)
+    if self.mapping is not None:
+        data += ",nps=" + repr(self.mapping.nps)
+    data += ">"
+    return data
+
+
+def layer_to_dict(self):
+    """Provide a dict representation of the Layer
+
+    Returns:
+        dict: a Layer dictionary.
+    """
+    params = {name: getattr(self.parameters, name) for name in dir(self.parameters)}
+    params["layer_type"] = self.parameters.layer_type.name
+    variables = {}
+    for name in self.variables.names:
+        var = self.variables[name]
+        variables[name] = {
+            "shape": var.shape,
+            "dtype": str(var.dtype),
+            "bitwidth": evaluate_bitwidth(var),
+            "data": var.tolist()
+        }
+    has_shapes = "input_channels" in dir(self.parameters) or len(self.inbounds) > 0
+    return {
+        "name": self.name,
+        "parameters": params,
+        "variables": variables,
+        "inbounds": [layer.name for layer in self.inbounds],
+        "input_shape": self.input_dims if has_shapes else None,
+        "output_shape": self.output_dims if has_shapes else None
+    }
+
+
+def set_variable(self, name, values):
+    """Set the value of a layer variable.
+
+    Layer variables are named entities representing the weights or
+    thresholds used during inference:
+
+    * Weights variables are typically integer arrays of shape:
+
+      (num_neurons, features/channels, y, x) col-major ordered ('F')
+
+    or equivalently:
+
+      (x, y, features/channels, num_neurons) row-major ('C').
+
+    * Threshold variables are typically integer or float arrays of shape:
+      (num_neurons).
+
+    Args:
+        name (str): the variable name.
+        values (:obj:`numpy.ndarray`): a numpy.ndarray containing the variable values.
+
+    """
+    self.variables[name] = np.ascontiguousarray(values)
+
+
+def get_variable(self, name):
+    """Get the value of a layer variable.
+
+    Layer variables are named entities representing the weights or
+    thresholds used during inference:
+
+    * Weights variables are typically integer arrays of shape:
+      (x, y, features/channels, num_neurons) row-major ('C').
+    * Threshold variables are typically integer or float arrays of shape:
+      (num_neurons).
+
+    Args:
+        name (str): the variable name.
+
+    Returns:
+        :obj:`numpy.ndarray`: an array containing the variable.
+
+    """
+    return self.variables[name]
+
+
+def get_variable_names(self):
+    """Get the list of variable names for this layer.
+
+    Returns:
+        a list of variable names.
+
+    """
+    return self.variables.names
+
+
+def get_learning_histogram(self):
+    """Returns an histogram of learning percentages.
+
+    Returns a list of learning percentages and the associated number of
+    neurons.
+
+    Returns:
+        :obj:`numpy.ndarray`: a (n,2) numpy.ndarray containing the learning
+        percentages and the number of neurons.
+
+    """
+    histogram = np.zeros((100, 2), dtype=np.uint32)
+    num_neurons = self.get_variable("weights").shape[3]
+    num_weights = np.count_nonzero(self.get_variable("weights")) / num_neurons
+    for i in range(num_neurons):
+        threshold_learn = self.get_variable("threshold_learning")[i]
+        learn_percentage = int(100 * threshold_learn / num_weights)
+        histogram[learn_percentage, 0] = learn_percentage
+        histogram[learn_percentage, 1] += 1
+    return histogram[histogram[:, 0] != 0, :]
```

## akida/model.py

```diff
@@ -1,356 +1,363 @@
-import json
-import numpy as np
-import re
-
-from .core import Model, Layer, LayerType, LayerParams, AkidaUnsupervised, __version__
-from .statistics import Statistics
-
-
-def model_str(self):
-    data = "akida.Model, layer_count=" + str(self.get_layer_count())
-    data += ", sequence_count=" + str(len(self.sequences))
-    out_dims = self.output_shape if self.get_layer_count() else []
-    data += ", output_shape=" + str(out_dims)
-    return data
-
-
-def model_repr(self):
-    out_dims = self.output_shape if self.get_layer_count() else []
-    data = "<akida.Model, layer_count=" + str(self.get_layer_count())
-    data += ", output_shape=" + str(out_dims)
-    data += ", sequences=" + repr(self.sequences) + ">"
-    return data
-
-
-def model_to_dict(self):
-    """Provide a dict representation of the Model
-
-    Returns:
-        dict: a Model dictionary.
-    """
-    learning = None
-    if self.learning:
-        learning = {name: getattr(self.learning, name) for name in dir(self.learning)}
-    return {
-        "version": __version__,
-        "layers": [layer.to_dict() for layer in self.layers],
-        "learning": learning,
-        "input_shape": self.input_shape,
-        "output_shape": self.output_shape
-    }
-
-
-def model_from_dict(model_dict):
-    """Instantiate a Model from a dict representation
-
-    Args:
-        model_dict(dict): a Model dictionary.
-
-    Returns:
-        :obj:`Model`: a Model.
-    """
-    # Check major and minor version
-    model_version = model_dict["version"]
-    major, minor, _ = __version__.split('.')
-    model_major, model_minor, _ = model_version.split('.')
-    if major != model_major or minor != model_minor:
-        raise ValueError(f"Serialized model was generated by version {model_version}, which"
-                         f" is incompatible with current version {__version__}")
-    # Instantiate an empty Model
-    model = Model()
-    # Add layers
-    layers = model_dict["layers"]
-    for layer_dict in layers:
-        layer_name = layer_dict["name"]
-        # Extract layer parameters
-        layer_params = layer_dict["parameters"]
-        if layer_params is None:
-            raise ValueError("Cannot deserialize a Layer without parameters")
-        layer_params = layer_params.copy()
-        # Evaluate the Layer type from its serialized name
-        layer_type = getattr(LayerType, layer_params["layer_type"])
-        # Remove layer_type
-        layer_params.pop("layer_type")
-        # Instantiate layer
-        layer = Layer(LayerParams(layer_type, layer_params), layer_name)
-        # Evaluate the layer inbounds
-        inbounds = [model.get_layer(ib) for ib in layer_dict["inbounds"]]
-        # Add it to the model
-        model.add(layer, inbounds)
-    # If needed compile it to set learning parameters and variables
-    if model_dict["learning"] is not None:
-        learning = model_dict["learning"].copy()
-        # Some parameters must be integer
-        for name in ["num_weights", "num_classes"]:
-            learning[name] = int(learning[name])
-        model.compile(AkidaUnsupervised(**learning))
-    # Now that the Model is fully initialized, load layer variables
-    for layer_dict in layers:
-        layer_name = layer_dict["name"]
-        # Get corresponding Layer in model
-        layer = model.get_layer(layer_name)
-        # Iterate over variables
-        variables_dict = layer_dict["variables"]
-        for name in variables_dict:
-            variable_dict = variables_dict[name]
-            dtype = variable_dict["dtype"]
-            variable = np.array(variable_dict["data"]).astype(dtype)
-            layer.variables[name] = variable
-    return model
-
-
-def model_to_json(self):
-    """Provide a JSON representation of the Model
-
-    Returns:
-        str: a JSON-formatted string corresponding to a Model.
-    """
-    # Pretty-print serialized model
-    model_str = json.dumps(self.to_dict(), indent=2)
-
-    # Remove spurious line jumps in serialized arrays of numbers
-    def align_arrays(m):
-        # Just return the extracted pattern
-        return m.group(1)
-    # Look for lines starting with whitespaces and:
-    # - a signed integer or float number with an optional opening square bracket,
-    # - stand-alone opening or closing square brackets
-    return re.sub(r'\n\s+(\[?-?[\d\.]+,?|\]|\[)', align_arrays, model_str)
-
-
-def model_from_json(model_str):
-    """Instantiate a Model from a JSON representation
-
-    Args:
-        model_str(str): a JSON-formatted string corresponding to a Model.
-
-    Returns:
-        :obj:`Model`: a Model.
-    """
-    return Model.from_dict(json.loads(model_str))
-
-
-@property
-def statistics(self):
-    """Get statistics by sequence for this model.
-
-    Returns:
-        a dictionary of :obj:`SequenceStatistics` indexed by name.
-
-    """
-    return Statistics(model=self)
-
-
-def summary(self):
-    """Prints a string summary of the model.
-
-    This method prints a summary of the model with details for every layer,
-    grouped by sequences:
-
-    - name and type in the first column
-    - output shape
-    - kernel shape
-
-    If there is any layer with unsupervised learning enabled, it will list
-    them, with these details:
-
-    - name of layer
-    - number of incoming connections
-    - number of weights per neuron
-
-    """
-
-    def _model_summary(model):
-        # prepare headers
-        headers = ['Input shape', 'Output shape', 'Sequences', 'Layers']
-        # prepare an empty table
-        table = [headers]
-
-        if not model.layers:
-            row = [
-                'N/A',
-                'N/A',
-                str(len(model.sequences)),
-                str(len(model.layers))
-            ]
-        else:
-            row = [
-                str(model.input_shape),
-                str(model.output_shape),
-                str(len(model.sequences)),
-                str(len(model.layers))
-            ]
-        # add the number of NPs if the model is mapped
-        has_program = np.any([s.program is not None for s in self.sequences])
-        if has_program:
-            headers.append('NPs')
-            nb_nps = 0
-            for sequence in model.sequences:
-                for current_pass in sequence.passes:
-                    for layer in current_pass.layers:
-                        if layer.parameters.layer_type != LayerType.InputConvolutional:
-                            nb_nps += 0 if layer.mapping is None else len(layer.mapping.nps)
-            row.append(nb_nps)
-
-        # prepare an empty table
-        table = [headers]
-        table.append(row)
-        print_table(table, "Model Summary")
-
-    def _get_backend_info(sequence):
-        backend = str(sequence.backend).split('.')[-1]
-        sequence_info = sequence.name + " (" + backend + ")"
-        if sequence.program is not None:
-            sequence_info += " - size: " + str(len(sequence.program)) + " bytes"
-        return sequence_info
-
-    def _layers_summary(sequences):
-        # Prepare headers
-        headers = ['Layer (type)', 'Output shape', 'Kernel shape']
-        has_program = np.any([s.program is not None for s in self.sequences])
-        if has_program:
-            headers.append('NPs')
-        # prepare an empty table
-        table = [headers]
-        new_splits = []
-        has_multi_pass = len(sequences[0].passes) > 1
-        nb_pass = 0
-        for s in sequences:
-            info = _get_backend_info(s)
-            new_splits.append(info)
-            for p in s.passes:
-                if has_multi_pass:
-                    nb_pass += 1
-                    if nb_pass > 1:
-                        new_splits.append(f"pass {nb_pass}")
-                for layer in p.layers:
-                    nps = None if layer.mapping is None else layer.mapping.nps
-                    # layer name (type)
-                    layer_type = layer.parameters.layer_type
-                    # kernel shape
-                    if "weights" in layer.get_variable_names():
-                        kernel_shape = layer.get_variable("weights").shape
-                    else:
-                        kernel_shape = "N/A"
-                    # Prepare row and add it
-                    row = [str(layer), str(layer.output_dims), str(kernel_shape)]
-                    if has_program:
-                        if layer_type == LayerType.InputConvolutional or nps is None:
-                            row.append('N/A')
-                        else:
-                            row.append(len(nps))
-                    table.append(row)
-                    if len(table) - 1 > len(new_splits):
-                        new_splits.append(False)
-                    if layer_type == LayerType.SeparableConvolutional:
-                        # Add pointwise weights on a second line
-                        kernel_pw_shape = layer.get_variable("weights_pw").shape
-                        row = ['', '', kernel_pw_shape]
-                        if has_program:
-                            row.append('')
-                        table.append(row)
-                        new_splits.append(False)
-        print_table(table, None, new_splits)
-
-    def _learning_summary(model):
-        layer = model.layers[-1]
-        # Prepare headers
-        headers = ["Learning Layer", "# Input Conn.", "# Weights"]
-        table = [headers]
-        name = layer.name
-        # Input connections is the product of input dims
-        input_connections = np.prod(layer.input_dims)
-        # Num non zero weights per neuron (counted on first neuron)
-        weights = layer.get_variable("weights")
-        incoming_conn = np.count_nonzero(weights[:, :, :, 0])
-        # Prepare row and add it
-        row = [name, str(input_connections), incoming_conn]
-        table.append(row)
-        print()
-        print_table(table, "Learning Summary")
-
-    # Print first the general Model summary
-    _model_summary(self)
-    # Print sequences summary
-    if self.sequences:
-        print()
-        _layers_summary(self.sequences)
-    # Print learning summary if we have more than one input layer
-    if len(self.layers) > 1 and self.learning:
-        # Only the last layer of a model can learn
-        print()
-        _learning_summary(self)
-
-
-def print_table(table, title, new_splits=None):
-    # Convert to np.array
-    to_str = np.vectorize(str, otypes=['O'])
-    table = to_str(table)
-    # get column lengths
-    str_len_f = np.vectorize(lambda cell: len(str(cell)))
-    str_lens = np.amax(str_len_f(table), 0)
-    line_len = np.sum(str_lens)
-    # Prepare format rows
-    size_formats = np.vectorize(lambda cell: f"{{:{cell}.{cell}}}")
-    format_strings = size_formats(str_lens)
-    format_row = "  ".join(format_strings)
-    # Generate separators
-    separator_len = line_len + 2 * len(table[0])
-    separator = "_" * separator_len
-    double_separator = "=" * separator_len
-
-    # Print header
-    center_format = f"{{:^{separator_len}}}"
-    if title is not None:
-        print(center_format.format(title))
-    print(separator)
-    print(format_row.format(*table[0]))
-
-    rows = table[1:, :]
-    if new_splits is None:
-        new_splits = [False] * len(rows)
-    assert len(rows) == len(new_splits)
-    if not any(new_splits):
-        print(double_separator)
-    # Print body
-    for row, new_split in zip(rows, new_splits):
-        if new_split:
-            # Display a line break only for sequences
-            if "pass" not in new_split:
-                print()
-            # Compute the number of char on each side of the text
-            space_len = max((separator_len - len(new_split)) / 2., 1.)
-            space_left = "=" * int(np.ceil(space_len - 1))
-            space_right = "=" * int(np.floor(space_len - 1))
-            print(space_left, new_split, space_right)
-            # Display a line break only for sequences
-            if "pass" not in new_split:
-                print()
-        # Don't use a separator line on first row
-        elif row[0] != rows[0][0]:
-            print(separator)
-        print(format_row.format(*row))
-    print(separator)
-
-
-def predict_classes(self, inputs, num_classes=0, batch_size=0):
-    """Predicts the class labels for the specified inputs.
-
-    Args:
-        inputs (:obj:`numpy.ndarray`): a (n, x, y, c) uint8 tensor
-        num_classes (int, optional): the number of output classes
-        batch_size (int, optional): maximum number of inputs that should be
-            processed at a time
-
-    Returns:
-        :obj:`numpy.ndarray`: an array of class labels
-
-    """
-
-    outputs = self.predict(inputs, batch_size)
-    classes = np.argmax(outputs, axis=-1).flatten()
-    num_neurons = outputs.shape[-1]
-    if num_classes != 0 and num_classes != num_neurons:
-        neurons_per_class = num_neurons // num_classes
-        classes = classes // neurons_per_class
-    return classes
+import json
+import numpy as np
+import re
+
+from .core import (Model, Layer, LayerType, LayerParams, AkidaUnsupervised, __version__,
+                   evaluate_bitwidth)
+from .statistics import Statistics
+
+
+def model_str(self):
+    data = "akida.Model, layer_count=" + str(self.get_layer_count())
+    data += ", sequence_count=" + str(len(self.sequences))
+    out_dims = self.output_shape if self.get_layer_count() else []
+    data += ", output_shape=" + str(out_dims)
+    return data
+
+
+def model_repr(self):
+    out_dims = self.output_shape if self.get_layer_count() else []
+    data = "<akida.Model, layer_count=" + str(self.get_layer_count())
+    data += ", output_shape=" + str(out_dims)
+    data += ", sequences=" + repr(self.sequences) + ">"
+    return data
+
+
+def model_to_dict(self):
+    """Provide a dict representation of the Model
+
+    Returns:
+        dict: a Model dictionary.
+    """
+    learning = None
+    if self.learning:
+        learning = {name: getattr(self.learning, name) for name in dir(self.learning)}
+    return {
+        "version": __version__,
+        "layers": [layer.to_dict() for layer in self.layers],
+        "learning": learning,
+        "input_shape": self.input_shape,
+        "output_shape": self.output_shape
+    }
+
+
+def model_from_dict(model_dict):
+    """Instantiate a Model from a dict representation
+
+    Args:
+        model_dict(dict): a Model dictionary.
+
+    Returns:
+        :obj:`Model`: a Model.
+    """
+    # Check major and minor version
+    model_version = model_dict["version"]
+    major, minor, _ = __version__.split('.')
+    model_major, model_minor, _ = model_version.split('.')
+    if major != model_major or minor != model_minor:
+        raise ValueError(f"Serialized model was generated by version {model_version}, which"
+                         f" is incompatible with current version {__version__}")
+    # Instantiate an empty Model
+    model = Model()
+    # Add layers
+    layers = model_dict["layers"]
+    for layer_dict in layers:
+        layer_name = layer_dict["name"]
+        # Extract layer parameters
+        layer_params = layer_dict["parameters"]
+        if layer_params is None:
+            raise ValueError("Cannot deserialize a Layer without parameters")
+        layer_params = layer_params.copy()
+        # Evaluate the Layer type from its serialized name
+        layer_type = getattr(LayerType, layer_params["layer_type"])
+        # Remove layer_type
+        layer_params.pop("layer_type")
+        # Instantiate layer
+        layer = Layer(LayerParams(layer_type, layer_params), layer_name)
+        # Evaluate the layer inbounds
+        inbounds = [model.get_layer(ib) for ib in layer_dict["inbounds"]]
+        # Add it to the model
+        model.add(layer, inbounds)
+    # If needed compile it to set learning parameters and variables
+    if model_dict["learning"] is not None:
+        learning = model_dict["learning"].copy()
+        # Some parameters must be integer
+        for name in ["num_weights", "num_classes"]:
+            learning[name] = int(learning[name])
+        model.compile(AkidaUnsupervised(**learning))
+    # Now that the Model is fully initialized, load layer variables
+    for layer_dict in layers:
+        layer_name = layer_dict["name"]
+        # Get corresponding Layer in model
+        layer = model.get_layer(layer_name)
+        # Iterate over variables
+        variables_dict = layer_dict["variables"]
+        for name in variables_dict:
+            variable_dict = variables_dict[name]
+            dtype = variable_dict["dtype"]
+            variable = np.array(variable_dict["data"]).astype(dtype)
+            bitwidth = variable_dict["bitwidth"]
+            actual_bitwidth = evaluate_bitwidth(variable)
+            if bitwidth < actual_bitwidth:
+                raise ValueError(f"The specified bitwidth ({bitwidth}) must be higher or equal to \
+                                 the actual bitwidth ({actual_bitwidth}) of {name} \
+                                 from the layer '{layer_name}'.")
+            layer.variables[name] = variable
+    return model
+
+
+def model_to_json(self):
+    """Provide a JSON representation of the Model
+
+    Returns:
+        str: a JSON-formatted string corresponding to a Model.
+    """
+    # Pretty-print serialized model
+    model_str = json.dumps(self.to_dict(), indent=2)
+
+    # Remove spurious line jumps in serialized arrays of numbers
+    def align_arrays(m):
+        # Just return the extracted pattern
+        return m.group(1)
+    # Look for lines starting with whitespaces and:
+    # - a signed integer or float number with an optional opening square bracket,
+    # - stand-alone opening or closing square brackets
+    return re.sub(r'\n\s+(\[?-?[\d\.]+,?|\]|\[)', align_arrays, model_str)
+
+
+def model_from_json(model_str):
+    """Instantiate a Model from a JSON representation
+
+    Args:
+        model_str(str): a JSON-formatted string corresponding to a Model.
+
+    Returns:
+        :obj:`Model`: a Model.
+    """
+    return Model.from_dict(json.loads(model_str))
+
+
+@property
+def statistics(self):
+    """Get statistics by sequence for this model.
+
+    Returns:
+        a dictionary of :obj:`SequenceStatistics` indexed by name.
+
+    """
+    return Statistics(model=self)
+
+
+def summary(self):
+    """Prints a string summary of the model.
+
+    This method prints a summary of the model with details for every layer,
+    grouped by sequences:
+
+    - name and type in the first column
+    - output shape
+    - kernel shape
+
+    If there is any layer with unsupervised learning enabled, it will list
+    them, with these details:
+
+    - name of layer
+    - number of incoming connections
+    - number of weights per neuron
+
+    """
+
+    def _model_summary(model):
+        # prepare headers
+        headers = ['Input shape', 'Output shape', 'Sequences', 'Layers']
+        # prepare an empty table
+        table = [headers]
+
+        if not model.layers:
+            row = [
+                'N/A',
+                'N/A',
+                str(len(model.sequences)),
+                str(len(model.layers))
+            ]
+        else:
+            row = [
+                str(model.input_shape),
+                str(model.output_shape),
+                str(len(model.sequences)),
+                str(len(model.layers))
+            ]
+        # add the number of NPs if the model is mapped
+        has_program = np.any([s.program is not None for s in self.sequences])
+        if has_program:
+            headers.append('NPs')
+            nb_nps = 0
+            for sequence in model.sequences:
+                for current_pass in sequence.passes:
+                    for layer in current_pass.layers:
+                        if layer.parameters.layer_type != LayerType.InputConvolutional:
+                            nb_nps += 0 if layer.mapping is None else len(layer.mapping.nps)
+            row.append(nb_nps)
+
+        # prepare an empty table
+        table = [headers]
+        table.append(row)
+        print_table(table, "Model Summary")
+
+    def _get_backend_info(sequence):
+        backend = str(sequence.backend).split('.')[-1]
+        sequence_info = sequence.name + " (" + backend + ")"
+        if sequence.program is not None:
+            sequence_info += " - size: " + str(len(sequence.program)) + " bytes"
+        return sequence_info
+
+    def _layers_summary(sequences):
+        # Prepare headers
+        headers = ['Layer (type)', 'Output shape', 'Kernel shape']
+        has_program = np.any([s.program is not None for s in self.sequences])
+        if has_program:
+            headers.append('NPs')
+        # prepare an empty table
+        table = [headers]
+        new_splits = []
+        has_multi_pass = len(sequences[0].passes) > 1
+        nb_pass = 0
+        for s in sequences:
+            info = _get_backend_info(s)
+            new_splits.append(info)
+            for p in s.passes:
+                if has_multi_pass:
+                    nb_pass += 1
+                    if nb_pass > 1:
+                        new_splits.append(f"pass {nb_pass}")
+                for layer in p.layers:
+                    nps = None if layer.mapping is None else layer.mapping.nps
+                    # layer name (type)
+                    layer_type = layer.parameters.layer_type
+                    # kernel shape
+                    if "weights" in layer.get_variable_names():
+                        kernel_shape = layer.get_variable("weights").shape
+                    else:
+                        kernel_shape = "N/A"
+                    # Prepare row and add it
+                    row = [str(layer), str(layer.output_dims), str(kernel_shape)]
+                    if has_program:
+                        if layer_type == LayerType.InputConvolutional or nps is None:
+                            row.append('N/A')
+                        else:
+                            row.append(len(nps))
+                    table.append(row)
+                    if len(table) - 1 > len(new_splits):
+                        new_splits.append(False)
+                    if layer_type == LayerType.SeparableConvolutional:
+                        # Add pointwise weights on a second line
+                        kernel_pw_shape = layer.get_variable("weights_pw").shape
+                        row = ['', '', kernel_pw_shape]
+                        if has_program:
+                            row.append('')
+                        table.append(row)
+                        new_splits.append(False)
+        print_table(table, None, new_splits)
+
+    def _learning_summary(model):
+        layer = model.layers[-1]
+        # Prepare headers
+        headers = ["Learning Layer", "# Input Conn.", "# Weights"]
+        table = [headers]
+        name = layer.name
+        # Input connections is the product of input dims
+        input_connections = np.prod(layer.input_dims)
+        # Num non zero weights per neuron (counted on first neuron)
+        weights = layer.get_variable("weights")
+        incoming_conn = np.count_nonzero(weights[:, :, :, 0])
+        # Prepare row and add it
+        row = [name, str(input_connections), incoming_conn]
+        table.append(row)
+        print()
+        print_table(table, "Learning Summary")
+
+    # Print first the general Model summary
+    _model_summary(self)
+    # Print sequences summary
+    if self.sequences:
+        print()
+        _layers_summary(self.sequences)
+    # Print learning summary if we have more than one input layer
+    if len(self.layers) > 1 and self.learning:
+        # Only the last layer of a model can learn
+        print()
+        _learning_summary(self)
+
+
+def print_table(table, title, new_splits=None):
+    # Convert to np.array
+    to_str = np.vectorize(str, otypes=['O'])
+    table = to_str(table)
+    # get column lengths
+    str_len_f = np.vectorize(lambda cell: len(str(cell)))
+    str_lens = np.amax(str_len_f(table), 0)
+    line_len = np.sum(str_lens)
+    # Prepare format rows
+    size_formats = np.vectorize(lambda cell: f"{{:{cell}.{cell}}}")
+    format_strings = size_formats(str_lens)
+    format_row = "  ".join(format_strings)
+    # Generate separators
+    separator_len = line_len + 2 * len(table[0])
+    separator = "_" * separator_len
+    double_separator = "=" * separator_len
+
+    # Print header
+    center_format = f"{{:^{separator_len}}}"
+    if title is not None:
+        print(center_format.format(title))
+    print(separator)
+    print(format_row.format(*table[0]))
+
+    rows = table[1:, :]
+    if new_splits is None:
+        new_splits = [False] * len(rows)
+    assert len(rows) == len(new_splits)
+    if not any(new_splits):
+        print(double_separator)
+    # Print body
+    for row, new_split in zip(rows, new_splits):
+        if new_split:
+            # Display a line break only for sequences
+            if "pass" not in new_split:
+                print()
+            # Compute the number of char on each side of the text
+            space_len = max((separator_len - len(new_split)) / 2., 1.)
+            space_left = "=" * int(np.ceil(space_len - 1))
+            space_right = "=" * int(np.floor(space_len - 1))
+            print(space_left, new_split, space_right)
+            # Display a line break only for sequences
+            if "pass" not in new_split:
+                print()
+        # Don't use a separator line on first row
+        elif row[0] != rows[0][0]:
+            print(separator)
+        print(format_row.format(*row))
+    print(separator)
+
+
+def predict_classes(self, inputs, num_classes=0, batch_size=0):
+    """Predicts the class labels for the specified inputs.
+
+    Args:
+        inputs (:obj:`numpy.ndarray`): a (n, x, y, c) uint8 tensor
+        num_classes (int, optional): the number of output classes
+        batch_size (int, optional): maximum number of inputs that should be
+            processed at a time
+
+    Returns:
+        :obj:`numpy.ndarray`: an array of class labels
+
+    """
+
+    outputs = self.predict(inputs, batch_size)
+    classes = np.argmax(outputs, axis=-1).flatten()
+    num_neurons = outputs.shape[-1]
+    if num_classes != 0 and num_classes != num_neurons:
+        neurons_per_class = num_neurons // num_classes
+        classes = classes // neurons_per_class
+    return classes
```

## akida/np.py

 * *Ordering differences only*

```diff
@@ -1,21 +1,21 @@
-def np_info_repr(self):
-    data = "<akida.NP.Info"
-    data += ", ident=" + str(self.ident)
-    data += ", types=" + str(self.types) + ">"
-    return data
-
-
-def np_mesh_repr(self):
-    data = "<akida.NP.Mesh"
-    data += ", dma_event=" + str(self.dma_event)
-    data += ", dma_conf=" + str(self.dma_conf)
-    data += ", nps=" + str(self.nps) + ">"
-    return data
-
-
-def np_mapping_repr(self):
-    return "<akida.NP.Mapping" + \
-        ", ident=" + str(self.ident) + \
-        ", type=" + str(self.type) + \
-        ", filters=" + str(self.filters) + \
-        ", single_buffer=" + str(self.single_buffer) + ">"
+def np_info_repr(self):
+    data = "<akida.NP.Info"
+    data += ", ident=" + str(self.ident)
+    data += ", types=" + str(self.types) + ">"
+    return data
+
+
+def np_mesh_repr(self):
+    data = "<akida.NP.Mesh"
+    data += ", dma_event=" + str(self.dma_event)
+    data += ", dma_conf=" + str(self.dma_conf)
+    data += ", nps=" + str(self.nps) + ">"
+    return data
+
+
+def np_mapping_repr(self):
+    return "<akida.NP.Mapping" + \
+        ", ident=" + str(self.ident) + \
+        ", type=" + str(self.type) + \
+        ", filters=" + str(self.filters) + \
+        ", single_buffer=" + str(self.single_buffer) + ">"
```

## akida/sequence.py

 * *Ordering differences only*

```diff
@@ -1,17 +1,17 @@
-def sequence_repr(self):
-    data = "<akida.Sequence"
-    data += ", name=" + self.name
-    data += ", backend=" + str(self.backend).split('.')[-1]
-    data += ", passes=" + repr(self.passes)
-    program = self.program
-    if program is not None:
-        data += ", program_size=" + str(len(program))
-    data += ">"
-    return data
-
-
-def pass_repr(self):
-    data = "<akida.Pass"
-    data += ", layers=" + repr(self.layers)
-    data += ">"
-    return data
+def sequence_repr(self):
+    data = "<akida.Sequence"
+    data += ", name=" + self.name
+    data += ", backend=" + str(self.backend).split('.')[-1]
+    data += ", passes=" + repr(self.passes)
+    program = self.program
+    if program is not None:
+        data += ", program_size=" + str(len(program))
+    data += ">"
+    return data
+
+
+def pass_repr(self):
+    data = "<akida.Pass"
+    data += ", layers=" + repr(self.layers)
+    data += ">"
+    return data
```

## akida/sparsity.py

 * *Ordering differences only*

```diff
@@ -1,29 +1,29 @@
-import numpy as np
-from .core import LayerType
-from .core import Model
-
-
-def evaluate_sparsity(model, inputs):
-    """Evaluate the sparsity of a Model on a set of inputs
-
-    Args:
-        model (:obj:`Model`): the model to evaluate
-        inputs (:obj:`numpy.ndarray`): a numpy.ndarray
-
-    Returns:
-        a dictionary of float sparsity values indexed by layers
-
-    """
-    sparsities = {}
-    current_inputs = inputs
-    for layer in model.layers:
-        params = layer.parameters
-        if params.layer_type != LayerType.InputData and layer.parameters.activation:
-            sub_model = Model(layers=[layer])
-            current_inputs = sub_model.forward(current_inputs)
-            output_size = np.prod(current_inputs.shape)
-            activations = np.count_nonzero(current_inputs)
-            sparsities[layer] = 1 - activations / output_size
-        else:
-            sparsities[layer] = None
-    return sparsities
+import numpy as np
+from .core import LayerType
+from .core import Model
+
+
+def evaluate_sparsity(model, inputs):
+    """Evaluate the sparsity of a Model on a set of inputs
+
+    Args:
+        model (:obj:`Model`): the model to evaluate
+        inputs (:obj:`numpy.ndarray`): a numpy.ndarray
+
+    Returns:
+        a dictionary of float sparsity values indexed by layers
+
+    """
+    sparsities = {}
+    current_inputs = inputs
+    for layer in model.layers:
+        params = layer.parameters
+        if params.layer_type != LayerType.InputData and layer.parameters.activation:
+            sub_model = Model(layers=[layer])
+            current_inputs = sub_model.forward(current_inputs)
+            output_size = np.prod(current_inputs.shape)
+            activations = np.count_nonzero(current_inputs)
+            sparsities[layer] = 1 - activations / output_size
+        else:
+            sparsities[layer] = None
+    return sparsities
```

## akida/statistics.py

 * *Ordering differences only*

```diff
@@ -1,114 +1,114 @@
-from statistics import stdev
-
-
-class InferenceStatistics():
-    """Provides inference statistics.
-    """
-
-    def __init__(self, metrics, power_events):
-        self._fps = None
-        self._powers = {}
-        self._energy = None
-        if "inference_start" in metrics.names:
-            inf_start = metrics["inference_start"]
-            inf_end = metrics["inference_end"]
-            frames = metrics["inference_frames"]
-            duration = (inf_end - inf_start) / 1000
-            if duration > 0:
-                self._fps = frames / duration
-            if power_events:
-                # get power events between inference start & end
-                powers = []
-                for event in power_events:
-                    if event.ts >= inf_start and event.ts <= inf_end:
-                        powers.append(event.power)
-                num_powers = len(powers)
-                if num_powers > 1:
-                    # Remove first value
-                    powers = powers[1:]
-                    # get avg/min/max
-                    self._powers["Avg"] = sum(powers) / len(powers)
-                    self._powers["Min"] = min(powers)
-                    self._powers["Max"] = max(powers)
-                    if len(powers) > 1:
-                        self._powers["Std"] = stdev(powers)
-                    # evaluate the energy consumed by frame
-                    # It is average power * duration / frame
-                    self._energy = self._powers["Avg"] * duration / frames
-
-    def __repr__(self):
-        fps = "N/A" if self.fps is None else "%.2f" % self._fps
-        data = "fps: " + fps
-        if self._powers:
-            data += ", powers: " + str(self._powers)
-        if self._energy:
-            data += ", energy: " + str(self._energy)
-        return data
-
-    def __str__(self):
-        fps = "N/A" if self.fps is None else "%.2f" % self._fps + " fps"
-        data = "Average framerate = " + fps
-        if self._powers:
-            data += "\nLast inference power range (mW): "
-            num_powers = len(self._powers)
-            for index, (key, value) in enumerate(self._powers.items()):
-                data += " {} {:.2f} ".format(key, value)
-                if index != num_powers - 1:
-                    data += "/"
-        if self._energy:
-            data += "\nLast inference energy consumed (mJ/frame): {:.2f}".format(
-                self._energy)
-        return data
-
-    @property
-    def fps(self):
-        """Returns the frames per seconds for the last inference batch.
-
-        Returns:
-            a float value in frames/s.
-        """
-        return self._fps
-
-    @property
-    def powers(self):
-        """Returns the power ranges during the last inference batch.
-
-        Note that the power measurements must be enabled for the device.
-
-        Note also that the inference must last long enough to provide meaningful
-        power measurements: try increasing the number of samples and/or batch
-        size if power ranges are missing.
-
-        Returns:
-            a dictionary of float power values in mW indexed by name (where
-            names are in ['Avg', 'Min', 'Max', 'Std']).
-        """
-        return self._powers
-
-    @property
-    def energy(self):
-        """Returns the energy consumed during the last inference batch.
-
-        This corresponds to the average amount of energy consumed to process one
-        frame.
-
-        Returns:
-            a float value in mJ/frame.
-        """
-        return self._energy
-
-
-class Statistics(InferenceStatistics):
-    """Provides statistics for a Model or a Device.
-    """
-
-    def __init__(self, model=None, device=None):
-        if model is not None:
-            super().__init__(model.metrics, model.power_events)
-        elif device is not None:
-            # Check if we have a soc device to get power events
-            soc = device.soc
-            power_events = None
-            if soc and soc.power_measurement_enabled:
-                power_events = device.inference_power_events
-            super().__init__(device.metrics, power_events)
+from statistics import stdev
+
+
+class InferenceStatistics():
+    """Provides inference statistics.
+    """
+
+    def __init__(self, metrics, power_events):
+        self._fps = None
+        self._powers = {}
+        self._energy = None
+        if "inference_start" in metrics.names:
+            inf_start = metrics["inference_start"]
+            inf_end = metrics["inference_end"]
+            frames = metrics["inference_frames"]
+            duration = (inf_end - inf_start) / 1000
+            if duration > 0:
+                self._fps = frames / duration
+            if power_events:
+                # get power events between inference start & end
+                powers = []
+                for event in power_events:
+                    if event.ts >= inf_start and event.ts <= inf_end:
+                        powers.append(event.power)
+                num_powers = len(powers)
+                if num_powers > 1:
+                    # Remove first value
+                    powers = powers[1:]
+                    # get avg/min/max
+                    self._powers["Avg"] = sum(powers) / len(powers)
+                    self._powers["Min"] = min(powers)
+                    self._powers["Max"] = max(powers)
+                    if len(powers) > 1:
+                        self._powers["Std"] = stdev(powers)
+                    # evaluate the energy consumed by frame
+                    # It is average power * duration / frame
+                    self._energy = self._powers["Avg"] * duration / frames
+
+    def __repr__(self):
+        fps = "N/A" if self.fps is None else "%.2f" % self._fps
+        data = "fps: " + fps
+        if self._powers:
+            data += ", powers: " + str(self._powers)
+        if self._energy:
+            data += ", energy: " + str(self._energy)
+        return data
+
+    def __str__(self):
+        fps = "N/A" if self.fps is None else "%.2f" % self._fps + " fps"
+        data = "Average framerate = " + fps
+        if self._powers:
+            data += "\nLast inference power range (mW): "
+            num_powers = len(self._powers)
+            for index, (key, value) in enumerate(self._powers.items()):
+                data += " {} {:.2f} ".format(key, value)
+                if index != num_powers - 1:
+                    data += "/"
+        if self._energy:
+            data += "\nLast inference energy consumed (mJ/frame): {:.2f}".format(
+                self._energy)
+        return data
+
+    @property
+    def fps(self):
+        """Returns the frames per seconds for the last inference batch.
+
+        Returns:
+            a float value in frames/s.
+        """
+        return self._fps
+
+    @property
+    def powers(self):
+        """Returns the power ranges during the last inference batch.
+
+        Note that the power measurements must be enabled for the device.
+
+        Note also that the inference must last long enough to provide meaningful
+        power measurements: try increasing the number of samples and/or batch
+        size if power ranges are missing.
+
+        Returns:
+            a dictionary of float power values in mW indexed by name (where
+            names are in ['Avg', 'Min', 'Max', 'Std']).
+        """
+        return self._powers
+
+    @property
+    def energy(self):
+        """Returns the energy consumed during the last inference batch.
+
+        This corresponds to the average amount of energy consumed to process one
+        frame.
+
+        Returns:
+            a float value in mJ/frame.
+        """
+        return self._energy
+
+
+class Statistics(InferenceStatistics):
+    """Provides statistics for a Model or a Device.
+    """
+
+    def __init__(self, model=None, device=None):
+        if model is not None:
+            super().__init__(model.metrics, model.power_events)
+        elif device is not None:
+            # Check if we have a soc device to get power events
+            soc = device.soc
+            power_events = None
+            if soc and soc.power_measurement_enabled:
+                power_events = device.inference_power_events
+            super().__init__(device.metrics, power_events)
```

## akida/virtual_devices.py

 * *Ordering differences only*

```diff
@@ -1,121 +1,121 @@
-from .core import Device, HwVersion, NSoC_v2, NP
-
-
-def AKD1000():
-    """Returns a virtual device for an AKD1000 NSoC.
-
-    This function returns a virtual device for the Brainchip's AKD1000
-    NSoC.
-
-    Returns:
-        :obj:`Device`: a virtual device.
-
-    """
-    dma_event = NP.Ident(3, 1, 0)
-    dma_conf = NP.Ident(3, 1, 1)
-    nps = [
-        NP.Info(NP.Ident(1, 3, 0), {NP.Type.CNP1, NP.Type.FNP3}),
-        NP.Info(NP.Ident(1, 3, 1), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(1, 3, 2), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(1, 3, 3), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(1, 4, 0), {NP.Type.CNP1, NP.Type.FNP3}),
-        NP.Info(NP.Ident(1, 4, 1), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(1, 4, 2), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(1, 4, 3), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(1, 5, 0), {NP.Type.CNP1, NP.Type.FNP3}),
-        NP.Info(NP.Ident(1, 5, 1), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(1, 5, 2), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(1, 5, 3), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(2, 3, 0), {NP.Type.CNP1, NP.Type.FNP3}),
-        NP.Info(NP.Ident(2, 3, 1), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(2, 3, 2), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(2, 3, 3), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(2, 4, 0), {NP.Type.CNP1, NP.Type.FNP3}),
-        NP.Info(NP.Ident(2, 4, 1), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(2, 4, 2), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(2, 4, 3), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(2, 5, 0), {NP.Type.CNP1, NP.Type.FNP3}),
-        NP.Info(NP.Ident(2, 5, 1), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(2, 5, 2), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(2, 5, 3), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(3, 1, 2), {NP.Type.CNP1}),
-        NP.Info(NP.Ident(3, 1, 3), {NP.Type.CNP1}),
-        NP.Info(NP.Ident(3, 2, 0), {NP.Type.CNP1, NP.Type.FNP3}),
-        NP.Info(NP.Ident(3, 2, 1), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(3, 2, 2), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(3, 2, 3), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(3, 3, 0), {NP.Type.CNP1, NP.Type.FNP3}),
-        NP.Info(NP.Ident(3, 3, 1), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(3, 3, 2), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(3, 3, 3), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(3, 4, 0), {NP.Type.CNP1, NP.Type.FNP3}),
-        NP.Info(NP.Ident(3, 4, 1), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(3, 4, 2), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(3, 4, 3), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(3, 5, 0), {NP.Type.CNP1, NP.Type.FNP3}),
-        NP.Info(NP.Ident(3, 5, 1), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(3, 5, 2), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(3, 5, 3), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(4, 1, 0), {NP.Type.CNP1, NP.Type.FNP2}),
-        NP.Info(NP.Ident(4, 1, 1), {NP.Type.CNP1, NP.Type.FNP2}),
-        NP.Info(NP.Ident(4, 1, 2), {NP.Type.CNP1, NP.Type.FNP2}),
-        NP.Info(NP.Ident(4, 1, 3), {NP.Type.CNP1, NP.Type.FNP2}),
-        NP.Info(NP.Ident(4, 2, 0), {NP.Type.CNP1, NP.Type.FNP3}),
-        NP.Info(NP.Ident(4, 2, 1), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(4, 2, 2), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(4, 2, 3), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(4, 3, 0), {NP.Type.CNP1, NP.Type.FNP3}),
-        NP.Info(NP.Ident(4, 3, 1), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(4, 3, 2), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(4, 3, 3), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(4, 4, 0), {NP.Type.CNP1, NP.Type.FNP3}),
-        NP.Info(NP.Ident(4, 4, 1), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(4, 4, 2), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(4, 4, 3), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(4, 5, 0), {NP.Type.CNP1, NP.Type.FNP3}),
-        NP.Info(NP.Ident(4, 5, 1), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(4, 5, 2), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(4, 5, 3), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(5, 2, 0), {NP.Type.CNP1, NP.Type.FNP3}),
-        NP.Info(NP.Ident(5, 2, 1), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(5, 2, 2), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(5, 2, 3), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(5, 3, 0), {NP.Type.CNP1, NP.Type.FNP3}),
-        NP.Info(NP.Ident(5, 3, 1), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(5, 3, 2), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(5, 3, 3), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(5, 4, 0), {NP.Type.CNP1, NP.Type.FNP3}),
-        NP.Info(NP.Ident(5, 4, 1), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(5, 4, 2), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(5, 4, 3), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(5, 5, 0), {NP.Type.CNP1, NP.Type.FNP3}),
-        NP.Info(NP.Ident(5, 5, 1), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(5, 5, 2), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(5, 5, 3), {NP.Type.CNP1, NP.Type.CNP2})
-    ]
-    mesh = NP.Mesh(dma_event, dma_conf, nps)
-    return Device(NSoC_v2, mesh)
-
-
-def TwoNodesIP():
-    """Returns a virtual device for a two nodes Akida IP.
-
-    Returns:
-        :obj:`Device`: a virtual device.
-
-    """
-    hw_version = HwVersion(0xBC, 0xA1, 3, 0)
-    dma_event = NP.Ident(1, 1, 0)
-    dma_conf = NP.Ident(1, 1, 1)
-    nps = [
-        NP.Info(NP.Ident(1, 2, 0), {NP.Type.CNP1, NP.Type.FNP2}),
-        NP.Info(NP.Ident(1, 2, 1), {NP.Type.CNP1}),
-        NP.Info(NP.Ident(1, 2, 2), {NP.Type.CNP1}),
-        NP.Info(NP.Ident(1, 2, 3), {NP.Type.CNP1}),
-        NP.Info(NP.Ident(1, 3, 0), {NP.Type.CNP1, NP.Type.FNP3}),
-        NP.Info(NP.Ident(1, 3, 1), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(1, 3, 2), {NP.Type.CNP1, NP.Type.CNP2}),
-        NP.Info(NP.Ident(1, 3, 3), {NP.Type.CNP1, NP.Type.CNP2})
-    ]
-    mesh = NP.Mesh(dma_event, dma_conf, nps)
-    return Device(hw_version, mesh)
+from .core import Device, HwVersion, NSoC_v2, NP
+
+
+def AKD1000():
+    """Returns a virtual device for an AKD1000 NSoC.
+
+    This function returns a virtual device for the Brainchip's AKD1000
+    NSoC.
+
+    Returns:
+        :obj:`Device`: a virtual device.
+
+    """
+    dma_event = NP.Ident(3, 1, 0)
+    dma_conf = NP.Ident(3, 1, 1)
+    nps = [
+        NP.Info(NP.Ident(1, 3, 0), {NP.Type.CNP1, NP.Type.FNP3}),
+        NP.Info(NP.Ident(1, 3, 1), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(1, 3, 2), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(1, 3, 3), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(1, 4, 0), {NP.Type.CNP1, NP.Type.FNP3}),
+        NP.Info(NP.Ident(1, 4, 1), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(1, 4, 2), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(1, 4, 3), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(1, 5, 0), {NP.Type.CNP1, NP.Type.FNP3}),
+        NP.Info(NP.Ident(1, 5, 1), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(1, 5, 2), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(1, 5, 3), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(2, 3, 0), {NP.Type.CNP1, NP.Type.FNP3}),
+        NP.Info(NP.Ident(2, 3, 1), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(2, 3, 2), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(2, 3, 3), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(2, 4, 0), {NP.Type.CNP1, NP.Type.FNP3}),
+        NP.Info(NP.Ident(2, 4, 1), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(2, 4, 2), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(2, 4, 3), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(2, 5, 0), {NP.Type.CNP1, NP.Type.FNP3}),
+        NP.Info(NP.Ident(2, 5, 1), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(2, 5, 2), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(2, 5, 3), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(3, 1, 2), {NP.Type.CNP1}),
+        NP.Info(NP.Ident(3, 1, 3), {NP.Type.CNP1}),
+        NP.Info(NP.Ident(3, 2, 0), {NP.Type.CNP1, NP.Type.FNP3}),
+        NP.Info(NP.Ident(3, 2, 1), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(3, 2, 2), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(3, 2, 3), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(3, 3, 0), {NP.Type.CNP1, NP.Type.FNP3}),
+        NP.Info(NP.Ident(3, 3, 1), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(3, 3, 2), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(3, 3, 3), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(3, 4, 0), {NP.Type.CNP1, NP.Type.FNP3}),
+        NP.Info(NP.Ident(3, 4, 1), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(3, 4, 2), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(3, 4, 3), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(3, 5, 0), {NP.Type.CNP1, NP.Type.FNP3}),
+        NP.Info(NP.Ident(3, 5, 1), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(3, 5, 2), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(3, 5, 3), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(4, 1, 0), {NP.Type.CNP1, NP.Type.FNP2}),
+        NP.Info(NP.Ident(4, 1, 1), {NP.Type.CNP1, NP.Type.FNP2}),
+        NP.Info(NP.Ident(4, 1, 2), {NP.Type.CNP1, NP.Type.FNP2}),
+        NP.Info(NP.Ident(4, 1, 3), {NP.Type.CNP1, NP.Type.FNP2}),
+        NP.Info(NP.Ident(4, 2, 0), {NP.Type.CNP1, NP.Type.FNP3}),
+        NP.Info(NP.Ident(4, 2, 1), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(4, 2, 2), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(4, 2, 3), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(4, 3, 0), {NP.Type.CNP1, NP.Type.FNP3}),
+        NP.Info(NP.Ident(4, 3, 1), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(4, 3, 2), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(4, 3, 3), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(4, 4, 0), {NP.Type.CNP1, NP.Type.FNP3}),
+        NP.Info(NP.Ident(4, 4, 1), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(4, 4, 2), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(4, 4, 3), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(4, 5, 0), {NP.Type.CNP1, NP.Type.FNP3}),
+        NP.Info(NP.Ident(4, 5, 1), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(4, 5, 2), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(4, 5, 3), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(5, 2, 0), {NP.Type.CNP1, NP.Type.FNP3}),
+        NP.Info(NP.Ident(5, 2, 1), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(5, 2, 2), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(5, 2, 3), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(5, 3, 0), {NP.Type.CNP1, NP.Type.FNP3}),
+        NP.Info(NP.Ident(5, 3, 1), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(5, 3, 2), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(5, 3, 3), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(5, 4, 0), {NP.Type.CNP1, NP.Type.FNP3}),
+        NP.Info(NP.Ident(5, 4, 1), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(5, 4, 2), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(5, 4, 3), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(5, 5, 0), {NP.Type.CNP1, NP.Type.FNP3}),
+        NP.Info(NP.Ident(5, 5, 1), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(5, 5, 2), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(5, 5, 3), {NP.Type.CNP1, NP.Type.CNP2})
+    ]
+    mesh = NP.Mesh(dma_event, dma_conf, nps)
+    return Device(NSoC_v2, mesh)
+
+
+def TwoNodesIP():
+    """Returns a virtual device for a two nodes Akida IP.
+
+    Returns:
+        :obj:`Device`: a virtual device.
+
+    """
+    hw_version = HwVersion(0xBC, 0xA1, 3, 0)
+    dma_event = NP.Ident(1, 1, 0)
+    dma_conf = NP.Ident(1, 1, 1)
+    nps = [
+        NP.Info(NP.Ident(1, 2, 0), {NP.Type.CNP1, NP.Type.FNP2}),
+        NP.Info(NP.Ident(1, 2, 1), {NP.Type.CNP1}),
+        NP.Info(NP.Ident(1, 2, 2), {NP.Type.CNP1}),
+        NP.Info(NP.Ident(1, 2, 3), {NP.Type.CNP1}),
+        NP.Info(NP.Ident(1, 3, 0), {NP.Type.CNP1, NP.Type.FNP3}),
+        NP.Info(NP.Ident(1, 3, 1), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(1, 3, 2), {NP.Type.CNP1, NP.Type.CNP2}),
+        NP.Info(NP.Ident(1, 3, 3), {NP.Type.CNP1, NP.Type.CNP2})
+    ]
+    mesh = NP.Mesh(dma_event, dma_conf, nps)
+    return Device(hw_version, mesh)
```

## akida/api/akida/backend_type.h

 * *Ordering differences only*

```diff
@@ -1,22 +1,22 @@
-#pragma once
-
-#include <string>
-
-namespace akida {
-
-enum class BackendType { Software, Hardware, Hybrid };
-
-inline std::string backend_name(BackendType backend) {
-  switch (backend) {
-    case BackendType::Software:
-      return "Software";
-    case BackendType::Hardware:
-      return "Hardware";
-    case BackendType::Hybrid:
-      return "Hybrid";
-    default:
-      return "Unknown";
-  }
-}
-
-}  // namespace akida
+#pragma once
+
+#include <string>
+
+namespace akida {
+
+enum class BackendType { Software, Hardware, Hybrid };
+
+inline std::string backend_name(BackendType backend) {
+  switch (backend) {
+    case BackendType::Software:
+      return "Software";
+    case BackendType::Hardware:
+      return "Hardware";
+    case BackendType::Hybrid:
+      return "Hybrid";
+    default:
+      return "Unknown";
+  }
+}
+
+}  // namespace akida
```

## akida/api/akida/backends.h

 * *Ordering differences only*

```diff
@@ -1,26 +1,26 @@
-#pragma once
-
-#include <map>
-#include <memory>
-
-#include "akida/backend_type.h"
-
-#include "infra/exports.h"
-
-namespace akida {
-
-class Backend;
-
-using BackendPtr = std::shared_ptr<Backend>;
-
-// Raises an exception if the key is not found, allowing to dereference
-// the result of this function safely
-AKIDASHAREDLIB_EXPORT BackendPtr get_backend(BackendType type);
-
-// Checks if a given backend type is available
-AKIDASHAREDLIB_EXPORT bool has_backend(BackendType type);
-
-// Return the full list of available backends
-AKIDASHAREDLIB_EXPORT const std::map<BackendType, BackendPtr>& get_backends();
-
-}  // namespace akida
+#pragma once
+
+#include <map>
+#include <memory>
+
+#include "akida/backend_type.h"
+
+#include "infra/exports.h"
+
+namespace akida {
+
+class Backend;
+
+using BackendPtr = std::shared_ptr<Backend>;
+
+// Raises an exception if the key is not found, allowing to dereference
+// the result of this function safely
+AKIDASHAREDLIB_EXPORT BackendPtr get_backend(BackendType type);
+
+// Checks if a given backend type is available
+AKIDASHAREDLIB_EXPORT bool has_backend(BackendType type);
+
+// Return the full list of available backends
+AKIDASHAREDLIB_EXPORT const std::map<BackendType, BackendPtr>& get_backends();
+
+}  // namespace akida
```

## akida/api/akida/device.h

 * *Ordering differences only*

```diff
@@ -1,73 +1,73 @@
-/*******************************************************************************
- * Copyright 2019 Brainchip Holdings Ltd.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- ********************************************************************************
- */
-
-#pragma once
-
-#include <cstdint>
-#include <memory>
-#include <vector>
-
-#include "akida/hw_version.h"
-#include "akida/np.h"
-#include "infra/exports.h"
-
-namespace akida {
-
-class HardwareDevice;
-
-class Device;
-
-using DevicePtr = std::shared_ptr<Device>;
-using DeviceConstPtr = std::shared_ptr<const Device>;
-
-/**
- * class Device
- *
- * Public interface to an Akida Device (real or virtual)
- *
- */
-class AKIDASHAREDLIB_EXPORT Device {
- public:
-  virtual ~Device() = default;
-  /**
-   * @brief Get the Device version
-   * @return a HwVersion
-   */
-  virtual HwVersion version() const = 0;
-
-  /**
-   * @brief Get the Device description
-   * @return a char*
-   */
-  virtual const char* desc() const = 0;
-
-  /**
-   * @brief Return the Device Neural Processor Mesh layout
-   *
-   * @return a reference to a np::Mesh structure
-   */
-  virtual const np::Mesh& mesh() const = 0;
-
-  /**
-   * @brief Return the Hardware Device if exist
-   *
-   * @return a pointer to a HardwareDevice
-   */
-  virtual HardwareDevice* hardware() const = 0;
-};
-
-}  // namespace akida
+/*******************************************************************************
+ * Copyright 2019 Brainchip Holdings Ltd.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ ********************************************************************************
+ */
+
+#pragma once
+
+#include <cstdint>
+#include <memory>
+#include <vector>
+
+#include "akida/hw_version.h"
+#include "akida/np.h"
+#include "infra/exports.h"
+
+namespace akida {
+
+class HardwareDevice;
+
+class Device;
+
+using DevicePtr = std::shared_ptr<Device>;
+using DeviceConstPtr = std::shared_ptr<const Device>;
+
+/**
+ * class Device
+ *
+ * Public interface to an Akida Device (real or virtual)
+ *
+ */
+class AKIDASHAREDLIB_EXPORT Device {
+ public:
+  virtual ~Device() = default;
+  /**
+   * @brief Get the Device version
+   * @return a HwVersion
+   */
+  virtual HwVersion version() const = 0;
+
+  /**
+   * @brief Get the Device description
+   * @return a char*
+   */
+  virtual const char* desc() const = 0;
+
+  /**
+   * @brief Return the Device Neural Processor Mesh layout
+   *
+   * @return a reference to a np::Mesh structure
+   */
+  virtual const np::Mesh& mesh() const = 0;
+
+  /**
+   * @brief Return the Hardware Device if exist
+   *
+   * @return a pointer to a HardwareDevice
+   */
+  virtual HardwareDevice* hardware() const = 0;
+};
+
+}  // namespace akida
```

## akida/api/akida/layer.h

 * *Ordering differences only*

```diff
@@ -1,143 +1,143 @@
-/*******************************************************************************
- * Copyright 2019 Brainchip Holdings Ltd.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- ********************************************************************************
- */
-#pragma once
-
-#include <cstdint>
-#include <memory>
-#include <string>
-#include <vector>
-
-#include "akida/layer_params.h"
-#include "akida/learning_params.h"
-#include "akida/np_mapping.h"
-#include "akida/shape.h"
-#include "akida/variables.h"
-#include "infra/exports.h"
-
-namespace akida {
-
-class Layer;
-
-/**
- * @brief A shared pointer to a Layer object
- */
-using LayerPtr = std::shared_ptr<Layer>;
-
-/**
- * @brief A shared pointer to a const Layer object
- */
-using LayerConstPtr = std::shared_ptr<const Layer>;
-
-/**
- * class Layer
- *
- * Public interface to an Akida Layer.
- *
- */
-
-class AKIDASHAREDLIB_EXPORT Layer {
- public:
-  using Shapes = std::vector<Shape>;
-
-  /**
-   * @brief Create a layer from a parameter structure and a name
-   * @param params : structure to initialize the layer
-   * @param name : name for the layer
-   * @return a LayerPtr object
-   */
-  static LayerPtr create(const LayerParams* params, const std::string& name);
-
-  virtual ~Layer() {}
-
-  /**
-   * @brief Returns the name of this layer
-   */
-  virtual std::string get_name() const = 0;
-
-  /**
-   * @brief Returns the input dimensions of this layer
-   */
-  virtual Shapes input_dimensions() const = 0;
-
-  /**
-   * @brief Returns the output dimensions of this layer
-   */
-  virtual Shape output_dimensions() const = 0;
-
-  /**
-   * @brief Returns the input bitwidth of this layer
-   */
-  virtual uint8_t input_bits() const = 0;
-
-  /**
-   * @brief Returns the output bitwidth of this layer
-   */
-  virtual uint8_t output_bits() const = 0;
-
-  /**
-   * @brief Returns true if output in this layer is signed
-   */
-  virtual bool output_signed() const = 0;
-
-  /**
-   * @brief Returns the parameters of this layer
-   */
-  virtual const LayerParams* params() const = 0;
-
-  /**
-   * @brief Returns the learning parameters of this layer
-   */
-  virtual const LearningParams* learning() const = 0;
-
-  /**
-   * @brief Returns true if this layer can learn
-   */
-  virtual bool can_learn() const = 0;
-
-  /**
-   * @brief Returns the Variables object attached to this layer
-   */
-  virtual Variables* variables() = 0;
-
-  /**
-   * @brief Returns the Variables object attached to this layer
-   */
-  virtual const Variables& variables() const = 0;
-
-  /**
-   * @brief Returns the inbound layers of this layer
-   */
-  virtual std::vector<LayerConstPtr> inbound_layers() const = 0;
-
-  /**
-   * The mapping of a Layer on one or more Neural Processors
-   */
-  struct Mapping {
-    virtual ~Mapping() = default;
-    std::vector<NPMapping> nps;
-  };
-
-  using MappingPtr = std::shared_ptr<Mapping>;
-  using MappingConstPtr = std::shared_ptr<const Mapping>;
-
-  /**
-   * Return the layer hardware mapping
-   */
-  virtual MappingConstPtr mapping() const = 0;
-};
-
-}  // namespace akida
+/*******************************************************************************
+ * Copyright 2019 Brainchip Holdings Ltd.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ ********************************************************************************
+ */
+#pragma once
+
+#include <cstdint>
+#include <memory>
+#include <string>
+#include <vector>
+
+#include "akida/layer_params.h"
+#include "akida/learning_params.h"
+#include "akida/np_mapping.h"
+#include "akida/shape.h"
+#include "akida/variables.h"
+#include "infra/exports.h"
+
+namespace akida {
+
+class Layer;
+
+/**
+ * @brief A shared pointer to a Layer object
+ */
+using LayerPtr = std::shared_ptr<Layer>;
+
+/**
+ * @brief A shared pointer to a const Layer object
+ */
+using LayerConstPtr = std::shared_ptr<const Layer>;
+
+/**
+ * class Layer
+ *
+ * Public interface to an Akida Layer.
+ *
+ */
+
+class AKIDASHAREDLIB_EXPORT Layer {
+ public:
+  using Shapes = std::vector<Shape>;
+
+  /**
+   * @brief Create a layer from a parameter structure and a name
+   * @param params : structure to initialize the layer
+   * @param name : name for the layer
+   * @return a LayerPtr object
+   */
+  static LayerPtr create(const LayerParams* params, const std::string& name);
+
+  virtual ~Layer() {}
+
+  /**
+   * @brief Returns the name of this layer
+   */
+  virtual std::string get_name() const = 0;
+
+  /**
+   * @brief Returns the input dimensions of this layer
+   */
+  virtual Shapes input_dimensions() const = 0;
+
+  /**
+   * @brief Returns the output dimensions of this layer
+   */
+  virtual Shape output_dimensions() const = 0;
+
+  /**
+   * @brief Returns the input bitwidth of this layer
+   */
+  virtual uint8_t input_bits() const = 0;
+
+  /**
+   * @brief Returns the output bitwidth of this layer
+   */
+  virtual uint8_t output_bits() const = 0;
+
+  /**
+   * @brief Returns true if output in this layer is signed
+   */
+  virtual bool output_signed() const = 0;
+
+  /**
+   * @brief Returns the parameters of this layer
+   */
+  virtual const LayerParams* params() const = 0;
+
+  /**
+   * @brief Returns the learning parameters of this layer
+   */
+  virtual const LearningParams* learning() const = 0;
+
+  /**
+   * @brief Returns true if this layer can learn
+   */
+  virtual bool can_learn() const = 0;
+
+  /**
+   * @brief Returns the Variables object attached to this layer
+   */
+  virtual Variables* variables() = 0;
+
+  /**
+   * @brief Returns the Variables object attached to this layer
+   */
+  virtual const Variables& variables() const = 0;
+
+  /**
+   * @brief Returns the inbound layers of this layer
+   */
+  virtual std::vector<LayerConstPtr> inbound_layers() const = 0;
+
+  /**
+   * The mapping of a Layer on one or more Neural Processors
+   */
+  struct Mapping {
+    virtual ~Mapping() = default;
+    std::vector<NPMapping> nps;
+  };
+
+  using MappingPtr = std::shared_ptr<Mapping>;
+  using MappingConstPtr = std::shared_ptr<const Mapping>;
+
+  /**
+   * Return the layer hardware mapping
+   */
+  virtual MappingConstPtr mapping() const = 0;
+};
+
+}  // namespace akida
```

## akida/api/akida/layer_params.h

 * *Ordering differences only*

```diff
@@ -1,144 +1,144 @@
-#pragma once
-
-#include <cstdint>
-#include <map>
-#include <memory>
-#include <stdexcept>
-#include <string>
-
-#include "akida/shape.h"
-#include "infra/exports.h"
-
-/** file akida/layer_params.h
- * Contains layer related enums and layers parameters structures.
- */
-
-namespace akida {
-
-// Layer enum definitions
-/**
- * @enum LayerType
- * @brief The layer type
- */
-enum class LayerType {
-  Unknown,
-  InputData,
-  InputConvolutional,
-  FullyConnected,
-  Convolutional,
-  SeparableConvolutional,
-  Add,
-  Dense2D,
-  Shiftmax,
-  Attention,
-  Stem,
-  MadNorm,
-  Concatenate,
-  BatchNormalization,
-  Conv2D,
-  InputConv2D,
-  DepthwiseConv2D,
-  Conv2DTranspose,
-  ExtractToken,
-  Dequantizer,
-  DepthwiseConv2DTranspose
-};
-
-/**
- * @enum Padding
- * @brief The padding type
- */
-enum class Padding {
-  Valid /**<No padding*/,
-  Same /**<Padded so that output size is input size divided by the stride*/,
-};
-
-/**
- * @enum PoolType
- * @brief The pooling type
- */
-enum class PoolType {
-  NoPooling /**<No pooling applied*/,
-  Max /**<Maximum pixel value is selected*/,
-  Average /**<Average pixel value is selected*/
-};
-
-/**
- * @class LayerParams
- * @brief Generic parameters that can be used for several layers
- */
-class AKIDASHAREDLIB_EXPORT LayerParams {
- public:
-  // Value is a basic type large enough to hold the data for every type in a
-  // lossless way.
-  using Value = int32_t;
-  using KeyType = const std::string;
-  using Dict = std::map<KeyType, Value>;
-
-  virtual ~LayerParams() = default;
-
-  /**
-   * @brief Simplified static cast to Value
-   * @param value: Value to be casted
-   * @return casted value, ready to be inserted
-   */
-  template<typename T>
-  static constexpr Value as_value(T value) {
-    return static_cast<Value>(value);
-  }
-
-  /**
-   * @brief Create a LayerParams object
-   * @param layer_type: layer type for these parameters
-   * @param entries: a map of key-entries
-   * @return a LayerParams object
-   */
-  static std::unique_ptr<LayerParams> create(LayerType type,
-                                             const Dict& entries);
-
-  /**
-   * @brief Copy a LayerParams object
-   * @param src: source object
-   * @return A copy of the source object
-   */
-  static std::unique_ptr<LayerParams> clone(const LayerParams* src);
-
-  /**
-   * @brief Reads a value from LayerParams.
-   * @param key: identifier for the value
-   * @return Value associated with the key
-   */
-  virtual Value get(KeyType const& key) const = 0;
-
-  /**
-   * @brief Check if the paramater exists in LayerParams.
-   * @param name: parameter name
-   * @return Boolean asserting the existence or not of the parameter
-   */
-  virtual bool has(const std::string& name) const = 0;
-
-  /**
-   * @brief Get all the keys in the LayerParams instance.
-   * @return Vector of keys
-   */
-  virtual std::vector<std::string> keys() const = 0;
-
-  /**
-   * @brief compare two LayerParams objects.
-   * @param other: object to compare with
-   * @return true if values and layer type are the same
-   */
-  virtual bool operator==(const LayerParams& other) const = 0;
-
-  /**
-   * @brief Get the layer type
-   * @return the layer type
-   */
-  virtual LayerType layer_type() const = 0;
-};
-
-inline bool param_activation(const LayerParams& params) {
-  return params.has("activation") && params.get("activation") != 0;
-}
-
-}  // namespace akida
+#pragma once
+
+#include <cstdint>
+#include <map>
+#include <memory>
+#include <stdexcept>
+#include <string>
+
+#include "akida/shape.h"
+#include "infra/exports.h"
+
+/** file akida/layer_params.h
+ * Contains layer related enums and layers parameters structures.
+ */
+
+namespace akida {
+
+// Layer enum definitions
+/**
+ * @enum LayerType
+ * @brief The layer type
+ */
+enum class LayerType {
+  Unknown,
+  InputData,
+  InputConvolutional,
+  FullyConnected,
+  Convolutional,
+  SeparableConvolutional,
+  Add,
+  Dense2D,
+  Shiftmax,
+  Attention,
+  Stem,
+  MadNorm,
+  Concatenate,
+  BatchNormalization,
+  Conv2D,
+  InputConv2D,
+  DepthwiseConv2D,
+  Conv2DTranspose,
+  ExtractToken,
+  Dequantizer,
+  DepthwiseConv2DTranspose
+};
+
+/**
+ * @enum Padding
+ * @brief The padding type
+ */
+enum class Padding {
+  Valid /**<No padding*/,
+  Same /**<Padded so that output size is input size divided by the stride*/,
+};
+
+/**
+ * @enum PoolType
+ * @brief The pooling type
+ */
+enum class PoolType {
+  NoPooling /**<No pooling applied*/,
+  Max /**<Maximum pixel value is selected*/,
+  Average /**<Average pixel value is selected*/
+};
+
+/**
+ * @class LayerParams
+ * @brief Generic parameters that can be used for several layers
+ */
+class AKIDASHAREDLIB_EXPORT LayerParams {
+ public:
+  // Value is a basic type large enough to hold the data for every type in a
+  // lossless way.
+  using Value = int32_t;
+  using KeyType = const std::string;
+  using Dict = std::map<KeyType, Value>;
+
+  virtual ~LayerParams() = default;
+
+  /**
+   * @brief Simplified static cast to Value
+   * @param value: Value to be casted
+   * @return casted value, ready to be inserted
+   */
+  template<typename T>
+  static constexpr Value as_value(T value) {
+    return static_cast<Value>(value);
+  }
+
+  /**
+   * @brief Create a LayerParams object
+   * @param layer_type: layer type for these parameters
+   * @param entries: a map of key-entries
+   * @return a LayerParams object
+   */
+  static std::unique_ptr<LayerParams> create(LayerType type,
+                                             const Dict& entries);
+
+  /**
+   * @brief Copy a LayerParams object
+   * @param src: source object
+   * @return A copy of the source object
+   */
+  static std::unique_ptr<LayerParams> clone(const LayerParams* src);
+
+  /**
+   * @brief Reads a value from LayerParams.
+   * @param key: identifier for the value
+   * @return Value associated with the key
+   */
+  virtual Value get(KeyType const& key) const = 0;
+
+  /**
+   * @brief Check if the paramater exists in LayerParams.
+   * @param name: parameter name
+   * @return Boolean asserting the existence or not of the parameter
+   */
+  virtual bool has(const std::string& name) const = 0;
+
+  /**
+   * @brief Get all the keys in the LayerParams instance.
+   * @return Vector of keys
+   */
+  virtual std::vector<std::string> keys() const = 0;
+
+  /**
+   * @brief compare two LayerParams objects.
+   * @param other: object to compare with
+   * @return true if values and layer type are the same
+   */
+  virtual bool operator==(const LayerParams& other) const = 0;
+
+  /**
+   * @brief Get the layer type
+   * @return the layer type
+   */
+  virtual LayerType layer_type() const = 0;
+};
+
+inline bool param_activation(const LayerParams& params) {
+  return params.has("activation") && params.get("activation") != 0;
+}
+
+}  // namespace akida
```

## akida/api/akida/learning_params.h

 * *Ordering differences only*

```diff
@@ -1,86 +1,86 @@
-#pragma once
-
-#include <map>
-#include <memory>
-#include <string>
-
-#include "akida/shape.h"
-
-namespace akida {
-
-/**
- * @enum LearningType
- * @brief The layer type
- */
-enum class LearningType { AkidaUnsupervised };
-
-class LearningParams;
-using LearningParamsPtr = std::shared_ptr<LearningParams>;
-
-/**
- * @class LearningParams
- * @brief Generic parameters that can be used for several learnings
- */
-class AKIDASHAREDLIB_EXPORT LearningParams {
- public:
-  // Value is a basic type large enough to hold the data for every type in a
-  // lossless way.
-  using Value = double;
-  using KeyType = const std::string;
-  using Dict = std::map<KeyType, Value>;
-
-  virtual ~LearningParams() = default;
-
-  /**
-   * @brief Simplified static cast to Value
-   * @param value: Value to be casted
-   * @return casted value, ready to be inserted
-   */
-  template<typename T>
-  static constexpr Value as_value(T value) {
-    return static_cast<Value>(value);
-  }
-
-  /**
-   * @brief Create a LearningParams object
-   * @param learning_type: learning type for these parameters
-   * @param entries: a map of key-entries
-   * @return a LearningParams object
-   */
-  static LearningParamsPtr create(LearningType type, const Dict& entries);
-
-  /**
-   * @brief Copy a LearningParams object
-   * @param src: source object
-   * @return A copy of the source object
-   */
-  static LearningParamsPtr clone(const LearningParams* src);
-
-  /**
-   * @brief Reads a value from LearningParams.
-   * @param key: identifier for the value
-   * @return Value associated with the key
-   */
-  virtual Value get(KeyType const& key) const = 0;
-
-  /**
-   * @brief Get all the keys in the LearningParams instance.
-   * @return Vector of keys
-   */
-  virtual std::vector<std::string> keys() const = 0;
-
-  /**
-   * @brief compare two LearningParams objects.
-   * @param other: object to compare with
-   * @return true if values and learning type are the same
-   */
-  virtual bool operator==(const LearningParams& other) const = 0;
-
-  /**
-   * @brief Get the learning type
-   * @return the learning type
-   */
-  virtual LearningType learning_type() const = 0;
-};
-
+#pragma once
+
+#include <map>
+#include <memory>
+#include <string>
+
+#include "akida/shape.h"
+
+namespace akida {
+
+/**
+ * @enum LearningType
+ * @brief The layer type
+ */
+enum class LearningType { AkidaUnsupervised };
+
+class LearningParams;
+using LearningParamsPtr = std::shared_ptr<LearningParams>;
+
+/**
+ * @class LearningParams
+ * @brief Generic parameters that can be used for several learnings
+ */
+class AKIDASHAREDLIB_EXPORT LearningParams {
+ public:
+  // Value is a basic type large enough to hold the data for every type in a
+  // lossless way.
+  using Value = double;
+  using KeyType = const std::string;
+  using Dict = std::map<KeyType, Value>;
+
+  virtual ~LearningParams() = default;
+
+  /**
+   * @brief Simplified static cast to Value
+   * @param value: Value to be casted
+   * @return casted value, ready to be inserted
+   */
+  template<typename T>
+  static constexpr Value as_value(T value) {
+    return static_cast<Value>(value);
+  }
+
+  /**
+   * @brief Create a LearningParams object
+   * @param learning_type: learning type for these parameters
+   * @param entries: a map of key-entries
+   * @return a LearningParams object
+   */
+  static LearningParamsPtr create(LearningType type, const Dict& entries);
+
+  /**
+   * @brief Copy a LearningParams object
+   * @param src: source object
+   * @return A copy of the source object
+   */
+  static LearningParamsPtr clone(const LearningParams* src);
+
+  /**
+   * @brief Reads a value from LearningParams.
+   * @param key: identifier for the value
+   * @return Value associated with the key
+   */
+  virtual Value get(KeyType const& key) const = 0;
+
+  /**
+   * @brief Get all the keys in the LearningParams instance.
+   * @return Vector of keys
+   */
+  virtual std::vector<std::string> keys() const = 0;
+
+  /**
+   * @brief compare two LearningParams objects.
+   * @param other: object to compare with
+   * @return true if values and learning type are the same
+   */
+  virtual bool operator==(const LearningParams& other) const = 0;
+
+  /**
+   * @brief Get the learning type
+   * @return the learning type
+   */
+  virtual LearningType learning_type() const = 0;
+};
+
 }  // namespace akida
```

## akida/api/akida/mesh.h

 * *Ordering differences only*

```diff
@@ -1,21 +1,21 @@
-#pragma once
-
-#include <memory>
-#include <vector>
-
-#include "akida/hardware_device.h"
-#include "akida/np.h"
-
-namespace akida {
-
-namespace mesh {
-
-/**
- * Discover the topology of a Device Mesh
- */
-AKIDASHAREDLIB_EXPORT std::unique_ptr<np::Mesh> discover(
-    HardwareDevice* device);
-
-}  // namespace mesh
-
-}  // namespace akida
+#pragma once
+
+#include <memory>
+#include <vector>
+
+#include "akida/hardware_device.h"
+#include "akida/np.h"
+
+namespace akida {
+
+namespace mesh {
+
+/**
+ * Discover the topology of a Device Mesh
+ */
+AKIDASHAREDLIB_EXPORT std::unique_ptr<np::Mesh> discover(
+    HardwareDevice* device);
+
+}  // namespace mesh
+
+}  // namespace akida
```

## akida/api/akida/mesh_mapper.h

 * *Ordering differences only*

```diff
@@ -1,54 +1,54 @@
-#pragma once
-
-#include <memory>
-#include <tuple>
-#include <vector>
-
-#include "akida/device.h"
-#include "akida/layer.h"
-#include "infra/exports.h"
-#include "np_mapping.h"
-
-namespace akida {
-
-class MeshMapper;
-
-using MeshMapperPtr = std::shared_ptr<MeshMapper>;
-
-class AKIDASHAREDLIB_EXPORT MeshMapper {
- public:
-  virtual ~MeshMapper() {}
-  /**
-   * @brief Select a set of Neural Processors (NP)
-   *
-   * This allows to select from a predefined list a specified number of NPs.
-   */
-  virtual np::IdentVector select_nps(const np::IdentVector& source_nps,
-                                     size_t num_nps, np::Type type) = 0;
-
-  /**
-   * @brief Get the maximum width allowed for a CNP
-   */
-  virtual uint32_t cnp_max_width() = 0;
-
-  /**
-   * @brief Get the maximum height allowed for a CNP
-   */
-  virtual uint32_t cnp_max_height() = 0;
-
-  /**
-   * @brief Get the maximum number of filters allowed for a CNP
-   */
-  virtual uint32_t cnp_max_filters() = 0;
-
-  /**
-   * @brief Override the default MeshMapper
-   *
-   * Passing nullptr to this method restores the default MeshMapper.
-   *
-   * @param : a pointer to a MeshMapper instance, or nullptr
-   */
-  static void replace(MeshMapperPtr mapper);
-};
-
-}  // namespace akida
+#pragma once
+
+#include <memory>
+#include <tuple>
+#include <vector>
+
+#include "akida/device.h"
+#include "akida/layer.h"
+#include "infra/exports.h"
+#include "np_mapping.h"
+
+namespace akida {
+
+class MeshMapper;
+
+using MeshMapperPtr = std::shared_ptr<MeshMapper>;
+
+class AKIDASHAREDLIB_EXPORT MeshMapper {
+ public:
+  virtual ~MeshMapper() {}
+  /**
+   * @brief Select a set of Neural Processors (NP)
+   *
+   * This allows to select from a predefined list a specified number of NPs.
+   */
+  virtual np::IdentVector select_nps(const np::IdentVector& source_nps,
+                                     size_t num_nps, np::Type type) = 0;
+
+  /**
+   * @brief Get the maximum width allowed for a CNP
+   */
+  virtual uint32_t cnp_max_width() = 0;
+
+  /**
+   * @brief Get the maximum height allowed for a CNP
+   */
+  virtual uint32_t cnp_max_height() = 0;
+
+  /**
+   * @brief Get the maximum number of filters allowed for a CNP
+   */
+  virtual uint32_t cnp_max_filters() = 0;
+
+  /**
+   * @brief Override the default MeshMapper
+   *
+   * Passing nullptr to this method restores the default MeshMapper.
+   *
+   * @param : a pointer to a MeshMapper instance, or nullptr
+   */
+  static void replace(MeshMapperPtr mapper);
+};
+
+}  // namespace akida
```

## akida/api/akida/model.h

 * *Ordering differences only*

```diff
@@ -1,226 +1,226 @@
-/*******************************************************************************
- * Copyright 2019 Brainchip Holdings Ltd.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- ********************************************************************************
- */
-
-#pragma once
-
-#include <cstddef>
-#include <cstdint>
-#include <memory>
-#include <string>
-#include <vector>
-
-#include "akida/dense.h"
-#include "akida/device.h"
-#include "akida/layer.h"
-#include "akida/layer_params.h"
-#include "akida/sequence.h"
-#include "akida/shape.h"
-#include "akida/tensor.h"
-#include "akida/variables.h"
-#include "infra/exports.h"
-
-namespace akida {
-
-class Model;
-
-/**
- * @brief A unique pointer to a Model object
- */
-using ModelPtr = std::unique_ptr<Model>;
-
-/**
- * class Model
- *
- * Public interface to an Akida Model. Users can create or load
- * models, save models, and propagate spike events through the full model.
- *
- */
-
-class AKIDASHAREDLIB_EXPORT Model {
- public:
-  virtual ~Model() {}
-  /**
-   * @brief Create a model object
-   */
-  static ModelPtr create();
-
-  /**
-   * @brief Create a model from a serialized model
-   * @param buffer : char buffer containing the model (a flatbuffer)
-   * @param size : size of the buffer
-   */
-  static ModelPtr from_buffer(const char* buffer, size_t size);
-
-  /**
-   * @brief Add a layer to the current model
-   *
-   * A list of inbound layers can optionally be specified.
-   *
-   * These layers must already be included in the model.
-   *
-   * If no inbound layer is specified, and the layer is not the first layer in
-   * the model, the last included layer will be used as inbound layer.
-   *
-   * @param layer : layer instance to be added to the model
-   * @param inbound_layers : the inbound layers for this layer
-   */
-  virtual void add(LayerPtr layer,
-                   const std::vector<LayerConstPtr> inbound_layers = {}) = 0;
-
-  /**
-   * @brief Remove the last layer of the current model
-   */
-  virtual void pop_layer() = 0;
-
-  /**
-   * @brief Return the serialized model configuration (all layers and weights)
-   * as a vector of char (bytes).
-   */
-  virtual std::vector<char> to_buffer() = 0;
-
-  /**
-   * @brief Prepare the internal parameters of the last layer of the model for
-   * training.
-   * @param params : the LearningParams
-   */
-  virtual void compile(const LearningParams& params) = 0;
-
-  /**
-   * @brief Propagates inputs to train the model.
-   * @param inputs       : pointer to Dense inputs
-   * @param input_labels : integer value labels of the input classes,
-   * for supervised learning
-   * @param batch_size   : maximum number of inputs that should be processed at
-   * a time. If 0, the whole input size will be taken.
-   * @return Dense outputs from the model last layer
-   */
-  virtual DensePtr fit(DenseConstPtr inputs,
-                       const std::vector<int32_t>& input_labels = {},
-                       uint32_t batch_size = 0) = 0;
-
-  /**
-   * @brief Propagates events through the model
-   * @param inputs       : pointer to Dense inputs
-   * @param batch_size   : maximum number of inputs that should be processed at
-   * a time. If 0, the whole input size will be taken.
-   * @return Dense outputs from the model last layer
-   */
-  virtual DensePtr forward(DenseConstPtr inputs, uint32_t batch_size = 0) = 0;
-
-  /**
-   * @brief Evaluates the results of events propagation through the model
-   *
-   * This method propagates a set of inputs through the model and returns the
-   * results in the form of a Tensor of float values.
-   * It applies ONLY on models whithout an activation on the last layer.
-   * The output values are obtained from the model discrete potentials by
-   * applying a shift and a scale.
-   *
-   * @param inputs : Dense inputs to be processed by model
-   * @param batch_size   : maximum number of inputs that should be processed at
-   * a time. If 0, the whole input size will be taken.
-   * @return rescaled output potentials from the model last layer
-   */
-  virtual DensePtr predict(DenseConstPtr inputs, uint32_t batch_size = 0) = 0;
-
-  /**
-   * @brief Maps the model to a Device
-   *
-   * This method tries to map a Model to the specified Device, implicitly
-   * identifying one or more layer sequences that are mapped individually on the
-   * Device Mesh.
-   *
-   * An optional hw_only parameter can be specified to force the mapping
-   * strategy to use only one hardware sequence, thus reducing software
-   * intervention on the inference.
-   *
-   * @param device: the target Device or nullptr
-   * @param hw_only: when true, the model should be mapped in one sequence
-   */
-  virtual void map(DevicePtr device, bool hw_only = false) = 0;
-
-  /**
-   * @brief Returns a pointer to a layer from its index
-   * @param index : index to the layer to retrieve
-   * @return a pointer to a Layer object
-   */
-  virtual LayerPtr get_layer(size_t index) = 0;
-
-  /**
-   * @brief Returns a pointer to a layer from its name
-   * @param name : name of the layer to retrieve
-   * @return a pointer to a Layer object
-   */
-  virtual LayerPtr get_layer(const std::string& name) = 0;
-
-  /**
-   * @brief Returns a pointer to a layer from its index
-   * @param index : index to the layer to retrieve
-   * @return a const pointer to a Layer object
-   */
-  virtual LayerConstPtr get_layer(size_t index) const = 0;
-
-  /**
-   * @brief Returns a pointer to a layer from its name
-   * @param name : name of the layer to retrieve
-   * @return a const pointer to a Layer object
-   */
-  virtual LayerConstPtr get_layer(const std::string& name) const = 0;
-
-  /**
-   * @brief Returns a vector of pointers to layers in the model
-   * @return a vector containing pointers to Layer objects
-   */
-  virtual const std::vector<LayerPtr>& get_layers() const = 0;
-
-  /**
-   * @brief Returns the learning parameters
-   * @return a pointer to LearningParams objects
-   */
-  virtual const LearningParams* learning() const = 0;
-
-  /**
-   * @brief Returns the input dimensions of the model
-   * @return : a Shape representing input dimensions
-   */
-  virtual Shape input_shape() const = 0;
-
-  /**
-   * @brief Returns output dimensions of the model
-   * @return : a Shape representing output dimensions
-   */
-  virtual Shape output_shape() const = 0;
-
-  /**
-   * @brief Returns the number of layers in the model
-   */
-  virtual size_t get_layer_count() const = 0;
-
-  /**
-   * @brief Retrieve the Model layer sequences
-   * @return a vector of const Sequence pointers
-   */
-  virtual const std::vector<SequencePtr>& sequences() = 0;
-
-  /**
-   * @brief Retrieve the Device the Model is mapped to
-   * @return a Device pointer or nullptr
-   */
-  virtual DevicePtr device() = 0;
-};
-
-}  // namespace akida
+/*******************************************************************************
+ * Copyright 2019 Brainchip Holdings Ltd.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ ********************************************************************************
+ */
+
+#pragma once
+
+#include <cstddef>
+#include <cstdint>
+#include <memory>
+#include <string>
+#include <vector>
+
+#include "akida/dense.h"
+#include "akida/device.h"
+#include "akida/layer.h"
+#include "akida/layer_params.h"
+#include "akida/sequence.h"
+#include "akida/shape.h"
+#include "akida/tensor.h"
+#include "akida/variables.h"
+#include "infra/exports.h"
+
+namespace akida {
+
+class Model;
+
+/**
+ * @brief A unique pointer to a Model object
+ */
+using ModelPtr = std::unique_ptr<Model>;
+
+/**
+ * class Model
+ *
+ * Public interface to an Akida Model. Users can create or load
+ * models, save models, and propagate spike events through the full model.
+ *
+ */
+
+class AKIDASHAREDLIB_EXPORT Model {
+ public:
+  virtual ~Model() {}
+  /**
+   * @brief Create a model object
+   */
+  static ModelPtr create();
+
+  /**
+   * @brief Create a model from a serialized model
+   * @param buffer : char buffer containing the model (a flatbuffer)
+   * @param size : size of the buffer
+   */
+  static ModelPtr from_buffer(const char* buffer, size_t size);
+
+  /**
+   * @brief Add a layer to the current model
+   *
+   * A list of inbound layers can optionally be specified.
+   *
+   * These layers must already be included in the model.
+   *
+   * If no inbound layer is specified, and the layer is not the first layer in
+   * the model, the last included layer will be used as inbound layer.
+   *
+   * @param layer : layer instance to be added to the model
+   * @param inbound_layers : the inbound layers for this layer
+   */
+  virtual void add(LayerPtr layer,
+                   const std::vector<LayerConstPtr> inbound_layers = {}) = 0;
+
+  /**
+   * @brief Remove the last layer of the current model
+   */
+  virtual void pop_layer() = 0;
+
+  /**
+   * @brief Return the serialized model configuration (all layers and weights)
+   * as a vector of char (bytes).
+   */
+  virtual std::vector<char> to_buffer() = 0;
+
+  /**
+   * @brief Prepare the internal parameters of the last layer of the model for
+   * training.
+   * @param params : the LearningParams
+   */
+  virtual void compile(const LearningParams& params) = 0;
+
+  /**
+   * @brief Propagates inputs to train the model.
+   * @param inputs       : pointer to Dense inputs
+   * @param input_labels : integer value labels of the input classes,
+   * for supervised learning
+   * @param batch_size   : maximum number of inputs that should be processed at
+   * a time. If 0, the whole input size will be taken.
+   * @return Dense outputs from the model last layer
+   */
+  virtual DensePtr fit(DenseConstPtr inputs,
+                       const std::vector<int32_t>& input_labels = {},
+                       uint32_t batch_size = 0) = 0;
+
+  /**
+   * @brief Propagates events through the model
+   * @param inputs       : pointer to Dense inputs
+   * @param batch_size   : maximum number of inputs that should be processed at
+   * a time. If 0, the whole input size will be taken.
+   * @return Dense outputs from the model last layer
+   */
+  virtual DensePtr forward(DenseConstPtr inputs, uint32_t batch_size = 0) = 0;
+
+  /**
+   * @brief Evaluates the results of events propagation through the model
+   *
+   * This method propagates a set of inputs through the model and returns the
+   * results in the form of a Tensor of float values.
+   * It applies ONLY on models whithout an activation on the last layer.
+   * The output values are obtained from the model discrete potentials by
+   * applying a shift and a scale.
+   *
+   * @param inputs : Dense inputs to be processed by model
+   * @param batch_size   : maximum number of inputs that should be processed at
+   * a time. If 0, the whole input size will be taken.
+   * @return rescaled output potentials from the model last layer
+   */
+  virtual DensePtr predict(DenseConstPtr inputs, uint32_t batch_size = 0) = 0;
+
+  /**
+   * @brief Maps the model to a Device
+   *
+   * This method tries to map a Model to the specified Device, implicitly
+   * identifying one or more layer sequences that are mapped individually on the
+   * Device Mesh.
+   *
+   * An optional hw_only parameter can be specified to force the mapping
+   * strategy to use only one hardware sequence, thus reducing software
+   * intervention on the inference.
+   *
+   * @param device: the target Device or nullptr
+   * @param hw_only: when true, the model should be mapped in one sequence
+   */
+  virtual void map(DevicePtr device, bool hw_only = false) = 0;
+
+  /**
+   * @brief Returns a pointer to a layer from its index
+   * @param index : index to the layer to retrieve
+   * @return a pointer to a Layer object
+   */
+  virtual LayerPtr get_layer(size_t index) = 0;
+
+  /**
+   * @brief Returns a pointer to a layer from its name
+   * @param name : name of the layer to retrieve
+   * @return a pointer to a Layer object
+   */
+  virtual LayerPtr get_layer(const std::string& name) = 0;
+
+  /**
+   * @brief Returns a pointer to a layer from its index
+   * @param index : index to the layer to retrieve
+   * @return a const pointer to a Layer object
+   */
+  virtual LayerConstPtr get_layer(size_t index) const = 0;
+
+  /**
+   * @brief Returns a pointer to a layer from its name
+   * @param name : name of the layer to retrieve
+   * @return a const pointer to a Layer object
+   */
+  virtual LayerConstPtr get_layer(const std::string& name) const = 0;
+
+  /**
+   * @brief Returns a vector of pointers to layers in the model
+   * @return a vector containing pointers to Layer objects
+   */
+  virtual const std::vector<LayerPtr>& get_layers() const = 0;
+
+  /**
+   * @brief Returns the learning parameters
+   * @return a pointer to LearningParams objects
+   */
+  virtual const LearningParams* learning() const = 0;
+
+  /**
+   * @brief Returns the input dimensions of the model
+   * @return : a Shape representing input dimensions
+   */
+  virtual Shape input_shape() const = 0;
+
+  /**
+   * @brief Returns output dimensions of the model
+   * @return : a Shape representing output dimensions
+   */
+  virtual Shape output_shape() const = 0;
+
+  /**
+   * @brief Returns the number of layers in the model
+   */
+  virtual size_t get_layer_count() const = 0;
+
+  /**
+   * @brief Retrieve the Model layer sequences
+   * @return a vector of const Sequence pointers
+   */
+  virtual const std::vector<SequencePtr>& sequences() = 0;
+
+  /**
+   * @brief Retrieve the Device the Model is mapped to
+   * @return a Device pointer or nullptr
+   */
+  virtual DevicePtr device() = 0;
+};
+
+}  // namespace akida
```

## akida/api/akida/np_mapping.h

 * *Ordering differences only*

```diff
@@ -1,64 +1,64 @@
-#pragma once
-
-#include <algorithm>
-#include <map>
-#include <vector>
-
-#include "akida/np.h"
-#include "akida/shape.h"
-
-namespace akida {
-
-struct NPSpace {
-  Index x;
-  Index y;
-  Shape shape;
-};
-
-struct NPMapping {
-  NPMapping(np::Type np_type, const NPSpace& in_int, const NPSpace& in_aug,
-            Index start_n, Index neurons, bool single_buf,
-            np::Ident np_id = np::HRC_IDENT)
-      : np(np_id),
-        type(np_type),
-        start_neuron(start_n),
-        num_neurons(neurons),
-        input_int(in_int),
-        input_aug(in_aug),
-        single_buffer(single_buf) {}
-  NPMapping(np::Type np_type, const Shape& shape, Index start_n, Index neurons,
-            bool single_buf, np::Ident np_id = np::HRC_IDENT)
-      : np(np_id),
-        type(np_type),
-        start_neuron(start_n),
-        num_neurons(neurons),
-        input_int{0, 0, shape},
-        input_aug{0, 0, shape},
-        single_buffer(single_buf) {}
-  explicit NPMapping(np::Ident np_id)
-      : NPMapping(np::Type::HRC, {}, 0, 0, false, np_id) {}
-  np::Ident np;
-  np::Type type;
-  Index start_neuron;
-  Index num_neurons;
-  // Internal input box
-  NPSpace input_int;
-  // Augmented input box
-  NPSpace input_aug;
-  // Use single buffer or dual ping-pong buffer
-  bool single_buffer;
-};
-
-// utility function to find the leftmost or rightmost NPs
-inline uint8_t find_border_column(const std::vector<NPMapping>& nps,
-                                  bool find_left) {
-  auto border_np = *std::min_element(
-      nps.begin(), nps.end(),
-      [find_left](const NPMapping& left, const NPMapping& right) {
-        return find_left ? left.np.col < right.np.col
-                         : left.np.col > right.np.col;
-      });
-  return border_np.np.col;
-}
-
-}  // namespace akida
+#pragma once
+
+#include <algorithm>
+#include <map>
+#include <vector>
+
+#include "akida/np.h"
+#include "akida/shape.h"
+
+namespace akida {
+
+struct NPSpace {
+  Index x;
+  Index y;
+  Shape shape;
+};
+
+struct NPMapping {
+  NPMapping(np::Type np_type, const NPSpace& in_int, const NPSpace& in_aug,
+            Index start_n, Index neurons, bool single_buf,
+            np::Ident np_id = np::HRC_IDENT)
+      : np(np_id),
+        type(np_type),
+        start_neuron(start_n),
+        num_neurons(neurons),
+        input_int(in_int),
+        input_aug(in_aug),
+        single_buffer(single_buf) {}
+  NPMapping(np::Type np_type, const Shape& shape, Index start_n, Index neurons,
+            bool single_buf, np::Ident np_id = np::HRC_IDENT)
+      : np(np_id),
+        type(np_type),
+        start_neuron(start_n),
+        num_neurons(neurons),
+        input_int{0, 0, shape},
+        input_aug{0, 0, shape},
+        single_buffer(single_buf) {}
+  explicit NPMapping(np::Ident np_id)
+      : NPMapping(np::Type::HRC, {}, 0, 0, false, np_id) {}
+  np::Ident np;
+  np::Type type;
+  Index start_neuron;
+  Index num_neurons;
+  // Internal input box
+  NPSpace input_int;
+  // Augmented input box
+  NPSpace input_aug;
+  // Use single buffer or dual ping-pong buffer
+  bool single_buffer;
+};
+
+// utility function to find the leftmost or rightmost NPs
+inline uint8_t find_border_column(const std::vector<NPMapping>& nps,
+                                  bool find_left) {
+  auto border_np = *std::min_element(
+      nps.begin(), nps.end(),
+      [find_left](const NPMapping& left, const NPMapping& right) {
+        return find_left ? left.np.col < right.np.col
+                         : left.np.col > right.np.col;
+      });
+  return border_np.np.col;
+}
+
+}  // namespace akida
```

## akida/api/akida/sequence.h

 * *Ordering differences only*

```diff
@@ -1,87 +1,87 @@
-/*******************************************************************************
- * Copyright 2021 Brainchip Holdings Ltd.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- ********************************************************************************
- */
-#pragma once
-
-#include <memory>
-#include <utility>
-
-#include "akida/backend_type.h"
-#include "akida/layer.h"
-
-namespace akida {
-
-using BytesBuffer = std::pair<const uint8_t*, size_t>;
-
-/**
- * class Sequence
- *
- * Represents a sequence of layers.
- * Sequences can be mapped in Software or on a Device.
- *
- */
-
-class AKIDASHAREDLIB_EXPORT Sequence {
- public:
-  virtual ~Sequence() {}
-
-  /**
-   * class Pass
-   *
-   * Represents a subset of the Sequence.
-   * Hardware Sequences can typically be split into multiple passes on devices
-   * that support hardware partial reconfiguration feature, reducing the
-   * intervention of the software during inference.
-   */
-  struct Pass {
-    virtual ~Pass() = default;
-    std::vector<LayerPtr> layers;
-  };
-  using PassPtr = std::shared_ptr<Pass>;
-
-  /**
-   * @brief Returns a vector of pointers to passes in the Sequence
-   * @return a vector containing pointers to Pass objects
-   */
-  virtual const std::vector<PassPtr>& passes() const = 0;
-
-  /**
-   * @brief Return the Sequence backend type
-   *
-   * @return : the Sequence BackendType
-   */
-  virtual BackendType backend() const = 0;
-
-  /**
-   * @brief Return the Sequence programmation
-   *
-   * @return: a pair with a pointer to the serialized program and its size, or
-   *          nullptr if the Sequence is not programmable
-   */
-  virtual BytesBuffer program() const = 0;
-
-  /**
-   * @brief Return the Sequence name
-   *
-   * @return a string representing the sequence
-   */
-  virtual std::string name() const = 0;
-};
-
-using SequenceConstPtr = std::shared_ptr<const Sequence>;
-using SequencePtr = std::shared_ptr<Sequence>;
-
-}  // namespace akida
+/*******************************************************************************
+ * Copyright 2021 Brainchip Holdings Ltd.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ ********************************************************************************
+ */
+#pragma once
+
+#include <memory>
+#include <utility>
+
+#include "akida/backend_type.h"
+#include "akida/layer.h"
+
+namespace akida {
+
+using BytesBuffer = std::pair<const uint8_t*, size_t>;
+
+/**
+ * class Sequence
+ *
+ * Represents a sequence of layers.
+ * Sequences can be mapped in Software or on a Device.
+ *
+ */
+
+class AKIDASHAREDLIB_EXPORT Sequence {
+ public:
+  virtual ~Sequence() {}
+
+  /**
+   * class Pass
+   *
+   * Represents a subset of the Sequence.
+   * Hardware Sequences can typically be split into multiple passes on devices
+   * that support hardware partial reconfiguration feature, reducing the
+   * intervention of the software during inference.
+   */
+  struct Pass {
+    virtual ~Pass() = default;
+    std::vector<LayerPtr> layers;
+  };
+  using PassPtr = std::shared_ptr<Pass>;
+
+  /**
+   * @brief Returns a vector of pointers to passes in the Sequence
+   * @return a vector containing pointers to Pass objects
+   */
+  virtual const std::vector<PassPtr>& passes() const = 0;
+
+  /**
+   * @brief Return the Sequence backend type
+   *
+   * @return : the Sequence BackendType
+   */
+  virtual BackendType backend() const = 0;
+
+  /**
+   * @brief Return the Sequence programmation
+   *
+   * @return: a pair with a pointer to the serialized program and its size, or
+   *          nullptr if the Sequence is not programmable
+   */
+  virtual BytesBuffer program() const = 0;
+
+  /**
+   * @brief Return the Sequence name
+   *
+   * @return a string representing the sequence
+   */
+  virtual std::string name() const = 0;
+};
+
+using SequenceConstPtr = std::shared_ptr<const Sequence>;
+using SequencePtr = std::shared_ptr<Sequence>;
+
+}  // namespace akida
```

## akida/api/akida/variables.h

 * *Ordering differences only*

```diff
@@ -1,55 +1,55 @@
-#pragma once
-
-#include <cstdint>
-#include <string>
-#include <vector>
-
-#include "akida/dense.h"
-#include "akida/shape.h"
-#include "infra/exports.h"
-
-namespace akida {
-
-/**
- * class Variables
- * @brief Public interface to access and use layer variables.
- */
-class AKIDASHAREDLIB_EXPORT Variables {
- public:
-  virtual ~Variables() {}
-
-  /**
-   * @brief Returns the names of available variables.
-   */
-  virtual std::vector<std::string> names() const = 0;
-
-  /**
-   * @brief Sets a variable.
-   * @param name   : name of the variable to set
-   * @param tensor : values to set as a Tensor shared pointer
-   */
-  virtual void set(const std::string& name, DenseConstPtr tensor) = 0;
-
-  /**
-   * @brief Gets a variable.
-   * @param name : name of the variable to get
-   * @return the requested variable
-   * @note The returned variable is read-only, to apply changes to it one must
-   * use the set method
-   */
-  virtual DenseConstPtr get(const std::string& name) const = 0;
-
-  /**
-   * @brief Check if variable exists.
-   * @param name : name of the variable to check
-   * @return boolean asserting the existence or not of the variable
-   */
-  virtual bool has(const std::string& name) const = 0;
-};
-
-/**
- * @brief A shared pointer to a Variables object
- */
-using VariablesPtr = std::shared_ptr<Variables>;
-
-}  // namespace akida
+#pragma once
+
+#include <cstdint>
+#include <string>
+#include <vector>
+
+#include "akida/dense.h"
+#include "akida/shape.h"
+#include "infra/exports.h"
+
+namespace akida {
+
+/**
+ * class Variables
+ * @brief Public interface to access and use layer variables.
+ */
+class AKIDASHAREDLIB_EXPORT Variables {
+ public:
+  virtual ~Variables() {}
+
+  /**
+   * @brief Returns the names of available variables.
+   */
+  virtual std::vector<std::string> names() const = 0;
+
+  /**
+   * @brief Sets a variable.
+   * @param name   : name of the variable to set
+   * @param tensor : values to set as a Tensor shared pointer
+   */
+  virtual void set(const std::string& name, DenseConstPtr tensor) = 0;
+
+  /**
+   * @brief Gets a variable.
+   * @param name : name of the variable to get
+   * @return the requested variable
+   * @note The returned variable is read-only, to apply changes to it one must
+   * use the set method
+   */
+  virtual DenseConstPtr get(const std::string& name) const = 0;
+
+  /**
+   * @brief Check if variable exists.
+   * @param name : name of the variable to check
+   * @return boolean asserting the existence or not of the variable
+   */
+  virtual bool has(const std::string& name) const = 0;
+};
+
+/**
+ * @brief A shared pointer to a Variables object
+ */
+using VariablesPtr = std::shared_ptr<Variables>;
+
+}  // namespace akida
```

## akida/api/host/circular_queue.h

 * *Ordering differences only*

```diff
@@ -1,133 +1,133 @@
-#pragma once
-
-#include <cstddef>
-#include <stdexcept>
-#include <vector>
-
-namespace akida {
-
-template<typename T>
-class CircularQueue {
- public:
-  explicit CircularQueue(size_t size);
-  void push(const T& elem);
-  bool empty() const;
-  bool full() const;
-  size_t size() const;
-  std::vector<T> get(
-      bool flush);  // returns items in the order they were inserted
-  T latest();       // returns the latest inserted item
-  void clear();     // clear the queue
- private:
-  std::vector<T> buffer_;
-  T* head_;             // pointer to the latest inserted item
-  T* tail_;             // pointer to the oldest inserted item
-  const T* const end_;  // pointer to the end of the buffer
-  bool full_;
-};
-
-template<typename T>
-CircularQueue<T>::CircularQueue(size_t size)
-    : buffer_(size),          // dimension buffer to expected size
-      head_(buffer_.data()),  // initialize both head & tail to beginning of the
-                              // buffer
-      tail_(buffer_.data()),
-      end_(buffer_.data() + size - 1),  // initialize end to the last element
-      full_(false) {
-  if (size == 0) {
-    throw std::invalid_argument("Size must be > 0");
-  }
-}
-
-template<typename T>
-static T* advance_ptr(T* const ptr, T* const begin, const T* const end) {
-  T* result;
-  if (ptr == end) {
-    result = begin;
-  } else {
-    result = ptr + 1;
-  }
-  return result;
-}
-
-template<typename T>
-void CircularQueue<T>::push(const T& elem) {
-  // insert elem at the head
-  *head_ = elem;
-  // if buffer is full, we just erased tail, so advance it
-  if (full()) {
-    tail_ = advance_ptr(tail_, buffer_.data(), end_);
-  }
-  // advance head
-  head_ = advance_ptr(head_, buffer_.data(), end_);
-
-  full_ = head_ == tail_;
-}
-
-template<typename T>
-bool CircularQueue<T>::empty() const {
-  return !full_ && head_ == tail_;
-}
-
-template<typename T>
-bool CircularQueue<T>::full() const {
-  return full_;
-}
-
-template<typename T>
-size_t CircularQueue<T>::size() const {
-  size_t size;
-
-  if (empty()) {
-    size = 0;
-  } else if (full()) {
-    size = buffer_.size();
-  } else {
-    if (head_ > tail_) {
-      // If head is ahead of tail, just return diff between them
-      size = head_ - tail_;
-    } else {
-      // else adds max size to "force" head to be ahead of tail
-      size = head_ + buffer_.size() - tail_;
-    }
-  }
-  return size;
-}
-
-template<typename T>
-std::vector<T> CircularQueue<T>::get(bool flush) {
-  std::vector<T> result;
-  const auto nb_elems = size();
-  result.reserve(nb_elems);
-  // get elems from tail to head
-  auto begin = tail_;
-  for (size_t i = 0; i < nb_elems; ++i) {
-    result.push_back(*begin);
-    begin = advance_ptr(begin, buffer_.data(), end_);
-  }
-
-  if (flush) {
-    clear();
-  }
-
-  return result;
-}
-
-template<typename T>
-void CircularQueue<T>::clear() {
-  // clear buffer
-  head_ = buffer_.data();
-  tail_ = buffer_.data();
-  full_ = false;
-}
-
-template<typename T>
-T CircularQueue<T>::latest() {
-  if (head_ == buffer_.data()) {
-    return *end_;
-  } else {
-    return *(head_ - 1);
-  }
-}
-
-}  // namespace akida
+#pragma once
+
+#include <cstddef>
+#include <stdexcept>
+#include <vector>
+
+namespace akida {
+
+template<typename T>
+class CircularQueue {
+ public:
+  explicit CircularQueue(size_t size);
+  void push(const T& elem);
+  bool empty() const;
+  bool full() const;
+  size_t size() const;
+  std::vector<T> get(
+      bool flush);  // returns items in the order they were inserted
+  T latest();       // returns the latest inserted item
+  void clear();     // clear the queue
+ private:
+  std::vector<T> buffer_;
+  T* head_;             // pointer to the latest inserted item
+  T* tail_;             // pointer to the oldest inserted item
+  const T* const end_;  // pointer to the end of the buffer
+  bool full_;
+};
+
+template<typename T>
+CircularQueue<T>::CircularQueue(size_t size)
+    : buffer_(size),          // dimension buffer to expected size
+      head_(buffer_.data()),  // initialize both head & tail to beginning of the
+                              // buffer
+      tail_(buffer_.data()),
+      end_(buffer_.data() + size - 1),  // initialize end to the last element
+      full_(false) {
+  if (size == 0) {
+    throw std::invalid_argument("Size must be > 0");
+  }
+}
+
+template<typename T>
+static T* advance_ptr(T* const ptr, T* const begin, const T* const end) {
+  T* result;
+  if (ptr == end) {
+    result = begin;
+  } else {
+    result = ptr + 1;
+  }
+  return result;
+}
+
+template<typename T>
+void CircularQueue<T>::push(const T& elem) {
+  // insert elem at the head
+  *head_ = elem;
+  // if buffer is full, we just erased tail, so advance it
+  if (full()) {
+    tail_ = advance_ptr(tail_, buffer_.data(), end_);
+  }
+  // advance head
+  head_ = advance_ptr(head_, buffer_.data(), end_);
+
+  full_ = head_ == tail_;
+}
+
+template<typename T>
+bool CircularQueue<T>::empty() const {
+  return !full_ && head_ == tail_;
+}
+
+template<typename T>
+bool CircularQueue<T>::full() const {
+  return full_;
+}
+
+template<typename T>
+size_t CircularQueue<T>::size() const {
+  size_t size;
+
+  if (empty()) {
+    size = 0;
+  } else if (full()) {
+    size = buffer_.size();
+  } else {
+    if (head_ > tail_) {
+      // If head is ahead of tail, just return diff between them
+      size = head_ - tail_;
+    } else {
+      // else adds max size to "force" head to be ahead of tail
+      size = head_ + buffer_.size() - tail_;
+    }
+  }
+  return size;
+}
+
+template<typename T>
+std::vector<T> CircularQueue<T>::get(bool flush) {
+  std::vector<T> result;
+  const auto nb_elems = size();
+  result.reserve(nb_elems);
+  // get elems from tail to head
+  auto begin = tail_;
+  for (size_t i = 0; i < nb_elems; ++i) {
+    result.push_back(*begin);
+    begin = advance_ptr(begin, buffer_.data(), end_);
+  }
+
+  if (flush) {
+    clear();
+  }
+
+  return result;
+}
+
+template<typename T>
+void CircularQueue<T>::clear() {
+  // clear buffer
+  head_ = buffer_.data();
+  tail_ = buffer_.data();
+  full_ = false;
+}
+
+template<typename T>
+T CircularQueue<T>::latest() {
+  if (head_ == buffer_.data()) {
+    return *end_;
+  } else {
+    return *(head_ - 1);
+  }
+}
+
+}  // namespace akida
```

## akida/api/host/hardware_devices.h

 * *Ordering differences only*

```diff
@@ -1,33 +1,33 @@
-/*******************************************************************************
- * Copyright 2021 Brainchip Holdings Ltd.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- ********************************************************************************
- */
-
-#pragma once
-
-#include <vector>
-
-#include "akida/device.h"
-#include "infra/exports.h"
-
-namespace akida {
-
-/**
- * @brief Return the full list of available hardware devices
- * @return vector of hardware devices found
- */
-AKIDASHAREDLIB_EXPORT const std::vector<DevicePtr>& get_devices();
-
-}  // namespace akida
+/*******************************************************************************
+ * Copyright 2021 Brainchip Holdings Ltd.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ ********************************************************************************
+ */
+
+#pragma once
+
+#include <vector>
+
+#include "akida/device.h"
+#include "infra/exports.h"
+
+namespace akida {
+
+/**
+ * @brief Return the full list of available hardware devices
+ * @return vector of hardware devices found
+ */
+AKIDASHAREDLIB_EXPORT const std::vector<DevicePtr>& get_devices();
+
+}  // namespace akida
```

## akida/api/host/hardware_drivers.h

 * *Ordering differences only*

```diff
@@ -1,17 +1,17 @@
-#pragma once
-
-#include "infra/exports.h"
-#include "infra/hardware_driver.h"
-
-#include <memory>
-#include <vector>
-
-namespace akida {
-
-/**
- * @brief Return a singleton vector containing all usable hardware drivers
- */
-AKIDASHAREDLIB_EXPORT const std::vector<std::unique_ptr<HardwareDriver>>&
-get_drivers();
-
-}  // namespace akida
+#pragma once
+
+#include "infra/exports.h"
+#include "infra/hardware_driver.h"
+
+#include <memory>
+#include <vector>
+
+namespace akida {
+
+/**
+ * @brief Return a singleton vector containing all usable hardware drivers
+ */
+AKIDASHAREDLIB_EXPORT const std::vector<std::unique_ptr<HardwareDriver>>&
+get_drivers();
+
+}  // namespace akida
```

## akida/api/host/host_device.h

 * *Ordering differences only*

```diff
@@ -1,35 +1,35 @@
-#pragma once
-
-#include <memory>
-
-#include "akida/device.h"
-#include "akida/hardware_device.h"
-#include "akida/mesh.h"
-
-namespace akida {
-
-class HostDevice;
-
-using HostDevicePtr = std::shared_ptr<HostDevice>;
-using HostDeviceConstPtr = std::shared_ptr<const HostDevice>;
-
-class AKIDASHAREDLIB_EXPORT HostDevice : public Device {
- public:
-  explicit HostDevice(HardwareDevicePtr hardware) : hardware_(hardware) {
-    mesh_ = mesh::discover(hardware_.get());
-  }
-
-  HwVersion version() const override { return hardware_->version(); };
-
-  const char* desc() const override { return hardware_->desc(); }
-
-  const np::Mesh& mesh() const override { return *mesh_.get(); }
-
-  HardwareDevice* hardware() const override { return hardware_.get(); }
-
- private:
-  HardwareDevicePtr hardware_;
-  std::unique_ptr<np::Mesh> mesh_;
-};
-
-}  // namespace akida
+#pragma once
+
+#include <memory>
+
+#include "akida/device.h"
+#include "akida/hardware_device.h"
+#include "akida/mesh.h"
+
+namespace akida {
+
+class HostDevice;
+
+using HostDevicePtr = std::shared_ptr<HostDevice>;
+using HostDeviceConstPtr = std::shared_ptr<const HostDevice>;
+
+class AKIDASHAREDLIB_EXPORT HostDevice : public Device {
+ public:
+  explicit HostDevice(HardwareDevicePtr hardware) : hardware_(hardware) {
+    mesh_ = mesh::discover(hardware_.get());
+  }
+
+  HwVersion version() const override { return hardware_->version(); };
+
+  const char* desc() const override { return hardware_->desc(); }
+
+  const np::Mesh& mesh() const override { return *mesh_.get(); }
+
+  HardwareDevice* hardware() const override { return hardware_.get(); }
+
+ private:
+  HardwareDevicePtr hardware_;
+  std::unique_ptr<np::Mesh> mesh_;
+};
+
+}  // namespace akida
```

## akida/api/host/power_meter.h

 * *Ordering differences only*

```diff
@@ -1,48 +1,48 @@
-#pragma once
-
-#include <cstdint>
-#include <memory>
-#include <mutex>
-#include <set>
-#include <vector>
-
-#include "host/circular_queue.h"
-#include "host/soc_clock_mode.h"
-
-namespace akida {
-
-class PowerMeter {
- public:
-  // the period at which power is updated by INA controller (140.8 ms)
-  static constexpr int64_t ina_period_ms = 141;
-  struct PowerEvent {
-    explicit PowerEvent(int64_t t = 0, uint32_t v = 0, uint32_t i = 0)
-        : ts(t), voltage(v), current(i) {}
-    int64_t ts;        // Milliseconds
-    uint32_t voltage;  // microVolt (10e-6)
-    uint32_t current;  // milliAmpere (10e-3)
-  };
-  PowerMeter();
-  // Log a new event
-  void log_event(PowerEvent&& event);
-  // Get all current events
-  std::vector<PowerEvent> events();
-  // Check if there is events
-  bool has_events();
-  // Get the current floor power in milliWatt
-  float floor();
-  // Reset the floor power at the specified time
-  void reset(int64_t time);
-  // Get PowerMeter instance
-  static PowerMeter* get();
-
- private:
-  CircularQueue<PowerEvent> events_;
-  float floor_;
-  int64_t next_update_;
-  std::mutex events_mutex_;
-};
-
-using PowerMeterPtr = std::shared_ptr<PowerMeter>;
-
-}  // namespace akida
+#pragma once
+
+#include <cstdint>
+#include <memory>
+#include <mutex>
+#include <set>
+#include <vector>
+
+#include "host/circular_queue.h"
+#include "host/soc_clock_mode.h"
+
+namespace akida {
+
+class PowerMeter {
+ public:
+  // the period at which power is updated by INA controller (140.8 ms)
+  static constexpr int64_t ina_period_ms = 141;
+  struct PowerEvent {
+    explicit PowerEvent(int64_t t = 0, uint32_t v = 0, uint32_t i = 0)
+        : ts(t), voltage(v), current(i) {}
+    int64_t ts;        // Milliseconds
+    uint32_t voltage;  // microVolt (10e-6)
+    uint32_t current;  // milliAmpere (10e-3)
+  };
+  PowerMeter();
+  // Log a new event
+  void log_event(PowerEvent&& event);
+  // Get all current events
+  std::vector<PowerEvent> events();
+  // Check if there is events
+  bool has_events();
+  // Get the current floor power in milliWatt
+  float floor();
+  // Reset the floor power at the specified time
+  void reset(int64_t time);
+  // Get PowerMeter instance
+  static PowerMeter* get();
+
+ private:
+  CircularQueue<PowerEvent> events_;
+  float floor_;
+  int64_t next_update_;
+  std::mutex events_mutex_;
+};
+
+using PowerMeterPtr = std::shared_ptr<PowerMeter>;
+
+}  // namespace akida
```

## akida/api/host/soc_clock_mode.h

 * *Ordering differences only*

```diff
@@ -1,21 +1,21 @@
-#pragma once
-
-#include "infra/hardware_driver.h"
-
-namespace akida {
-namespace soc {
-
-// These clock modes correspond to frequency values for the Akida IP in the SoC
-enum class ClockMode {
-  Performance = 300000000,
-  Economy = 100000000,
-  LowPower = 5000000,
-};
-
-// Get clock mode of SoC currently connected
-ClockMode get_clock_mode(HardwareDriver* driver);
-// Set clock mode of SoC currently connected
-void set_clock_mode(HardwareDriver* driver, const ClockMode& clock_mode);
-
-}  // namespace soc
-}  // namespace akida
+#pragma once
+
+#include "infra/hardware_driver.h"
+
+namespace akida {
+namespace soc {
+
+// These clock modes correspond to frequency values for the Akida IP in the SoC
+enum class ClockMode {
+  Performance = 300000000,
+  Economy = 100000000,
+  LowPower = 5000000,
+};
+
+// Get clock mode of SoC currently connected
+ClockMode get_clock_mode(HardwareDriver* driver);
+// Set clock mode of SoC currently connected
+void set_clock_mode(HardwareDriver* driver, const ClockMode& clock_mode);
+
+}  // namespace soc
+}  // namespace akida
```

## akida/api/host/soc_driver.h

 * *Ordering differences only*

```diff
@@ -1,22 +1,22 @@
-#pragma once
-
-#include "host/power_meter.h"
-#include "host/soc_clock_mode.h"
-
-namespace akida {
-
-// Interface class to define an API specific to a SoC driver (as opposed to an
-// FPGA one)
-class SocDriver {
- public:
-  virtual ~SocDriver() = default;
-  // Get/Set clock mode
-  virtual soc::ClockMode get_clock_mode() = 0;
-  virtual void set_clock_mode(const soc::ClockMode& clock_mode) = 0;
-  // Toggle power measurement on or off
-  virtual void toggle_power_measurement(bool enable) = 0;
-  // Get power meter
-  virtual PowerMeterPtr power_meter() = 0;
-};
-
-}  // namespace akida
+#pragma once
+
+#include "host/power_meter.h"
+#include "host/soc_clock_mode.h"
+
+namespace akida {
+
+// Interface class to define an API specific to a SoC driver (as opposed to an
+// FPGA one)
+class SocDriver {
+ public:
+  virtual ~SocDriver() = default;
+  // Get/Set clock mode
+  virtual soc::ClockMode get_clock_mode() = 0;
+  virtual void set_clock_mode(const soc::ClockMode& clock_mode) = 0;
+  // Toggle power measurement on or off
+  virtual void toggle_power_measurement(bool enable) = 0;
+  // Get power meter
+  virtual PowerMeterPtr power_meter() = 0;
+};
+
+}  // namespace akida
```

## akida/api/infra/exports.h

 * *Ordering differences only*

```diff
@@ -1,9 +1,9 @@
-#pragma once
-
-#ifdef _WIN32
-#define AKIDASHAREDLIB_EXPORT __declspec(dllexport)
-#elif (__GNUC__ || __clang__)
-#define AKIDASHAREDLIB_EXPORT __attribute__((visibility("default")))
-#else
-#define AKIDASHAREDLIB_EXPORT
-#endif
+#pragma once
+
+#ifdef _WIN32
+#define AKIDASHAREDLIB_EXPORT __declspec(dllexport)
+#elif (__GNUC__ || __clang__)
+#define AKIDASHAREDLIB_EXPORT __attribute__((visibility("default")))
+#else
+#define AKIDASHAREDLIB_EXPORT
+#endif
```

## akida/api/infra/hardware_driver.h

 * *Ordering differences only*

```diff
@@ -1,83 +1,83 @@
-#pragma once
-
-#include <cstddef>
-#include <cstdint>
-#include <vector>
-
-#include "infra/exports.h"
-
-namespace akida {
-
-class BlockDevice {
- public:
-  virtual ~BlockDevice() = default;
-  /**
-   * @brief read operation.
-   * @param address: address where data should be read
-   * @param data: pointer data that will store the result
-   * @param size: size data to be read
-   */
-  virtual void read(uint32_t address, void* data, size_t size) const = 0;
-
-  /**
-   * @brief read operation.
-   * @param address: address where data should be read
-   */
-  uint32_t read32(uint32_t address) const {
-    uint32_t ret;
-    read(address, &ret, sizeof(uint32_t));
-    return ret;
-  }
-
-  /**
-   * @brief write operation
-   * @param address: address where data should be written
-   * @param data: pointer data to be written
-   * @param size: data size in number of 32 bit words
-   */
-  virtual void write(uint32_t address, const void* data, size_t size) = 0;
-
-  /**
-   * @brief write operation
-   * @param address: address where data should be written
-   * @param data: uint32_t data value to be written
-   */
-  void write32(uint32_t address, const uint32_t data) {
-    write(address, &data, sizeof(uint32_t));
-  }
-};
-
-class HardwareDriver : public BlockDevice {
- public:
-  /**
-   * @brief Return a null terminated string with driver description.
-   */
-  virtual const char* desc() const = 0;
-
-  /**
-   * @brief Return address used for scratch memory.
-   */
-  virtual uint32_t scratch_memory() const = 0;
-
-  /**
-   * @brief Return size (in bytes) available as scratch memory.
-   */
-  virtual uint32_t scratch_size() const = 0;
-
-  /**
-   * @brief Return address used for top level registers.
-   */
-  virtual uint32_t top_level_reg() const = 0;
-
-  /**
-   * @brief return the address of data that are directly accessible by akida
-   */
-  virtual uint32_t akida_visible_memory() const = 0;
-  /**
-   * @brief return the size (in bytes) of data that are directly accessible by
-   * akida
-   */
-  virtual uint32_t akida_visible_memory_size() const = 0;
-};
-
-}  // namespace akida
+#pragma once
+
+#include <cstddef>
+#include <cstdint>
+#include <vector>
+
+#include "infra/exports.h"
+
+namespace akida {
+
+class BlockDevice {
+ public:
+  virtual ~BlockDevice() = default;
+  /**
+   * @brief read operation.
+   * @param address: address where data should be read
+   * @param data: pointer data that will store the result
+   * @param size: size data to be read
+   */
+  virtual void read(uint32_t address, void* data, size_t size) const = 0;
+
+  /**
+   * @brief read operation.
+   * @param address: address where data should be read
+   */
+  uint32_t read32(uint32_t address) const {
+    uint32_t ret;
+    read(address, &ret, sizeof(uint32_t));
+    return ret;
+  }
+
+  /**
+   * @brief write operation
+   * @param address: address where data should be written
+   * @param data: pointer data to be written
+   * @param size: data size in number of 32 bit words
+   */
+  virtual void write(uint32_t address, const void* data, size_t size) = 0;
+
+  /**
+   * @brief write operation
+   * @param address: address where data should be written
+   * @param data: uint32_t data value to be written
+   */
+  void write32(uint32_t address, const uint32_t data) {
+    write(address, &data, sizeof(uint32_t));
+  }
+};
+
+class HardwareDriver : public BlockDevice {
+ public:
+  /**
+   * @brief Return a null terminated string with driver description.
+   */
+  virtual const char* desc() const = 0;
+
+  /**
+   * @brief Return address used for scratch memory.
+   */
+  virtual uint32_t scratch_memory() const = 0;
+
+  /**
+   * @brief Return size (in bytes) available as scratch memory.
+   */
+  virtual uint32_t scratch_size() const = 0;
+
+  /**
+   * @brief Return address used for top level registers.
+   */
+  virtual uint32_t top_level_reg() const = 0;
+
+  /**
+   * @brief return the address of data that are directly accessible by akida
+   */
+  virtual uint32_t akida_visible_memory() const = 0;
+  /**
+   * @brief return the size (in bytes) of data that are directly accessible by
+   * akida
+   */
+  virtual uint32_t akida_visible_memory_size() const = 0;
+};
+
+}  // namespace akida
```

## akida/api/infra/int_ops.h

 * *Ordering differences only*

```diff
@@ -1,26 +1,26 @@
-#pragma once
-
-#include <cstdint>
-
-namespace akida {
-
-/**
- * @brief Perform a division and a ceiling on the result.
- * @param n numerator
- * @param d denominator
- */
-inline constexpr uint32_t div_round_up(uint32_t n, uint32_t d) {
-  return static_cast<uint32_t>((n + d - 1) / d);
-}
-
-/**
- * @brief Increases an integer value until evenly divisible by a given alignment
- * value.
- * @param v input value
- * @param alignment alignment value
- */
-inline constexpr uint32_t align_up(uint32_t v, uint32_t alignment) {
-  return div_round_up(v, alignment) * alignment;
-}
-
-}  // namespace akida
+#pragma once
+
+#include <cstdint>
+
+namespace akida {
+
+/**
+ * @brief Perform a division and a ceiling on the result.
+ * @param n numerator
+ * @param d denominator
+ */
+inline constexpr uint32_t div_round_up(uint32_t n, uint32_t d) {
+  return static_cast<uint32_t>((n + d - 1) / d);
+}
+
+/**
+ * @brief Increases an integer value until evenly divisible by a given alignment
+ * value.
+ * @param v input value
+ * @param alignment alignment value
+ */
+inline constexpr uint32_t align_up(uint32_t v, uint32_t alignment) {
+  return div_round_up(v, alignment) * alignment;
+}
+
+}  // namespace akida
```

## akida/api/infra/registers_common.h

 * *Ordering differences only*

```diff
@@ -1,38 +1,38 @@
-#pragma once
-
-#include <cassert>
-#include <cstdint>
-
-namespace akida {
-
-struct RegDetail {
-  uint32_t offset;
-  uint32_t nb_bits;
-
-  explicit constexpr RegDetail(uint32_t first, uint32_t last)
-      : offset(first), nb_bits(last - first + 1) {}
-
-  explicit constexpr RegDetail(uint32_t first) : offset(first), nb_bits(1) {}
-};
-
-// Util function to set a range of bit to a value
-inline void set_field(uint32_t* bits, const RegDetail& field, uint32_t value) {
-  uint32_t max_value = (1 << field.nb_bits) - 1;
-  assert(value <= max_value);
-  // Mask value to avoid writing outside the field
-  value &= max_value;
-  // first clear bits
-  *bits &= ~(max_value << field.offset);
-  // Then set bits to value
-  *bits |= value << field.offset;
-}
-
-inline uint32_t get_field(const uint32_t& bits, const RegDetail& field) {
-  // create a mask
-  uint32_t max_value = (1 << field.nb_bits) - 1;
-  // shift and mask the value
-  uint32_t ret = (bits >> field.offset) & max_value;
-  return ret;
-}
-
-}  // namespace akida
+#pragma once
+
+#include <cassert>
+#include <cstdint>
+
+namespace akida {
+
+struct RegDetail {
+  uint32_t offset;
+  uint32_t nb_bits;
+
+  explicit constexpr RegDetail(uint32_t first, uint32_t last)
+      : offset(first), nb_bits(last - first + 1) {}
+
+  explicit constexpr RegDetail(uint32_t first) : offset(first), nb_bits(1) {}
+};
+
+// Util function to set a range of bit to a value
+inline void set_field(uint32_t* bits, const RegDetail& field, uint32_t value) {
+  uint32_t max_value = (1 << field.nb_bits) - 1;
+  assert(value <= max_value);
+  // Mask value to avoid writing outside the field
+  value &= max_value;
+  // first clear bits
+  *bits &= ~(max_value << field.offset);
+  // Then set bits to value
+  *bits |= value << field.offset;
+}
+
+inline uint32_t get_field(const uint32_t& bits, const RegDetail& field) {
+  // create a mask
+  uint32_t max_value = (1 << field.nb_bits) - 1;
+  // shift and mask the value
+  uint32_t ret = (bits >> field.offset) & max_value;
+  return ret;
+}
+
+}  // namespace akida
```

## akida/api/infra/system.h

 * *Ordering differences only*

```diff
@@ -1,35 +1,35 @@
-#pragma once
-
-#if defined(__cplusplus)
-extern "C" { /* C-declarations in C++ programs */
-#endif
-
-#include <cstdint>
-
-#include "infra/exports.h"
-
-/**
- * @brief wait for a given duration
- * @param duration: how long should be waited, in milliseconds
- */
-AKIDASHAREDLIB_EXPORT void msleep(uint32_t duration);
-
-/**
- * @brief return monotone time in milliseconds
- */
-AKIDASHAREDLIB_EXPORT int64_t time_ms();
-
-/**
- * @brief signal watchdog while the library is busy during long processing.
- */
-AKIDASHAREDLIB_EXPORT void kick_watchdog();
-
-/**
- * @brief generate an unrecoverable error (exception), whose description is
- * formatted as a string.
- */
-AKIDASHAREDLIB_EXPORT void panic [[noreturn]] (const char* format, ...);
-
-#if defined(__cplusplus)
-} /* C-declarations in C++ programs */
-#endif
+#pragma once
+
+#if defined(__cplusplus)
+extern "C" { /* C-declarations in C++ programs */
+#endif
+
+#include <cstdint>
+
+#include "infra/exports.h"
+
+/**
+ * @brief wait for a given duration
+ * @param duration: how long should be waited, in milliseconds
+ */
+AKIDASHAREDLIB_EXPORT void msleep(uint32_t duration);
+
+/**
+ * @brief return monotone time in milliseconds
+ */
+AKIDASHAREDLIB_EXPORT int64_t time_ms();
+
+/**
+ * @brief signal watchdog while the library is busy during long processing.
+ */
+AKIDASHAREDLIB_EXPORT void kick_watchdog();
+
+/**
+ * @brief generate an unrecoverable error (exception), whose description is
+ * formatted as a string.
+ */
+AKIDASHAREDLIB_EXPORT void panic [[noreturn]] (const char* format, ...);
+
+#if defined(__cplusplus)
+} /* C-declarations in C++ programs */
+#endif
```

## akida/compatibility/__init__.py

 * *Ordering differences only*

```diff
@@ -1 +1 @@
-from .conversion import *
+from .conversion import *
```

## akida/compatibility/conversion.py

 * *Ordering differences only*

```diff
@@ -1,162 +1,162 @@
-import copy
-import numpy as np
-import akida
-
-
-def _get_weights_params_identity(layer):
-    """
-    Creates an 'identity' convolutional layer parameters and its weights.
-    """
-    out_dims = layer.output_dims
-    nb_chan = out_dims[2]
-    dw_weights = np.zeros((3, 3, nb_chan, 1), dtype=np.int8)
-    pw_weights = np.zeros((1, 1, nb_chan, nb_chan), dtype=np.int8)
-    act_step_val = 2**layer.parameters.act_bits / 16
-    act_step = np.full((nb_chan), act_step_val, dtype=np.float32)
-    for i in range(nb_chan):
-        dw_weights[1, 1, i, 0] = 1
-        pw_weights[0, 0, i, i] = 1
-
-    # create a layer to have default parameters
-    identity_layer = akida.SeparableConvolutional(
-        name=f"{layer.name}_pooling",
-        kernel_size=(3, 3),
-        filters=nb_chan,
-        act_bits=layer.parameters.act_bits)
-    return copy.copy(
-        identity_layer.parameters), dw_weights, pw_weights, act_step
-
-
-def _copy_layer_variables(layer, copied_layer):
-    for var in copied_layer.get_variable_names():
-        layer.set_variable(var, copied_layer.get_variable(var))
-
-
-def _copy_layer(model, layer):
-    new_layer = akida.Layer(layer.parameters, layer.name)
-    model.add(new_layer)
-    if model.learning:
-        # Recompile model with layer parameters
-        learn_params = {
-            attr: getattr(model.learning, attr)
-            for attr in dir(model.learning)
-            if '__' not in attr
-        }
-        model.compile(**learn_params)
-    _copy_layer_variables(new_layer, layer)
-
-
-def _add_identity_cnp_after_max_pooling(model, layer):
-    """
-    Adds the layer and an identity CNP to the model
-    """
-    ident_params, ident_dw_weights, ident_pw_weights, act_step = \
-        _get_weights_params_identity(layer)
-    identity_layer = akida.Layer(ident_params, f"{layer.name}_identity")
-    model.add(identity_layer)
-    identity_layer.set_variable("weights", ident_dw_weights)
-    identity_layer.set_variable("weights_pw", ident_pw_weights)
-    identity_layer.set_variable("act_step", act_step)
-
-
-def _cnp_max_pooling(layer):
-    return layer.parameters.layer_type in [
-        akida.LayerType.Convolutional, akida.LayerType.SeparableConvolutional
-    ] and akida.PoolType(layer.parameters.pool_type) == akida.PoolType.Max
-
-
-def _cnp_pooling_needs_identity_cnp(model, layer_index):
-    """
-    Returns True if the layer is CNP with max pooling not followed by another
-    CNP, and we can add an identity CNP layer after it without altering result
-    """
-    result = False
-    layer = model.get_layer(layer_index)
-    if _cnp_max_pooling(layer):
-        # if it is not the last layer, check the layer is not followed by
-        # another cnp
-        if layer_index != model.get_layer_count() - 1:
-            next_layer = model.get_layer(layer_index + 1)
-            if next_layer.parameters.layer_type not in [
-                    akida.LayerType.Convolutional,
-                    akida.LayerType.SeparableConvolutional
-            ]:
-                result = True
-        # if it is the last layer, we can add an identity layer only if it has
-        # activations enabled
-        elif layer.parameters.activation:
-            result = True
-    return result
-
-
-def create_from_model(model):
-    """Tries to create a HW compatible model from an incompatible one
-
-    Tries to create a HW compatible model from an incompatible one, using SW
-    workarounds for known limitations. It returns a converted model that is not
-    guaranteed to be HW compatible, depending if workaround have been found.
-
-    Args:
-        model (:obj:`Model`): a Model object to convert
-
-    Returns:
-        :obj:`Model`: a new Model with no guarantee that it is HW compatible.
-    """
-    new_model = akida.Model()
-    nb_layers = model.get_layer_count()
-    for i in range(nb_layers):
-        layer = model.get_layer(i)
-        if _cnp_max_pooling(layer):
-            # On hardware, any CNP with max pooling must be followed by a CNP
-            # (to perform vertical pooling). If not, an identity CNP layer is
-            # then added.
-            _copy_layer(new_model, layer)
-            # If CNP has max pooling and is not followed by another CNP, we can
-            # add an identity CNP layer
-            if _cnp_pooling_needs_identity_cnp(model, i):
-                _add_identity_cnp_after_max_pooling(new_model, layer)
-            continue
-
-        # if no particular case is found, copy the layer into the new model
-        _copy_layer(new_model, layer)
-
-    return new_model
-
-
-def transpose(model):
-    """Transpose the weights of a legacy (pre-2.1.4) model
-
-    This only applies to:
-
-    - models converted using cnn2snn,
-    - models instantiated using the Sequential API starting with an
-      InputConvolutional.
-
-    Models instantiated using the Sequential API starting with an InputData
-    don't need to have their weights transposed.
-
-    Args:
-        model (:obj:`Model`): a Model object whose weights need transposing
-
-    Returns:
-        :obj:`Model`: a new Model with transposed weights
-    """
-    # Clone the model
-    t_model = akida.Model(layers=model.layers)
-    for layer in t_model.layers:
-        if layer.parameters.layer_type == akida.LayerType.InputData:
-            continue
-        if layer.parameters.layer_type == akida.LayerType.FullyConnected:
-            # Inflate, transpose and flatten weights
-            n = layer.parameters.units
-            x, y, c = layer.input_dims
-            w = layer.variables["weights"]
-            w = w.reshape((c, y, x, n)) \
-                .transpose((0, 2, 1, 3)) \
-                .reshape((1, 1, x * y * c, n))
-            layer.variables["weights"] = w
-        else:
-            # Transpose weights
-            w = layer.variables["weights"]
-            layer.variables["weights"] = np.transpose(w, axes=(1, 0, 2, 3))
-    return t_model
+import copy
+import numpy as np
+import akida
+
+
+def _get_weights_params_identity(layer):
+    """
+    Creates an 'identity' convolutional layer parameters and its weights.
+    """
+    out_dims = layer.output_dims
+    nb_chan = out_dims[2]
+    dw_weights = np.zeros((3, 3, nb_chan, 1), dtype=np.int8)
+    pw_weights = np.zeros((1, 1, nb_chan, nb_chan), dtype=np.int8)
+    act_step_val = 2**layer.parameters.act_bits / 16
+    act_step = np.full((nb_chan), act_step_val, dtype=np.float32)
+    for i in range(nb_chan):
+        dw_weights[1, 1, i, 0] = 1
+        pw_weights[0, 0, i, i] = 1
+
+    # create a layer to have default parameters
+    identity_layer = akida.SeparableConvolutional(
+        name=f"{layer.name}_pooling",
+        kernel_size=(3, 3),
+        filters=nb_chan,
+        act_bits=layer.parameters.act_bits)
+    return copy.copy(
+        identity_layer.parameters), dw_weights, pw_weights, act_step
+
+
+def _copy_layer_variables(layer, copied_layer):
+    for var in copied_layer.get_variable_names():
+        layer.set_variable(var, copied_layer.get_variable(var))
+
+
+def _copy_layer(model, layer):
+    new_layer = akida.Layer(layer.parameters, layer.name)
+    model.add(new_layer)
+    if model.learning:
+        # Recompile model with layer parameters
+        learn_params = {
+            attr: getattr(model.learning, attr)
+            for attr in dir(model.learning)
+            if '__' not in attr
+        }
+        model.compile(**learn_params)
+    _copy_layer_variables(new_layer, layer)
+
+
+def _add_identity_cnp_after_max_pooling(model, layer):
+    """
+    Adds the layer and an identity CNP to the model
+    """
+    ident_params, ident_dw_weights, ident_pw_weights, act_step = \
+        _get_weights_params_identity(layer)
+    identity_layer = akida.Layer(ident_params, f"{layer.name}_identity")
+    model.add(identity_layer)
+    identity_layer.set_variable("weights", ident_dw_weights)
+    identity_layer.set_variable("weights_pw", ident_pw_weights)
+    identity_layer.set_variable("act_step", act_step)
+
+
+def _cnp_max_pooling(layer):
+    return layer.parameters.layer_type in [
+        akida.LayerType.Convolutional, akida.LayerType.SeparableConvolutional
+    ] and akida.PoolType(layer.parameters.pool_type) == akida.PoolType.Max
+
+
+def _cnp_pooling_needs_identity_cnp(model, layer_index):
+    """
+    Returns True if the layer is CNP with max pooling not followed by another
+    CNP, and we can add an identity CNP layer after it without altering result
+    """
+    result = False
+    layer = model.get_layer(layer_index)
+    if _cnp_max_pooling(layer):
+        # if it is not the last layer, check the layer is not followed by
+        # another cnp
+        if layer_index != model.get_layer_count() - 1:
+            next_layer = model.get_layer(layer_index + 1)
+            if next_layer.parameters.layer_type not in [
+                    akida.LayerType.Convolutional,
+                    akida.LayerType.SeparableConvolutional
+            ]:
+                result = True
+        # if it is the last layer, we can add an identity layer only if it has
+        # activations enabled
+        elif layer.parameters.activation:
+            result = True
+    return result
+
+
+def create_from_model(model):
+    """Tries to create a HW compatible model from an incompatible one
+
+    Tries to create a HW compatible model from an incompatible one, using SW
+    workarounds for known limitations. It returns a converted model that is not
+    guaranteed to be HW compatible, depending if workaround have been found.
+
+    Args:
+        model (:obj:`Model`): a Model object to convert
+
+    Returns:
+        :obj:`Model`: a new Model with no guarantee that it is HW compatible.
+    """
+    new_model = akida.Model()
+    nb_layers = model.get_layer_count()
+    for i in range(nb_layers):
+        layer = model.get_layer(i)
+        if _cnp_max_pooling(layer):
+            # On hardware, any CNP with max pooling must be followed by a CNP
+            # (to perform vertical pooling). If not, an identity CNP layer is
+            # then added.
+            _copy_layer(new_model, layer)
+            # If CNP has max pooling and is not followed by another CNP, we can
+            # add an identity CNP layer
+            if _cnp_pooling_needs_identity_cnp(model, i):
+                _add_identity_cnp_after_max_pooling(new_model, layer)
+            continue
+
+        # if no particular case is found, copy the layer into the new model
+        _copy_layer(new_model, layer)
+
+    return new_model
+
+
+def transpose(model):
+    """Transpose the weights of a legacy (pre-2.1.4) model
+
+    This only applies to:
+
+    - models converted using cnn2snn,
+    - models instantiated using the Sequential API starting with an
+      InputConvolutional.
+
+    Models instantiated using the Sequential API starting with an InputData
+    don't need to have their weights transposed.
+
+    Args:
+        model (:obj:`Model`): a Model object whose weights need transposing
+
+    Returns:
+        :obj:`Model`: a new Model with transposed weights
+    """
+    # Clone the model
+    t_model = akida.Model(layers=model.layers)
+    for layer in t_model.layers:
+        if layer.parameters.layer_type == akida.LayerType.InputData:
+            continue
+        if layer.parameters.layer_type == akida.LayerType.FullyConnected:
+            # Inflate, transpose and flatten weights
+            n = layer.parameters.units
+            x, y, c = layer.input_dims
+            w = layer.variables["weights"]
+            w = w.reshape((c, y, x, n)) \
+                .transpose((0, 2, 1, 3)) \
+                .reshape((1, 1, x * y * c, n))
+            layer.variables["weights"] = w
+        else:
+            # Transpose weights
+            w = layer.variables["weights"]
+            layer.variables["weights"] = np.transpose(w, axes=(1, 0, 2, 3))
+    return t_model
```

## akida/deploy/__init__.py

 * *Ordering differences only*

```diff
@@ -1,16 +1,16 @@
-# ******************************************************************************
-# Copyright 2020 Brainchip Holdings Ltd.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ******************************************************************************
-from .engine import deploy_engine
+# ******************************************************************************
+# Copyright 2020 Brainchip Holdings Ltd.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ******************************************************************************
+from .engine import deploy_engine
```

## akida/deploy/engine.py

 * *Ordering differences only*

```diff
@@ -1,40 +1,40 @@
-# ******************************************************************************
-# Copyright 2020 Brainchip Holdings Ltd.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ******************************************************************************
-import os
-from pathlib import Path
-import shutil
-
-
-def copytree_force(src, dest):
-    # Note: we can't use shutil.copytree because the dirs_exist_ok parameter that forces the copy
-    # is not available before Python 3.8
-    if os.path.isdir(src):
-        if not os.path.isdir(dest):
-            os.makedirs(dest)
-        files = os.listdir(src)
-        for f in files:
-            copytree_force(os.path.join(src, f),
-                           os.path.join(dest, f))
-    else:
-        shutil.copyfile(src, dest)
-
-
-def deploy_engine(dest_path):
-    # Note: we cannot use importlib.resources because it has incompatible behaviours
-    # when dealing with directories between python 3.7/3.8 and 3.9/3.10
-    engine_path = os.path.join(Path(__file__).parent, "../engine")
-    dest_path = os.path.join(dest_path, "engine")
-    copytree_force(engine_path, dest_path)
+# ******************************************************************************
+# Copyright 2020 Brainchip Holdings Ltd.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ******************************************************************************
+import os
+from pathlib import Path
+import shutil
+
+
+def copytree_force(src, dest):
+    # Note: we can't use shutil.copytree because the dirs_exist_ok parameter that forces the copy
+    # is not available before Python 3.8
+    if os.path.isdir(src):
+        if not os.path.isdir(dest):
+            os.makedirs(dest)
+        files = os.listdir(src)
+        for f in files:
+            copytree_force(os.path.join(src, f),
+                           os.path.join(dest, f))
+    else:
+        shutil.copyfile(src, dest)
+
+
+def deploy_engine(dest_path):
+    # Note: we cannot use importlib.resources because it has incompatible behaviours
+    # when dealing with directories between python 3.7/3.8 and 3.9/3.10
+    engine_path = os.path.join(Path(__file__).parent, "../engine")
+    dest_path = os.path.join(dest_path, "engine")
+    copytree_force(engine_path, dest_path)
```

## akida/engine/CMakeLists.txt

 * *Ordering differences only*

```diff
@@ -1,7 +1,7 @@
-cmake_minimum_required(VERSION 3.20)
-
-set(CMAKE_MODULE_PATH ${CMAKE_CURRENT_LIST_DIR}/cmake)
-
-# build engine
-include(akida-engine)
-
+cmake_minimum_required(VERSION 3.20)
+
+set(CMAKE_MODULE_PATH ${CMAKE_CURRENT_LIST_DIR}/cmake)
+
+# build engine
+include(akida-engine)
+
```

## akida/engine/README.md

 * *Ordering differences only*

```diff
@@ -1,378 +1,378 @@
-# Akida Engine library
-
-The **Akida Engine** library is a C++ library allowing to perform an inference
-on an Akida model loaded into an Akida compatible device.
-
-As a prerequisite, the Akida model program has to be generated on a host using
-the [Akida python package](https://pypi.org/project/Akida/).
-
-## Building the library
-
-### Build system
-
-The Akida Engine library does not have any specific build system requirements.
-
-### Toolchain/Compiler
-
-The Akida Engine library requires a 32 or 64 bit little endian toolchain with
-support for floating point numbers.
-
-The toolchain must include a C++ compiler supporting at least C++ 11.
-
-The toolchain must provide a minimal implementation of the C standard library,
-like for instance GCC newlib and the corresponding target-specific stubs.
-
-The Akida Engine library does not use C++ exceptions, so exception support can
-be disabled when generating binaries for embedded targets.
-
-### Dependencies
-
-The library relies on external symbols whose target-specific implementation must
-be provided by the build system.
-
-#### Google flatbuffers
-
-The library has a dependency towards the Google flatbuffer header-only library:
-   https://github.com/google/flatbuffers/releases/tag/v2.0.6
-
-The sources must be downloaded from the link above and made available to the
-library as an additional include path.
-
-#### Standard Template Library (STL)
-
-The Akida Engine library relies on a few C++ STL classes whose implementation
-must be provided by the build system.
-
-The following headers must be supported:
-
-~~~~cpp
-<algorithm>
-<array>
-<functional>
-<memory>
-<queue>
-<set>
-<tuple>
-<typeindex>
-<vector>
-~~~~
-
-#### System infra
-
-The Akida Engine library requires a few system methods to be implemented.
-
-Please refer to api/infra/system.h for a list and description of the methods to
-implement.
-
-### Building a static library
-
-A basic cmake build file is provided for convenience:
-
-~~~~
-mkdir build
-cmake . -B build
-make -C build
-~~~~
-
-### Using the library
-
-#### Hardware driver
-
-The Akida Engine library primary entry point is the HardwareDevice class.
-
-One can obtain an HardwareDevice instance by passing a target-specific
-HardwareDriver instance to HardwareDevice::create().
-
-~~~~cpp
-#include "akida/hardware_device.h"
-#include "infra/hardware_driver.h"
-
-class MyDriver : public HardwareDriver {
-...
-}
-
-...
-
-auto driver = MyDriver();
-auto device = HardwareDevice::create(driver);
-~~~~
-
-The HardwareDriver main purpose is to abstract the read and write operations
-into the Akida DMA.
-It also provides:
-- the base address for Akida blocks registers
-- the 'scratch' memory region where the library can allocate buffers to
-communicate with the Akida DMA (runtime configuration, inputs/outputs).
-- the 'visible from akida' memory region, defining the memory region directly
-accessible by Akida DMAs.
-Data put in this area will not be copied to scratch memory.
-
-Please refer to api/infra/hardware_driver.h for a complete description of the
-HardwareDriver API and how to implement your specific driver.
-
-##### BareMetalDriver class
-
-BareMetalDriver, a custom HardwareDriver implementation, is provided as an
-example.
-Its constructor takes 4 arguments that you can use to customize your
-application:
-- scratch_base_address: the base 'scratch' memory address.
-- scratch_size: the size of 'scratch' memory that can be used. In case of
- overflow, application will panic at runtime.
-- akida_visible_memory_base: the base address where Akida DMAs can directly
-acccess data. This can be the location where program and inputs are put,
-so they don't need to be copied to 'scratch' memory
-- akida_visible_memory_size: the size of the 'visible' memory region.
-
-#### Generate model programs
-
-The akida engine library allows to program an akida device with machine
-learning models previously trained and compiled.
-
-The akida and cnn2snn python packages are required to generate model programs
-from pre-trained keras models.
-
-The akida_models python package contains helpers to fetch models from the akida
-model zoo.
-
-~~~~python
-#!/usr/bin/env python
-import os
-from akida import AKD1000
-from akida.generate array_to_cpp
-from cnn2snn import convert
-from akida_models import ds_cnn_kws_pretrained
-
-# Load Keras pre-trained model from Akida model zoo
-model_keras = ds_cnn_kws_pretrained()
-
-# Convert Keras model to Akida
-model_akida = convert(model_keras)
-
-# Map/compile converted model for an AKD1000 device
-model_akida.map(device=AKD1000(), hw_only=True)
-
-# Check model mapping: NP allocation and binary size
-model_akida.summary()
-
-# Retrieve model program binary
-program = model_akida.sequences[0].program
-
-# Generate a binary that can be flashed
-with open('kws_model.bin', 'wb') as file:
-    file.write(program)
-    file.close()
-
-# Or generate source files to be included -> kws_model.{cpp,h}
-    array_to_cpp('., program, 'kws_model')
-~~~~
-
-#### Load model programs
-
-Once loaded in memory, the raw bytes buffer corresponding to a model program
-can be passed to the HardwareDevice to program the model on the device.
-
-~~~~cpp
-#include "kws_model.h"
-
-// Load program
-device->program(kws_model, kws_model_size);
-~~~~
-
-#### Set batch size
-
-After programming, one must configure the batch size to the number of inputs
-that can be enqueued before fetching for outputs. A higher batch size has
-higher memory requirements, but can allow for inputs pipelining. The batch size
-must be set by calling `set_batch_size` function.
-This function takes a `requested_batch_size` parameter, which is the batch size
-one wants to set, and a boolean `allocate_inputs` to allocate space for inputs
-in scratch memory.
-`allocate_inputs` must be set to `true` if inputs are not in the range
-specified by `akida_visible_memory_base` and `akida_visible_memory_size`.
-Outputs are always allocated in scratch memory.
-
-Note: the maximum effective batch size is 15 for a single pass program, or 1 if
-the program is multi pass, or learning mode have been enabled. Passing a higher
-value is allowed, but the effective batch size that has been set is returned by
-`set_batch_size` method.
-
-~~~~cpp
-// Assume device has been programmed
-
-// set a batch size of 2, and do not allocate space for inputs (they must be in
-// the range specified by driver->akida_visible_memory_base(),
-// driver->akida_visible_memory_size())
-device->set_batch_size(2, false);
-~~~~
-
-#### Perform an inference
-
-Once a model has been programmed and a batch size is set, inputs Tensor can be
-enqueued into the device inference pipeline, by calling `enqueue` function. The
-function immediately returns a boolean set to true if the tensor was
-successfully enqueued, or false if the pipeline was full.
-Depending on your program and device, you may have to convert your inputs to a
-sparse format.
-
-Two types of tensors can be passed as input and returned as outputs:
-- Dense tensors are standard contiguous buffers whose items are ordered either
-using the row-major or col-major convention,
-- Sparse tensors are list of coordinates and values. These cannot be directly
-constructed, they are output from Akida, or converted from a Dense.
-
-Several static constructors are available to create tensors, depending on your
-use case: pre-allocate, copy, etc.
-Conversion functions also exist.
-
-~~~~cpp
-#include "akida/dense.h"
-
-// Enqueue a single input from a raw input dense buffer
-auto input = Dense::create_view(input_buf, TensorType::uint8, {49, 10, 1},
-                                TensorLayout::RowMajor);
-bool success = device->enqueue(*input);
-~~~~
-
-Then you can periodically check for outputs, by calling `fetch` function.
-The function returns a pointer to an output Tensor, or nullptr if no output
-was available.
-
-~~~~cpp
-auto output = device->fetch();
-if (output != nullptr) {
-    // do something with output
-}
-~~~~
-
-If your program last layer ends with an activation, the outputs are normalized
-n-bit values.
-Otherwise, if your program does not end with an activation, you must dequantize
-the output potentials to float values to project them in the same scale before
-processing them. You can use the `dequantize` method to do so.
-
-#### Perform edge learning
-
-To perform edge learning, the generated model should have been compiled
-beforehand.
-
-Example:
-~~~~python
-model.compile(
-    optimizer=akida.AkidaUnsupervised(
-        num_weights=num_weights,
-        num_classes=num_classes,
-        learning_competition=learning_competition
-        )
-    )
-~~~~
-
-See https://doc.brainchipinc.com/examples/index.html#edge-examples
-for more informations about learning and learning parameters.
-
-Then, to activate edge learning in your application, you must call
-`toggle_learn` method with the `learn_en` parameter set to `true`.
-You can then pass a label corresponding to your input when calling `enqueue`.
-Learning can be disabled by calling `toggle_learn` again with the `learn_en`
-parameter set to `false`.
-~~~~cpp
-#include "akida/dense.h"
-
-#include "kws_model.h"
-
-// Program device (this is not required if you already did it previously)
-device->program(kws_model, kws_model_size);
-
-// Turn learning mode on because learning is disabled by default
-device->toggle_learn(true);
-
-// Set batch size to 1 because learning is on
-device->set_batch_size(1, false);
-
-// Enqueue a single input from a raw input dense buffer
-auto input = Dense::create_view(input_buf, TensorType::uint8, {49, 10, 1},
-                                TensorLayout::RowMajor);
-// passing a single label
-bool success = device->enqueue(input, 1);
-~~~~
-
-##### Updating weights
-
-Weights used for learn are initially stored in the program buffer, then used to
-program the FNP (FullyConnected Neural Processor) that will perform the learn.
-In order to retrieve updated weights when learn is used, there are two possible
-situations, depending on the FNP type used:
-
-- FNP3 type: Akida has weights stored in its own SRAM, so a copy back from
-internal SRAM to application memory must be done. To do that, a buffer must be
-allocated with the correct size. `HardwareDevice::learn_mem_size()` method can
-be called to know this size.
-The output of this function is the number of 32 bits words required.
-Then you can call `HardwareDevice::learn_mem(output_buffer)` that will copy
-weights to the `output_buffer` parameter.
-
-- FNP2 type: Akida reads directly weights from memory,
-so that memory will be updated automatically after each fit call.
-Note that if weights were not in 'akida visible memory', they have been copied
-to 'scratch' memory, so only scratch memory have been updated. To update your
-initial program, you will have to proceed the same way as FNP3.
-
-~~~~cpp
-std::vector<uint32_t> updated_weights(device->learn_mem_size());
-device->learn_mem(updated_weights.data());
-~~~~
-
-The application can then store this buffer.
-This can be reprogrammed by calling
-`HardwareDevice::update_learn_mem(stored_weights)`, e.g. to restore learned
- weights after a power cycle.
-
-~~~~cpp
-// program Akida, e.g. after power cycle
-device->program(model_buffer, model_size);
-
-// toggle learning mode on
-device->toggle_learn(true);
-
-// set batch size to 1 because learning is on
-device->set_batch_size(1, false);
-
-std::vector<uint32_t> stored_weights = /* load weights from application */
-// program previously saved weights to Akida
-device->update_learn_mem(stored_weights.data());
-~~~~
-
-#### Higher level API
-
-A higher level API is available as convenience:
-- `forward` method that perform inference on vector of input Tensor, and
-returns a vector of output Tensor.
-- `predict` method, similar to forward but also performing the dequantize step.
-- `fit` method that perform inference on vector of input Tensor, along with a
-vector of labels, and returns a vector of output Tensor.
-
-### Generating test applications
-
-The test directory contains several 'fixtures' allowing to generate unit test
-applications using the akida package.
-
-~~~~
-akida generate --fixture-files test/fixtures/*.py
-               --dest-path <path>
-~~~~
-
-## Licensing
-
-Copyright 2022 Brainchip, Inc
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS,
-    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-    See the License for the specific language governing permissions and
-    limitations under the License.
+# Akida Engine library
+
+The **Akida Engine** library is a C++ library allowing to perform an inference
+on an Akida model loaded into an Akida compatible device.
+
+As a prerequisite, the Akida model program has to be generated on a host using
+the [Akida python package](https://pypi.org/project/Akida/).
+
+## Building the library
+
+### Build system
+
+The Akida Engine library does not have any specific build system requirements.
+
+### Toolchain/Compiler
+
+The Akida Engine library requires a 32 or 64 bit little endian toolchain with
+support for floating point numbers.
+
+The toolchain must include a C++ compiler supporting at least C++ 11.
+
+The toolchain must provide a minimal implementation of the C standard library,
+like for instance GCC newlib and the corresponding target-specific stubs.
+
+The Akida Engine library does not use C++ exceptions, so exception support can
+be disabled when generating binaries for embedded targets.
+
+### Dependencies
+
+The library relies on external symbols whose target-specific implementation must
+be provided by the build system.
+
+#### Google flatbuffers
+
+The library has a dependency towards the Google flatbuffer header-only library:
+   https://github.com/google/flatbuffers/releases/tag/v2.0.6
+
+The sources must be downloaded from the link above and made available to the
+library as an additional include path.
+
+#### Standard Template Library (STL)
+
+The Akida Engine library relies on a few C++ STL classes whose implementation
+must be provided by the build system.
+
+The following headers must be supported:
+
+~~~~cpp
+<algorithm>
+<array>
+<functional>
+<memory>
+<queue>
+<set>
+<tuple>
+<typeindex>
+<vector>
+~~~~
+
+#### System infra
+
+The Akida Engine library requires a few system methods to be implemented.
+
+Please refer to api/infra/system.h for a list and description of the methods to
+implement.
+
+### Building a static library
+
+A basic cmake build file is provided for convenience:
+
+~~~~
+mkdir build
+cmake . -B build
+make -C build
+~~~~
+
+### Using the library
+
+#### Hardware driver
+
+The Akida Engine library primary entry point is the HardwareDevice class.
+
+One can obtain an HardwareDevice instance by passing a target-specific
+HardwareDriver instance to HardwareDevice::create().
+
+~~~~cpp
+#include "akida/hardware_device.h"
+#include "infra/hardware_driver.h"
+
+class MyDriver : public HardwareDriver {
+...
+}
+
+...
+
+auto driver = MyDriver();
+auto device = HardwareDevice::create(driver);
+~~~~
+
+The HardwareDriver main purpose is to abstract the read and write operations
+into the Akida DMA.
+It also provides:
+- the base address for Akida blocks registers
+- the 'scratch' memory region where the library can allocate buffers to
+communicate with the Akida DMA (runtime configuration, inputs/outputs).
+- the 'visible from akida' memory region, defining the memory region directly
+accessible by Akida DMAs.
+Data put in this area will not be copied to scratch memory.
+
+Please refer to api/infra/hardware_driver.h for a complete description of the
+HardwareDriver API and how to implement your specific driver.
+
+##### BareMetalDriver class
+
+BareMetalDriver, a custom HardwareDriver implementation, is provided as an
+example.
+Its constructor takes 4 arguments that you can use to customize your
+application:
+- scratch_base_address: the base 'scratch' memory address.
+- scratch_size: the size of 'scratch' memory that can be used. In case of
+ overflow, application will panic at runtime.
+- akida_visible_memory_base: the base address where Akida DMAs can directly
+acccess data. This can be the location where program and inputs are put,
+so they don't need to be copied to 'scratch' memory
+- akida_visible_memory_size: the size of the 'visible' memory region.
+
+#### Generate model programs
+
+The akida engine library allows to program an akida device with machine
+learning models previously trained and compiled.
+
+The akida and cnn2snn python packages are required to generate model programs
+from pre-trained keras models.
+
+The akida_models python package contains helpers to fetch models from the akida
+model zoo.
+
+~~~~python
+#!/usr/bin/env python
+import os
+from akida import AKD1000
+from akida.generate array_to_cpp
+from cnn2snn import convert
+from akida_models import ds_cnn_kws_pretrained
+
+# Load Keras pre-trained model from Akida model zoo
+model_keras = ds_cnn_kws_pretrained()
+
+# Convert Keras model to Akida
+model_akida = convert(model_keras)
+
+# Map/compile converted model for an AKD1000 device
+model_akida.map(device=AKD1000(), hw_only=True)
+
+# Check model mapping: NP allocation and binary size
+model_akida.summary()
+
+# Retrieve model program binary
+program = model_akida.sequences[0].program
+
+# Generate a binary that can be flashed
+with open('kws_model.bin', 'wb') as file:
+    file.write(program)
+    file.close()
+
+# Or generate source files to be included -> kws_model.{cpp,h}
+    array_to_cpp('., program, 'kws_model')
+~~~~
+
+#### Load model programs
+
+Once loaded in memory, the raw bytes buffer corresponding to a model program
+can be passed to the HardwareDevice to program the model on the device.
+
+~~~~cpp
+#include "kws_model.h"
+
+// Load program
+device->program(kws_model, kws_model_size);
+~~~~
+
+#### Set batch size
+
+After programming, one must configure the batch size to the number of inputs
+that can be enqueued before fetching for outputs. A higher batch size has
+higher memory requirements, but can allow for inputs pipelining. The batch size
+must be set by calling `set_batch_size` function.
+This function takes a `requested_batch_size` parameter, which is the batch size
+one wants to set, and a boolean `allocate_inputs` to allocate space for inputs
+in scratch memory.
+`allocate_inputs` must be set to `true` if inputs are not in the range
+specified by `akida_visible_memory_base` and `akida_visible_memory_size`.
+Outputs are always allocated in scratch memory.
+
+Note: the maximum effective batch size is 15 for a single pass program, or 1 if
+the program is multi pass, or learning mode have been enabled. Passing a higher
+value is allowed, but the effective batch size that has been set is returned by
+`set_batch_size` method.
+
+~~~~cpp
+// Assume device has been programmed
+
+// set a batch size of 2, and do not allocate space for inputs (they must be in
+// the range specified by driver->akida_visible_memory_base(),
+// driver->akida_visible_memory_size())
+device->set_batch_size(2, false);
+~~~~
+
+#### Perform an inference
+
+Once a model has been programmed and a batch size is set, inputs Tensor can be
+enqueued into the device inference pipeline, by calling `enqueue` function. The
+function immediately returns a boolean set to true if the tensor was
+successfully enqueued, or false if the pipeline was full.
+Depending on your program and device, you may have to convert your inputs to a
+sparse format.
+
+Two types of tensors can be passed as input and returned as outputs:
+- Dense tensors are standard contiguous buffers whose items are ordered either
+using the row-major or col-major convention,
+- Sparse tensors are list of coordinates and values. These cannot be directly
+constructed, they are output from Akida, or converted from a Dense.
+
+Several static constructors are available to create tensors, depending on your
+use case: pre-allocate, copy, etc.
+Conversion functions also exist.
+
+~~~~cpp
+#include "akida/dense.h"
+
+// Enqueue a single input from a raw input dense buffer
+auto input = Dense::create_view(input_buf, TensorType::uint8, {49, 10, 1},
+                                TensorLayout::RowMajor);
+bool success = device->enqueue(*input);
+~~~~
+
+Then you can periodically check for outputs, by calling `fetch` function.
+The function returns a pointer to an output Tensor, or nullptr if no output
+was available.
+
+~~~~cpp
+auto output = device->fetch();
+if (output != nullptr) {
+    // do something with output
+}
+~~~~
+
+If your program last layer ends with an activation, the outputs are normalized
+n-bit values.
+Otherwise, if your program does not end with an activation, you must dequantize
+the output potentials to float values to project them in the same scale before
+processing them. You can use the `dequantize` method to do so.
+
+#### Perform edge learning
+
+To perform edge learning, the generated model should have been compiled
+beforehand.
+
+Example:
+~~~~python
+model.compile(
+    optimizer=akida.AkidaUnsupervised(
+        num_weights=num_weights,
+        num_classes=num_classes,
+        learning_competition=learning_competition
+        )
+    )
+~~~~
+
+See https://doc.brainchipinc.com/examples/index.html#edge-examples
+for more informations about learning and learning parameters.
+
+Then, to activate edge learning in your application, you must call
+`toggle_learn` method with the `learn_en` parameter set to `true`.
+You can then pass a label corresponding to your input when calling `enqueue`.
+Learning can be disabled by calling `toggle_learn` again with the `learn_en`
+parameter set to `false`.
+~~~~cpp
+#include "akida/dense.h"
+
+#include "kws_model.h"
+
+// Program device (this is not required if you already did it previously)
+device->program(kws_model, kws_model_size);
+
+// Turn learning mode on because learning is disabled by default
+device->toggle_learn(true);
+
+// Set batch size to 1 because learning is on
+device->set_batch_size(1, false);
+
+// Enqueue a single input from a raw input dense buffer
+auto input = Dense::create_view(input_buf, TensorType::uint8, {49, 10, 1},
+                                TensorLayout::RowMajor);
+// passing a single label
+bool success = device->enqueue(input, 1);
+~~~~
+
+##### Updating weights
+
+Weights used for learn are initially stored in the program buffer, then used to
+program the FNP (FullyConnected Neural Processor) that will perform the learn.
+In order to retrieve updated weights when learn is used, there are two possible
+situations, depending on the FNP type used:
+
+- FNP3 type: Akida has weights stored in its own SRAM, so a copy back from
+internal SRAM to application memory must be done. To do that, a buffer must be
+allocated with the correct size. `HardwareDevice::learn_mem_size()` method can
+be called to know this size.
+The output of this function is the number of 32 bits words required.
+Then you can call `HardwareDevice::learn_mem(output_buffer)` that will copy
+weights to the `output_buffer` parameter.
+
+- FNP2 type: Akida reads directly weights from memory,
+so that memory will be updated automatically after each fit call.
+Note that if weights were not in 'akida visible memory', they have been copied
+to 'scratch' memory, so only scratch memory have been updated. To update your
+initial program, you will have to proceed the same way as FNP3.
+
+~~~~cpp
+std::vector<uint32_t> updated_weights(device->learn_mem_size());
+device->learn_mem(updated_weights.data());
+~~~~
+
+The application can then store this buffer.
+This can be reprogrammed by calling
+`HardwareDevice::update_learn_mem(stored_weights)`, e.g. to restore learned
+ weights after a power cycle.
+
+~~~~cpp
+// program Akida, e.g. after power cycle
+device->program(model_buffer, model_size);
+
+// toggle learning mode on
+device->toggle_learn(true);
+
+// set batch size to 1 because learning is on
+device->set_batch_size(1, false);
+
+std::vector<uint32_t> stored_weights = /* load weights from application */
+// program previously saved weights to Akida
+device->update_learn_mem(stored_weights.data());
+~~~~
+
+#### Higher level API
+
+A higher level API is available as convenience:
+- `forward` method that perform inference on vector of input Tensor, and
+returns a vector of output Tensor.
+- `predict` method, similar to forward but also performing the dequantize step.
+- `fit` method that perform inference on vector of input Tensor, along with a
+vector of labels, and returns a vector of output Tensor.
+
+### Generating test applications
+
+The test directory contains several 'fixtures' allowing to generate unit test
+applications using the akida package.
+
+~~~~
+akida generate --fixture-files test/fixtures/*.py
+               --dest-path <path>
+~~~~
+
+## Licensing
+
+Copyright 2022 Brainchip, Inc
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an "AS IS" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
```

## akida/engine/api/akd1000/bare_metal_driver.h

 * *Ordering differences only*

```diff
@@ -1,55 +1,55 @@
-#pragma once
-
-// this need to be in an extern "C" block because C++ will generate name
-// mangling for nested extern declarations
-extern "C" {
-#include "cmsis_compiler.h"
-}
-#if (defined(__ARM_ARCH_7EM__) && (__ARM_ARCH_7EM__ == 1))
-#include <cstdint>
-
-#include "akd1000/memory_mapping.h"
-#include "infra/hardware_driver.h"
-
-namespace akida {
-
-class BareMetalDriver final : public HardwareDriver {
- public:
-  BareMetalDriver(uint32_t scratch_base_address, uint32_t scratch_size,
-                  uint32_t akida_visible_memory_base,
-                  uint32_t akida_visible_memory_size);
-
-  void read(uint32_t address, void* data, size_t size) const override;
-
-  void write(uint32_t address, const void* data, size_t size) override;
-
-  const char* desc() const override;
-
-  uint32_t scratch_memory() const override { return scratch_base_addr_; }
-
-  uint32_t scratch_size() const override { return scratch_size_; }
-
-  uint32_t top_level_reg() const override {
-    return soc::akd1000::kTopLevelRegBase;
-  }
-
-  uint32_t akida_visible_memory() const override {
-    return akida_visible_mem_base_;
-  }
-
-  uint32_t akida_visible_memory_size() const override {
-    return akida_visible_mem_size_;
-  }
-
- protected:
-  uint32_t scratch_base_addr_;
-  uint32_t scratch_size_;
-  uint32_t akida_visible_mem_base_;
-  uint32_t akida_visible_mem_size_;
-};
-
-}  // namespace akida
-
-#else
-#error This file should only be built for Cortex M4F (armv7e-m) target
-#endif
+#pragma once
+
+// this need to be in an extern "C" block because C++ will generate name
+// mangling for nested extern declarations
+extern "C" {
+#include "cmsis_compiler.h"
+}
+#if (defined(__ARM_ARCH_7EM__) && (__ARM_ARCH_7EM__ == 1))
+#include <cstdint>
+
+#include "akd1000/memory_mapping.h"
+#include "infra/hardware_driver.h"
+
+namespace akida {
+
+class BareMetalDriver final : public HardwareDriver {
+ public:
+  BareMetalDriver(uint32_t scratch_base_address, uint32_t scratch_size,
+                  uint32_t akida_visible_memory_base,
+                  uint32_t akida_visible_memory_size);
+
+  void read(uint32_t address, void* data, size_t size) const override;
+
+  void write(uint32_t address, const void* data, size_t size) override;
+
+  const char* desc() const override;
+
+  uint32_t scratch_memory() const override { return scratch_base_addr_; }
+
+  uint32_t scratch_size() const override { return scratch_size_; }
+
+  uint32_t top_level_reg() const override {
+    return soc::akd1000::kTopLevelRegBase;
+  }
+
+  uint32_t akida_visible_memory() const override {
+    return akida_visible_mem_base_;
+  }
+
+  uint32_t akida_visible_memory_size() const override {
+    return akida_visible_mem_size_;
+  }
+
+ protected:
+  uint32_t scratch_base_addr_;
+  uint32_t scratch_size_;
+  uint32_t akida_visible_mem_base_;
+  uint32_t akida_visible_mem_size_;
+};
+
+}  // namespace akida
+
+#else
+#error This file should only be built for Cortex M4F (armv7e-m) target
+#endif
```

## akida/engine/api/akd1000/memory_mapping.h

 * *Ordering differences only*

```diff
@@ -1,27 +1,27 @@
-#pragma once
-
-#include <cstdint>
-
-namespace akida {
-namespace soc {
-namespace akd1000 {
-// NSoC top level address
-constexpr uint32_t kTopLevelRegBase = 0xFCC00000;
-
-// Typical scratch memory values
-// DDR offset in NSoC
-constexpr uint32_t kDdrBase = 0x20000000;
-// First 64 MB of DDR could be used by a firmware
-constexpr uint32_t kMemOffset = 64 * 1024 * 1024;
-constexpr uint32_t kScratchBase = kDdrBase + kMemOffset;
-// Allow scratch to go up to 256 MB after the DDR base, i.e.: 192 MB.
-// On a model like Mobilenet 0.5, forwarding more than 16 frames, the NSoC
-// scratch memory usage hits a maximum peak of 17 MB, so 192 MB is
-// probably large enough for inference on most models.
-constexpr uint32_t kScratchSize = 192 * 1024 * 1024;
-
-}  // namespace akd1000
-
-}  // namespace soc
-
-}  // namespace akida
+#pragma once
+
+#include <cstdint>
+
+namespace akida {
+namespace soc {
+namespace akd1000 {
+// NSoC top level address
+constexpr uint32_t kTopLevelRegBase = 0xFCC00000;
+
+// Typical scratch memory values
+// DDR offset in NSoC
+constexpr uint32_t kDdrBase = 0x20000000;
+// First 64 MB of DDR could be used by a firmware
+constexpr uint32_t kMemOffset = 64 * 1024 * 1024;
+constexpr uint32_t kScratchBase = kDdrBase + kMemOffset;
+// Allow scratch to go up to 256 MB after the DDR base, i.e.: 192 MB.
+// On a model like Mobilenet 0.5, forwarding more than 16 frames, the NSoC
+// scratch memory usage hits a maximum peak of 17 MB, so 192 MB is
+// probably large enough for inference on most models.
+constexpr uint32_t kScratchSize = 192 * 1024 * 1024;
+
+}  // namespace akd1000
+
+}  // namespace soc
+
+}  // namespace akida
```

## akida/engine/api/akd1000/registers_soc.h

 * *Ordering differences only*

```diff
@@ -1,57 +1,57 @@
-#pragma once
-
-#include <cstdint>
-#include "infra/registers_common.h"
-
-namespace akida {
-
-// Chip info register
-static constexpr uint32_t REG_SYS_CONF_BASE = 0xF0000000;
-
-static constexpr uint32_t REG_CHIP_INFO = REG_SYS_CONF_BASE + 0x10;
-static constexpr RegDetail REG_CHIP_VERSION(0, 3);
-
-static constexpr uint32_t APB_BASE = 0xE8010000;
-
-// I2C registers
-static constexpr uint32_t I2C_BASE = APB_BASE + 0x07FFA000;
-static constexpr uint32_t I2C_BSR = I2C_BASE + 0x0;  // Bus Status Register
-static constexpr RegDetail I2C_BSR_TRX(3);
-static constexpr RegDetail I2C_BSR_LRB(4);
-static constexpr RegDetail I2C_BSR_BB(7);
-static constexpr uint32_t I2C_BCR = I2C_BASE + 0x4;  // Bus Control Register
-static constexpr RegDetail I2C_BCR_INT(0);
-static constexpr RegDetail I2C_BCR_ACK(3);
-static constexpr RegDetail I2C_BCR_MSS(4);
-static constexpr RegDetail I2C_BCR_BER(7);
-static constexpr uint32_t I2C_CCR = I2C_BASE + 0x8;  // Clock Control Register
-static constexpr RegDetail I2C_CCR_EN(5);
-static constexpr uint32_t I2C_ADR = I2C_BASE + 0xc;   // Address Regsiter
-static constexpr uint32_t I2C_DAR = I2C_BASE + 0x10;  // Data Register
-static constexpr uint32_t I2C_CSR =
-    I2C_BASE + 0x14;  // Expand Clock Period Select Register
-static constexpr uint32_t I2C_FSR =
-    I2C_BASE + 0x18;  // Macro System Clock Frequency Select Register
-static constexpr uint32_t I2C_BCR2 = I2C_BASE + 0x1c;  // Bus Control Register 2
-
-// I3C registers
-static constexpr uint32_t I3C_BASE = APB_BASE + 0x07FFB000;
-static constexpr uint32_t I3C_CTRL = I3C_BASE + 0x10;
-static constexpr uint32_t I3C_PRESCL_CTRL0 = I3C_BASE + 0x14;
-static constexpr uint32_t I3C_PRESCL_CTRL1 = I3C_BASE + 0x18;
-static constexpr uint32_t I3C_MST_INTR_IER = I3C_BASE + 0x20;
-static constexpr uint32_t I3C_MST_INTR_ICR = I3C_BASE + 0x2C;
-static constexpr uint32_t I3C_MST_INTR_ISR = I3C_BASE + 0x30;
-static constexpr uint32_t I3C_MST_STATUS0 = I3C_BASE + 0x34;
-static constexpr uint32_t I3C_CMDR = I3C_BASE + 0x38;
-static constexpr uint32_t I3C_CMD0_FIFO = I3C_BASE + 0x60;
-static constexpr uint32_t I3C_CMD1_FIFO = I3C_BASE + 0x64;
-static constexpr uint32_t I3C_TX_FIFO = I3C_BASE + 0x68;
-static constexpr uint32_t I3C_RX_FIFO = I3C_BASE + 0x80;
-static constexpr uint32_t I3C_CMD_IBI_THR_CTRL = I3C_BASE + 0x90;
-static constexpr uint32_t I3C_FLUSH_CTRL = I3C_BASE + 0x9C;
-static constexpr uint32_t I3C_DEVS_CTRL = I3C_BASE + 0xB8;
-static constexpr uint32_t I3C_DEVICE_ID_0_RR0 = I3C_BASE + 0xC0;
-static constexpr uint32_t I3C_DEVICE_ID_1_RR0 = I3C_BASE + 0xD0;
-
-}  // namespace akida
+#pragma once
+
+#include <cstdint>
+#include "infra/registers_common.h"
+
+namespace akida {
+
+// Chip info register
+static constexpr uint32_t REG_SYS_CONF_BASE = 0xF0000000;
+
+static constexpr uint32_t REG_CHIP_INFO = REG_SYS_CONF_BASE + 0x10;
+static constexpr RegDetail REG_CHIP_VERSION(0, 3);
+
+static constexpr uint32_t APB_BASE = 0xE8010000;
+
+// I2C registers
+static constexpr uint32_t I2C_BASE = APB_BASE + 0x07FFA000;
+static constexpr uint32_t I2C_BSR = I2C_BASE + 0x0;  // Bus Status Register
+static constexpr RegDetail I2C_BSR_TRX(3);
+static constexpr RegDetail I2C_BSR_LRB(4);
+static constexpr RegDetail I2C_BSR_BB(7);
+static constexpr uint32_t I2C_BCR = I2C_BASE + 0x4;  // Bus Control Register
+static constexpr RegDetail I2C_BCR_INT(0);
+static constexpr RegDetail I2C_BCR_ACK(3);
+static constexpr RegDetail I2C_BCR_MSS(4);
+static constexpr RegDetail I2C_BCR_BER(7);
+static constexpr uint32_t I2C_CCR = I2C_BASE + 0x8;  // Clock Control Register
+static constexpr RegDetail I2C_CCR_EN(5);
+static constexpr uint32_t I2C_ADR = I2C_BASE + 0xc;   // Address Regsiter
+static constexpr uint32_t I2C_DAR = I2C_BASE + 0x10;  // Data Register
+static constexpr uint32_t I2C_CSR =
+    I2C_BASE + 0x14;  // Expand Clock Period Select Register
+static constexpr uint32_t I2C_FSR =
+    I2C_BASE + 0x18;  // Macro System Clock Frequency Select Register
+static constexpr uint32_t I2C_BCR2 = I2C_BASE + 0x1c;  // Bus Control Register 2
+
+// I3C registers
+static constexpr uint32_t I3C_BASE = APB_BASE + 0x07FFB000;
+static constexpr uint32_t I3C_CTRL = I3C_BASE + 0x10;
+static constexpr uint32_t I3C_PRESCL_CTRL0 = I3C_BASE + 0x14;
+static constexpr uint32_t I3C_PRESCL_CTRL1 = I3C_BASE + 0x18;
+static constexpr uint32_t I3C_MST_INTR_IER = I3C_BASE + 0x20;
+static constexpr uint32_t I3C_MST_INTR_ICR = I3C_BASE + 0x2C;
+static constexpr uint32_t I3C_MST_INTR_ISR = I3C_BASE + 0x30;
+static constexpr uint32_t I3C_MST_STATUS0 = I3C_BASE + 0x34;
+static constexpr uint32_t I3C_CMDR = I3C_BASE + 0x38;
+static constexpr uint32_t I3C_CMD0_FIFO = I3C_BASE + 0x60;
+static constexpr uint32_t I3C_CMD1_FIFO = I3C_BASE + 0x64;
+static constexpr uint32_t I3C_TX_FIFO = I3C_BASE + 0x68;
+static constexpr uint32_t I3C_RX_FIFO = I3C_BASE + 0x80;
+static constexpr uint32_t I3C_CMD_IBI_THR_CTRL = I3C_BASE + 0x90;
+static constexpr uint32_t I3C_FLUSH_CTRL = I3C_BASE + 0x9C;
+static constexpr uint32_t I3C_DEVS_CTRL = I3C_BASE + 0xB8;
+static constexpr uint32_t I3C_DEVICE_ID_0_RR0 = I3C_BASE + 0xC0;
+static constexpr uint32_t I3C_DEVICE_ID_1_RR0 = I3C_BASE + 0xD0;
+
+}  // namespace akida
```

## akida/engine/api/akida/dense.h

 * *Ordering differences only*

```diff
@@ -1,207 +1,207 @@
-#pragma once
-
-#include <cstdint>
-#include <cstring>
-#include <memory>
-#include <vector>
-
-#include "akida/shape.h"
-#include "akida/tensor.h"
-#include "infra/exports.h"
-
-/** file akida/dense.h
- * Contains the abstract Dense object and its related types
- */
-
-namespace akida {
-
-/**
- * @brief A shared pointer to a Dense object
- */
-using DensePtr = std::shared_ptr<Dense>;
-
-/**
- * @brief A shared pointer to a const Dense object
- */
-using DenseConstPtr = std::shared_ptr<const Dense>;
-
-/**
- * @bried A unique pointer to a Dense object
- */
-using DenseUniquePtr = std::unique_ptr<Dense>;
-
-/**
- * class Dense
- *
- * An abstraction of a multi-dimensional dense array
- *
- * Stores data using a column-major (default) or row-major layout
- *
- * To iterate over the values of a Dense of type T, one would typically call
- * the data<T>() templated member.
- *
- */
-class AKIDASHAREDLIB_EXPORT Dense : public Tensor {
- public:
-  virtual ~Dense() {}
-
-  bool operator==(const Tensor& ref) const override;
-
-  bool operator==(const Dense& ref) const {
-    auto shape = dimensions();
-    return type() == ref.type() && layout() == ref.layout() &&
-           std::equal(shape.begin(), shape.end(), ref.dimensions().begin()) &&
-           size() == ref.size() &&
-           std::memcmp(buffer()->data(), ref.buffer()->data(),
-                       buffer()->size()) == 0;
-  }
-
-  /**
-   * @enum  Layout
-   * @brief The Dense memory layout (storage order)
-   * The memory layout of a Dense tensor has an impact on how the linear index
-   * of the elements is calculated from each element coordinates.
-   * For a row-major Dense, when you increment the first coordinate, the element
-   * index is incremented by a factor corresponding to the product of all
-   * dimensions but the first one.
-   * On the contrary, for a column-major Dense, when you increment the first
-   * coordinate, the index is just incremented by one.
-   */
-  enum class Layout {
-    RowMajor /**<RowMajor, or 'biggest stride first'*/,
-    ColMajor /**<ColMajor, or 'smallest stride first'*/
-  };
-
-  /**
-   * @brief returns the Dense tensor layout
-   * @return : Layout::ColMajor or Layout::RowMajor
-   */
-  virtual Layout layout() const = 0;
-
-  /**
-   * @brief returns the Dense strides for each dimension
-   * @return : a vector of strides for direct access to the tensor data
-   */
-  virtual const std::vector<uint32_t>& strides() const = 0;
-
-  /**
-   * @brief Modifies a Dense by setting a new shape
-   * The shape of the dense is modified, its data is untouched, but a new shape
-   * is set. New shape should contain the same number of dimensions as the old
-   * one, and the product of its dimensions should be the same as the product of
-   * the old one.
-   * @param : the new shape
-   */
-  virtual void reshape(const Shape& new_shape) = 0;
-
-  /**
-   * @brief Get the value at the specified coordinates
-   * @param coords : the set of coordinates
-   * @return the Dense value at these coordinates
-   */
-  template<typename T>
-  T get(const std::vector<Index>& coords) const {
-    auto index = linear_index(coords, strides());
-    if (index > size() - 1) {
-      panic("Coordinates are out-of-range");
-    }
-    return data<T>()[index];
-  }
-
-  /**
-   * @brief Set the value at the specified coordinates
-   * @param coords : the set of coordinates
-   * @param value : the value at these coordinates
-   */
-  template<typename T>
-  void set(const std::vector<Index>& coords, T value) {
-    auto index = linear_index(coords, strides());
-    if (index > size() - 1) {
-      panic("Coordinates are out-of-range");
-    }
-    data<T>()[index] = value;
-  }
-
-  /**
-   * @brief Set the same value at all coordinates
-   * @param value : the value
-   */
-  template<typename T>
-  void fill(T value) {
-    auto cached_size = size();
-    for (size_t i = 0; i < cached_size; ++i) {
-      data<T>()[i] = value;
-    }
-  }
-
-  /**
-   * @brief Returns the strides corresponding to the given shape
-   * @param shape : the Dense tensor dimensions
-   * @param layout : the Dense tensor layout
-   * @return : a vector of strides for direct access to the tensor data
-   */
-  static std::vector<uint32_t> eval_strides(const Shape& shape, Layout layout);
-
-  /**
-   * @brief Create a Dense, allocating its internal buffer
-   *
-   * Note that the internal buffer is not zero-initialized.
-   *
-   * @param type   : the Tensor data type, as an akida::TensorType
-   * @param dims   : the Tensor dimensions
-   * @param layout : the source array layout, can be one of akida::RowMajor,
-   * akida::ColMajor
-   *
-   * If the source array is Row-major, then the order of dimensions of the
-   * created Tensor will be reversed.
-   */
-  static DenseUniquePtr create(TensorType type, const Shape& dims,
-                               Dense::Layout layout);
-
-  /**
-   * @brief Create a Dense whose internal buffer contains a copy of a byte array
-   * @param array  : a pointer to the source byte array
-   * @param size   : the array size in bytes
-   * @param type   : the Tensor data type, as an akida::TensorType
-   * @param dims   : the Tensor dimensions
-   * @param layout : the source array layout, can be one of akida::RowMajor,
-   * akida::ColMajor
-   *
-   * If the source array is Row-major, then the order of dimensions of the
-   * created Tensor will be reversed.
-   */
-  static DenseUniquePtr copy(const char* array, size_t bytes_size,
-                             TensorType type, const Shape& dims,
-                             Dense::Layout layout);
-
-  /**
-   * @brief Create a Dense from Sparse
-   * @param sparse : the Sparse object to clone
-   */
-  static DenseUniquePtr from_sparse(const Sparse& sparse, Layout layout);
-
-  /**
-   * @brief Create a Dense view from a C/C++ byte array
-   * @param array  : a pointer to the source byte array
-   * @param type   : the Tensor data type, as an akida::TensorType
-   * @param dims   : the Tensor dimensions
-   * @param layout : the source array layout, can be one of akida::RowMajor,
-   * akida::ColMajor
-   *
-   * Constant version of create_view.
-   */
-  static DenseUniquePtr create_view(const char* array, TensorType type,
-                                    const Shape& dims, Dense::Layout layout);
-
-  /**
-   * @brief Splits a Dense along longest stride axis
-   * The resulting sub-tensors might be just a view on the original data,
-   * meaning that they will not own their buffer and that original buffer should
-   * not be deleted while the subtensors are in use.
-   * @param : the tensor to split
-   * @return : a vector of sub-tensors
-   */
-  static std::vector<TensorConstPtr> split(const Dense& t);
-};
-
-}  // namespace akida
+#pragma once
+
+#include <cstdint>
+#include <cstring>
+#include <memory>
+#include <vector>
+
+#include "akida/shape.h"
+#include "akida/tensor.h"
+#include "infra/exports.h"
+
+/** file akida/dense.h
+ * Contains the abstract Dense object and its related types
+ */
+
+namespace akida {
+
+/**
+ * @brief A shared pointer to a Dense object
+ */
+using DensePtr = std::shared_ptr<Dense>;
+
+/**
+ * @brief A shared pointer to a const Dense object
+ */
+using DenseConstPtr = std::shared_ptr<const Dense>;
+
+/**
+ * @bried A unique pointer to a Dense object
+ */
+using DenseUniquePtr = std::unique_ptr<Dense>;
+
+/**
+ * class Dense
+ *
+ * An abstraction of a multi-dimensional dense array
+ *
+ * Stores data using a column-major (default) or row-major layout
+ *
+ * To iterate over the values of a Dense of type T, one would typically call
+ * the data<T>() templated member.
+ *
+ */
+class AKIDASHAREDLIB_EXPORT Dense : public Tensor {
+ public:
+  virtual ~Dense() {}
+
+  bool operator==(const Tensor& ref) const override;
+
+  bool operator==(const Dense& ref) const {
+    auto shape = dimensions();
+    return type() == ref.type() && layout() == ref.layout() &&
+           std::equal(shape.begin(), shape.end(), ref.dimensions().begin()) &&
+           size() == ref.size() &&
+           std::memcmp(buffer()->data(), ref.buffer()->data(),
+                       buffer()->size()) == 0;
+  }
+
+  /**
+   * @enum  Layout
+   * @brief The Dense memory layout (storage order)
+   * The memory layout of a Dense tensor has an impact on how the linear index
+   * of the elements is calculated from each element coordinates.
+   * For a row-major Dense, when you increment the first coordinate, the element
+   * index is incremented by a factor corresponding to the product of all
+   * dimensions but the first one.
+   * On the contrary, for a column-major Dense, when you increment the first
+   * coordinate, the index is just incremented by one.
+   */
+  enum class Layout {
+    RowMajor /**<RowMajor, or 'biggest stride first'*/,
+    ColMajor /**<ColMajor, or 'smallest stride first'*/
+  };
+
+  /**
+   * @brief returns the Dense tensor layout
+   * @return : Layout::ColMajor or Layout::RowMajor
+   */
+  virtual Layout layout() const = 0;
+
+  /**
+   * @brief returns the Dense strides for each dimension
+   * @return : a vector of strides for direct access to the tensor data
+   */
+  virtual const std::vector<uint32_t>& strides() const = 0;
+
+  /**
+   * @brief Modifies a Dense by setting a new shape
+   * The shape of the dense is modified, its data is untouched, but a new shape
+   * is set. New shape should contain the same number of dimensions as the old
+   * one, and the product of its dimensions should be the same as the product of
+   * the old one.
+   * @param : the new shape
+   */
+  virtual void reshape(const Shape& new_shape) = 0;
+
+  /**
+   * @brief Get the value at the specified coordinates
+   * @param coords : the set of coordinates
+   * @return the Dense value at these coordinates
+   */
+  template<typename T>
+  T get(const std::vector<Index>& coords) const {
+    auto index = linear_index(coords, strides());
+    if (index > size() - 1) {
+      panic("Coordinates are out-of-range");
+    }
+    return data<T>()[index];
+  }
+
+  /**
+   * @brief Set the value at the specified coordinates
+   * @param coords : the set of coordinates
+   * @param value : the value at these coordinates
+   */
+  template<typename T>
+  void set(const std::vector<Index>& coords, T value) {
+    auto index = linear_index(coords, strides());
+    if (index > size() - 1) {
+      panic("Coordinates are out-of-range");
+    }
+    data<T>()[index] = value;
+  }
+
+  /**
+   * @brief Set the same value at all coordinates
+   * @param value : the value
+   */
+  template<typename T>
+  void fill(T value) {
+    auto cached_size = size();
+    for (size_t i = 0; i < cached_size; ++i) {
+      data<T>()[i] = value;
+    }
+  }
+
+  /**
+   * @brief Returns the strides corresponding to the given shape
+   * @param shape : the Dense tensor dimensions
+   * @param layout : the Dense tensor layout
+   * @return : a vector of strides for direct access to the tensor data
+   */
+  static std::vector<uint32_t> eval_strides(const Shape& shape, Layout layout);
+
+  /**
+   * @brief Create a Dense, allocating its internal buffer
+   *
+   * Note that the internal buffer is not zero-initialized.
+   *
+   * @param type   : the Tensor data type, as an akida::TensorType
+   * @param dims   : the Tensor dimensions
+   * @param layout : the source array layout, can be one of akida::RowMajor,
+   * akida::ColMajor
+   *
+   * If the source array is Row-major, then the order of dimensions of the
+   * created Tensor will be reversed.
+   */
+  static DenseUniquePtr create(TensorType type, const Shape& dims,
+                               Dense::Layout layout);
+
+  /**
+   * @brief Create a Dense whose internal buffer contains a copy of a byte array
+   * @param array  : a pointer to the source byte array
+   * @param size   : the array size in bytes
+   * @param type   : the Tensor data type, as an akida::TensorType
+   * @param dims   : the Tensor dimensions
+   * @param layout : the source array layout, can be one of akida::RowMajor,
+   * akida::ColMajor
+   *
+   * If the source array is Row-major, then the order of dimensions of the
+   * created Tensor will be reversed.
+   */
+  static DenseUniquePtr copy(const char* array, size_t bytes_size,
+                             TensorType type, const Shape& dims,
+                             Dense::Layout layout);
+
+  /**
+   * @brief Create a Dense from Sparse
+   * @param sparse : the Sparse object to clone
+   */
+  static DenseUniquePtr from_sparse(const Sparse& sparse, Layout layout);
+
+  /**
+   * @brief Create a Dense view from a C/C++ byte array
+   * @param array  : a pointer to the source byte array
+   * @param type   : the Tensor data type, as an akida::TensorType
+   * @param dims   : the Tensor dimensions
+   * @param layout : the source array layout, can be one of akida::RowMajor,
+   * akida::ColMajor
+   *
+   * Constant version of create_view.
+   */
+  static DenseUniquePtr create_view(const char* array, TensorType type,
+                                    const Shape& dims, Dense::Layout layout);
+
+  /**
+   * @brief Splits a Dense along longest stride axis
+   * The resulting sub-tensors might be just a view on the original data,
+   * meaning that they will not own their buffer and that original buffer should
+   * not be deleted while the subtensors are in use.
+   * @param : the tensor to split
+   * @return : a vector of sub-tensors
+   */
+  static std::vector<TensorConstPtr> split(const Dense& t);
+};
+
+}  // namespace akida
```

## akida/engine/api/akida/hardware_device.h

 * *Ordering differences only*

```diff
@@ -1,253 +1,253 @@
-/*******************************************************************************
- * Copyright 2021 Brainchip Holdings Ltd.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- ********************************************************************************
- */
-
-#pragma once
-
-#include <cstdint>
-#include <memory>
-#include <vector>
-
-#include "akida/dense.h"
-#include "akida/hw_version.h"
-#include "akida/np.h"
-#include "akida/tensor.h"
-#include "infra/exports.h"
-#include "infra/hardware_driver.h"
-
-namespace akida {
-
-class HardwareDevice;
-
-using HardwareDevicePtr = std::shared_ptr<HardwareDevice>;
-using HardwareDeviceConstPtr = std::shared_ptr<const HardwareDevice>;
-
-/**
- * class HardwareDevice
- *
- * Public interface to an Akida Hardware Device.
- *
- * The main difference with Akida Device is the fact that HardwareDevice
- * objects driver real hardware, so they are capable of programming, performing
- * inference and few other hardware-specific calls.
- *
- */
-class AKIDASHAREDLIB_EXPORT HardwareDevice {
- public:
-  /**
-   * @brief Get the Device version
-   * @return a HwVersion
-   */
-  virtual HwVersion version() const = 0;
-
-  /**
-   * @brief Get the Device description
-   * @return a char*
-   */
-  virtual const char* desc() const = 0;
-
-  /**
-   * @brief Creates a Hardware Device
-   *
-   * @param driver : the driver that should be used by the hardware device
-   *
-   * @return a HardwareDevice
-   */
-  static HardwareDevicePtr create(HardwareDriver* driver);
-
-  /**
-   * @brief Toggle the HardwareDevice clock counter on/off
-   * @param enable : boolean to enable/disable clock counter
-   */
-  virtual void toggle_clock_counter(bool enable) = 0;
-
-  /**
-   * @brief Read the current HardwareDevice clock counter
-   * @return a uint32 representing the clock count (can overlap)
-   */
-  virtual uint32_t read_clock_counter() = 0;
-
-  /**
-   * @brief Read the current HardwareDevice configuration clock counter
-   * @return a uint32 representing the clock count (can overlap)
-   */
-  virtual uint32_t read_config_clock_counter() = 0;
-
-  /**
-   * @brief Return memory information
-   * @return a tuple containing current memory usage (in bytes) and top memory
-   * usage (in bytes)
-   */
-  using MemoryInfo = std::pair<uint32_t, uint32_t>;
-  virtual MemoryInfo memory() const = 0;
-
-  /**
-   * @brief Reset top memory usage to current one
-   */
-  virtual void reset_top_memory() = 0;
-
-  /**
-   * @brief Perform hardware device programming
-   * @param program : serialized buffer containing the Device program
-   * @param size : byte size of the buffer
-   */
-  virtual void program(const uint8_t* program, size_t size) = 0;
-
-  /**
-   * @brief Retrieve current program buffer
-   * @return a pair of pointer and size of current program
-   */
-  using BytesBuffer = std::pair<const uint8_t*, size_t>;
-  virtual const BytesBuffer& program() const = 0;
-
-  /**
-   * @brief Read the registers of a NP
-   * @param output : A pointer to a buffer to contain the registers read
-   * @param np : The NP to read
-   * @param nb_registers : The number of registers to read
-   */
-  virtual void read_np_registers(uint32_t* output, const struct np::Ident& np,
-                                 uint32_t nb_registers) = 0;
-
-  /**
-   * @brief Configure the number of inputs that can be sent at the same time
-   * (the number of enqueue calls without calling fetch). It is 15 max for a
-   * single pass program without learning, or 1 for multipass program and when
-   * learning is enabled. It will return the effective batch size applied.
-   * @param requested_batch_size : the requested batch size
-   * @param alloc_inputs : boolean to allocate memory for inputs. Required if
-   * inputs are not directly accessible from akida
-   * @return the effective batch size applied (can be lower than
-   * requested_batch_size)
-   */
-  virtual size_t set_batch_size(size_t requested_batch_size,
-                                bool allocate_inputs) = 0;
-
-  /**
-   * @brief Enable or disable learning mode of the current program
-   * @param learn_en : boolean to enable learning on the last layer
-   */
-  virtual void toggle_learn(bool learn_en) = 0;
-
-  /**
-   * @brief Tells if current program has learning enabled
-   * @return true if current program has learning enabled
-   */
-  virtual bool learn_enabled() const = 0;
-
-  /**
-   * @brief Tells current program learning memory size
-   * @return memory size, in number of 32 bit words
-   */
-  virtual size_t learn_mem_size() const = 0;
-
-  /**
-   * @brief Writes a copy of the learn memory of current program in the given
-   * buffer.
-   * @param output_buffer : A pointer to a buffer large enough to contain
-   * learning memory.
-   */
-  virtual void learn_mem(uint32_t* output_buffer) = 0;
-
-  std::vector<uint32_t> learn_mem() {
-    auto size = learn_mem_size();
-    std::vector<uint32_t> ret(size);
-    learn_mem(ret.data());
-    return ret;
-  }
-
-  /**
-   * @brief Updated learn memory from buffer containg a previously saved one
-   * @param input_buffer : A pointer to a buffer containing updated learning
-   * memory buffer
-   */
-  virtual void update_learn_mem(const uint32_t* input_buffer) = 0;
-
-  /**
-   * @brief Clear current program from hardware device, restoring its initial
-   * state
-   */
-  virtual void unprogram() = 0;
-
-  /**
-   * @brief Processes inputs to train on a programmed device
-   * @param inputs       : vector of 3D inputs Tensor
-   * @param input_labels : integer value labels of the input classes,
-   * for supervised learning
-   * @return Sparse or Dense outputs from the model last layer
-   */
-  virtual std::vector<TensorUniquePtr> fit(
-      const std::vector<TensorConstPtr>& inputs,
-      const std::vector<int32_t>& input_labels) = 0;
-
-  /**
-   * @brief Processes inputs on a programmed device
-   * @param inputs : vector of 3D inputs Tensor
-   * @return vector of 3D outputs from the device
-   */
-  virtual std::vector<TensorUniquePtr> forward(
-      const std::vector<TensorConstPtr>& inputs) = 0;
-
-  /**
-   * @brief Evaluates the results of processing on a programmed device
-   *
-   * This method propagates a set of inputs through a programmed device and
-   * returns the results in the form of a Tensor of float values.
-   * It applies ONLY on programs coming from models that do not have an
-   * activation on the last layer.
-   * The output values are obtained from the outputs discrete potentials by
-   * applying a shift and a scale.
-   *
-   * @param inputs : vector of 3D inputs Tensor
-   * @return vector of 3D rescaled output potentials from the programmed device
-   */
-  virtual std::vector<TensorUniquePtr> predict(
-      const std::vector<TensorConstPtr>& inputs) = 0;
-
-  /**
-   * @brief Try to put the input in the pipeline queue on a programmed device,
-   * starting queue execution if required
-   * @param input: 3D input Tensor
-   * @param label: integer value of the input class, for supervised learning
-   * @return True if the tensor was successfuly inserted in the pipeline, False
-   * if the pipeline was full
-   */
-  virtual bool enqueue(const Tensor& input, const int32_t* label = nullptr) = 0;
-
-  /**
-   * @brief Fetch the pipeline queue for eventual result. This function will pop
-   * one input from the pipeline queue, so it must be called once for each input
-   * @return A 3D Tensor output, or nullptr if no output is available
-   */
-  virtual TensorUniquePtr fetch() = 0;
-
-  /**
-   * @brief Transform an output Dense Tensor in the form of a Tensor of float
-   * values. It applies ONLY on outputs coming from programs that do not have an
-   * activation on the last layer.
-   * The output values are obtained from the outputs discrete potentials by
-   * applying a shift and a scale.
-   * */
-  virtual DenseUniquePtr dequantize(const Dense& potentials) = 0;
-
-  /**
-   * @brief Get the driver used by the device
-   * @return The HardwareDriver object used by the device
-   */
-  virtual HardwareDriver* driver() const = 0;
-};
-
-}  // namespace akida
+/*******************************************************************************
+ * Copyright 2021 Brainchip Holdings Ltd.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ ********************************************************************************
+ */
+
+#pragma once
+
+#include <cstdint>
+#include <memory>
+#include <vector>
+
+#include "akida/dense.h"
+#include "akida/hw_version.h"
+#include "akida/np.h"
+#include "akida/tensor.h"
+#include "infra/exports.h"
+#include "infra/hardware_driver.h"
+
+namespace akida {
+
+class HardwareDevice;
+
+using HardwareDevicePtr = std::shared_ptr<HardwareDevice>;
+using HardwareDeviceConstPtr = std::shared_ptr<const HardwareDevice>;
+
+/**
+ * class HardwareDevice
+ *
+ * Public interface to an Akida Hardware Device.
+ *
+ * The main difference with Akida Device is the fact that HardwareDevice
+ * objects driver real hardware, so they are capable of programming, performing
+ * inference and few other hardware-specific calls.
+ *
+ */
+class AKIDASHAREDLIB_EXPORT HardwareDevice {
+ public:
+  /**
+   * @brief Get the Device version
+   * @return a HwVersion
+   */
+  virtual HwVersion version() const = 0;
+
+  /**
+   * @brief Get the Device description
+   * @return a char*
+   */
+  virtual const char* desc() const = 0;
+
+  /**
+   * @brief Creates a Hardware Device
+   *
+   * @param driver : the driver that should be used by the hardware device
+   *
+   * @return a HardwareDevice
+   */
+  static HardwareDevicePtr create(HardwareDriver* driver);
+
+  /**
+   * @brief Toggle the HardwareDevice clock counter on/off
+   * @param enable : boolean to enable/disable clock counter
+   */
+  virtual void toggle_clock_counter(bool enable) = 0;
+
+  /**
+   * @brief Read the current HardwareDevice clock counter
+   * @return a uint32 representing the clock count (can overlap)
+   */
+  virtual uint32_t read_clock_counter() = 0;
+
+  /**
+   * @brief Read the current HardwareDevice configuration clock counter
+   * @return a uint32 representing the clock count (can overlap)
+   */
+  virtual uint32_t read_config_clock_counter() = 0;
+
+  /**
+   * @brief Return memory information
+   * @return a tuple containing current memory usage (in bytes) and top memory
+   * usage (in bytes)
+   */
+  using MemoryInfo = std::pair<uint32_t, uint32_t>;
+  virtual MemoryInfo memory() const = 0;
+
+  /**
+   * @brief Reset top memory usage to current one
+   */
+  virtual void reset_top_memory() = 0;
+
+  /**
+   * @brief Perform hardware device programming
+   * @param program : serialized buffer containing the Device program
+   * @param size : byte size of the buffer
+   */
+  virtual void program(const uint8_t* program, size_t size) = 0;
+
+  /**
+   * @brief Retrieve current program buffer
+   * @return a pair of pointer and size of current program
+   */
+  using BytesBuffer = std::pair<const uint8_t*, size_t>;
+  virtual const BytesBuffer& program() const = 0;
+
+  /**
+   * @brief Read the registers of a NP
+   * @param output : A pointer to a buffer to contain the registers read
+   * @param np : The NP to read
+   * @param nb_registers : The number of registers to read
+   */
+  virtual void read_np_registers(uint32_t* output, const struct np::Ident& np,
+                                 uint32_t nb_registers) = 0;
+
+  /**
+   * @brief Configure the number of inputs that can be sent at the same time
+   * (the number of enqueue calls without calling fetch). It is 15 max for a
+   * single pass program without learning, or 1 for multipass program and when
+   * learning is enabled. It will return the effective batch size applied.
+   * @param requested_batch_size : the requested batch size
+   * @param alloc_inputs : boolean to allocate memory for inputs. Required if
+   * inputs are not directly accessible from akida
+   * @return the effective batch size applied (can be lower than
+   * requested_batch_size)
+   */
+  virtual size_t set_batch_size(size_t requested_batch_size,
+                                bool allocate_inputs) = 0;
+
+  /**
+   * @brief Enable or disable learning mode of the current program
+   * @param learn_en : boolean to enable learning on the last layer
+   */
+  virtual void toggle_learn(bool learn_en) = 0;
+
+  /**
+   * @brief Tells if current program has learning enabled
+   * @return true if current program has learning enabled
+   */
+  virtual bool learn_enabled() const = 0;
+
+  /**
+   * @brief Tells current program learning memory size
+   * @return memory size, in number of 32 bit words
+   */
+  virtual size_t learn_mem_size() const = 0;
+
+  /**
+   * @brief Writes a copy of the learn memory of current program in the given
+   * buffer.
+   * @param output_buffer : A pointer to a buffer large enough to contain
+   * learning memory.
+   */
+  virtual void learn_mem(uint32_t* output_buffer) = 0;
+
+  std::vector<uint32_t> learn_mem() {
+    auto size = learn_mem_size();
+    std::vector<uint32_t> ret(size);
+    learn_mem(ret.data());
+    return ret;
+  }
+
+  /**
+   * @brief Updated learn memory from buffer containg a previously saved one
+   * @param input_buffer : A pointer to a buffer containing updated learning
+   * memory buffer
+   */
+  virtual void update_learn_mem(const uint32_t* input_buffer) = 0;
+
+  /**
+   * @brief Clear current program from hardware device, restoring its initial
+   * state
+   */
+  virtual void unprogram() = 0;
+
+  /**
+   * @brief Processes inputs to train on a programmed device
+   * @param inputs       : vector of 3D inputs Tensor
+   * @param input_labels : integer value labels of the input classes,
+   * for supervised learning
+   * @return Sparse or Dense outputs from the model last layer
+   */
+  virtual std::vector<TensorUniquePtr> fit(
+      const std::vector<TensorConstPtr>& inputs,
+      const std::vector<int32_t>& input_labels) = 0;
+
+  /**
+   * @brief Processes inputs on a programmed device
+   * @param inputs : vector of 3D inputs Tensor
+   * @return vector of 3D outputs from the device
+   */
+  virtual std::vector<TensorUniquePtr> forward(
+      const std::vector<TensorConstPtr>& inputs) = 0;
+
+  /**
+   * @brief Evaluates the results of processing on a programmed device
+   *
+   * This method propagates a set of inputs through a programmed device and
+   * returns the results in the form of a Tensor of float values.
+   * It applies ONLY on programs coming from models that do not have an
+   * activation on the last layer.
+   * The output values are obtained from the outputs discrete potentials by
+   * applying a shift and a scale.
+   *
+   * @param inputs : vector of 3D inputs Tensor
+   * @return vector of 3D rescaled output potentials from the programmed device
+   */
+  virtual std::vector<TensorUniquePtr> predict(
+      const std::vector<TensorConstPtr>& inputs) = 0;
+
+  /**
+   * @brief Try to put the input in the pipeline queue on a programmed device,
+   * starting queue execution if required
+   * @param input: 3D input Tensor
+   * @param label: integer value of the input class, for supervised learning
+   * @return True if the tensor was successfuly inserted in the pipeline, False
+   * if the pipeline was full
+   */
+  virtual bool enqueue(const Tensor& input, const int32_t* label = nullptr) = 0;
+
+  /**
+   * @brief Fetch the pipeline queue for eventual result. This function will pop
+   * one input from the pipeline queue, so it must be called once for each input
+   * @return A 3D Tensor output, or nullptr if no output is available
+   */
+  virtual TensorUniquePtr fetch() = 0;
+
+  /**
+   * @brief Transform an output Dense Tensor in the form of a Tensor of float
+   * values. It applies ONLY on outputs coming from programs that do not have an
+   * activation on the last layer.
+   * The output values are obtained from the outputs discrete potentials by
+   * applying a shift and a scale.
+   * */
+  virtual DenseUniquePtr dequantize(const Dense& potentials) = 0;
+
+  /**
+   * @brief Get the driver used by the device
+   * @return The HardwareDriver object used by the device
+   */
+  virtual HardwareDriver* driver() const = 0;
+};
+
+}  // namespace akida
```

## akida/engine/api/akida/hw_version.h

 * *Ordering differences only*

```diff
@@ -1,36 +1,36 @@
-#pragma once
-
-#include "infra/exports.h"
-#include "infra/hardware_driver.h"
-
-namespace akida {
-
-/**
- * The hardware version identifier
- * Vendor_id / Product_id / Major_rev / Minor_rev
- */
-struct AKIDASHAREDLIB_EXPORT HwVersion {
-  uint8_t vendor_id;
-  uint8_t product_id;
-  uint8_t major_rev;
-  uint8_t minor_rev;
-
-  bool operator==(const HwVersion& ref) const {
-    return (vendor_id == ref.vendor_id) && (product_id == ref.product_id) &&
-           (major_rev == ref.major_rev) && (minor_rev == ref.minor_rev);
-  }
-
-  bool operator!=(const HwVersion& ref) const { return !(*this == ref); }
-};
-
-static constexpr HwVersion NSoC_v1 = {0xBC, 0, 0, 1};
-static constexpr HwVersion NSoC_v2 = {0xBC, 0, 0, 2};
-static constexpr HwVersion TwoNodesIP_v1 = {0xBC, 0xA1, 3, 6};
-static constexpr HwVersion Latest = {0xBC, 0xA1, 3, 7};
-static constexpr HwVersion AKD500_v1 = {0xBC, 0xA1, 3, 9};
-
-// This method reads hardware version using the given driver
-AKIDASHAREDLIB_EXPORT
-HwVersion read_hw_version(const HardwareDriver& driver);
-
-}  // namespace akida
+#pragma once
+
+#include "infra/exports.h"
+#include "infra/hardware_driver.h"
+
+namespace akida {
+
+/**
+ * The hardware version identifier
+ * Vendor_id / Product_id / Major_rev / Minor_rev
+ */
+struct AKIDASHAREDLIB_EXPORT HwVersion {
+  uint8_t vendor_id;
+  uint8_t product_id;
+  uint8_t major_rev;
+  uint8_t minor_rev;
+
+  bool operator==(const HwVersion& ref) const {
+    return (vendor_id == ref.vendor_id) && (product_id == ref.product_id) &&
+           (major_rev == ref.major_rev) && (minor_rev == ref.minor_rev);
+  }
+
+  bool operator!=(const HwVersion& ref) const { return !(*this == ref); }
+};
+
+static constexpr HwVersion NSoC_v1 = {0xBC, 0, 0, 1};
+static constexpr HwVersion NSoC_v2 = {0xBC, 0, 0, 2};
+static constexpr HwVersion TwoNodesIP_v1 = {0xBC, 0xA1, 3, 6};
+static constexpr HwVersion Latest = {0xBC, 0xA1, 3, 7};
+static constexpr HwVersion AKD500_v1 = {0xBC, 0xA1, 3, 9};
+
+// This method reads hardware version using the given driver
+AKIDASHAREDLIB_EXPORT
+HwVersion read_hw_version(const HardwareDriver& driver);
+
+}  // namespace akida
```

## akida/engine/api/akida/input_conversion.h

 * *Ordering differences only*

```diff
@@ -1,23 +1,23 @@
-#pragma once
-
-#include <cstdint>
-
-#include "akida/dense.h"
-#include "akida/sparse.h"
-
-namespace akida {
-
-namespace conversion {
-
-const Sparse* as_sparse(const Tensor& input);
-
-SparseUniquePtr to_sparse(const Dense& input, const uint8_t* program);
-
-const Dense* as_dense(const Tensor& input);
-
-DenseUniquePtr to_dense(const Sparse& input);
-
-bool dense_input_expected(const uint8_t* program);
-
-}  // namespace conversion
-}  // namespace akida
+#pragma once
+
+#include <cstdint>
+
+#include "akida/dense.h"
+#include "akida/sparse.h"
+
+namespace akida {
+
+namespace conversion {
+
+const Sparse* as_sparse(const Tensor& input);
+
+SparseUniquePtr to_sparse(const Dense& input, const uint8_t* program);
+
+const Dense* as_dense(const Tensor& input);
+
+DenseUniquePtr to_dense(const Sparse& input);
+
+bool dense_input_expected(const uint8_t* program);
+
+}  // namespace conversion
+}  // namespace akida
```

## akida/engine/api/akida/np.h

 * *Ordering differences only*

```diff
@@ -1,48 +1,48 @@
-#pragma once
-
-#include <cstdint>
-#include <set>
-#include <vector>
-
-namespace akida {
-
-namespace np {
-
-struct Ident {
-  uint8_t col;
-  uint8_t row;
-  uint8_t id;
-  bool operator==(const Ident& other) const {
-    return col == other.col && row == other.row && id == other.id;
-  }
-  bool operator!=(const Ident& other) const { return !(*this == other); }
-  bool operator<(const Ident& other) const {
-    return (col < other.col) || ((col == other.col) && (row < other.row)) ||
-           ((col == other.col) && (row == other.row) && (id < other.id));
-  }
-};
-
-using IdentVector = std::vector<Ident>;
-
-constexpr Ident HRC_IDENT = Ident{0, 0, 0};
-
-enum class Type { HRC, CNP1, CNP2, FNP2, FNP3 };
-using Types = std::set<Type>;
-
-struct Info {
-  Ident ident;
-  Types types;
-};
-
-/**
- * The layout of a mesh of Neural Processors
- */
-struct Mesh {
-  np::Ident dma_event;       /**<The DMA event endpoint */
-  np::Ident dma_conf;        /**<The DMA configuration endpoint */
-  std::vector<np::Info> nps; /**<The available Neural Processors */
-};
-
-}  // namespace np
-
-}  // namespace akida
+#pragma once
+
+#include <cstdint>
+#include <set>
+#include <vector>
+
+namespace akida {
+
+namespace np {
+
+struct Ident {
+  uint8_t col;
+  uint8_t row;
+  uint8_t id;
+  bool operator==(const Ident& other) const {
+    return col == other.col && row == other.row && id == other.id;
+  }
+  bool operator!=(const Ident& other) const { return !(*this == other); }
+  bool operator<(const Ident& other) const {
+    return (col < other.col) || ((col == other.col) && (row < other.row)) ||
+           ((col == other.col) && (row == other.row) && (id < other.id));
+  }
+};
+
+using IdentVector = std::vector<Ident>;
+
+constexpr Ident HRC_IDENT = Ident{0, 0, 0};
+
+enum class Type { HRC, CNP1, CNP2, FNP2, FNP3 };
+using Types = std::set<Type>;
+
+struct Info {
+  Ident ident;
+  Types types;
+};
+
+/**
+ * The layout of a mesh of Neural Processors
+ */
+struct Mesh {
+  np::Ident dma_event;       /**<The DMA event endpoint */
+  np::Ident dma_conf;        /**<The DMA configuration endpoint */
+  std::vector<np::Info> nps; /**<The available Neural Processors */
+};
+
+}  // namespace np
+
+}  // namespace akida
```

## akida/engine/api/akida/program_memory_info.h

 * *Ordering differences only*

```diff
@@ -1,55 +1,55 @@
-#pragma once
-
-#include <cstddef>
-#include <cstdint>
-
-#include "infra/exports.h"
-
-namespace akida {
-
-/**
- * @brief Return the number of bytes required to store 1 input for the given
- * program
- * @param program: the program whose input size will be requested
- */
-AKIDASHAREDLIB_EXPORT
-size_t input_memory_required(const uint8_t* program);
-
-/**
- * @brief Return the number of bytes required to store 1 output for the given
- * program
- * @param program: the program whose output size will be requested
- */
-AKIDASHAREDLIB_EXPORT
-size_t output_memory_required(const uint8_t* program);
-
-/**
- * @brief Return the number of bytes required for 1 input descriptor for the
- * given program
- * @param program: the program whose required input dma size will be requested
- */
-AKIDASHAREDLIB_EXPORT
-size_t input_descriptor_memory_required(const uint8_t* program);
-
-/**
- * @brief Return the number of bytes required for program descriptors
- * @param program: the program whose descriptors required size will be requested
- */
-AKIDASHAREDLIB_EXPORT
-size_t program_descriptors_memory_required(const uint8_t* program);
-
-/**
- * @brief Return the number of bytes required for program
- * @param program: the program whose size will be requested
- */
-AKIDASHAREDLIB_EXPORT
-size_t program_data_memory_required(const uint8_t* program);
-
-/**
- * @brief Return the number of bytes required for extra program data
- * @param program: the program whose extra data size will be requested
- */
-AKIDASHAREDLIB_EXPORT
-size_t extra_program_memory_required(const uint8_t* program);
-
-}  // namespace akida
+#pragma once
+
+#include <cstddef>
+#include <cstdint>
+
+#include "infra/exports.h"
+
+namespace akida {
+
+/**
+ * @brief Return the number of bytes required to store 1 input for the given
+ * program
+ * @param program: the program whose input size will be requested
+ */
+AKIDASHAREDLIB_EXPORT
+size_t input_memory_required(const uint8_t* program);
+
+/**
+ * @brief Return the number of bytes required to store 1 output for the given
+ * program
+ * @param program: the program whose output size will be requested
+ */
+AKIDASHAREDLIB_EXPORT
+size_t output_memory_required(const uint8_t* program);
+
+/**
+ * @brief Return the number of bytes required for 1 input descriptor for the
+ * given program
+ * @param program: the program whose required input dma size will be requested
+ */
+AKIDASHAREDLIB_EXPORT
+size_t input_descriptor_memory_required(const uint8_t* program);
+
+/**
+ * @brief Return the number of bytes required for program descriptors
+ * @param program: the program whose descriptors required size will be requested
+ */
+AKIDASHAREDLIB_EXPORT
+size_t program_descriptors_memory_required(const uint8_t* program);
+
+/**
+ * @brief Return the number of bytes required for program
+ * @param program: the program whose size will be requested
+ */
+AKIDASHAREDLIB_EXPORT
+size_t program_data_memory_required(const uint8_t* program);
+
+/**
+ * @brief Return the number of bytes required for extra program data
+ * @param program: the program whose extra data size will be requested
+ */
+AKIDASHAREDLIB_EXPORT
+size_t extra_program_memory_required(const uint8_t* program);
+
+}  // namespace akida
```

## akida/engine/api/akida/registers_top_level.h

 * *Ordering differences only*

```diff
@@ -1,65 +1,65 @@
-#pragma once
-
-#include <cstdint>
-#include "infra/registers_common.h"
-
-namespace akida {
-
-static constexpr uint32_t REG_IP_VERSION = 0x0;
-static constexpr RegDetail MINOR_REV(0, 7);
-static constexpr RegDetail MAJOR_REV(8, 15);
-static constexpr RegDetail PROD_ID(16, 23);
-static constexpr RegDetail VENDOR_ID(24, 31);
-
-static constexpr uint32_t REG_GENERAL_CONTROL = 0x4;
-static constexpr RegDetail REWIND_MODE(0);
-static constexpr RegDetail PR_MESH_RST_END(1);
-static constexpr RegDetail AK_LOGIC_RST(8);
-static constexpr RegDetail AK_CORE_RST(9);
-static constexpr RegDetail AK_MESH_RST(10);
-static constexpr RegDetail SCC_CORE_RESET(12);
-static constexpr RegDetail AK_CORE_CLKPD(16);
-static constexpr RegDetail AK_C2C_USP_CLKPD(17);
-static constexpr RegDetail AK_C2C_DSP_CLKPD(18);
-static constexpr RegDetail SCC_CORE_CLKPD(20);
-
-// Mesh info registers definition
-static constexpr uint32_t REG_MESH_INFO1 = 0x50;
-static constexpr RegDetail MESH_ROWS(0, 7);
-static constexpr RegDetail MESH_COLS(8, 15);
-static constexpr RegDetail R1_START_COL(16, 23);
-static constexpr RegDetail R2_START_COL(24, 31);
-static constexpr uint32_t REG_MESH_INFO2 = 0x54;
-static constexpr RegDetail NP_PER_NODE(0, 2);
-static constexpr RegDetail DMA_NODE_EMPTY(4);
-static constexpr RegDetail DMA_NODE_ROW(8, 15);
-static constexpr RegDetail DMA_NODE_COL(16, 23);
-static constexpr RegDetail DMA_AE_NP(24, 27);
-static constexpr RegDetail DMA_CFG_NP(28, 31);
-static constexpr uint32_t REG_MESH_INFO3 = 0x58;
-static constexpr RegDetail FNP2_ROW(0, 7);
-static constexpr RegDetail FNP2_COL(8, 15);
-static constexpr RegDetail FNP2_NUM(16, 17);
-static constexpr RegDetail COL_NUM_LAST_NP(24, 31);
-
-// Interrupt controller registers
-static constexpr uint32_t INTERRUPT_CONTROLLER_OFFSET = 0x70000;
-static constexpr uint32_t REG_INTERRUPT_CONTROLLER_GENERAL_CONTROL =
-    INTERRUPT_CONTROLLER_OFFSET + 0x0;
-static constexpr RegDetail INTERRUPT_CONTROLLER_GENERAL_CONTROL_GLB_INT_EN(0);
-static constexpr uint32_t REG_INTERRUPT_CONTROLLER_SOURCE_MASK =
-    INTERRUPT_CONTROLLER_OFFSET + 0x4;
-static constexpr RegDetail REG_INTERRUPT_CONTROLLER_SOURCE_MASK_AEDMA(0);
-static constexpr RegDetail REG_INTERRUPT_CONTROLLER_SOURCE_MASK_AEIF(1);
-static constexpr RegDetail REG_INTERRUPT_CONTROLLER_SOURCE_MASK_CFGDMA(2);
-static constexpr RegDetail REG_INTERRUPT_CONTROLLER_SOURCE_MASK_CFGIF(3);
-static constexpr RegDetail REG_INTERRUPT_CONTROLLER_SOURCE_MASK_SCC_HRC(5);
-static constexpr uint32_t REG_INTERRUPT_CONTROLLER_SOURCE =
-    INTERRUPT_CONTROLLER_OFFSET + 0x8;
-static constexpr RegDetail REG_INTERRUPT_CONTROLLER_SOURCE_AEDMA(0);
-static constexpr RegDetail REG_INTERRUPT_CONTROLLER_SOURCE_AEIF(1);
-static constexpr RegDetail REG_INTERRUPT_CONTROLLER_SOURCE_CFGDMA(2);
-static constexpr RegDetail REG_INTERRUPT_CONTROLLER_SOURCE_CFGIF(3);
-static constexpr RegDetail REG_INTERRUPT_CONTROLLER_SOURCESCC_HRC(5);
-
-}  // namespace akida
+#pragma once
+
+#include <cstdint>
+#include "infra/registers_common.h"
+
+namespace akida {
+
+static constexpr uint32_t REG_IP_VERSION = 0x0;
+static constexpr RegDetail MINOR_REV(0, 7);
+static constexpr RegDetail MAJOR_REV(8, 15);
+static constexpr RegDetail PROD_ID(16, 23);
+static constexpr RegDetail VENDOR_ID(24, 31);
+
+static constexpr uint32_t REG_GENERAL_CONTROL = 0x4;
+static constexpr RegDetail REWIND_MODE(0);
+static constexpr RegDetail PR_MESH_RST_END(1);
+static constexpr RegDetail AK_LOGIC_RST(8);
+static constexpr RegDetail AK_CORE_RST(9);
+static constexpr RegDetail AK_MESH_RST(10);
+static constexpr RegDetail SCC_CORE_RESET(12);
+static constexpr RegDetail AK_CORE_CLKPD(16);
+static constexpr RegDetail AK_C2C_USP_CLKPD(17);
+static constexpr RegDetail AK_C2C_DSP_CLKPD(18);
+static constexpr RegDetail SCC_CORE_CLKPD(20);
+
+// Mesh info registers definition
+static constexpr uint32_t REG_MESH_INFO1 = 0x50;
+static constexpr RegDetail MESH_ROWS(0, 7);
+static constexpr RegDetail MESH_COLS(8, 15);
+static constexpr RegDetail R1_START_COL(16, 23);
+static constexpr RegDetail R2_START_COL(24, 31);
+static constexpr uint32_t REG_MESH_INFO2 = 0x54;
+static constexpr RegDetail NP_PER_NODE(0, 2);
+static constexpr RegDetail DMA_NODE_EMPTY(4);
+static constexpr RegDetail DMA_NODE_ROW(8, 15);
+static constexpr RegDetail DMA_NODE_COL(16, 23);
+static constexpr RegDetail DMA_AE_NP(24, 27);
+static constexpr RegDetail DMA_CFG_NP(28, 31);
+static constexpr uint32_t REG_MESH_INFO3 = 0x58;
+static constexpr RegDetail FNP2_ROW(0, 7);
+static constexpr RegDetail FNP2_COL(8, 15);
+static constexpr RegDetail FNP2_NUM(16, 17);
+static constexpr RegDetail COL_NUM_LAST_NP(24, 31);
+
+// Interrupt controller registers
+static constexpr uint32_t INTERRUPT_CONTROLLER_OFFSET = 0x70000;
+static constexpr uint32_t REG_INTERRUPT_CONTROLLER_GENERAL_CONTROL =
+    INTERRUPT_CONTROLLER_OFFSET + 0x0;
+static constexpr RegDetail INTERRUPT_CONTROLLER_GENERAL_CONTROL_GLB_INT_EN(0);
+static constexpr uint32_t REG_INTERRUPT_CONTROLLER_SOURCE_MASK =
+    INTERRUPT_CONTROLLER_OFFSET + 0x4;
+static constexpr RegDetail REG_INTERRUPT_CONTROLLER_SOURCE_MASK_AEDMA(0);
+static constexpr RegDetail REG_INTERRUPT_CONTROLLER_SOURCE_MASK_AEIF(1);
+static constexpr RegDetail REG_INTERRUPT_CONTROLLER_SOURCE_MASK_CFGDMA(2);
+static constexpr RegDetail REG_INTERRUPT_CONTROLLER_SOURCE_MASK_CFGIF(3);
+static constexpr RegDetail REG_INTERRUPT_CONTROLLER_SOURCE_MASK_SCC_HRC(5);
+static constexpr uint32_t REG_INTERRUPT_CONTROLLER_SOURCE =
+    INTERRUPT_CONTROLLER_OFFSET + 0x8;
+static constexpr RegDetail REG_INTERRUPT_CONTROLLER_SOURCE_AEDMA(0);
+static constexpr RegDetail REG_INTERRUPT_CONTROLLER_SOURCE_AEIF(1);
+static constexpr RegDetail REG_INTERRUPT_CONTROLLER_SOURCE_CFGDMA(2);
+static constexpr RegDetail REG_INTERRUPT_CONTROLLER_SOURCE_CFGIF(3);
+static constexpr RegDetail REG_INTERRUPT_CONTROLLER_SOURCESCC_HRC(5);
+
+}  // namespace akida
```

## akida/engine/api/akida/shape.h

 * *Ordering differences only*

```diff
@@ -1,145 +1,145 @@
-#pragma once
-
-#include <array>
-#include <cassert>
-#include <cstddef>
-#include <cstdint>
-#include <initializer_list>
-#include <limits>
-#include <vector>
-
-#include "infra/exports.h"
-#include "infra/system.h"
-
-namespace akida {
-
-/**
- * @brief An abstract type to represent dimensions
- */
-using Index = uint32_t;
-
-/**
- * @brief A class representing shapes (an array of dimensions). It can contain
- * from 1 to 4 dimensions, which are non zero values
- */
-class AKIDASHAREDLIB_EXPORT Shape {
-  using buffer_type = std::array<Index, 4>;
-
- public:
-  using iterator = buffer_type::iterator;
-  using const_iterator = buffer_type::const_iterator;
-
-  /**
-   * @brief Builds an empty shape
-   */
-  Shape() : data_{}, size_(0) {}
-  /**
-   * @brief Builds a shape from a list of values.
-   * It must have from 1 to 4 values (included). None of the values can be 0
-   */
-  Shape(const std::initializer_list<buffer_type::value_type>& values)
-      : Shape(values.begin(), values.size()) {}
-  /**
-   * @brief Builds a shape from an array of values.
-   * It must have from 1 to 4 values (included). None of the values can be 0
-   */
-  template<typename IntBuffer>
-  Shape(const IntBuffer* buffer, size_t nb_elems) : data_{}, size_(nb_elems) {
-    if (nb_elems == 0 || nb_elems > data_.size()) {
-      panic("Shape number of dimensions must be in range [1, 4]");
-    }
-    for (size_t i = 0; i < nb_elems; ++i) {
-      if (buffer[i] == 0) {
-        panic("Cannot have a shape with a dimension set to 0");
-      }
-      data_[i] = static_cast<buffer_type::value_type>(buffer[i]);
-    }
-  }
-
-  /**
-   * @brief Get an iterator to the 1st dimension
-   */
-  iterator begin() { return data_.begin(); }
-  /**
-   * @brief Get a const iterator to the 1st dimension
-   */
-  const_iterator begin() const { return data_.begin(); }
-  /**
-   * @brief Get an iterator to the last dimension
-   */
-  iterator end() { return begin() + size_; }
-  /**
-   * @brief Get a const iterator to the last dimension
-   */
-  const_iterator end() const { return begin() + size_; }
-
-  /**
-   * @brief Get the number of dimensions
-   */
-  size_t size() const { return size_; }
-  /**
-   * @brief Get the value of any dimension
-   */
-  buffer_type::value_type operator[](size_t dim_number) const {
-    assert(dim_number < size());
-    return *(begin() + dim_number);
-  }
-  /**
-   * @brief Get a pointer to the beginning of the shape
-   */
-  buffer_type::const_pointer data() const { return data_.data(); }
-  /**
-   * @brief Access to the 1st dimension
-   */
-  buffer_type::value_type front() const { return *begin(); }
-  /**
-   * @brief Access to the last dimension
-   */
-  buffer_type::value_type back() const { return *(begin() + size_ - 1); }
-
-  bool operator==(const Shape& other) const { return data_ == other.data_; }
-  bool operator!=(const Shape& other) const { return !(*this == other); }
-
- protected:
-  buffer_type data_;
-  size_t size_;
-};
-
-/**
- * @brief Returns the total size of the shape (product of its dimensions)
- */
-inline uint32_t shape_size(const Shape& s) {
-  uint64_t size = 1;
-  for (auto dim : s) {
-    size *= dim;
-  }
-  constexpr size_t max_size = std::numeric_limits<uint32_t>::max();
-  if (size > max_size) {
-    panic("Tensor shape size %lu exceeds maximum shape size (%u)", size,
-          max_size);
-  }
-  return static_cast<uint32_t>(size);
-}
-
-/**
- * @brief Returns the linear index for the given coords and strides
- */
-template<typename T>
-inline size_t linear_index(const T* coords,
-                           const std::vector<uint32_t>& strides) {
-  size_t index = 0;
-  for (size_t i = 0; i < strides.size(); ++i) {
-    index += coords[i] * strides[i];
-  }
-  return index;
-}
-
-/**
- * @brief Returns the linear index for the given coords and strides
- */
-inline size_t linear_index(const std::vector<Index>& coords,
-                           const std::vector<uint32_t>& strides) {
-  return linear_index(coords.data(), strides);
-}
-
-}  // namespace akida
+#pragma once
+
+#include <array>
+#include <cassert>
+#include <cstddef>
+#include <cstdint>
+#include <initializer_list>
+#include <limits>
+#include <vector>
+
+#include "infra/exports.h"
+#include "infra/system.h"
+
+namespace akida {
+
+/**
+ * @brief An abstract type to represent dimensions
+ */
+using Index = uint32_t;
+
+/**
+ * @brief A class representing shapes (an array of dimensions). It can contain
+ * from 1 to 4 dimensions, which are non zero values
+ */
+class AKIDASHAREDLIB_EXPORT Shape {
+  using buffer_type = std::array<Index, 4>;
+
+ public:
+  using iterator = buffer_type::iterator;
+  using const_iterator = buffer_type::const_iterator;
+
+  /**
+   * @brief Builds an empty shape
+   */
+  Shape() : data_{}, size_(0) {}
+  /**
+   * @brief Builds a shape from a list of values.
+   * It must have from 1 to 4 values (included). None of the values can be 0
+   */
+  Shape(const std::initializer_list<buffer_type::value_type>& values)
+      : Shape(values.begin(), values.size()) {}
+  /**
+   * @brief Builds a shape from an array of values.
+   * It must have from 1 to 4 values (included). None of the values can be 0
+   */
+  template<typename IntBuffer>
+  Shape(const IntBuffer* buffer, size_t nb_elems) : data_{}, size_(nb_elems) {
+    if (nb_elems == 0 || nb_elems > data_.size()) {
+      panic("Shape number of dimensions must be in range [1, 4]");
+    }
+    for (size_t i = 0; i < nb_elems; ++i) {
+      if (buffer[i] == 0) {
+        panic("Cannot have a shape with a dimension set to 0");
+      }
+      data_[i] = static_cast<buffer_type::value_type>(buffer[i]);
+    }
+  }
+
+  /**
+   * @brief Get an iterator to the 1st dimension
+   */
+  iterator begin() { return data_.begin(); }
+  /**
+   * @brief Get a const iterator to the 1st dimension
+   */
+  const_iterator begin() const { return data_.begin(); }
+  /**
+   * @brief Get an iterator to the last dimension
+   */
+  iterator end() { return begin() + size_; }
+  /**
+   * @brief Get a const iterator to the last dimension
+   */
+  const_iterator end() const { return begin() + size_; }
+
+  /**
+   * @brief Get the number of dimensions
+   */
+  size_t size() const { return size_; }
+  /**
+   * @brief Get the value of any dimension
+   */
+  buffer_type::value_type operator[](size_t dim_number) const {
+    assert(dim_number < size());
+    return *(begin() + dim_number);
+  }
+  /**
+   * @brief Get a pointer to the beginning of the shape
+   */
+  buffer_type::const_pointer data() const { return data_.data(); }
+  /**
+   * @brief Access to the 1st dimension
+   */
+  buffer_type::value_type front() const { return *begin(); }
+  /**
+   * @brief Access to the last dimension
+   */
+  buffer_type::value_type back() const { return *(begin() + size_ - 1); }
+
+  bool operator==(const Shape& other) const { return data_ == other.data_; }
+  bool operator!=(const Shape& other) const { return !(*this == other); }
+
+ protected:
+  buffer_type data_;
+  size_t size_;
+};
+
+/**
+ * @brief Returns the total size of the shape (product of its dimensions)
+ */
+inline uint32_t shape_size(const Shape& s) {
+  uint64_t size = 1;
+  for (auto dim : s) {
+    size *= dim;
+  }
+  constexpr size_t max_size = std::numeric_limits<uint32_t>::max();
+  if (size > max_size) {
+    panic("Tensor shape size %lu exceeds maximum shape size (%u)", size,
+          max_size);
+  }
+  return static_cast<uint32_t>(size);
+}
+
+/**
+ * @brief Returns the linear index for the given coords and strides
+ */
+template<typename T>
+inline size_t linear_index(const T* coords,
+                           const std::vector<uint32_t>& strides) {
+  size_t index = 0;
+  for (size_t i = 0; i < strides.size(); ++i) {
+    index += coords[i] * strides[i];
+  }
+  return index;
+}
+
+/**
+ * @brief Returns the linear index for the given coords and strides
+ */
+inline size_t linear_index(const std::vector<Index>& coords,
+                           const std::vector<uint32_t>& strides) {
+  return linear_index(coords.data(), strides);
+}
+
+}  // namespace akida
```

## akida/engine/api/akida/sparse.h

 * *Ordering differences only*

```diff
@@ -1,106 +1,106 @@
-#pragma once
-
-#include <cstddef>
-#include <memory>
-#include <vector>
-
-#include "akida/dense.h"
-#include "akida/shape.h"
-#include "akida/tensor.h"
-#include "infra/exports.h"
-
-/** file akida/sparse.h
- * Contains the abstract Sparse object and its related types
- */
-
-namespace akida {
-
-/**
- * @brief A shared pointer to a Sparse object
- */
-using SparsePtr = std::shared_ptr<Sparse>;
-
-/**
- * @brief A shared pointer to a const Sparse object
- */
-using SparseConstPtr = std::shared_ptr<const Sparse>;
-
-/**
- * @bried A unique pointer to a Sparse object
- */
-using SparseUniquePtr = std::unique_ptr<Sparse>;
-
-namespace sparse {
-
-/**
- * class SparseIterator
- *
- * Allows to iterate over the items of a Sparse
- *
- * To iterate over the items of a n-dimensionsal Sparse, one would typically
- * call the begin() member to obtain an iterator giving access to the first
- * item of the Sparse, and then iteratively call SparseIterator::next() to get
- * other items. When next() goes past the last item, SparseIterator::end() will
- * return true.
- *
- * The coordinates of each item are available as a vector of Index.
- *
- * The value of each item is available as a raw bytes pointer that needs to
- * be cast explicitly to the correct tensor type.
- *
- */
-class Iterator {
- public:
-  /**
-   * @brief Returns the current item coordinates
-   */
-  virtual std::vector<Index> coords() const = 0;
-  /**
-   * @brief Returns a pointer to the current item value bytes buffer
-   */
-  virtual const char* bytes() const = 0;
-  /**
-   * @brief Unravel the coordinates into a linear index
-   *
-   * @param strides :  the strides applied to each dimension
-   */
-  virtual size_t unravel(const std::vector<uint32_t>& strides) const = 0;
-  /**
-   * @brief Returns the current item value
-   */
-  template<typename T>
-  T value() const {
-    return *reinterpret_cast<const T*>(bytes());
-  }
-  /**
-   * @brief Move the iterator to the next Sparse item
-   */
-  virtual void next() = 0;
-  /**
-   * @brief Check if the iterator reached the end of the Sparse items
-   */
-  virtual bool end() const = 0;
-};
-using IteratorPtr = std::shared_ptr<Iterator>;
-
-}  // namespace sparse
-
-/**
- * class Sparse
- *
- * An abstraction of a multi-dimensional sparse array
- *
- * Contains a list of (coordinates, data) tuples.
- *
- */
-class AKIDASHAREDLIB_EXPORT Sparse : public Tensor {
- public:
-  bool operator==(const Tensor& ref) const override;
-
-  /**
-   * @brief Returns an iterator on Sparse items
-   */
-  virtual sparse::IteratorPtr begin() const = 0;
-};
-
-}  // namespace akida
+#pragma once
+
+#include <cstddef>
+#include <memory>
+#include <vector>
+
+#include "akida/dense.h"
+#include "akida/shape.h"
+#include "akida/tensor.h"
+#include "infra/exports.h"
+
+/** file akida/sparse.h
+ * Contains the abstract Sparse object and its related types
+ */
+
+namespace akida {
+
+/**
+ * @brief A shared pointer to a Sparse object
+ */
+using SparsePtr = std::shared_ptr<Sparse>;
+
+/**
+ * @brief A shared pointer to a const Sparse object
+ */
+using SparseConstPtr = std::shared_ptr<const Sparse>;
+
+/**
+ * @bried A unique pointer to a Sparse object
+ */
+using SparseUniquePtr = std::unique_ptr<Sparse>;
+
+namespace sparse {
+
+/**
+ * class SparseIterator
+ *
+ * Allows to iterate over the items of a Sparse
+ *
+ * To iterate over the items of a n-dimensionsal Sparse, one would typically
+ * call the begin() member to obtain an iterator giving access to the first
+ * item of the Sparse, and then iteratively call SparseIterator::next() to get
+ * other items. When next() goes past the last item, SparseIterator::end() will
+ * return true.
+ *
+ * The coordinates of each item are available as a vector of Index.
+ *
+ * The value of each item is available as a raw bytes pointer that needs to
+ * be cast explicitly to the correct tensor type.
+ *
+ */
+class Iterator {
+ public:
+  /**
+   * @brief Returns the current item coordinates
+   */
+  virtual std::vector<Index> coords() const = 0;
+  /**
+   * @brief Returns a pointer to the current item value bytes buffer
+   */
+  virtual const char* bytes() const = 0;
+  /**
+   * @brief Unravel the coordinates into a linear index
+   *
+   * @param strides :  the strides applied to each dimension
+   */
+  virtual size_t unravel(const std::vector<uint32_t>& strides) const = 0;
+  /**
+   * @brief Returns the current item value
+   */
+  template<typename T>
+  T value() const {
+    return *reinterpret_cast<const T*>(bytes());
+  }
+  /**
+   * @brief Move the iterator to the next Sparse item
+   */
+  virtual void next() = 0;
+  /**
+   * @brief Check if the iterator reached the end of the Sparse items
+   */
+  virtual bool end() const = 0;
+};
+using IteratorPtr = std::shared_ptr<Iterator>;
+
+}  // namespace sparse
+
+/**
+ * class Sparse
+ *
+ * An abstraction of a multi-dimensional sparse array
+ *
+ * Contains a list of (coordinates, data) tuples.
+ *
+ */
+class AKIDASHAREDLIB_EXPORT Sparse : public Tensor {
+ public:
+  bool operator==(const Tensor& ref) const override;
+
+  /**
+   * @brief Returns an iterator on Sparse items
+   */
+  virtual sparse::IteratorPtr begin() const = 0;
+};
+
+}  // namespace akida
```

## akida/engine/api/akida/tensor.h

 * *Ordering differences only*

```diff
@@ -1,266 +1,266 @@
-#pragma once
-
-#include <cstddef>
-#include <cstdint>
-#include <memory>
-#include <typeindex>
-#include <vector>
-
-#include "akida/shape.h"
-#include "infra/exports.h"
-
-#include "infra/system.h"
-
-/** file akida/tensor.h
- * Contains the abstract Tensor object and its related types
- */
-
-namespace akida {
-
-class Tensor;
-class Dense;
-class Sparse;
-
-/**
- * @brief A shared pointer to a Tensor object
- */
-using TensorPtr = std::shared_ptr<Tensor>;
-
-/**
- * @brief A shared pointer to a const Tensor object
- */
-using TensorConstPtr = std::shared_ptr<const Tensor>;
-
-/**
- * @brief A unique pointer to a Tensor object
- */
-using TensorUniquePtr = std::unique_ptr<Tensor>;
-
-/**
- * @enum  TensorType
- * @brief The data type of a Tensor
- */
-enum class TensorType {
-  int32 /**<Signed 32 bits integer*/,
-  float32 /**<32 bits floating point number*/,
-  uint8 /**<Unsigned 8 bits integer*/,
-  int8 /**<Signed 8 bits integer*/,
-  int4 /**<Signed 4 bits integer, range: [-7, 7] */,
-  int2 /**<Signed 2 bits integer, range: [-1, 1] */,
-  uint4 /**<Unsigned 4 bits integer, range: [0, 15] */,
-  uint2 /**<Unsigned 2 bits integer, range: [0, 3] */,
-  bit /**<Binary, range: [0, 1] */,
-};
-
-/**
- * @brief Get the byte size of a TensorType
- * @return the TensorType size in bytes
- */
-inline size_t tensor_type_size(TensorType type) {
-  size_t type_size = 0;
-  switch (type) {
-    case TensorType::int32: {
-      type_size = sizeof(int32_t);
-      break;
-    }
-    case TensorType::float32: {
-      type_size = sizeof(float);
-      break;
-    }
-    case TensorType::uint8:
-    case TensorType::uint4:
-    case TensorType::uint2:
-    case TensorType::bit: {
-      type_size = sizeof(uint8_t);
-      break;
-    }
-    case TensorType::int8:
-    case TensorType::int4:
-    case TensorType::int2: {
-      type_size = sizeof(int8_t);
-      break;
-    }
-    default: {
-      panic("Unsupported Tensor type");
-    }
-  }
-  return type_size;
-}
-
-/**
- * class Tensor
- *
- * An abstraction of a multi-dimensional array
- *
- */
-class AKIDASHAREDLIB_EXPORT Tensor {
- public:
-  virtual ~Tensor() {}
-
-  /**
-   * @brief Returns the tensor data type
-   */
-  virtual TensorType type() const = 0;
-
-  /**
-   * @brief Returns the tensor number of data elements
-   */
-  virtual size_t size() const = 0;
-
-  /**
-   * @brief Returns the tensor dimensions
-   */
-  virtual Shape dimensions() const = 0;
-
-  /**
-   * @brief Returns a human-readable representation of a Tensor data type
-   */
-  static const char* type_name(TensorType type) {
-    switch (type) {
-      case TensorType::int32: {
-        return "int32";
-      }
-      case TensorType::float32: {
-        return "float32";
-      }
-      case TensorType::uint8: {
-        return "uint8";
-      }
-      case TensorType::uint4: {
-        return "uint4";
-      }
-      case TensorType::uint2: {
-        return "uint2";
-      }
-      case TensorType::bit: {
-        return "bit";
-      }
-      case TensorType::int8: {
-        return "int8";
-      }
-      case TensorType::int4: {
-        return "int4";
-      }
-      case TensorType::int2: {
-        return "int2";
-        break;
-      }
-    }
-    return "unknown";
-  }
-
-  /**
-   * @brief Returns True if the Tensor has the specified templated data type
-   */
-  template<typename T>
-  bool has_type() const {
-    std::type_index type_T = std::type_index(typeid(T));
-    std::type_index type = std::type_index(typeid(void));
-    auto this_type = this->type();
-    switch (this_type) {
-      case TensorType::int32:
-        type = std::type_index(typeid(int32_t));
-        break;
-      case TensorType::float32:
-        type = std::type_index(typeid(float));
-        break;
-      // NOTE: all uint types that fit in 8 bit are handled by the same C++
-      // type, similarly for signed types. Only exception is bit, because 1
-      // bit weights were historically handled in a int8_t.
-      case TensorType::uint8:
-      case TensorType::uint4:
-      case TensorType::uint2:
-        type = std::type_index(typeid(uint8_t));
-        break;
-      case TensorType::int8:
-      case TensorType::int4:
-      case TensorType::int2:
-      case TensorType::bit:
-        type = std::type_index(typeid(int8_t));
-        break;
-      default:
-        break;
-    }
-    return (type_T == type);
-  }
-
-  /**
-   * @brief Throw an exception if the Tensor doesn't have the specified
-   * templated data type
-   */
-  template<typename T>
-  void check_type() const {
-    if (!has_type<T>()) {
-      panic("Wrong requested type %s for a tensor of type %s.",
-            typeid(T).name(), type_name(type()));
-    }
-  }
-
-  virtual bool operator==(const Tensor& ref) const = 0;
-
-  class Buffer {
-   public:
-    virtual ~Buffer() = default;
-
-    /**
-     * @brief Returns the size of the buffer data in bytes
-     */
-    virtual size_t size() const = 0;
-    /**
-     * @brief Returns a raw pointer to the buffer data
-     */
-    virtual char* data() = 0;
-    /**
-     * @brief Returns a raw pointer to the buffer data
-     */
-    virtual const char* data() const = 0;
-  };
-
-  /**
-   * @brief Returns the underlying buffer
-   */
-  virtual Buffer* buffer() = 0;
-
-  /**
-   * @brief Returns the underlying buffer
-   */
-  virtual const Buffer* buffer() const = 0;
-
-  /**
-   * @brief Returns a data pointer corresponding to the specified templated type
-   */
-  template<typename T>
-  T* data() {
-    check_type<T>();
-    return reinterpret_cast<T*>(buffer()->data());
-  }
-
-  /**
-   * @brief Returns a data pointer corresponding to the specified templated type
-   */
-  template<typename T>
-  const T* data() const {
-    check_type<T>();
-    return reinterpret_cast<const T*>(buffer()->data());
-  }
-
-  /**
-   * @brief Downcast a Tensor to a Dense
-   * @return : a pointer to the underlying Dense or a null pointer
-   */
-  static std::shared_ptr<const Dense> as_dense(TensorConstPtr tensor);
-
-  /**
-   * @brief Downcast a Tensor to a Sparse
-   * @return : a pointer to the underlying Sparse or a null pointer
-   */
-  static std::shared_ptr<const Sparse> as_sparse(TensorConstPtr tensor);
-
-  /**
-   * @brief Downcast a Tensor to a Dense or create a Dense copy
-   * @return : a pointer to the underlying Dense
-   */
-  static std::shared_ptr<const Dense> ensure_dense(TensorConstPtr tensor);
-};
-
-}  // namespace akida
+#pragma once
+
+#include <cstddef>
+#include <cstdint>
+#include <memory>
+#include <typeindex>
+#include <vector>
+
+#include "akida/shape.h"
+#include "infra/exports.h"
+
+#include "infra/system.h"
+
+/** file akida/tensor.h
+ * Contains the abstract Tensor object and its related types
+ */
+
+namespace akida {
+
+class Tensor;
+class Dense;
+class Sparse;
+
+/**
+ * @brief A shared pointer to a Tensor object
+ */
+using TensorPtr = std::shared_ptr<Tensor>;
+
+/**
+ * @brief A shared pointer to a const Tensor object
+ */
+using TensorConstPtr = std::shared_ptr<const Tensor>;
+
+/**
+ * @brief A unique pointer to a Tensor object
+ */
+using TensorUniquePtr = std::unique_ptr<Tensor>;
+
+/**
+ * @enum  TensorType
+ * @brief The data type of a Tensor
+ */
+enum class TensorType {
+  int32 /**<Signed 32 bits integer*/,
+  float32 /**<32 bits floating point number*/,
+  uint8 /**<Unsigned 8 bits integer*/,
+  int8 /**<Signed 8 bits integer*/,
+  int4 /**<Signed 4 bits integer, range: [-7, 7] */,
+  int2 /**<Signed 2 bits integer, range: [-1, 1] */,
+  uint4 /**<Unsigned 4 bits integer, range: [0, 15] */,
+  uint2 /**<Unsigned 2 bits integer, range: [0, 3] */,
+  bit /**<Binary, range: [0, 1] */,
+};
+
+/**
+ * @brief Get the byte size of a TensorType
+ * @return the TensorType size in bytes
+ */
+inline size_t tensor_type_size(TensorType type) {
+  size_t type_size = 0;
+  switch (type) {
+    case TensorType::int32: {
+      type_size = sizeof(int32_t);
+      break;
+    }
+    case TensorType::float32: {
+      type_size = sizeof(float);
+      break;
+    }
+    case TensorType::uint8:
+    case TensorType::uint4:
+    case TensorType::uint2:
+    case TensorType::bit: {
+      type_size = sizeof(uint8_t);
+      break;
+    }
+    case TensorType::int8:
+    case TensorType::int4:
+    case TensorType::int2: {
+      type_size = sizeof(int8_t);
+      break;
+    }
+    default: {
+      panic("Unsupported Tensor type");
+    }
+  }
+  return type_size;
+}
+
+/**
+ * class Tensor
+ *
+ * An abstraction of a multi-dimensional array
+ *
+ */
+class AKIDASHAREDLIB_EXPORT Tensor {
+ public:
+  virtual ~Tensor() {}
+
+  /**
+   * @brief Returns the tensor data type
+   */
+  virtual TensorType type() const = 0;
+
+  /**
+   * @brief Returns the tensor number of data elements
+   */
+  virtual size_t size() const = 0;
+
+  /**
+   * @brief Returns the tensor dimensions
+   */
+  virtual Shape dimensions() const = 0;
+
+  /**
+   * @brief Returns a human-readable representation of a Tensor data type
+   */
+  static const char* type_name(TensorType type) {
+    switch (type) {
+      case TensorType::int32: {
+        return "int32";
+      }
+      case TensorType::float32: {
+        return "float32";
+      }
+      case TensorType::uint8: {
+        return "uint8";
+      }
+      case TensorType::uint4: {
+        return "uint4";
+      }
+      case TensorType::uint2: {
+        return "uint2";
+      }
+      case TensorType::bit: {
+        return "bit";
+      }
+      case TensorType::int8: {
+        return "int8";
+      }
+      case TensorType::int4: {
+        return "int4";
+      }
+      case TensorType::int2: {
+        return "int2";
+        break;
+      }
+    }
+    return "unknown";
+  }
+
+  /**
+   * @brief Returns True if the Tensor has the specified templated data type
+   */
+  template<typename T>
+  bool has_type() const {
+    std::type_index type_T = std::type_index(typeid(T));
+    std::type_index type = std::type_index(typeid(void));
+    auto this_type = this->type();
+    switch (this_type) {
+      case TensorType::int32:
+        type = std::type_index(typeid(int32_t));
+        break;
+      case TensorType::float32:
+        type = std::type_index(typeid(float));
+        break;
+      // NOTE: all uint types that fit in 8 bit are handled by the same C++
+      // type, similarly for signed types. Only exception is bit, because 1
+      // bit weights were historically handled in a int8_t.
+      case TensorType::uint8:
+      case TensorType::uint4:
+      case TensorType::uint2:
+        type = std::type_index(typeid(uint8_t));
+        break;
+      case TensorType::int8:
+      case TensorType::int4:
+      case TensorType::int2:
+      case TensorType::bit:
+        type = std::type_index(typeid(int8_t));
+        break;
+      default:
+        break;
+    }
+    return (type_T == type);
+  }
+
+  /**
+   * @brief Throw an exception if the Tensor doesn't have the specified
+   * templated data type
+   */
+  template<typename T>
+  void check_type() const {
+    if (!has_type<T>()) {
+      panic("Wrong requested type %s for a tensor of type %s.",
+            typeid(T).name(), type_name(type()));
+    }
+  }
+
+  virtual bool operator==(const Tensor& ref) const = 0;
+
+  class Buffer {
+   public:
+    virtual ~Buffer() = default;
+
+    /**
+     * @brief Returns the size of the buffer data in bytes
+     */
+    virtual size_t size() const = 0;
+    /**
+     * @brief Returns a raw pointer to the buffer data
+     */
+    virtual char* data() = 0;
+    /**
+     * @brief Returns a raw pointer to the buffer data
+     */
+    virtual const char* data() const = 0;
+  };
+
+  /**
+   * @brief Returns the underlying buffer
+   */
+  virtual Buffer* buffer() = 0;
+
+  /**
+   * @brief Returns the underlying buffer
+   */
+  virtual const Buffer* buffer() const = 0;
+
+  /**
+   * @brief Returns a data pointer corresponding to the specified templated type
+   */
+  template<typename T>
+  T* data() {
+    check_type<T>();
+    return reinterpret_cast<T*>(buffer()->data());
+  }
+
+  /**
+   * @brief Returns a data pointer corresponding to the specified templated type
+   */
+  template<typename T>
+  const T* data() const {
+    check_type<T>();
+    return reinterpret_cast<const T*>(buffer()->data());
+  }
+
+  /**
+   * @brief Downcast a Tensor to a Dense
+   * @return : a pointer to the underlying Dense or a null pointer
+   */
+  static std::shared_ptr<const Dense> as_dense(TensorConstPtr tensor);
+
+  /**
+   * @brief Downcast a Tensor to a Sparse
+   * @return : a pointer to the underlying Sparse or a null pointer
+   */
+  static std::shared_ptr<const Sparse> as_sparse(TensorConstPtr tensor);
+
+  /**
+   * @brief Downcast a Tensor to a Dense or create a Dense copy
+   * @return : a pointer to the underlying Dense
+   */
+  static std::shared_ptr<const Dense> ensure_dense(TensorConstPtr tensor);
+};
+
+}  // namespace akida
```

## akida/engine/api/akida/version.h

 * *Ordering differences only*

```diff
@@ -1,9 +1,9 @@
-#pragma once
-
-#include "infra/exports.h"
-
-namespace akida {
-
-AKIDASHAREDLIB_EXPORT const char* version();
-
-}  // namespace akida
+#pragma once
+
+#include "infra/exports.h"
+
+namespace akida {
+
+AKIDASHAREDLIB_EXPORT const char* version();
+
+}  // namespace akida
```

## akida/engine/api/infra/exports.h

 * *Ordering differences only*

```diff
@@ -1,9 +1,9 @@
-#pragma once
-
-#ifdef _WIN32
-#define AKIDASHAREDLIB_EXPORT __declspec(dllexport)
-#elif (__GNUC__ || __clang__)
-#define AKIDASHAREDLIB_EXPORT __attribute__((visibility("default")))
-#else
-#define AKIDASHAREDLIB_EXPORT
-#endif
+#pragma once
+
+#ifdef _WIN32
+#define AKIDASHAREDLIB_EXPORT __declspec(dllexport)
+#elif (__GNUC__ || __clang__)
+#define AKIDASHAREDLIB_EXPORT __attribute__((visibility("default")))
+#else
+#define AKIDASHAREDLIB_EXPORT
+#endif
```

## akida/engine/api/infra/hardware_driver.h

 * *Ordering differences only*

```diff
@@ -1,83 +1,83 @@
-#pragma once
-
-#include <cstddef>
-#include <cstdint>
-#include <vector>
-
-#include "infra/exports.h"
-
-namespace akida {
-
-class BlockDevice {
- public:
-  virtual ~BlockDevice() = default;
-  /**
-   * @brief read operation.
-   * @param address: address where data should be read
-   * @param data: pointer data that will store the result
-   * @param size: size data to be read
-   */
-  virtual void read(uint32_t address, void* data, size_t size) const = 0;
-
-  /**
-   * @brief read operation.
-   * @param address: address where data should be read
-   */
-  uint32_t read32(uint32_t address) const {
-    uint32_t ret;
-    read(address, &ret, sizeof(uint32_t));
-    return ret;
-  }
-
-  /**
-   * @brief write operation
-   * @param address: address where data should be written
-   * @param data: pointer data to be written
-   * @param size: data size in number of 32 bit words
-   */
-  virtual void write(uint32_t address, const void* data, size_t size) = 0;
-
-  /**
-   * @brief write operation
-   * @param address: address where data should be written
-   * @param data: uint32_t data value to be written
-   */
-  void write32(uint32_t address, const uint32_t data) {
-    write(address, &data, sizeof(uint32_t));
-  }
-};
-
-class HardwareDriver : public BlockDevice {
- public:
-  /**
-   * @brief Return a null terminated string with driver description.
-   */
-  virtual const char* desc() const = 0;
-
-  /**
-   * @brief Return address used for scratch memory.
-   */
-  virtual uint32_t scratch_memory() const = 0;
-
-  /**
-   * @brief Return size (in bytes) available as scratch memory.
-   */
-  virtual uint32_t scratch_size() const = 0;
-
-  /**
-   * @brief Return address used for top level registers.
-   */
-  virtual uint32_t top_level_reg() const = 0;
-
-  /**
-   * @brief return the address of data that are directly accessible by akida
-   */
-  virtual uint32_t akida_visible_memory() const = 0;
-  /**
-   * @brief return the size (in bytes) of data that are directly accessible by
-   * akida
-   */
-  virtual uint32_t akida_visible_memory_size() const = 0;
-};
-
-}  // namespace akida
+#pragma once
+
+#include <cstddef>
+#include <cstdint>
+#include <vector>
+
+#include "infra/exports.h"
+
+namespace akida {
+
+class BlockDevice {
+ public:
+  virtual ~BlockDevice() = default;
+  /**
+   * @brief read operation.
+   * @param address: address where data should be read
+   * @param data: pointer data that will store the result
+   * @param size: size data to be read
+   */
+  virtual void read(uint32_t address, void* data, size_t size) const = 0;
+
+  /**
+   * @brief read operation.
+   * @param address: address where data should be read
+   */
+  uint32_t read32(uint32_t address) const {
+    uint32_t ret;
+    read(address, &ret, sizeof(uint32_t));
+    return ret;
+  }
+
+  /**
+   * @brief write operation
+   * @param address: address where data should be written
+   * @param data: pointer data to be written
+   * @param size: data size in number of 32 bit words
+   */
+  virtual void write(uint32_t address, const void* data, size_t size) = 0;
+
+  /**
+   * @brief write operation
+   * @param address: address where data should be written
+   * @param data: uint32_t data value to be written
+   */
+  void write32(uint32_t address, const uint32_t data) {
+    write(address, &data, sizeof(uint32_t));
+  }
+};
+
+class HardwareDriver : public BlockDevice {
+ public:
+  /**
+   * @brief Return a null terminated string with driver description.
+   */
+  virtual const char* desc() const = 0;
+
+  /**
+   * @brief Return address used for scratch memory.
+   */
+  virtual uint32_t scratch_memory() const = 0;
+
+  /**
+   * @brief Return size (in bytes) available as scratch memory.
+   */
+  virtual uint32_t scratch_size() const = 0;
+
+  /**
+   * @brief Return address used for top level registers.
+   */
+  virtual uint32_t top_level_reg() const = 0;
+
+  /**
+   * @brief return the address of data that are directly accessible by akida
+   */
+  virtual uint32_t akida_visible_memory() const = 0;
+  /**
+   * @brief return the size (in bytes) of data that are directly accessible by
+   * akida
+   */
+  virtual uint32_t akida_visible_memory_size() const = 0;
+};
+
+}  // namespace akida
```

## akida/engine/api/infra/int_ops.h

 * *Ordering differences only*

```diff
@@ -1,26 +1,26 @@
-#pragma once
-
-#include <cstdint>
-
-namespace akida {
-
-/**
- * @brief Perform a division and a ceiling on the result.
- * @param n numerator
- * @param d denominator
- */
-inline constexpr uint32_t div_round_up(uint32_t n, uint32_t d) {
-  return static_cast<uint32_t>((n + d - 1) / d);
-}
-
-/**
- * @brief Increases an integer value until evenly divisible by a given alignment
- * value.
- * @param v input value
- * @param alignment alignment value
- */
-inline constexpr uint32_t align_up(uint32_t v, uint32_t alignment) {
-  return div_round_up(v, alignment) * alignment;
-}
-
-}  // namespace akida
+#pragma once
+
+#include <cstdint>
+
+namespace akida {
+
+/**
+ * @brief Perform a division and a ceiling on the result.
+ * @param n numerator
+ * @param d denominator
+ */
+inline constexpr uint32_t div_round_up(uint32_t n, uint32_t d) {
+  return static_cast<uint32_t>((n + d - 1) / d);
+}
+
+/**
+ * @brief Increases an integer value until evenly divisible by a given alignment
+ * value.
+ * @param v input value
+ * @param alignment alignment value
+ */
+inline constexpr uint32_t align_up(uint32_t v, uint32_t alignment) {
+  return div_round_up(v, alignment) * alignment;
+}
+
+}  // namespace akida
```

## akida/engine/api/infra/registers_common.h

 * *Ordering differences only*

```diff
@@ -1,38 +1,38 @@
-#pragma once
-
-#include <cassert>
-#include <cstdint>
-
-namespace akida {
-
-struct RegDetail {
-  uint32_t offset;
-  uint32_t nb_bits;
-
-  explicit constexpr RegDetail(uint32_t first, uint32_t last)
-      : offset(first), nb_bits(last - first + 1) {}
-
-  explicit constexpr RegDetail(uint32_t first) : offset(first), nb_bits(1) {}
-};
-
-// Util function to set a range of bit to a value
-inline void set_field(uint32_t* bits, const RegDetail& field, uint32_t value) {
-  uint32_t max_value = (1 << field.nb_bits) - 1;
-  assert(value <= max_value);
-  // Mask value to avoid writing outside the field
-  value &= max_value;
-  // first clear bits
-  *bits &= ~(max_value << field.offset);
-  // Then set bits to value
-  *bits |= value << field.offset;
-}
-
-inline uint32_t get_field(const uint32_t& bits, const RegDetail& field) {
-  // create a mask
-  uint32_t max_value = (1 << field.nb_bits) - 1;
-  // shift and mask the value
-  uint32_t ret = (bits >> field.offset) & max_value;
-  return ret;
-}
-
-}  // namespace akida
+#pragma once
+
+#include <cassert>
+#include <cstdint>
+
+namespace akida {
+
+struct RegDetail {
+  uint32_t offset;
+  uint32_t nb_bits;
+
+  explicit constexpr RegDetail(uint32_t first, uint32_t last)
+      : offset(first), nb_bits(last - first + 1) {}
+
+  explicit constexpr RegDetail(uint32_t first) : offset(first), nb_bits(1) {}
+};
+
+// Util function to set a range of bit to a value
+inline void set_field(uint32_t* bits, const RegDetail& field, uint32_t value) {
+  uint32_t max_value = (1 << field.nb_bits) - 1;
+  assert(value <= max_value);
+  // Mask value to avoid writing outside the field
+  value &= max_value;
+  // first clear bits
+  *bits &= ~(max_value << field.offset);
+  // Then set bits to value
+  *bits |= value << field.offset;
+}
+
+inline uint32_t get_field(const uint32_t& bits, const RegDetail& field) {
+  // create a mask
+  uint32_t max_value = (1 << field.nb_bits) - 1;
+  // shift and mask the value
+  uint32_t ret = (bits >> field.offset) & max_value;
+  return ret;
+}
+
+}  // namespace akida
```

## akida/engine/api/infra/system.h

 * *Ordering differences only*

```diff
@@ -1,35 +1,35 @@
-#pragma once
-
-#if defined(__cplusplus)
-extern "C" { /* C-declarations in C++ programs */
-#endif
-
-#include <cstdint>
-
-#include "infra/exports.h"
-
-/**
- * @brief wait for a given duration
- * @param duration: how long should be waited, in milliseconds
- */
-AKIDASHAREDLIB_EXPORT void msleep(uint32_t duration);
-
-/**
- * @brief return monotone time in milliseconds
- */
-AKIDASHAREDLIB_EXPORT int64_t time_ms();
-
-/**
- * @brief signal watchdog while the library is busy during long processing.
- */
-AKIDASHAREDLIB_EXPORT void kick_watchdog();
-
-/**
- * @brief generate an unrecoverable error (exception), whose description is
- * formatted as a string.
- */
-AKIDASHAREDLIB_EXPORT void panic [[noreturn]] (const char* format, ...);
-
-#if defined(__cplusplus)
-} /* C-declarations in C++ programs */
-#endif
+#pragma once
+
+#if defined(__cplusplus)
+extern "C" { /* C-declarations in C++ programs */
+#endif
+
+#include <cstdint>
+
+#include "infra/exports.h"
+
+/**
+ * @brief wait for a given duration
+ * @param duration: how long should be waited, in milliseconds
+ */
+AKIDASHAREDLIB_EXPORT void msleep(uint32_t duration);
+
+/**
+ * @brief return monotone time in milliseconds
+ */
+AKIDASHAREDLIB_EXPORT int64_t time_ms();
+
+/**
+ * @brief signal watchdog while the library is busy during long processing.
+ */
+AKIDASHAREDLIB_EXPORT void kick_watchdog();
+
+/**
+ * @brief generate an unrecoverable error (exception), whose description is
+ * formatted as a string.
+ */
+AKIDASHAREDLIB_EXPORT void panic [[noreturn]] (const char* format, ...);
+
+#if defined(__cplusplus)
+} /* C-declarations in C++ programs */
+#endif
```

## akida/engine/cmake/akida-engine.cmake

```diff
@@ -1,38 +1,38 @@
-# Fetch Flatbuffer
-include(FetchContent)
-
-set(FETCHCONTENT_QUIET FALSE)
-
-FetchContent_Declare(
-    flatbuffers
-    URL https://github.com/google/flatbuffers/archive/v2.0.8.tar.gz
-)
-
-FetchContent_GetProperties(flatbuffers)
-
-if(NOT flatbuffers_POPULATED)
-    FetchContent_Populate(flatbuffers)
-endif()
-
-# Create an akida engine static library
-set(AKIDA_ENGINE akida_engine)
-
-file(GLOB ENGINE_CPP "${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp")
-add_library(${AKIDA_ENGINE} STATIC
-  ${ENGINE_CPP}
-)
-
-target_include_directories(${AKIDA_ENGINE}
-    PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/api>
-    PRIVATE $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/inc>
-    PRIVATE $<BUILD_INTERFACE:${flatbuffers_SOURCE_DIR}/include>
-)
-
-set_property(
-   SOURCE  ${CMAKE_CURRENT_SOURCE_DIR}/src/version.cpp
-   APPEND PROPERTY COMPILE_DEFINITIONS AKIDA_VERSION="2.3.3"
-)
-
-set_target_properties(${AKIDA_ENGINE} PROPERTIES
-    VERSION 2.3.3
-)
+# Fetch Flatbuffer
+include(FetchContent)
+
+set(FETCHCONTENT_QUIET FALSE)
+
+FetchContent_Declare(
+    flatbuffers
+    URL https://github.com/google/flatbuffers/archive/v2.0.8.tar.gz
+)
+
+FetchContent_GetProperties(flatbuffers)
+
+if(NOT flatbuffers_POPULATED)
+    FetchContent_Populate(flatbuffers)
+endif()
+
+# Create an akida engine static library
+set(AKIDA_ENGINE akida_engine)
+
+file(GLOB ENGINE_CPP "${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp")
+add_library(${AKIDA_ENGINE} STATIC
+  ${ENGINE_CPP}
+)
+
+target_include_directories(${AKIDA_ENGINE}
+    PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/api>
+    PRIVATE $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/inc>
+    PRIVATE $<BUILD_INTERFACE:${flatbuffers_SOURCE_DIR}/include>
+)
+
+set_property(
+   SOURCE  ${CMAKE_CURRENT_SOURCE_DIR}/src/version.cpp
+   APPEND PROPERTY COMPILE_DEFINITIONS AKIDA_VERSION="2.3.4"
+)
+
+set_target_properties(${AKIDA_ENGINE} PROPERTIES
+    VERSION 2.3.4
+)
```

## akida/engine/devices/akd1000/bare_metal_driver.cpp

 * *Ordering differences only*

```diff
@@ -1,40 +1,40 @@
-#include "akd1000/bare_metal_driver.h"
-
-#include <cstdio>
-#include <cstring>
-#include <vector>
-
-#include "infra/registers_common.h"
-
-#include "akd1000/registers_soc.h"
-
-namespace akida {
-
-static constexpr uint32_t regs_offset = 0xFCC00000u;
-
-BareMetalDriver::BareMetalDriver(uint32_t scratch_base_address,
-                                 uint32_t scratch_size,
-                                 uint32_t akida_visible_memory_base,
-                                 uint32_t akida_visible_memory_size)
-    : scratch_base_addr_(scratch_base_address),
-      scratch_size_(scratch_size),
-      akida_visible_mem_base_(akida_visible_memory_base),
-      akida_visible_mem_size_(akida_visible_memory_size) {}
-
-void BareMetalDriver::read(uint32_t address, void* data, size_t size) const {
-  memcpy(data, reinterpret_cast<void*>(address), size);
-}
-
-void BareMetalDriver::write(uint32_t address, const void* data, size_t size) {
-  memcpy(reinterpret_cast<void*>(address), data, size);
-}
-
-const char* BareMetalDriver::desc() const {
-  static char version_str[32];
-  auto reg = read32(REG_CHIP_INFO);
-  auto version = get_field(reg, REG_CHIP_VERSION);
-  snprintf(version_str, sizeof(version_str), "Embedded/NSoC_v%ld", version);
-  return version_str;
-}
-
-}  // namespace akida
+#include "akd1000/bare_metal_driver.h"
+
+#include <cstdio>
+#include <cstring>
+#include <vector>
+
+#include "infra/registers_common.h"
+
+#include "akd1000/registers_soc.h"
+
+namespace akida {
+
+static constexpr uint32_t regs_offset = 0xFCC00000u;
+
+BareMetalDriver::BareMetalDriver(uint32_t scratch_base_address,
+                                 uint32_t scratch_size,
+                                 uint32_t akida_visible_memory_base,
+                                 uint32_t akida_visible_memory_size)
+    : scratch_base_addr_(scratch_base_address),
+      scratch_size_(scratch_size),
+      akida_visible_mem_base_(akida_visible_memory_base),
+      akida_visible_mem_size_(akida_visible_memory_size) {}
+
+void BareMetalDriver::read(uint32_t address, void* data, size_t size) const {
+  memcpy(data, reinterpret_cast<void*>(address), size);
+}
+
+void BareMetalDriver::write(uint32_t address, const void* data, size_t size) {
+  memcpy(reinterpret_cast<void*>(address), data, size);
+}
+
+const char* BareMetalDriver::desc() const {
+  static char version_str[32];
+  auto reg = read32(REG_CHIP_INFO);
+  auto version = get_field(reg, REG_CHIP_VERSION);
+  snprintf(version_str, sizeof(version_str), "Embedded/NSoC_v%ld", version);
+  return version_str;
+}
+
+}  // namespace akida
```

## akida/engine/inc/engine/akida_device_program_fb_generated.h

 * *Ordering differences only*

```diff
@@ -1,897 +1,897 @@
-// automatically generated by the FlatBuffers compiler, do not modify
-
-
-#ifndef FLATBUFFERS_GENERATED_AKIDADEVICEPROGRAMFB_AKIDA_FB_H_
-#define FLATBUFFERS_GENERATED_AKIDADEVICEPROGRAMFB_AKIDA_FB_H_
-
-#include "flatbuffers/flatbuffers.h"
-
-// Ensure the included flatbuffers.h is the same version as when this file was
-// generated, otherwise it may not be compatible.
-static_assert(FLATBUFFERS_VERSION_MAJOR == 2 &&
-              FLATBUFFERS_VERSION_MINOR == 0 &&
-              FLATBUFFERS_VERSION_REVISION == 8,
-             "Non-compatible flatbuffers version included");
-
-namespace akida {
-namespace fb {
-
-struct DeviceVersion;
-struct DeviceVersionBuilder;
-
-struct Np;
-struct NpBuilder;
-
-struct NpTrack;
-struct NpTrackBuilder;
-
-struct Fnp2FilterTrack;
-struct Fnp2FilterTrackBuilder;
-
-struct EpgTrack;
-struct EpgTrackBuilder;
-
-struct Record;
-struct RecordBuilder;
-
-struct Pass;
-struct PassBuilder;
-
-struct DmaConfigHeader;
-
-struct LearningLayer;
-struct LearningLayerBuilder;
-
-struct Program;
-struct ProgramBuilder;
-
-enum IoType : int8_t {
-  IoType_dense = 0,
-  IoType_fnp_sparse = 1,
-  IoType_cnp_sparse = 2,
-  IoType_hrc_sparse = 3,
-  IoType_MIN = IoType_dense,
-  IoType_MAX = IoType_hrc_sparse
-};
-
-inline const IoType (&EnumValuesIoType())[4] {
-  static const IoType values[] = {
-    IoType_dense,
-    IoType_fnp_sparse,
-    IoType_cnp_sparse,
-    IoType_hrc_sparse
-  };
-  return values;
-}
-
-inline const char * const *EnumNamesIoType() {
-  static const char * const names[5] = {
-    "dense",
-    "fnp_sparse",
-    "cnp_sparse",
-    "hrc_sparse",
-    nullptr
-  };
-  return names;
-}
-
-inline const char *EnumNameIoType(IoType e) {
-  if (flatbuffers::IsOutRange(e, IoType_dense, IoType_hrc_sparse)) return "";
-  const size_t index = static_cast<size_t>(e);
-  return EnumNamesIoType()[index];
-}
-
-FLATBUFFERS_MANUALLY_ALIGNED_STRUCT(4) DmaConfigHeader FLATBUFFERS_FINAL_CLASS {
- private:
-  uint32_t w1_;
-  uint32_t w2_;
-
- public:
-  DmaConfigHeader()
-      : w1_(0),
-        w2_(0) {
-  }
-  DmaConfigHeader(uint32_t _w1, uint32_t _w2)
-      : w1_(flatbuffers::EndianScalar(_w1)),
-        w2_(flatbuffers::EndianScalar(_w2)) {
-  }
-  uint32_t w1() const {
-    return flatbuffers::EndianScalar(w1_);
-  }
-  uint32_t w2() const {
-    return flatbuffers::EndianScalar(w2_);
-  }
-};
-FLATBUFFERS_STRUCT_END(DmaConfigHeader, 8);
-
-struct DeviceVersion FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
-  typedef DeviceVersionBuilder Builder;
-  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
-    VT_VENDOR_ID = 4,
-    VT_PRODUCT_ID = 6,
-    VT_MAJOR_REV = 8,
-    VT_MINOR_REV = 10
-  };
-  uint8_t vendor_id() const {
-    return GetField<uint8_t>(VT_VENDOR_ID, 0);
-  }
-  uint8_t product_id() const {
-    return GetField<uint8_t>(VT_PRODUCT_ID, 0);
-  }
-  uint8_t major_rev() const {
-    return GetField<uint8_t>(VT_MAJOR_REV, 0);
-  }
-  uint8_t minor_rev() const {
-    return GetField<uint8_t>(VT_MINOR_REV, 0);
-  }
-  bool Verify(flatbuffers::Verifier &verifier) const {
-    return VerifyTableStart(verifier) &&
-           VerifyField<uint8_t>(verifier, VT_VENDOR_ID, 1) &&
-           VerifyField<uint8_t>(verifier, VT_PRODUCT_ID, 1) &&
-           VerifyField<uint8_t>(verifier, VT_MAJOR_REV, 1) &&
-           VerifyField<uint8_t>(verifier, VT_MINOR_REV, 1) &&
-           verifier.EndTable();
-  }
-};
-
-struct DeviceVersionBuilder {
-  typedef DeviceVersion Table;
-  flatbuffers::FlatBufferBuilder &fbb_;
-  flatbuffers::uoffset_t start_;
-  void add_vendor_id(uint8_t vendor_id) {
-    fbb_.AddElement<uint8_t>(DeviceVersion::VT_VENDOR_ID, vendor_id, 0);
-  }
-  void add_product_id(uint8_t product_id) {
-    fbb_.AddElement<uint8_t>(DeviceVersion::VT_PRODUCT_ID, product_id, 0);
-  }
-  void add_major_rev(uint8_t major_rev) {
-    fbb_.AddElement<uint8_t>(DeviceVersion::VT_MAJOR_REV, major_rev, 0);
-  }
-  void add_minor_rev(uint8_t minor_rev) {
-    fbb_.AddElement<uint8_t>(DeviceVersion::VT_MINOR_REV, minor_rev, 0);
-  }
-  explicit DeviceVersionBuilder(flatbuffers::FlatBufferBuilder &_fbb)
-        : fbb_(_fbb) {
-    start_ = fbb_.StartTable();
-  }
-  flatbuffers::Offset<DeviceVersion> Finish() {
-    const auto end = fbb_.EndTable(start_);
-    auto o = flatbuffers::Offset<DeviceVersion>(end);
-    return o;
-  }
-};
-
-inline flatbuffers::Offset<DeviceVersion> CreateDeviceVersion(
-    flatbuffers::FlatBufferBuilder &_fbb,
-    uint8_t vendor_id = 0,
-    uint8_t product_id = 0,
-    uint8_t major_rev = 0,
-    uint8_t minor_rev = 0) {
-  DeviceVersionBuilder builder_(_fbb);
-  builder_.add_minor_rev(minor_rev);
-  builder_.add_major_rev(major_rev);
-  builder_.add_product_id(product_id);
-  builder_.add_vendor_id(vendor_id);
-  return builder_.Finish();
-}
-
-struct Np FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
-  typedef NpBuilder Builder;
-  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
-    VT_COL = 4,
-    VT_ROW = 6,
-    VT_ID = 8
-  };
-  uint8_t col() const {
-    return GetField<uint8_t>(VT_COL, 0);
-  }
-  uint8_t row() const {
-    return GetField<uint8_t>(VT_ROW, 0);
-  }
-  uint8_t id() const {
-    return GetField<uint8_t>(VT_ID, 0);
-  }
-  bool Verify(flatbuffers::Verifier &verifier) const {
-    return VerifyTableStart(verifier) &&
-           VerifyField<uint8_t>(verifier, VT_COL, 1) &&
-           VerifyField<uint8_t>(verifier, VT_ROW, 1) &&
-           VerifyField<uint8_t>(verifier, VT_ID, 1) &&
-           verifier.EndTable();
-  }
-};
-
-struct NpBuilder {
-  typedef Np Table;
-  flatbuffers::FlatBufferBuilder &fbb_;
-  flatbuffers::uoffset_t start_;
-  void add_col(uint8_t col) {
-    fbb_.AddElement<uint8_t>(Np::VT_COL, col, 0);
-  }
-  void add_row(uint8_t row) {
-    fbb_.AddElement<uint8_t>(Np::VT_ROW, row, 0);
-  }
-  void add_id(uint8_t id) {
-    fbb_.AddElement<uint8_t>(Np::VT_ID, id, 0);
-  }
-  explicit NpBuilder(flatbuffers::FlatBufferBuilder &_fbb)
-        : fbb_(_fbb) {
-    start_ = fbb_.StartTable();
-  }
-  flatbuffers::Offset<Np> Finish() {
-    const auto end = fbb_.EndTable(start_);
-    auto o = flatbuffers::Offset<Np>(end);
-    return o;
-  }
-};
-
-inline flatbuffers::Offset<Np> CreateNp(
-    flatbuffers::FlatBufferBuilder &_fbb,
-    uint8_t col = 0,
-    uint8_t row = 0,
-    uint8_t id = 0) {
-  NpBuilder builder_(_fbb);
-  builder_.add_id(id);
-  builder_.add_row(row);
-  builder_.add_col(col);
-  return builder_.Finish();
-}
-
-struct NpTrack FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
-  typedef NpTrackBuilder Builder;
-  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
-    VT_DATA = 4
-  };
-  const flatbuffers::Vector<uint32_t> *data() const {
-    return GetPointer<const flatbuffers::Vector<uint32_t> *>(VT_DATA);
-  }
-  bool Verify(flatbuffers::Verifier &verifier) const {
-    return VerifyTableStart(verifier) &&
-           VerifyOffset(verifier, VT_DATA) &&
-           verifier.VerifyVector(data()) &&
-           verifier.EndTable();
-  }
-};
-
-struct NpTrackBuilder {
-  typedef NpTrack Table;
-  flatbuffers::FlatBufferBuilder &fbb_;
-  flatbuffers::uoffset_t start_;
-  void add_data(flatbuffers::Offset<flatbuffers::Vector<uint32_t>> data) {
-    fbb_.AddOffset(NpTrack::VT_DATA, data);
-  }
-  explicit NpTrackBuilder(flatbuffers::FlatBufferBuilder &_fbb)
-        : fbb_(_fbb) {
-    start_ = fbb_.StartTable();
-  }
-  flatbuffers::Offset<NpTrack> Finish() {
-    const auto end = fbb_.EndTable(start_);
-    auto o = flatbuffers::Offset<NpTrack>(end);
-    return o;
-  }
-};
-
-inline flatbuffers::Offset<NpTrack> CreateNpTrack(
-    flatbuffers::FlatBufferBuilder &_fbb,
-    flatbuffers::Offset<flatbuffers::Vector<uint32_t>> data = 0) {
-  NpTrackBuilder builder_(_fbb);
-  builder_.add_data(data);
-  return builder_.Finish();
-}
-
-inline flatbuffers::Offset<NpTrack> CreateNpTrackDirect(
-    flatbuffers::FlatBufferBuilder &_fbb,
-    const std::vector<uint32_t> *data = nullptr) {
-  auto data__ = data ? _fbb.CreateVector<uint32_t>(*data) : 0;
-  return akida::fb::CreateNpTrack(
-      _fbb,
-      data__);
-}
-
-struct Fnp2FilterTrack FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
-  typedef Fnp2FilterTrackBuilder Builder;
-  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
-    VT_NP = 4,
-    VT_DATA = 6
-  };
-  const akida::fb::Np *np() const {
-    return GetPointer<const akida::fb::Np *>(VT_NP);
-  }
-  const flatbuffers::Vector<uint32_t> *data() const {
-    return GetPointer<const flatbuffers::Vector<uint32_t> *>(VT_DATA);
-  }
-  bool Verify(flatbuffers::Verifier &verifier) const {
-    return VerifyTableStart(verifier) &&
-           VerifyOffset(verifier, VT_NP) &&
-           verifier.VerifyTable(np()) &&
-           VerifyOffset(verifier, VT_DATA) &&
-           verifier.VerifyVector(data()) &&
-           verifier.EndTable();
-  }
-};
-
-struct Fnp2FilterTrackBuilder {
-  typedef Fnp2FilterTrack Table;
-  flatbuffers::FlatBufferBuilder &fbb_;
-  flatbuffers::uoffset_t start_;
-  void add_np(flatbuffers::Offset<akida::fb::Np> np) {
-    fbb_.AddOffset(Fnp2FilterTrack::VT_NP, np);
-  }
-  void add_data(flatbuffers::Offset<flatbuffers::Vector<uint32_t>> data) {
-    fbb_.AddOffset(Fnp2FilterTrack::VT_DATA, data);
-  }
-  explicit Fnp2FilterTrackBuilder(flatbuffers::FlatBufferBuilder &_fbb)
-        : fbb_(_fbb) {
-    start_ = fbb_.StartTable();
-  }
-  flatbuffers::Offset<Fnp2FilterTrack> Finish() {
-    const auto end = fbb_.EndTable(start_);
-    auto o = flatbuffers::Offset<Fnp2FilterTrack>(end);
-    return o;
-  }
-};
-
-inline flatbuffers::Offset<Fnp2FilterTrack> CreateFnp2FilterTrack(
-    flatbuffers::FlatBufferBuilder &_fbb,
-    flatbuffers::Offset<akida::fb::Np> np = 0,
-    flatbuffers::Offset<flatbuffers::Vector<uint32_t>> data = 0) {
-  Fnp2FilterTrackBuilder builder_(_fbb);
-  builder_.add_data(data);
-  builder_.add_np(np);
-  return builder_.Finish();
-}
-
-inline flatbuffers::Offset<Fnp2FilterTrack> CreateFnp2FilterTrackDirect(
-    flatbuffers::FlatBufferBuilder &_fbb,
-    flatbuffers::Offset<akida::fb::Np> np = 0,
-    const std::vector<uint32_t> *data = nullptr) {
-  auto data__ = data ? _fbb.CreateVector<uint32_t>(*data) : 0;
-  return akida::fb::CreateFnp2FilterTrack(
-      _fbb,
-      np,
-      data__);
-}
-
-struct EpgTrack FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
-  typedef EpgTrackBuilder Builder;
-  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
-    VT_ADDRESS = 4,
-    VT_DATA = 6
-  };
-  uint32_t address() const {
-    return GetField<uint32_t>(VT_ADDRESS, 0);
-  }
-  uint32_t data() const {
-    return GetField<uint32_t>(VT_DATA, 0);
-  }
-  bool Verify(flatbuffers::Verifier &verifier) const {
-    return VerifyTableStart(verifier) &&
-           VerifyField<uint32_t>(verifier, VT_ADDRESS, 4) &&
-           VerifyField<uint32_t>(verifier, VT_DATA, 4) &&
-           verifier.EndTable();
-  }
-};
-
-struct EpgTrackBuilder {
-  typedef EpgTrack Table;
-  flatbuffers::FlatBufferBuilder &fbb_;
-  flatbuffers::uoffset_t start_;
-  void add_address(uint32_t address) {
-    fbb_.AddElement<uint32_t>(EpgTrack::VT_ADDRESS, address, 0);
-  }
-  void add_data(uint32_t data) {
-    fbb_.AddElement<uint32_t>(EpgTrack::VT_DATA, data, 0);
-  }
-  explicit EpgTrackBuilder(flatbuffers::FlatBufferBuilder &_fbb)
-        : fbb_(_fbb) {
-    start_ = fbb_.StartTable();
-  }
-  flatbuffers::Offset<EpgTrack> Finish() {
-    const auto end = fbb_.EndTable(start_);
-    auto o = flatbuffers::Offset<EpgTrack>(end);
-    return o;
-  }
-};
-
-inline flatbuffers::Offset<EpgTrack> CreateEpgTrack(
-    flatbuffers::FlatBufferBuilder &_fbb,
-    uint32_t address = 0,
-    uint32_t data = 0) {
-  EpgTrackBuilder builder_(_fbb);
-  builder_.add_data(data);
-  builder_.add_address(address);
-  return builder_.Finish();
-}
-
-struct Record FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
-  typedef RecordBuilder Builder;
-  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
-    VT_NP_TRACKS = 4,
-    VT_FNP2_TRACK = 6
-  };
-  const flatbuffers::Vector<flatbuffers::Offset<akida::fb::NpTrack>> *np_tracks() const {
-    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<akida::fb::NpTrack>> *>(VT_NP_TRACKS);
-  }
-  const akida::fb::Fnp2FilterTrack *fnp2_track() const {
-    return GetPointer<const akida::fb::Fnp2FilterTrack *>(VT_FNP2_TRACK);
-  }
-  bool Verify(flatbuffers::Verifier &verifier) const {
-    return VerifyTableStart(verifier) &&
-           VerifyOffset(verifier, VT_NP_TRACKS) &&
-           verifier.VerifyVector(np_tracks()) &&
-           verifier.VerifyVectorOfTables(np_tracks()) &&
-           VerifyOffset(verifier, VT_FNP2_TRACK) &&
-           verifier.VerifyTable(fnp2_track()) &&
-           verifier.EndTable();
-  }
-};
-
-struct RecordBuilder {
-  typedef Record Table;
-  flatbuffers::FlatBufferBuilder &fbb_;
-  flatbuffers::uoffset_t start_;
-  void add_np_tracks(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<akida::fb::NpTrack>>> np_tracks) {
-    fbb_.AddOffset(Record::VT_NP_TRACKS, np_tracks);
-  }
-  void add_fnp2_track(flatbuffers::Offset<akida::fb::Fnp2FilterTrack> fnp2_track) {
-    fbb_.AddOffset(Record::VT_FNP2_TRACK, fnp2_track);
-  }
-  explicit RecordBuilder(flatbuffers::FlatBufferBuilder &_fbb)
-        : fbb_(_fbb) {
-    start_ = fbb_.StartTable();
-  }
-  flatbuffers::Offset<Record> Finish() {
-    const auto end = fbb_.EndTable(start_);
-    auto o = flatbuffers::Offset<Record>(end);
-    return o;
-  }
-};
-
-inline flatbuffers::Offset<Record> CreateRecord(
-    flatbuffers::FlatBufferBuilder &_fbb,
-    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<akida::fb::NpTrack>>> np_tracks = 0,
-    flatbuffers::Offset<akida::fb::Fnp2FilterTrack> fnp2_track = 0) {
-  RecordBuilder builder_(_fbb);
-  builder_.add_fnp2_track(fnp2_track);
-  builder_.add_np_tracks(np_tracks);
-  return builder_.Finish();
-}
-
-inline flatbuffers::Offset<Record> CreateRecordDirect(
-    flatbuffers::FlatBufferBuilder &_fbb,
-    const std::vector<flatbuffers::Offset<akida::fb::NpTrack>> *np_tracks = nullptr,
-    flatbuffers::Offset<akida::fb::Fnp2FilterTrack> fnp2_track = 0) {
-  auto np_tracks__ = np_tracks ? _fbb.CreateVector<flatbuffers::Offset<akida::fb::NpTrack>>(*np_tracks) : 0;
-  return akida::fb::CreateRecord(
-      _fbb,
-      np_tracks__,
-      fnp2_track);
-}
-
-struct Pass FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
-  typedef PassBuilder Builder;
-  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
-    VT_RECORDS = 4
-  };
-  const flatbuffers::Vector<flatbuffers::Offset<akida::fb::Record>> *records() const {
-    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<akida::fb::Record>> *>(VT_RECORDS);
-  }
-  bool Verify(flatbuffers::Verifier &verifier) const {
-    return VerifyTableStart(verifier) &&
-           VerifyOffset(verifier, VT_RECORDS) &&
-           verifier.VerifyVector(records()) &&
-           verifier.VerifyVectorOfTables(records()) &&
-           verifier.EndTable();
-  }
-};
-
-struct PassBuilder {
-  typedef Pass Table;
-  flatbuffers::FlatBufferBuilder &fbb_;
-  flatbuffers::uoffset_t start_;
-  void add_records(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<akida::fb::Record>>> records) {
-    fbb_.AddOffset(Pass::VT_RECORDS, records);
-  }
-  explicit PassBuilder(flatbuffers::FlatBufferBuilder &_fbb)
-        : fbb_(_fbb) {
-    start_ = fbb_.StartTable();
-  }
-  flatbuffers::Offset<Pass> Finish() {
-    const auto end = fbb_.EndTable(start_);
-    auto o = flatbuffers::Offset<Pass>(end);
-    return o;
-  }
-};
-
-inline flatbuffers::Offset<Pass> CreatePass(
-    flatbuffers::FlatBufferBuilder &_fbb,
-    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<akida::fb::Record>>> records = 0) {
-  PassBuilder builder_(_fbb);
-  builder_.add_records(records);
-  return builder_.Finish();
-}
-
-inline flatbuffers::Offset<Pass> CreatePassDirect(
-    flatbuffers::FlatBufferBuilder &_fbb,
-    const std::vector<flatbuffers::Offset<akida::fb::Record>> *records = nullptr) {
-  auto records__ = records ? _fbb.CreateVector<flatbuffers::Offset<akida::fb::Record>>(*records) : 0;
-  return akida::fb::CreatePass(
-      _fbb,
-      records__);
-}
-
-struct LearningLayer FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
-  typedef LearningLayerBuilder Builder;
-  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
-    VT_NP = 4,
-    VT_LEARNING_REGISTERS = 6,
-    VT_INFERENCE_REGISTERS = 8,
-    VT_RAM = 10,
-    VT_LEARN_MEM_SIZE = 12,
-    VT_UPDATE_LEARN_MEM_HDR = 14
-  };
-  const akida::fb::Np *np() const {
-    return GetPointer<const akida::fb::Np *>(VT_NP);
-  }
-  const akida::fb::Record *learning_registers() const {
-    return GetPointer<const akida::fb::Record *>(VT_LEARNING_REGISTERS);
-  }
-  const akida::fb::Record *inference_registers() const {
-    return GetPointer<const akida::fb::Record *>(VT_INFERENCE_REGISTERS);
-  }
-  const akida::fb::Record *ram() const {
-    return GetPointer<const akida::fb::Record *>(VT_RAM);
-  }
-  uint32_t learn_mem_size() const {
-    return GetField<uint32_t>(VT_LEARN_MEM_SIZE, 0);
-  }
-  const akida::fb::DmaConfigHeader *update_learn_mem_hdr() const {
-    return GetStruct<const akida::fb::DmaConfigHeader *>(VT_UPDATE_LEARN_MEM_HDR);
-  }
-  bool Verify(flatbuffers::Verifier &verifier) const {
-    return VerifyTableStart(verifier) &&
-           VerifyOffset(verifier, VT_NP) &&
-           verifier.VerifyTable(np()) &&
-           VerifyOffset(verifier, VT_LEARNING_REGISTERS) &&
-           verifier.VerifyTable(learning_registers()) &&
-           VerifyOffset(verifier, VT_INFERENCE_REGISTERS) &&
-           verifier.VerifyTable(inference_registers()) &&
-           VerifyOffset(verifier, VT_RAM) &&
-           verifier.VerifyTable(ram()) &&
-           VerifyField<uint32_t>(verifier, VT_LEARN_MEM_SIZE, 4) &&
-           VerifyField<akida::fb::DmaConfigHeader>(verifier, VT_UPDATE_LEARN_MEM_HDR, 4) &&
-           verifier.EndTable();
-  }
-};
-
-struct LearningLayerBuilder {
-  typedef LearningLayer Table;
-  flatbuffers::FlatBufferBuilder &fbb_;
-  flatbuffers::uoffset_t start_;
-  void add_np(flatbuffers::Offset<akida::fb::Np> np) {
-    fbb_.AddOffset(LearningLayer::VT_NP, np);
-  }
-  void add_learning_registers(flatbuffers::Offset<akida::fb::Record> learning_registers) {
-    fbb_.AddOffset(LearningLayer::VT_LEARNING_REGISTERS, learning_registers);
-  }
-  void add_inference_registers(flatbuffers::Offset<akida::fb::Record> inference_registers) {
-    fbb_.AddOffset(LearningLayer::VT_INFERENCE_REGISTERS, inference_registers);
-  }
-  void add_ram(flatbuffers::Offset<akida::fb::Record> ram) {
-    fbb_.AddOffset(LearningLayer::VT_RAM, ram);
-  }
-  void add_learn_mem_size(uint32_t learn_mem_size) {
-    fbb_.AddElement<uint32_t>(LearningLayer::VT_LEARN_MEM_SIZE, learn_mem_size, 0);
-  }
-  void add_update_learn_mem_hdr(const akida::fb::DmaConfigHeader *update_learn_mem_hdr) {
-    fbb_.AddStruct(LearningLayer::VT_UPDATE_LEARN_MEM_HDR, update_learn_mem_hdr);
-  }
-  explicit LearningLayerBuilder(flatbuffers::FlatBufferBuilder &_fbb)
-        : fbb_(_fbb) {
-    start_ = fbb_.StartTable();
-  }
-  flatbuffers::Offset<LearningLayer> Finish() {
-    const auto end = fbb_.EndTable(start_);
-    auto o = flatbuffers::Offset<LearningLayer>(end);
-    return o;
-  }
-};
-
-inline flatbuffers::Offset<LearningLayer> CreateLearningLayer(
-    flatbuffers::FlatBufferBuilder &_fbb,
-    flatbuffers::Offset<akida::fb::Np> np = 0,
-    flatbuffers::Offset<akida::fb::Record> learning_registers = 0,
-    flatbuffers::Offset<akida::fb::Record> inference_registers = 0,
-    flatbuffers::Offset<akida::fb::Record> ram = 0,
-    uint32_t learn_mem_size = 0,
-    const akida::fb::DmaConfigHeader *update_learn_mem_hdr = nullptr) {
-  LearningLayerBuilder builder_(_fbb);
-  builder_.add_update_learn_mem_hdr(update_learn_mem_hdr);
-  builder_.add_learn_mem_size(learn_mem_size);
-  builder_.add_ram(ram);
-  builder_.add_inference_registers(inference_registers);
-  builder_.add_learning_registers(learning_registers);
-  builder_.add_np(np);
-  return builder_.Finish();
-}
-
-struct Program FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
-  typedef ProgramBuilder Builder;
-  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
-    VT_VERSION = 4,
-    VT_DEVICE_VERSION = 6,
-    VT_INPUT_DIMS = 8,
-    VT_OUTPUT_DIMS = 10,
-    VT_INPUT_TYPE = 12,
-    VT_OUTPUT_TYPE = 14,
-    VT_ACTIVATION = 16,
-    VT_DENSE_WINDOW_W = 18,
-    VT_DENSE_WINDOW_H = 20,
-    VT_PASSES = 22,
-    VT_LEARNING_LAYER = 24,
-    VT_EPG_TRACKS = 26,
-    VT_SHIFTS = 28,
-    VT_SCALES = 30,
-    VT_DUMMY_DESC_HDR = 32,
-    VT_MAX_NUM_DESC = 34
-  };
-  const flatbuffers::String *version() const {
-    return GetPointer<const flatbuffers::String *>(VT_VERSION);
-  }
-  const akida::fb::DeviceVersion *device_version() const {
-    return GetPointer<const akida::fb::DeviceVersion *>(VT_DEVICE_VERSION);
-  }
-  const flatbuffers::Vector<uint32_t> *input_dims() const {
-    return GetPointer<const flatbuffers::Vector<uint32_t> *>(VT_INPUT_DIMS);
-  }
-  const flatbuffers::Vector<uint32_t> *output_dims() const {
-    return GetPointer<const flatbuffers::Vector<uint32_t> *>(VT_OUTPUT_DIMS);
-  }
-  akida::fb::IoType input_type() const {
-    return static_cast<akida::fb::IoType>(GetField<int8_t>(VT_INPUT_TYPE, 0));
-  }
-  akida::fb::IoType output_type() const {
-    return static_cast<akida::fb::IoType>(GetField<int8_t>(VT_OUTPUT_TYPE, 0));
-  }
-  bool activation() const {
-    return GetField<uint8_t>(VT_ACTIVATION, 0) != 0;
-  }
-  uint32_t dense_window_w() const {
-    return GetField<uint32_t>(VT_DENSE_WINDOW_W, 0);
-  }
-  uint32_t dense_window_h() const {
-    return GetField<uint32_t>(VT_DENSE_WINDOW_H, 0);
-  }
-  const flatbuffers::Vector<flatbuffers::Offset<akida::fb::Pass>> *passes() const {
-    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<akida::fb::Pass>> *>(VT_PASSES);
-  }
-  const akida::fb::LearningLayer *learning_layer() const {
-    return GetPointer<const akida::fb::LearningLayer *>(VT_LEARNING_LAYER);
-  }
-  const flatbuffers::Vector<flatbuffers::Offset<akida::fb::EpgTrack>> *epg_tracks() const {
-    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<akida::fb::EpgTrack>> *>(VT_EPG_TRACKS);
-  }
-  const flatbuffers::Vector<int32_t> *shifts() const {
-    return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_SHIFTS);
-  }
-  const flatbuffers::Vector<float> *scales() const {
-    return GetPointer<const flatbuffers::Vector<float> *>(VT_SCALES);
-  }
-  const akida::fb::DmaConfigHeader *dummy_desc_hdr() const {
-    return GetStruct<const akida::fb::DmaConfigHeader *>(VT_DUMMY_DESC_HDR);
-  }
-  uint8_t max_num_desc() const {
-    return GetField<uint8_t>(VT_MAX_NUM_DESC, 0);
-  }
-  bool Verify(flatbuffers::Verifier &verifier) const {
-    return VerifyTableStart(verifier) &&
-           VerifyOffset(verifier, VT_VERSION) &&
-           verifier.VerifyString(version()) &&
-           VerifyOffset(verifier, VT_DEVICE_VERSION) &&
-           verifier.VerifyTable(device_version()) &&
-           VerifyOffset(verifier, VT_INPUT_DIMS) &&
-           verifier.VerifyVector(input_dims()) &&
-           VerifyOffset(verifier, VT_OUTPUT_DIMS) &&
-           verifier.VerifyVector(output_dims()) &&
-           VerifyField<int8_t>(verifier, VT_INPUT_TYPE, 1) &&
-           VerifyField<int8_t>(verifier, VT_OUTPUT_TYPE, 1) &&
-           VerifyField<uint8_t>(verifier, VT_ACTIVATION, 1) &&
-           VerifyField<uint32_t>(verifier, VT_DENSE_WINDOW_W, 4) &&
-           VerifyField<uint32_t>(verifier, VT_DENSE_WINDOW_H, 4) &&
-           VerifyOffset(verifier, VT_PASSES) &&
-           verifier.VerifyVector(passes()) &&
-           verifier.VerifyVectorOfTables(passes()) &&
-           VerifyOffset(verifier, VT_LEARNING_LAYER) &&
-           verifier.VerifyTable(learning_layer()) &&
-           VerifyOffset(verifier, VT_EPG_TRACKS) &&
-           verifier.VerifyVector(epg_tracks()) &&
-           verifier.VerifyVectorOfTables(epg_tracks()) &&
-           VerifyOffset(verifier, VT_SHIFTS) &&
-           verifier.VerifyVector(shifts()) &&
-           VerifyOffset(verifier, VT_SCALES) &&
-           verifier.VerifyVector(scales()) &&
-           VerifyField<akida::fb::DmaConfigHeader>(verifier, VT_DUMMY_DESC_HDR, 4) &&
-           VerifyField<uint8_t>(verifier, VT_MAX_NUM_DESC, 1) &&
-           verifier.EndTable();
-  }
-};
-
-struct ProgramBuilder {
-  typedef Program Table;
-  flatbuffers::FlatBufferBuilder &fbb_;
-  flatbuffers::uoffset_t start_;
-  void add_version(flatbuffers::Offset<flatbuffers::String> version) {
-    fbb_.AddOffset(Program::VT_VERSION, version);
-  }
-  void add_device_version(flatbuffers::Offset<akida::fb::DeviceVersion> device_version) {
-    fbb_.AddOffset(Program::VT_DEVICE_VERSION, device_version);
-  }
-  void add_input_dims(flatbuffers::Offset<flatbuffers::Vector<uint32_t>> input_dims) {
-    fbb_.AddOffset(Program::VT_INPUT_DIMS, input_dims);
-  }
-  void add_output_dims(flatbuffers::Offset<flatbuffers::Vector<uint32_t>> output_dims) {
-    fbb_.AddOffset(Program::VT_OUTPUT_DIMS, output_dims);
-  }
-  void add_input_type(akida::fb::IoType input_type) {
-    fbb_.AddElement<int8_t>(Program::VT_INPUT_TYPE, static_cast<int8_t>(input_type), 0);
-  }
-  void add_output_type(akida::fb::IoType output_type) {
-    fbb_.AddElement<int8_t>(Program::VT_OUTPUT_TYPE, static_cast<int8_t>(output_type), 0);
-  }
-  void add_activation(bool activation) {
-    fbb_.AddElement<uint8_t>(Program::VT_ACTIVATION, static_cast<uint8_t>(activation), 0);
-  }
-  void add_dense_window_w(uint32_t dense_window_w) {
-    fbb_.AddElement<uint32_t>(Program::VT_DENSE_WINDOW_W, dense_window_w, 0);
-  }
-  void add_dense_window_h(uint32_t dense_window_h) {
-    fbb_.AddElement<uint32_t>(Program::VT_DENSE_WINDOW_H, dense_window_h, 0);
-  }
-  void add_passes(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<akida::fb::Pass>>> passes) {
-    fbb_.AddOffset(Program::VT_PASSES, passes);
-  }
-  void add_learning_layer(flatbuffers::Offset<akida::fb::LearningLayer> learning_layer) {
-    fbb_.AddOffset(Program::VT_LEARNING_LAYER, learning_layer);
-  }
-  void add_epg_tracks(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<akida::fb::EpgTrack>>> epg_tracks) {
-    fbb_.AddOffset(Program::VT_EPG_TRACKS, epg_tracks);
-  }
-  void add_shifts(flatbuffers::Offset<flatbuffers::Vector<int32_t>> shifts) {
-    fbb_.AddOffset(Program::VT_SHIFTS, shifts);
-  }
-  void add_scales(flatbuffers::Offset<flatbuffers::Vector<float>> scales) {
-    fbb_.AddOffset(Program::VT_SCALES, scales);
-  }
-  void add_dummy_desc_hdr(const akida::fb::DmaConfigHeader *dummy_desc_hdr) {
-    fbb_.AddStruct(Program::VT_DUMMY_DESC_HDR, dummy_desc_hdr);
-  }
-  void add_max_num_desc(uint8_t max_num_desc) {
-    fbb_.AddElement<uint8_t>(Program::VT_MAX_NUM_DESC, max_num_desc, 0);
-  }
-  explicit ProgramBuilder(flatbuffers::FlatBufferBuilder &_fbb)
-        : fbb_(_fbb) {
-    start_ = fbb_.StartTable();
-  }
-  flatbuffers::Offset<Program> Finish() {
-    const auto end = fbb_.EndTable(start_);
-    auto o = flatbuffers::Offset<Program>(end);
-    return o;
-  }
-};
-
-inline flatbuffers::Offset<Program> CreateProgram(
-    flatbuffers::FlatBufferBuilder &_fbb,
-    flatbuffers::Offset<flatbuffers::String> version = 0,
-    flatbuffers::Offset<akida::fb::DeviceVersion> device_version = 0,
-    flatbuffers::Offset<flatbuffers::Vector<uint32_t>> input_dims = 0,
-    flatbuffers::Offset<flatbuffers::Vector<uint32_t>> output_dims = 0,
-    akida::fb::IoType input_type = akida::fb::IoType_dense,
-    akida::fb::IoType output_type = akida::fb::IoType_dense,
-    bool activation = false,
-    uint32_t dense_window_w = 0,
-    uint32_t dense_window_h = 0,
-    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<akida::fb::Pass>>> passes = 0,
-    flatbuffers::Offset<akida::fb::LearningLayer> learning_layer = 0,
-    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<akida::fb::EpgTrack>>> epg_tracks = 0,
-    flatbuffers::Offset<flatbuffers::Vector<int32_t>> shifts = 0,
-    flatbuffers::Offset<flatbuffers::Vector<float>> scales = 0,
-    const akida::fb::DmaConfigHeader *dummy_desc_hdr = nullptr,
-    uint8_t max_num_desc = 0) {
-  ProgramBuilder builder_(_fbb);
-  builder_.add_dummy_desc_hdr(dummy_desc_hdr);
-  builder_.add_scales(scales);
-  builder_.add_shifts(shifts);
-  builder_.add_epg_tracks(epg_tracks);
-  builder_.add_learning_layer(learning_layer);
-  builder_.add_passes(passes);
-  builder_.add_dense_window_h(dense_window_h);
-  builder_.add_dense_window_w(dense_window_w);
-  builder_.add_output_dims(output_dims);
-  builder_.add_input_dims(input_dims);
-  builder_.add_device_version(device_version);
-  builder_.add_version(version);
-  builder_.add_max_num_desc(max_num_desc);
-  builder_.add_activation(activation);
-  builder_.add_output_type(output_type);
-  builder_.add_input_type(input_type);
-  return builder_.Finish();
-}
-
-inline flatbuffers::Offset<Program> CreateProgramDirect(
-    flatbuffers::FlatBufferBuilder &_fbb,
-    const char *version = nullptr,
-    flatbuffers::Offset<akida::fb::DeviceVersion> device_version = 0,
-    const std::vector<uint32_t> *input_dims = nullptr,
-    const std::vector<uint32_t> *output_dims = nullptr,
-    akida::fb::IoType input_type = akida::fb::IoType_dense,
-    akida::fb::IoType output_type = akida::fb::IoType_dense,
-    bool activation = false,
-    uint32_t dense_window_w = 0,
-    uint32_t dense_window_h = 0,
-    const std::vector<flatbuffers::Offset<akida::fb::Pass>> *passes = nullptr,
-    flatbuffers::Offset<akida::fb::LearningLayer> learning_layer = 0,
-    const std::vector<flatbuffers::Offset<akida::fb::EpgTrack>> *epg_tracks = nullptr,
-    const std::vector<int32_t> *shifts = nullptr,
-    const std::vector<float> *scales = nullptr,
-    const akida::fb::DmaConfigHeader *dummy_desc_hdr = nullptr,
-    uint8_t max_num_desc = 0) {
-  auto version__ = version ? _fbb.CreateString(version) : 0;
-  auto input_dims__ = input_dims ? _fbb.CreateVector<uint32_t>(*input_dims) : 0;
-  auto output_dims__ = output_dims ? _fbb.CreateVector<uint32_t>(*output_dims) : 0;
-  auto passes__ = passes ? _fbb.CreateVector<flatbuffers::Offset<akida::fb::Pass>>(*passes) : 0;
-  auto epg_tracks__ = epg_tracks ? _fbb.CreateVector<flatbuffers::Offset<akida::fb::EpgTrack>>(*epg_tracks) : 0;
-  auto shifts__ = shifts ? _fbb.CreateVector<int32_t>(*shifts) : 0;
-  auto scales__ = scales ? _fbb.CreateVector<float>(*scales) : 0;
-  return akida::fb::CreateProgram(
-      _fbb,
-      version__,
-      device_version,
-      input_dims__,
-      output_dims__,
-      input_type,
-      output_type,
-      activation,
-      dense_window_w,
-      dense_window_h,
-      passes__,
-      learning_layer,
-      epg_tracks__,
-      shifts__,
-      scales__,
-      dummy_desc_hdr,
-      max_num_desc);
-}
-
-inline const akida::fb::Program *GetProgram(const void *buf) {
-  return flatbuffers::GetRoot<akida::fb::Program>(buf);
-}
-
-inline const akida::fb::Program *GetSizePrefixedProgram(const void *buf) {
-  return flatbuffers::GetSizePrefixedRoot<akida::fb::Program>(buf);
-}
-
-inline bool VerifyProgramBuffer(
-    flatbuffers::Verifier &verifier) {
-  return verifier.VerifyBuffer<akida::fb::Program>(nullptr);
-}
-
-inline bool VerifySizePrefixedProgramBuffer(
-    flatbuffers::Verifier &verifier) {
-  return verifier.VerifySizePrefixedBuffer<akida::fb::Program>(nullptr);
-}
-
-inline void FinishProgramBuffer(
-    flatbuffers::FlatBufferBuilder &fbb,
-    flatbuffers::Offset<akida::fb::Program> root) {
-  fbb.Finish(root);
-}
-
-inline void FinishSizePrefixedProgramBuffer(
-    flatbuffers::FlatBufferBuilder &fbb,
-    flatbuffers::Offset<akida::fb::Program> root) {
-  fbb.FinishSizePrefixed(root);
-}
-
-}  // namespace fb
-}  // namespace akida
-
-#endif  // FLATBUFFERS_GENERATED_AKIDADEVICEPROGRAMFB_AKIDA_FB_H_
+// automatically generated by the FlatBuffers compiler, do not modify
+
+
+#ifndef FLATBUFFERS_GENERATED_AKIDADEVICEPROGRAMFB_AKIDA_FB_H_
+#define FLATBUFFERS_GENERATED_AKIDADEVICEPROGRAMFB_AKIDA_FB_H_
+
+#include "flatbuffers/flatbuffers.h"
+
+// Ensure the included flatbuffers.h is the same version as when this file was
+// generated, otherwise it may not be compatible.
+static_assert(FLATBUFFERS_VERSION_MAJOR == 2 &&
+              FLATBUFFERS_VERSION_MINOR == 0 &&
+              FLATBUFFERS_VERSION_REVISION == 8,
+             "Non-compatible flatbuffers version included");
+
+namespace akida {
+namespace fb {
+
+struct DeviceVersion;
+struct DeviceVersionBuilder;
+
+struct Np;
+struct NpBuilder;
+
+struct NpTrack;
+struct NpTrackBuilder;
+
+struct Fnp2FilterTrack;
+struct Fnp2FilterTrackBuilder;
+
+struct EpgTrack;
+struct EpgTrackBuilder;
+
+struct Record;
+struct RecordBuilder;
+
+struct Pass;
+struct PassBuilder;
+
+struct DmaConfigHeader;
+
+struct LearningLayer;
+struct LearningLayerBuilder;
+
+struct Program;
+struct ProgramBuilder;
+
+enum IoType : int8_t {
+  IoType_dense = 0,
+  IoType_fnp_sparse = 1,
+  IoType_cnp_sparse = 2,
+  IoType_hrc_sparse = 3,
+  IoType_MIN = IoType_dense,
+  IoType_MAX = IoType_hrc_sparse
+};
+
+inline const IoType (&EnumValuesIoType())[4] {
+  static const IoType values[] = {
+    IoType_dense,
+    IoType_fnp_sparse,
+    IoType_cnp_sparse,
+    IoType_hrc_sparse
+  };
+  return values;
+}
+
+inline const char * const *EnumNamesIoType() {
+  static const char * const names[5] = {
+    "dense",
+    "fnp_sparse",
+    "cnp_sparse",
+    "hrc_sparse",
+    nullptr
+  };
+  return names;
+}
+
+inline const char *EnumNameIoType(IoType e) {
+  if (flatbuffers::IsOutRange(e, IoType_dense, IoType_hrc_sparse)) return "";
+  const size_t index = static_cast<size_t>(e);
+  return EnumNamesIoType()[index];
+}
+
+FLATBUFFERS_MANUALLY_ALIGNED_STRUCT(4) DmaConfigHeader FLATBUFFERS_FINAL_CLASS {
+ private:
+  uint32_t w1_;
+  uint32_t w2_;
+
+ public:
+  DmaConfigHeader()
+      : w1_(0),
+        w2_(0) {
+  }
+  DmaConfigHeader(uint32_t _w1, uint32_t _w2)
+      : w1_(flatbuffers::EndianScalar(_w1)),
+        w2_(flatbuffers::EndianScalar(_w2)) {
+  }
+  uint32_t w1() const {
+    return flatbuffers::EndianScalar(w1_);
+  }
+  uint32_t w2() const {
+    return flatbuffers::EndianScalar(w2_);
+  }
+};
+FLATBUFFERS_STRUCT_END(DmaConfigHeader, 8);
+
+struct DeviceVersion FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
+  typedef DeviceVersionBuilder Builder;
+  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
+    VT_VENDOR_ID = 4,
+    VT_PRODUCT_ID = 6,
+    VT_MAJOR_REV = 8,
+    VT_MINOR_REV = 10
+  };
+  uint8_t vendor_id() const {
+    return GetField<uint8_t>(VT_VENDOR_ID, 0);
+  }
+  uint8_t product_id() const {
+    return GetField<uint8_t>(VT_PRODUCT_ID, 0);
+  }
+  uint8_t major_rev() const {
+    return GetField<uint8_t>(VT_MAJOR_REV, 0);
+  }
+  uint8_t minor_rev() const {
+    return GetField<uint8_t>(VT_MINOR_REV, 0);
+  }
+  bool Verify(flatbuffers::Verifier &verifier) const {
+    return VerifyTableStart(verifier) &&
+           VerifyField<uint8_t>(verifier, VT_VENDOR_ID, 1) &&
+           VerifyField<uint8_t>(verifier, VT_PRODUCT_ID, 1) &&
+           VerifyField<uint8_t>(verifier, VT_MAJOR_REV, 1) &&
+           VerifyField<uint8_t>(verifier, VT_MINOR_REV, 1) &&
+           verifier.EndTable();
+  }
+};
+
+struct DeviceVersionBuilder {
+  typedef DeviceVersion Table;
+  flatbuffers::FlatBufferBuilder &fbb_;
+  flatbuffers::uoffset_t start_;
+  void add_vendor_id(uint8_t vendor_id) {
+    fbb_.AddElement<uint8_t>(DeviceVersion::VT_VENDOR_ID, vendor_id, 0);
+  }
+  void add_product_id(uint8_t product_id) {
+    fbb_.AddElement<uint8_t>(DeviceVersion::VT_PRODUCT_ID, product_id, 0);
+  }
+  void add_major_rev(uint8_t major_rev) {
+    fbb_.AddElement<uint8_t>(DeviceVersion::VT_MAJOR_REV, major_rev, 0);
+  }
+  void add_minor_rev(uint8_t minor_rev) {
+    fbb_.AddElement<uint8_t>(DeviceVersion::VT_MINOR_REV, minor_rev, 0);
+  }
+  explicit DeviceVersionBuilder(flatbuffers::FlatBufferBuilder &_fbb)
+        : fbb_(_fbb) {
+    start_ = fbb_.StartTable();
+  }
+  flatbuffers::Offset<DeviceVersion> Finish() {
+    const auto end = fbb_.EndTable(start_);
+    auto o = flatbuffers::Offset<DeviceVersion>(end);
+    return o;
+  }
+};
+
+inline flatbuffers::Offset<DeviceVersion> CreateDeviceVersion(
+    flatbuffers::FlatBufferBuilder &_fbb,
+    uint8_t vendor_id = 0,
+    uint8_t product_id = 0,
+    uint8_t major_rev = 0,
+    uint8_t minor_rev = 0) {
+  DeviceVersionBuilder builder_(_fbb);
+  builder_.add_minor_rev(minor_rev);
+  builder_.add_major_rev(major_rev);
+  builder_.add_product_id(product_id);
+  builder_.add_vendor_id(vendor_id);
+  return builder_.Finish();
+}
+
+struct Np FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
+  typedef NpBuilder Builder;
+  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
+    VT_COL = 4,
+    VT_ROW = 6,
+    VT_ID = 8
+  };
+  uint8_t col() const {
+    return GetField<uint8_t>(VT_COL, 0);
+  }
+  uint8_t row() const {
+    return GetField<uint8_t>(VT_ROW, 0);
+  }
+  uint8_t id() const {
+    return GetField<uint8_t>(VT_ID, 0);
+  }
+  bool Verify(flatbuffers::Verifier &verifier) const {
+    return VerifyTableStart(verifier) &&
+           VerifyField<uint8_t>(verifier, VT_COL, 1) &&
+           VerifyField<uint8_t>(verifier, VT_ROW, 1) &&
+           VerifyField<uint8_t>(verifier, VT_ID, 1) &&
+           verifier.EndTable();
+  }
+};
+
+struct NpBuilder {
+  typedef Np Table;
+  flatbuffers::FlatBufferBuilder &fbb_;
+  flatbuffers::uoffset_t start_;
+  void add_col(uint8_t col) {
+    fbb_.AddElement<uint8_t>(Np::VT_COL, col, 0);
+  }
+  void add_row(uint8_t row) {
+    fbb_.AddElement<uint8_t>(Np::VT_ROW, row, 0);
+  }
+  void add_id(uint8_t id) {
+    fbb_.AddElement<uint8_t>(Np::VT_ID, id, 0);
+  }
+  explicit NpBuilder(flatbuffers::FlatBufferBuilder &_fbb)
+        : fbb_(_fbb) {
+    start_ = fbb_.StartTable();
+  }
+  flatbuffers::Offset<Np> Finish() {
+    const auto end = fbb_.EndTable(start_);
+    auto o = flatbuffers::Offset<Np>(end);
+    return o;
+  }
+};
+
+inline flatbuffers::Offset<Np> CreateNp(
+    flatbuffers::FlatBufferBuilder &_fbb,
+    uint8_t col = 0,
+    uint8_t row = 0,
+    uint8_t id = 0) {
+  NpBuilder builder_(_fbb);
+  builder_.add_id(id);
+  builder_.add_row(row);
+  builder_.add_col(col);
+  return builder_.Finish();
+}
+
+struct NpTrack FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
+  typedef NpTrackBuilder Builder;
+  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
+    VT_DATA = 4
+  };
+  const flatbuffers::Vector<uint32_t> *data() const {
+    return GetPointer<const flatbuffers::Vector<uint32_t> *>(VT_DATA);
+  }
+  bool Verify(flatbuffers::Verifier &verifier) const {
+    return VerifyTableStart(verifier) &&
+           VerifyOffset(verifier, VT_DATA) &&
+           verifier.VerifyVector(data()) &&
+           verifier.EndTable();
+  }
+};
+
+struct NpTrackBuilder {
+  typedef NpTrack Table;
+  flatbuffers::FlatBufferBuilder &fbb_;
+  flatbuffers::uoffset_t start_;
+  void add_data(flatbuffers::Offset<flatbuffers::Vector<uint32_t>> data) {
+    fbb_.AddOffset(NpTrack::VT_DATA, data);
+  }
+  explicit NpTrackBuilder(flatbuffers::FlatBufferBuilder &_fbb)
+        : fbb_(_fbb) {
+    start_ = fbb_.StartTable();
+  }
+  flatbuffers::Offset<NpTrack> Finish() {
+    const auto end = fbb_.EndTable(start_);
+    auto o = flatbuffers::Offset<NpTrack>(end);
+    return o;
+  }
+};
+
+inline flatbuffers::Offset<NpTrack> CreateNpTrack(
+    flatbuffers::FlatBufferBuilder &_fbb,
+    flatbuffers::Offset<flatbuffers::Vector<uint32_t>> data = 0) {
+  NpTrackBuilder builder_(_fbb);
+  builder_.add_data(data);
+  return builder_.Finish();
+}
+
+inline flatbuffers::Offset<NpTrack> CreateNpTrackDirect(
+    flatbuffers::FlatBufferBuilder &_fbb,
+    const std::vector<uint32_t> *data = nullptr) {
+  auto data__ = data ? _fbb.CreateVector<uint32_t>(*data) : 0;
+  return akida::fb::CreateNpTrack(
+      _fbb,
+      data__);
+}
+
+struct Fnp2FilterTrack FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
+  typedef Fnp2FilterTrackBuilder Builder;
+  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
+    VT_NP = 4,
+    VT_DATA = 6
+  };
+  const akida::fb::Np *np() const {
+    return GetPointer<const akida::fb::Np *>(VT_NP);
+  }
+  const flatbuffers::Vector<uint32_t> *data() const {
+    return GetPointer<const flatbuffers::Vector<uint32_t> *>(VT_DATA);
+  }
+  bool Verify(flatbuffers::Verifier &verifier) const {
+    return VerifyTableStart(verifier) &&
+           VerifyOffset(verifier, VT_NP) &&
+           verifier.VerifyTable(np()) &&
+           VerifyOffset(verifier, VT_DATA) &&
+           verifier.VerifyVector(data()) &&
+           verifier.EndTable();
+  }
+};
+
+struct Fnp2FilterTrackBuilder {
+  typedef Fnp2FilterTrack Table;
+  flatbuffers::FlatBufferBuilder &fbb_;
+  flatbuffers::uoffset_t start_;
+  void add_np(flatbuffers::Offset<akida::fb::Np> np) {
+    fbb_.AddOffset(Fnp2FilterTrack::VT_NP, np);
+  }
+  void add_data(flatbuffers::Offset<flatbuffers::Vector<uint32_t>> data) {
+    fbb_.AddOffset(Fnp2FilterTrack::VT_DATA, data);
+  }
+  explicit Fnp2FilterTrackBuilder(flatbuffers::FlatBufferBuilder &_fbb)
+        : fbb_(_fbb) {
+    start_ = fbb_.StartTable();
+  }
+  flatbuffers::Offset<Fnp2FilterTrack> Finish() {
+    const auto end = fbb_.EndTable(start_);
+    auto o = flatbuffers::Offset<Fnp2FilterTrack>(end);
+    return o;
+  }
+};
+
+inline flatbuffers::Offset<Fnp2FilterTrack> CreateFnp2FilterTrack(
+    flatbuffers::FlatBufferBuilder &_fbb,
+    flatbuffers::Offset<akida::fb::Np> np = 0,
+    flatbuffers::Offset<flatbuffers::Vector<uint32_t>> data = 0) {
+  Fnp2FilterTrackBuilder builder_(_fbb);
+  builder_.add_data(data);
+  builder_.add_np(np);
+  return builder_.Finish();
+}
+
+inline flatbuffers::Offset<Fnp2FilterTrack> CreateFnp2FilterTrackDirect(
+    flatbuffers::FlatBufferBuilder &_fbb,
+    flatbuffers::Offset<akida::fb::Np> np = 0,
+    const std::vector<uint32_t> *data = nullptr) {
+  auto data__ = data ? _fbb.CreateVector<uint32_t>(*data) : 0;
+  return akida::fb::CreateFnp2FilterTrack(
+      _fbb,
+      np,
+      data__);
+}
+
+struct EpgTrack FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
+  typedef EpgTrackBuilder Builder;
+  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
+    VT_ADDRESS = 4,
+    VT_DATA = 6
+  };
+  uint32_t address() const {
+    return GetField<uint32_t>(VT_ADDRESS, 0);
+  }
+  uint32_t data() const {
+    return GetField<uint32_t>(VT_DATA, 0);
+  }
+  bool Verify(flatbuffers::Verifier &verifier) const {
+    return VerifyTableStart(verifier) &&
+           VerifyField<uint32_t>(verifier, VT_ADDRESS, 4) &&
+           VerifyField<uint32_t>(verifier, VT_DATA, 4) &&
+           verifier.EndTable();
+  }
+};
+
+struct EpgTrackBuilder {
+  typedef EpgTrack Table;
+  flatbuffers::FlatBufferBuilder &fbb_;
+  flatbuffers::uoffset_t start_;
+  void add_address(uint32_t address) {
+    fbb_.AddElement<uint32_t>(EpgTrack::VT_ADDRESS, address, 0);
+  }
+  void add_data(uint32_t data) {
+    fbb_.AddElement<uint32_t>(EpgTrack::VT_DATA, data, 0);
+  }
+  explicit EpgTrackBuilder(flatbuffers::FlatBufferBuilder &_fbb)
+        : fbb_(_fbb) {
+    start_ = fbb_.StartTable();
+  }
+  flatbuffers::Offset<EpgTrack> Finish() {
+    const auto end = fbb_.EndTable(start_);
+    auto o = flatbuffers::Offset<EpgTrack>(end);
+    return o;
+  }
+};
+
+inline flatbuffers::Offset<EpgTrack> CreateEpgTrack(
+    flatbuffers::FlatBufferBuilder &_fbb,
+    uint32_t address = 0,
+    uint32_t data = 0) {
+  EpgTrackBuilder builder_(_fbb);
+  builder_.add_data(data);
+  builder_.add_address(address);
+  return builder_.Finish();
+}
+
+struct Record FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
+  typedef RecordBuilder Builder;
+  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
+    VT_NP_TRACKS = 4,
+    VT_FNP2_TRACK = 6
+  };
+  const flatbuffers::Vector<flatbuffers::Offset<akida::fb::NpTrack>> *np_tracks() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<akida::fb::NpTrack>> *>(VT_NP_TRACKS);
+  }
+  const akida::fb::Fnp2FilterTrack *fnp2_track() const {
+    return GetPointer<const akida::fb::Fnp2FilterTrack *>(VT_FNP2_TRACK);
+  }
+  bool Verify(flatbuffers::Verifier &verifier) const {
+    return VerifyTableStart(verifier) &&
+           VerifyOffset(verifier, VT_NP_TRACKS) &&
+           verifier.VerifyVector(np_tracks()) &&
+           verifier.VerifyVectorOfTables(np_tracks()) &&
+           VerifyOffset(verifier, VT_FNP2_TRACK) &&
+           verifier.VerifyTable(fnp2_track()) &&
+           verifier.EndTable();
+  }
+};
+
+struct RecordBuilder {
+  typedef Record Table;
+  flatbuffers::FlatBufferBuilder &fbb_;
+  flatbuffers::uoffset_t start_;
+  void add_np_tracks(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<akida::fb::NpTrack>>> np_tracks) {
+    fbb_.AddOffset(Record::VT_NP_TRACKS, np_tracks);
+  }
+  void add_fnp2_track(flatbuffers::Offset<akida::fb::Fnp2FilterTrack> fnp2_track) {
+    fbb_.AddOffset(Record::VT_FNP2_TRACK, fnp2_track);
+  }
+  explicit RecordBuilder(flatbuffers::FlatBufferBuilder &_fbb)
+        : fbb_(_fbb) {
+    start_ = fbb_.StartTable();
+  }
+  flatbuffers::Offset<Record> Finish() {
+    const auto end = fbb_.EndTable(start_);
+    auto o = flatbuffers::Offset<Record>(end);
+    return o;
+  }
+};
+
+inline flatbuffers::Offset<Record> CreateRecord(
+    flatbuffers::FlatBufferBuilder &_fbb,
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<akida::fb::NpTrack>>> np_tracks = 0,
+    flatbuffers::Offset<akida::fb::Fnp2FilterTrack> fnp2_track = 0) {
+  RecordBuilder builder_(_fbb);
+  builder_.add_fnp2_track(fnp2_track);
+  builder_.add_np_tracks(np_tracks);
+  return builder_.Finish();
+}
+
+inline flatbuffers::Offset<Record> CreateRecordDirect(
+    flatbuffers::FlatBufferBuilder &_fbb,
+    const std::vector<flatbuffers::Offset<akida::fb::NpTrack>> *np_tracks = nullptr,
+    flatbuffers::Offset<akida::fb::Fnp2FilterTrack> fnp2_track = 0) {
+  auto np_tracks__ = np_tracks ? _fbb.CreateVector<flatbuffers::Offset<akida::fb::NpTrack>>(*np_tracks) : 0;
+  return akida::fb::CreateRecord(
+      _fbb,
+      np_tracks__,
+      fnp2_track);
+}
+
+struct Pass FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
+  typedef PassBuilder Builder;
+  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
+    VT_RECORDS = 4
+  };
+  const flatbuffers::Vector<flatbuffers::Offset<akida::fb::Record>> *records() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<akida::fb::Record>> *>(VT_RECORDS);
+  }
+  bool Verify(flatbuffers::Verifier &verifier) const {
+    return VerifyTableStart(verifier) &&
+           VerifyOffset(verifier, VT_RECORDS) &&
+           verifier.VerifyVector(records()) &&
+           verifier.VerifyVectorOfTables(records()) &&
+           verifier.EndTable();
+  }
+};
+
+struct PassBuilder {
+  typedef Pass Table;
+  flatbuffers::FlatBufferBuilder &fbb_;
+  flatbuffers::uoffset_t start_;
+  void add_records(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<akida::fb::Record>>> records) {
+    fbb_.AddOffset(Pass::VT_RECORDS, records);
+  }
+  explicit PassBuilder(flatbuffers::FlatBufferBuilder &_fbb)
+        : fbb_(_fbb) {
+    start_ = fbb_.StartTable();
+  }
+  flatbuffers::Offset<Pass> Finish() {
+    const auto end = fbb_.EndTable(start_);
+    auto o = flatbuffers::Offset<Pass>(end);
+    return o;
+  }
+};
+
+inline flatbuffers::Offset<Pass> CreatePass(
+    flatbuffers::FlatBufferBuilder &_fbb,
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<akida::fb::Record>>> records = 0) {
+  PassBuilder builder_(_fbb);
+  builder_.add_records(records);
+  return builder_.Finish();
+}
+
+inline flatbuffers::Offset<Pass> CreatePassDirect(
+    flatbuffers::FlatBufferBuilder &_fbb,
+    const std::vector<flatbuffers::Offset<akida::fb::Record>> *records = nullptr) {
+  auto records__ = records ? _fbb.CreateVector<flatbuffers::Offset<akida::fb::Record>>(*records) : 0;
+  return akida::fb::CreatePass(
+      _fbb,
+      records__);
+}
+
+struct LearningLayer FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
+  typedef LearningLayerBuilder Builder;
+  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
+    VT_NP = 4,
+    VT_LEARNING_REGISTERS = 6,
+    VT_INFERENCE_REGISTERS = 8,
+    VT_RAM = 10,
+    VT_LEARN_MEM_SIZE = 12,
+    VT_UPDATE_LEARN_MEM_HDR = 14
+  };
+  const akida::fb::Np *np() const {
+    return GetPointer<const akida::fb::Np *>(VT_NP);
+  }
+  const akida::fb::Record *learning_registers() const {
+    return GetPointer<const akida::fb::Record *>(VT_LEARNING_REGISTERS);
+  }
+  const akida::fb::Record *inference_registers() const {
+    return GetPointer<const akida::fb::Record *>(VT_INFERENCE_REGISTERS);
+  }
+  const akida::fb::Record *ram() const {
+    return GetPointer<const akida::fb::Record *>(VT_RAM);
+  }
+  uint32_t learn_mem_size() const {
+    return GetField<uint32_t>(VT_LEARN_MEM_SIZE, 0);
+  }
+  const akida::fb::DmaConfigHeader *update_learn_mem_hdr() const {
+    return GetStruct<const akida::fb::DmaConfigHeader *>(VT_UPDATE_LEARN_MEM_HDR);
+  }
+  bool Verify(flatbuffers::Verifier &verifier) const {
+    return VerifyTableStart(verifier) &&
+           VerifyOffset(verifier, VT_NP) &&
+           verifier.VerifyTable(np()) &&
+           VerifyOffset(verifier, VT_LEARNING_REGISTERS) &&
+           verifier.VerifyTable(learning_registers()) &&
+           VerifyOffset(verifier, VT_INFERENCE_REGISTERS) &&
+           verifier.VerifyTable(inference_registers()) &&
+           VerifyOffset(verifier, VT_RAM) &&
+           verifier.VerifyTable(ram()) &&
+           VerifyField<uint32_t>(verifier, VT_LEARN_MEM_SIZE, 4) &&
+           VerifyField<akida::fb::DmaConfigHeader>(verifier, VT_UPDATE_LEARN_MEM_HDR, 4) &&
+           verifier.EndTable();
+  }
+};
+
+struct LearningLayerBuilder {
+  typedef LearningLayer Table;
+  flatbuffers::FlatBufferBuilder &fbb_;
+  flatbuffers::uoffset_t start_;
+  void add_np(flatbuffers::Offset<akida::fb::Np> np) {
+    fbb_.AddOffset(LearningLayer::VT_NP, np);
+  }
+  void add_learning_registers(flatbuffers::Offset<akida::fb::Record> learning_registers) {
+    fbb_.AddOffset(LearningLayer::VT_LEARNING_REGISTERS, learning_registers);
+  }
+  void add_inference_registers(flatbuffers::Offset<akida::fb::Record> inference_registers) {
+    fbb_.AddOffset(LearningLayer::VT_INFERENCE_REGISTERS, inference_registers);
+  }
+  void add_ram(flatbuffers::Offset<akida::fb::Record> ram) {
+    fbb_.AddOffset(LearningLayer::VT_RAM, ram);
+  }
+  void add_learn_mem_size(uint32_t learn_mem_size) {
+    fbb_.AddElement<uint32_t>(LearningLayer::VT_LEARN_MEM_SIZE, learn_mem_size, 0);
+  }
+  void add_update_learn_mem_hdr(const akida::fb::DmaConfigHeader *update_learn_mem_hdr) {
+    fbb_.AddStruct(LearningLayer::VT_UPDATE_LEARN_MEM_HDR, update_learn_mem_hdr);
+  }
+  explicit LearningLayerBuilder(flatbuffers::FlatBufferBuilder &_fbb)
+        : fbb_(_fbb) {
+    start_ = fbb_.StartTable();
+  }
+  flatbuffers::Offset<LearningLayer> Finish() {
+    const auto end = fbb_.EndTable(start_);
+    auto o = flatbuffers::Offset<LearningLayer>(end);
+    return o;
+  }
+};
+
+inline flatbuffers::Offset<LearningLayer> CreateLearningLayer(
+    flatbuffers::FlatBufferBuilder &_fbb,
+    flatbuffers::Offset<akida::fb::Np> np = 0,
+    flatbuffers::Offset<akida::fb::Record> learning_registers = 0,
+    flatbuffers::Offset<akida::fb::Record> inference_registers = 0,
+    flatbuffers::Offset<akida::fb::Record> ram = 0,
+    uint32_t learn_mem_size = 0,
+    const akida::fb::DmaConfigHeader *update_learn_mem_hdr = nullptr) {
+  LearningLayerBuilder builder_(_fbb);
+  builder_.add_update_learn_mem_hdr(update_learn_mem_hdr);
+  builder_.add_learn_mem_size(learn_mem_size);
+  builder_.add_ram(ram);
+  builder_.add_inference_registers(inference_registers);
+  builder_.add_learning_registers(learning_registers);
+  builder_.add_np(np);
+  return builder_.Finish();
+}
+
+struct Program FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
+  typedef ProgramBuilder Builder;
+  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
+    VT_VERSION = 4,
+    VT_DEVICE_VERSION = 6,
+    VT_INPUT_DIMS = 8,
+    VT_OUTPUT_DIMS = 10,
+    VT_INPUT_TYPE = 12,
+    VT_OUTPUT_TYPE = 14,
+    VT_ACTIVATION = 16,
+    VT_DENSE_WINDOW_W = 18,
+    VT_DENSE_WINDOW_H = 20,
+    VT_PASSES = 22,
+    VT_LEARNING_LAYER = 24,
+    VT_EPG_TRACKS = 26,
+    VT_SHIFTS = 28,
+    VT_SCALES = 30,
+    VT_DUMMY_DESC_HDR = 32,
+    VT_MAX_NUM_DESC = 34
+  };
+  const flatbuffers::String *version() const {
+    return GetPointer<const flatbuffers::String *>(VT_VERSION);
+  }
+  const akida::fb::DeviceVersion *device_version() const {
+    return GetPointer<const akida::fb::DeviceVersion *>(VT_DEVICE_VERSION);
+  }
+  const flatbuffers::Vector<uint32_t> *input_dims() const {
+    return GetPointer<const flatbuffers::Vector<uint32_t> *>(VT_INPUT_DIMS);
+  }
+  const flatbuffers::Vector<uint32_t> *output_dims() const {
+    return GetPointer<const flatbuffers::Vector<uint32_t> *>(VT_OUTPUT_DIMS);
+  }
+  akida::fb::IoType input_type() const {
+    return static_cast<akida::fb::IoType>(GetField<int8_t>(VT_INPUT_TYPE, 0));
+  }
+  akida::fb::IoType output_type() const {
+    return static_cast<akida::fb::IoType>(GetField<int8_t>(VT_OUTPUT_TYPE, 0));
+  }
+  bool activation() const {
+    return GetField<uint8_t>(VT_ACTIVATION, 0) != 0;
+  }
+  uint32_t dense_window_w() const {
+    return GetField<uint32_t>(VT_DENSE_WINDOW_W, 0);
+  }
+  uint32_t dense_window_h() const {
+    return GetField<uint32_t>(VT_DENSE_WINDOW_H, 0);
+  }
+  const flatbuffers::Vector<flatbuffers::Offset<akida::fb::Pass>> *passes() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<akida::fb::Pass>> *>(VT_PASSES);
+  }
+  const akida::fb::LearningLayer *learning_layer() const {
+    return GetPointer<const akida::fb::LearningLayer *>(VT_LEARNING_LAYER);
+  }
+  const flatbuffers::Vector<flatbuffers::Offset<akida::fb::EpgTrack>> *epg_tracks() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<akida::fb::EpgTrack>> *>(VT_EPG_TRACKS);
+  }
+  const flatbuffers::Vector<int32_t> *shifts() const {
+    return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_SHIFTS);
+  }
+  const flatbuffers::Vector<float> *scales() const {
+    return GetPointer<const flatbuffers::Vector<float> *>(VT_SCALES);
+  }
+  const akida::fb::DmaConfigHeader *dummy_desc_hdr() const {
+    return GetStruct<const akida::fb::DmaConfigHeader *>(VT_DUMMY_DESC_HDR);
+  }
+  uint8_t max_num_desc() const {
+    return GetField<uint8_t>(VT_MAX_NUM_DESC, 0);
+  }
+  bool Verify(flatbuffers::Verifier &verifier) const {
+    return VerifyTableStart(verifier) &&
+           VerifyOffset(verifier, VT_VERSION) &&
+           verifier.VerifyString(version()) &&
+           VerifyOffset(verifier, VT_DEVICE_VERSION) &&
+           verifier.VerifyTable(device_version()) &&
+           VerifyOffset(verifier, VT_INPUT_DIMS) &&
+           verifier.VerifyVector(input_dims()) &&
+           VerifyOffset(verifier, VT_OUTPUT_DIMS) &&
+           verifier.VerifyVector(output_dims()) &&
+           VerifyField<int8_t>(verifier, VT_INPUT_TYPE, 1) &&
+           VerifyField<int8_t>(verifier, VT_OUTPUT_TYPE, 1) &&
+           VerifyField<uint8_t>(verifier, VT_ACTIVATION, 1) &&
+           VerifyField<uint32_t>(verifier, VT_DENSE_WINDOW_W, 4) &&
+           VerifyField<uint32_t>(verifier, VT_DENSE_WINDOW_H, 4) &&
+           VerifyOffset(verifier, VT_PASSES) &&
+           verifier.VerifyVector(passes()) &&
+           verifier.VerifyVectorOfTables(passes()) &&
+           VerifyOffset(verifier, VT_LEARNING_LAYER) &&
+           verifier.VerifyTable(learning_layer()) &&
+           VerifyOffset(verifier, VT_EPG_TRACKS) &&
+           verifier.VerifyVector(epg_tracks()) &&
+           verifier.VerifyVectorOfTables(epg_tracks()) &&
+           VerifyOffset(verifier, VT_SHIFTS) &&
+           verifier.VerifyVector(shifts()) &&
+           VerifyOffset(verifier, VT_SCALES) &&
+           verifier.VerifyVector(scales()) &&
+           VerifyField<akida::fb::DmaConfigHeader>(verifier, VT_DUMMY_DESC_HDR, 4) &&
+           VerifyField<uint8_t>(verifier, VT_MAX_NUM_DESC, 1) &&
+           verifier.EndTable();
+  }
+};
+
+struct ProgramBuilder {
+  typedef Program Table;
+  flatbuffers::FlatBufferBuilder &fbb_;
+  flatbuffers::uoffset_t start_;
+  void add_version(flatbuffers::Offset<flatbuffers::String> version) {
+    fbb_.AddOffset(Program::VT_VERSION, version);
+  }
+  void add_device_version(flatbuffers::Offset<akida::fb::DeviceVersion> device_version) {
+    fbb_.AddOffset(Program::VT_DEVICE_VERSION, device_version);
+  }
+  void add_input_dims(flatbuffers::Offset<flatbuffers::Vector<uint32_t>> input_dims) {
+    fbb_.AddOffset(Program::VT_INPUT_DIMS, input_dims);
+  }
+  void add_output_dims(flatbuffers::Offset<flatbuffers::Vector<uint32_t>> output_dims) {
+    fbb_.AddOffset(Program::VT_OUTPUT_DIMS, output_dims);
+  }
+  void add_input_type(akida::fb::IoType input_type) {
+    fbb_.AddElement<int8_t>(Program::VT_INPUT_TYPE, static_cast<int8_t>(input_type), 0);
+  }
+  void add_output_type(akida::fb::IoType output_type) {
+    fbb_.AddElement<int8_t>(Program::VT_OUTPUT_TYPE, static_cast<int8_t>(output_type), 0);
+  }
+  void add_activation(bool activation) {
+    fbb_.AddElement<uint8_t>(Program::VT_ACTIVATION, static_cast<uint8_t>(activation), 0);
+  }
+  void add_dense_window_w(uint32_t dense_window_w) {
+    fbb_.AddElement<uint32_t>(Program::VT_DENSE_WINDOW_W, dense_window_w, 0);
+  }
+  void add_dense_window_h(uint32_t dense_window_h) {
+    fbb_.AddElement<uint32_t>(Program::VT_DENSE_WINDOW_H, dense_window_h, 0);
+  }
+  void add_passes(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<akida::fb::Pass>>> passes) {
+    fbb_.AddOffset(Program::VT_PASSES, passes);
+  }
+  void add_learning_layer(flatbuffers::Offset<akida::fb::LearningLayer> learning_layer) {
+    fbb_.AddOffset(Program::VT_LEARNING_LAYER, learning_layer);
+  }
+  void add_epg_tracks(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<akida::fb::EpgTrack>>> epg_tracks) {
+    fbb_.AddOffset(Program::VT_EPG_TRACKS, epg_tracks);
+  }
+  void add_shifts(flatbuffers::Offset<flatbuffers::Vector<int32_t>> shifts) {
+    fbb_.AddOffset(Program::VT_SHIFTS, shifts);
+  }
+  void add_scales(flatbuffers::Offset<flatbuffers::Vector<float>> scales) {
+    fbb_.AddOffset(Program::VT_SCALES, scales);
+  }
+  void add_dummy_desc_hdr(const akida::fb::DmaConfigHeader *dummy_desc_hdr) {
+    fbb_.AddStruct(Program::VT_DUMMY_DESC_HDR, dummy_desc_hdr);
+  }
+  void add_max_num_desc(uint8_t max_num_desc) {
+    fbb_.AddElement<uint8_t>(Program::VT_MAX_NUM_DESC, max_num_desc, 0);
+  }
+  explicit ProgramBuilder(flatbuffers::FlatBufferBuilder &_fbb)
+        : fbb_(_fbb) {
+    start_ = fbb_.StartTable();
+  }
+  flatbuffers::Offset<Program> Finish() {
+    const auto end = fbb_.EndTable(start_);
+    auto o = flatbuffers::Offset<Program>(end);
+    return o;
+  }
+};
+
+inline flatbuffers::Offset<Program> CreateProgram(
+    flatbuffers::FlatBufferBuilder &_fbb,
+    flatbuffers::Offset<flatbuffers::String> version = 0,
+    flatbuffers::Offset<akida::fb::DeviceVersion> device_version = 0,
+    flatbuffers::Offset<flatbuffers::Vector<uint32_t>> input_dims = 0,
+    flatbuffers::Offset<flatbuffers::Vector<uint32_t>> output_dims = 0,
+    akida::fb::IoType input_type = akida::fb::IoType_dense,
+    akida::fb::IoType output_type = akida::fb::IoType_dense,
+    bool activation = false,
+    uint32_t dense_window_w = 0,
+    uint32_t dense_window_h = 0,
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<akida::fb::Pass>>> passes = 0,
+    flatbuffers::Offset<akida::fb::LearningLayer> learning_layer = 0,
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<akida::fb::EpgTrack>>> epg_tracks = 0,
+    flatbuffers::Offset<flatbuffers::Vector<int32_t>> shifts = 0,
+    flatbuffers::Offset<flatbuffers::Vector<float>> scales = 0,
+    const akida::fb::DmaConfigHeader *dummy_desc_hdr = nullptr,
+    uint8_t max_num_desc = 0) {
+  ProgramBuilder builder_(_fbb);
+  builder_.add_dummy_desc_hdr(dummy_desc_hdr);
+  builder_.add_scales(scales);
+  builder_.add_shifts(shifts);
+  builder_.add_epg_tracks(epg_tracks);
+  builder_.add_learning_layer(learning_layer);
+  builder_.add_passes(passes);
+  builder_.add_dense_window_h(dense_window_h);
+  builder_.add_dense_window_w(dense_window_w);
+  builder_.add_output_dims(output_dims);
+  builder_.add_input_dims(input_dims);
+  builder_.add_device_version(device_version);
+  builder_.add_version(version);
+  builder_.add_max_num_desc(max_num_desc);
+  builder_.add_activation(activation);
+  builder_.add_output_type(output_type);
+  builder_.add_input_type(input_type);
+  return builder_.Finish();
+}
+
+inline flatbuffers::Offset<Program> CreateProgramDirect(
+    flatbuffers::FlatBufferBuilder &_fbb,
+    const char *version = nullptr,
+    flatbuffers::Offset<akida::fb::DeviceVersion> device_version = 0,
+    const std::vector<uint32_t> *input_dims = nullptr,
+    const std::vector<uint32_t> *output_dims = nullptr,
+    akida::fb::IoType input_type = akida::fb::IoType_dense,
+    akida::fb::IoType output_type = akida::fb::IoType_dense,
+    bool activation = false,
+    uint32_t dense_window_w = 0,
+    uint32_t dense_window_h = 0,
+    const std::vector<flatbuffers::Offset<akida::fb::Pass>> *passes = nullptr,
+    flatbuffers::Offset<akida::fb::LearningLayer> learning_layer = 0,
+    const std::vector<flatbuffers::Offset<akida::fb::EpgTrack>> *epg_tracks = nullptr,
+    const std::vector<int32_t> *shifts = nullptr,
+    const std::vector<float> *scales = nullptr,
+    const akida::fb::DmaConfigHeader *dummy_desc_hdr = nullptr,
+    uint8_t max_num_desc = 0) {
+  auto version__ = version ? _fbb.CreateString(version) : 0;
+  auto input_dims__ = input_dims ? _fbb.CreateVector<uint32_t>(*input_dims) : 0;
+  auto output_dims__ = output_dims ? _fbb.CreateVector<uint32_t>(*output_dims) : 0;
+  auto passes__ = passes ? _fbb.CreateVector<flatbuffers::Offset<akida::fb::Pass>>(*passes) : 0;
+  auto epg_tracks__ = epg_tracks ? _fbb.CreateVector<flatbuffers::Offset<akida::fb::EpgTrack>>(*epg_tracks) : 0;
+  auto shifts__ = shifts ? _fbb.CreateVector<int32_t>(*shifts) : 0;
+  auto scales__ = scales ? _fbb.CreateVector<float>(*scales) : 0;
+  return akida::fb::CreateProgram(
+      _fbb,
+      version__,
+      device_version,
+      input_dims__,
+      output_dims__,
+      input_type,
+      output_type,
+      activation,
+      dense_window_w,
+      dense_window_h,
+      passes__,
+      learning_layer,
+      epg_tracks__,
+      shifts__,
+      scales__,
+      dummy_desc_hdr,
+      max_num_desc);
+}
+
+inline const akida::fb::Program *GetProgram(const void *buf) {
+  return flatbuffers::GetRoot<akida::fb::Program>(buf);
+}
+
+inline const akida::fb::Program *GetSizePrefixedProgram(const void *buf) {
+  return flatbuffers::GetSizePrefixedRoot<akida::fb::Program>(buf);
+}
+
+inline bool VerifyProgramBuffer(
+    flatbuffers::Verifier &verifier) {
+  return verifier.VerifyBuffer<akida::fb::Program>(nullptr);
+}
+
+inline bool VerifySizePrefixedProgramBuffer(
+    flatbuffers::Verifier &verifier) {
+  return verifier.VerifySizePrefixedBuffer<akida::fb::Program>(nullptr);
+}
+
+inline void FinishProgramBuffer(
+    flatbuffers::FlatBufferBuilder &fbb,
+    flatbuffers::Offset<akida::fb::Program> root) {
+  fbb.Finish(root);
+}
+
+inline void FinishSizePrefixedProgramBuffer(
+    flatbuffers::FlatBufferBuilder &fbb,
+    flatbuffers::Offset<akida::fb::Program> root) {
+  fbb.FinishSizePrefixed(root);
+}
+
+}  // namespace fb
+}  // namespace akida
+
+#endif  // FLATBUFFERS_GENERATED_AKIDADEVICEPROGRAMFB_AKIDA_FB_H_
```

## akida/engine/inc/engine/dma.h

 * *Ordering differences only*

```diff
@@ -1,32 +1,32 @@
-#pragma once
-
-#include <cstddef>
-#include <cstdint>
-#include <vector>
-
-namespace akida {
-namespace dma {
-
-using w32 = uint32_t;
-using wbuffer = std::vector<w32>;
-
-// Akida memory addresses are stored in uint32_t
-using addr = uint32_t;
-// Many operations require address alignment to 32 bit.
-// Inputs and outputs for all inbound buffers for DMA controllers (except for
-// HRC, that can be just byte aligned), and for all outbound buffers used by DMA
-// controllers.
-constexpr uint32_t kAlignment = sizeof(addr);
-
-// Sparse tensors use 2 words per item
-constexpr uint32_t kSparseEventWordSize = 2;
-constexpr size_t kSparseEventByteSize = kSparseEventWordSize * sizeof(dma::w32);
-// Output from DMA has a header
-constexpr uint32_t kOutputHeaderByteSize = 0x20;
-
-static constexpr uint32_t kMinNbDescriptors = 2;
-static constexpr uint32_t kMaxNbDescriptors = 16;
-static constexpr uint32_t kMaxNbDescriptorsMultipass = 256;
-
-}  // namespace dma
-}  // namespace akida
+#pragma once
+
+#include <cstddef>
+#include <cstdint>
+#include <vector>
+
+namespace akida {
+namespace dma {
+
+using w32 = uint32_t;
+using wbuffer = std::vector<w32>;
+
+// Akida memory addresses are stored in uint32_t
+using addr = uint32_t;
+// Many operations require address alignment to 32 bit.
+// Inputs and outputs for all inbound buffers for DMA controllers (except for
+// HRC, that can be just byte aligned), and for all outbound buffers used by DMA
+// controllers.
+constexpr uint32_t kAlignment = sizeof(addr);
+
+// Sparse tensors use 2 words per item
+constexpr uint32_t kSparseEventWordSize = 2;
+constexpr size_t kSparseEventByteSize = kSparseEventWordSize * sizeof(dma::w32);
+// Output from DMA has a header
+constexpr uint32_t kOutputHeaderByteSize = 0x20;
+
+static constexpr uint32_t kMinNbDescriptors = 2;
+static constexpr uint32_t kMaxNbDescriptors = 16;
+static constexpr uint32_t kMaxNbDescriptorsMultipass = 256;
+
+}  // namespace dma
+}  // namespace akida
```

## akida/engine/inc/engine/dma_config_ops.h

 * *Ordering differences only*

```diff
@@ -1,41 +1,41 @@
-#pragma once
-
-#include <cstdint>
-
-#include "akida/np.h"
-#include "engine/dma.h"
-
-namespace akida {
-
-namespace dma {
-
-enum class Target {
-  CnpFilter,
-  CnpLearnThres,
-  CnpFireThres,
-  FnpWeights,
-  NpRegisters,
-  HrcRegisters,
-  HrcSram,
-};
-
-// Return vector of 2 elements containing the DMA header
-wbuffer format_config_header(const struct np::Ident& np, Target target,
-                             uint32_t size, uint16_t dest_addr,
-                             bool xl = false);
-
-uint32_t parse_config_read_size(const wbuffer& read_header);
-
-bool config_block_size_needs_xl(uint32_t block_size);
-
-// Size of address
-constexpr uint32_t kXlIncrementSz = 16;
-constexpr uint32_t kConfigReadPacketHdrSz = 1;
-constexpr uint32_t kConfigReadPacketOffset = 32;
-constexpr uint32_t kConfigWriteHdrWordLen = 2;
-constexpr uint32_t kConfigWritePacketOffset =
-    kConfigWriteHdrWordLen * sizeof(dma::w32);
-
-}  // namespace dma
-
-}  // namespace akida
+#pragma once
+
+#include <cstdint>
+
+#include "akida/np.h"
+#include "engine/dma.h"
+
+namespace akida {
+
+namespace dma {
+
+enum class Target {
+  CnpFilter,
+  CnpLearnThres,
+  CnpFireThres,
+  FnpWeights,
+  NpRegisters,
+  HrcRegisters,
+  HrcSram,
+};
+
+// Return vector of 2 elements containing the DMA header
+wbuffer format_config_header(const struct np::Ident& np, Target target,
+                             uint32_t size, uint16_t dest_addr,
+                             bool xl = false);
+
+uint32_t parse_config_read_size(const wbuffer& read_header);
+
+bool config_block_size_needs_xl(uint32_t block_size);
+
+// Size of address
+constexpr uint32_t kXlIncrementSz = 16;
+constexpr uint32_t kConfigReadPacketHdrSz = 1;
+constexpr uint32_t kConfigReadPacketOffset = 32;
+constexpr uint32_t kConfigWriteHdrWordLen = 2;
+constexpr uint32_t kConfigWritePacketOffset =
+    kConfigWriteHdrWordLen * sizeof(dma::w32);
+
+}  // namespace dma
+
+}  // namespace akida
```

## akida/engine/inc/engine/int_conversion.h

 * *Ordering differences only*

```diff
@@ -1,60 +1,60 @@
-#pragma once
-
-#include <cstdint>
-
-namespace akida {
-
-template<int N>
-constexpr int32_t get_min_signed_value_N_bits() {
-  // Min signed value is the leftmost bit set to 1
-  return 1 << (N - 1);
-}
-
-template<int N>
-constexpr int32_t get_mask_N_bits() {
-  // Mask for N bits is all N bits set to 1
-  return (1 << N) - 1;
-}
-
-template<int N>
-inline uint32_t int32_to_intN(int32_t value) {
-  // Min signed value is the leftmost bit set to 1 (when represented by 32 bits,
-  // it is actually the absolute value of the minimum value represented by
-  // N-bits)
-  constexpr int32_t MIN_SIGNED_VALUE_N_BITS = get_min_signed_value_N_bits<N>();
-  // Max signed value is N - 1 bits set to 1 (so it is the absolute value of
-  // minimum minus 1)
-  constexpr int32_t MAX_SIGNED_VALUE_N_BITS = MIN_SIGNED_VALUE_N_BITS - 1;
-  // The sign bit must be shifted by 32 - N
-  constexpr int32_t SIGN_OFFSET = 32 - N;
-
-  // check value range is ok
-  if (value > MAX_SIGNED_VALUE_N_BITS || value < -MIN_SIGNED_VALUE_N_BITS) {
-    panic("%d cannot fit in a %d bits signed integer", value, N);
-  }
-  // get the (N - 1) bits
-  uint32_t result = value & get_mask_N_bits<N - 1>();
-  // and append sign bit
-  result |= (value >> SIGN_OFFSET) & MIN_SIGNED_VALUE_N_BITS;
-  return result;
-}
-
-template<int N>
-inline int32_t intN_to_int32(uint32_t val) {
-  // The mask for negative part is all leftmosts bits set to 1
-  constexpr int32_t MASK_NEGATIVE_PART_32_BITS = ~(get_mask_N_bits<N>());
-
-  // check if we have sign bit
-  uint32_t neg_sign = (val & get_min_signed_value_N_bits<N>());
-
-  // For negative value, propagate the bit sign to the leftmosts bits
-  if (neg_sign != 0) {
-    // Set most significant bits
-    return val | MASK_NEGATIVE_PART_32_BITS;
-  } else {
-    // Positives values are untouched
-    return static_cast<int32_t>(val);
-  }
-}
-
-}  // namespace akida
+#pragma once
+
+#include <cstdint>
+
+namespace akida {
+
+template<int N>
+constexpr int32_t get_min_signed_value_N_bits() {
+  // Min signed value is the leftmost bit set to 1
+  return 1 << (N - 1);
+}
+
+template<int N>
+constexpr int32_t get_mask_N_bits() {
+  // Mask for N bits is all N bits set to 1
+  return (1 << N) - 1;
+}
+
+template<int N>
+inline uint32_t int32_to_intN(int32_t value) {
+  // Min signed value is the leftmost bit set to 1 (when represented by 32 bits,
+  // it is actually the absolute value of the minimum value represented by
+  // N-bits)
+  constexpr int32_t MIN_SIGNED_VALUE_N_BITS = get_min_signed_value_N_bits<N>();
+  // Max signed value is N - 1 bits set to 1 (so it is the absolute value of
+  // minimum minus 1)
+  constexpr int32_t MAX_SIGNED_VALUE_N_BITS = MIN_SIGNED_VALUE_N_BITS - 1;
+  // The sign bit must be shifted by 32 - N
+  constexpr int32_t SIGN_OFFSET = 32 - N;
+
+  // check value range is ok
+  if (value > MAX_SIGNED_VALUE_N_BITS || value < -MIN_SIGNED_VALUE_N_BITS) {
+    panic("%d cannot fit in a %d bits signed integer", value, N);
+  }
+  // get the (N - 1) bits
+  uint32_t result = value & get_mask_N_bits<N - 1>();
+  // and append sign bit
+  result |= (value >> SIGN_OFFSET) & MIN_SIGNED_VALUE_N_BITS;
+  return result;
+}
+
+template<int N>
+inline int32_t intN_to_int32(uint32_t val) {
+  // The mask for negative part is all leftmosts bits set to 1
+  constexpr int32_t MASK_NEGATIVE_PART_32_BITS = ~(get_mask_N_bits<N>());
+
+  // check if we have sign bit
+  uint32_t neg_sign = (val & get_min_signed_value_N_bits<N>());
+
+  // For negative value, propagate the bit sign to the leftmosts bits
+  if (neg_sign != 0) {
+    // Set most significant bits
+    return val | MASK_NEGATIVE_PART_32_BITS;
+  } else {
+    // Positives values are untouched
+    return static_cast<int32_t>(val);
+  }
+}
+
+}  // namespace akida
```

## akida/engine/src/dense.cpp

 * *Ordering differences only*

```diff
@@ -1,274 +1,274 @@
-#include "akida/dense.h"
-
-#include <cassert>
-#include <cstdint>
-#include <limits>
-
-#include "akida/shape.h"
-#include "akida/sparse.h"
-#include "akida/tensor.h"
-
-#include "infra/system.h"
-
-namespace akida {
-
-using DenseBufferPtr = std::unique_ptr<Dense::Buffer>;
-
-class DenseOwnedBuffer final : public Dense::Buffer {
- private:
-  char* data_;
-  const size_t size_;
-
- public:
-  // Constructor creates a 0 initialized data array of the given size
-  DenseOwnedBuffer(size_t size) : data_(new char[size]{}), size_(size) {}
-  ~DenseOwnedBuffer() { delete[] data_; }
-
-  size_t size() const override { return size_; }
-  char* data() override { return data_; }
-  const char* data() const override { return data_; }
-};
-
-class DenseViewBuffer final : public Dense::Buffer {
- private:
-  char* data_;
-  size_t size_;
-
- public:
-  DenseViewBuffer(char* data, size_t size) : data_(data), size_(size) {}
-
-  size_t size() const override { return size_; }
-  char* data() override { return data_; }
-  const char* data() const override { return data_; }
-};
-
-class DenseViewConstBuffer final : public Dense::Buffer {
- private:
-  const char* data_;
-  size_t size_;
-
- public:
-  DenseViewConstBuffer(const char* data, size_t size)
-      : data_(data), size_(size) {}
-
-  size_t size() const override { return size_; }
-  char* data() override {
-    assert(false);  // this should never be user, This dense view should always
-                    // be const
-    return nullptr;
-  };
-  const char* data() const override { return data_; }
-};
-
-class DenseImpl : public Dense {
- public:
-  DenseImpl(TensorType type, const Shape& dims,
-            Dense::Layout layout = Dense::Layout::ColMajor)
-      : type_(type),
-        dims_(dims),
-        layout_(layout),
-        size_(shape_size(dims)),
-        strides_(eval_strides(dims, layout)) {
-    // Evaluate element size
-    auto elem_size = tensor_type_size(type_);
-    // Allocate the tensor memory (zero-initialized)
-    bytes_ = DenseBufferPtr(new DenseOwnedBuffer(size_ * elem_size));
-  }
-
-  DenseImpl(const char* bytes, size_t bytes_size, TensorType type,
-            const Shape& dims, Dense::Layout layout = Dense::Layout::ColMajor)
-      : DenseImpl(type, dims, layout) {
-    // Sanity check: verify that the bytes size matches the tensor size
-    if (bytes_size != bytes_->size()) {
-      if (dims.size() == 1) {
-        panic(
-            "Size mismatch for tensor of shape (%d), expected %d but got "
-            "%d bytes.",
-            dims[0], bytes_->size(), bytes_size);
-      } else if (dims.size() == 3) {
-        panic(
-            "Size mismatch for tensor of shape (%d,%d,%d), expected %d but "
-            "got "
-            "%d bytes.",
-            dims[0], dims[1], dims[2], bytes_->size(), bytes_size);
-      } else if (dims.size() > 1) {
-        panic(
-            "Size mismatch for tensor of shape (%d,...,%d), expected %d but "
-            "got "
-            "%d bytes.",
-            dims.front(), dims.back(), bytes_->size(), bytes_size);
-      } else {
-        panic("Error in tensor shape, it can't be empty.");
-      }
-    }
-    // Copy data
-    std::copy(bytes, bytes + bytes_size, bytes_->data());
-  }
-
-  // Contstructor from DenseBuffer
-  DenseImpl(DenseBufferPtr&& buffer, TensorType type, const Shape& dims,
-            Dense::Layout layout = Dense::Layout::ColMajor)
-      : type_(type),
-        dims_(dims),
-        layout_(layout),
-        size_(shape_size(dims)),
-        strides_(eval_strides(dims, layout)),
-        bytes_(std::move(buffer)) {}
-
-  TensorType type() const override { return type_; }
-
-  size_t size() const override { return size_; }
-
-  Shape dimensions() const override { return dims_; }
-
-  Buffer* buffer() override { return bytes_.get(); }
-
-  const Buffer* buffer() const override { return bytes_.get(); }
-
-  Layout layout() const override { return layout_; }
-
-  const std::vector<uint32_t>& strides() const override { return strides_; }
-
-  void reshape(const Shape& new_shape) override {
-    auto dims_size = dims_.size();
-    if (new_shape.size() != dims_size) {
-      panic("Shape size mismatch for dense");
-    }
-    Index product1 = 1, product2 = 1;
-    for (Index i = 0; i < dims_size; i++) {
-      product1 *= dims_[i];
-      product2 *= new_shape[i];
-    }
-    if (product1 != product2) {
-      panic("Cannot reshape with incompatible shape");
-    }
-    dims_ = new_shape;
-  }
-
- protected:
-  TensorType type_;
-  Shape dims_;
-  Layout layout_;
-  size_t size_;
-  std::vector<uint32_t> strides_;
-  DenseBufferPtr bytes_;
-};
-
-bool Dense::operator==(const Tensor& ref) const {
-  // Try to downcast to a Dense pointer
-  auto dense = dynamic_cast<const Dense*>(&ref);
-  // If downcast was successful, return Dense comparison
-  if (dense) {
-    return *this == *dense;
-  } else {
-    // Try to downcast to a Sparse pointer
-    auto sparse = dynamic_cast<const Sparse*>(&ref);
-    if (sparse) {
-      // If downcast was successful, convert the sparse to a Dense
-      auto dense_clone = from_sparse(*sparse, layout());
-      // Return dense comparison
-      return *this == *dense_clone;
-    }
-  }
-  return false;
-}
-
-DenseUniquePtr Dense::create(TensorType type, const Shape& dims,
-                             Dense::Layout layout) {
-  return DenseUniquePtr(new DenseImpl(type, dims, layout));
-}
-
-DenseUniquePtr Dense::copy(const char* array, size_t size, TensorType type,
-                           const Shape& dims, Dense::Layout layout) {
-  return DenseUniquePtr(new DenseImpl(array, size, type, dims, layout));
-}
-
-DenseUniquePtr Dense::from_sparse(const Sparse& sparse, Dense::Layout layout) {
-  const auto& shape = sparse.dimensions();
-  auto dense = DenseUniquePtr(new DenseImpl(sparse.type(), shape, layout));
-  size_t v_size = tensor_type_size(sparse.type());
-  auto strides = dense->strides();
-  // Iterate over the sparse coordinates and values
-  auto sparse_it = sparse.begin();
-  auto dense_bytes = dense->buffer()->data();
-  while (!sparse_it->end()) {
-    // Evaluate the linear index for these coordinates
-    auto index = sparse_it->unravel(strides);
-    // Multiply by the size of each value to get a bytes offset
-    size_t offset = index * v_size;
-    // Copy bytes values
-    std::memcpy(dense_bytes + offset, sparse_it->bytes(), v_size);
-    sparse_it->next();
-  }
-  return dense;
-}
-
-DenseUniquePtr Dense::create_view(const char* array, TensorType type,
-                                  const Shape& dims, Dense::Layout layout) {
-  // Evaluate element size
-  auto elem_size = tensor_type_size(type);
-  auto size = shape_size(dims);
-  auto buffer =
-      DenseBufferPtr(new DenseViewConstBuffer(array, size * elem_size));
-  return DenseUniquePtr(new DenseImpl(std::move(buffer), type, dims, layout));
-}
-
-std::vector<uint32_t> Dense::eval_strides(const Shape& shape, Layout layout) {
-  auto ndims = shape.size();
-  std::vector<uint32_t> strides(ndims);
-  uint64_t cur_stride = 1;
-  for (size_t i = 0; i < ndims; ++i) {
-    if (layout == Layout::ColMajor) {
-      strides[i] = static_cast<uint32_t>(cur_stride);
-      cur_stride *= shape[i];
-    } else {
-      strides[ndims - 1 - i] = static_cast<uint32_t>(cur_stride);
-      cur_stride *= shape[ndims - 1 - i];
-    }
-    constexpr size_t max_stride = std::numeric_limits<Index>::max();
-    if (cur_stride > max_stride) {
-      panic("Stride of dimension %i out of range (%lu/%u)", i, cur_stride,
-            max_stride);
-    }
-  }
-  return strides;
-}
-
-std::vector<TensorConstPtr> Dense::split(const Dense& dense) {
-  auto initial_shape = dense.dimensions();
-  auto n_dims = initial_shape.size();
-  if (n_dims == 1) {
-    panic("Cannot split a one-dimension tensor");
-  }
-  // We only support splitting a Dense along its highest stride dimension, as
-  // it is a buffer copy
-  auto layout = dense.layout();
-
-  // we split along the dimension with the biggest stride
-  bool is_row_major = layout == Dense::Layout::RowMajor;
-  Index dim = static_cast<Index>(is_row_major ? 0 : (initial_shape.size() - 1));
-  // Sub-tensors have one dimension less
-  Shape shape;
-  if (is_row_major) {
-    shape = Shape(initial_shape.data() + 1, initial_shape.size() - 1);
-  } else {
-    shape = Shape(initial_shape.data(), initial_shape.size() - 1);
-  }
-
-  // Prepare empty Dense sub-tensors
-  auto n_dense = initial_shape[dim];
-  std::vector<TensorConstPtr> denses;
-  denses.reserve(n_dense);
-
-  // Create sub-denses from underlying contiguous buffers
-  auto bytes = dense.buffer()->data();
-  auto byte_size = dense.buffer()->size() / n_dense;
-  auto type = dense.type();
-  for (size_t n = 0; n < n_dense; ++n) {
-    denses.push_back(Dense::create_view(bytes, type, shape, layout));
-    bytes += byte_size;
-  }
-  return denses;
-}
-
-}  // namespace akida
+#include "akida/dense.h"
+
+#include <cassert>
+#include <cstdint>
+#include <limits>
+
+#include "akida/shape.h"
+#include "akida/sparse.h"
+#include "akida/tensor.h"
+
+#include "infra/system.h"
+
+namespace akida {
+
+using DenseBufferPtr = std::unique_ptr<Dense::Buffer>;
+
+class DenseOwnedBuffer final : public Dense::Buffer {
+ private:
+  char* data_;
+  const size_t size_;
+
+ public:
+  // Constructor creates a 0 initialized data array of the given size
+  DenseOwnedBuffer(size_t size) : data_(new char[size]{}), size_(size) {}
+  ~DenseOwnedBuffer() { delete[] data_; }
+
+  size_t size() const override { return size_; }
+  char* data() override { return data_; }
+  const char* data() const override { return data_; }
+};
+
+class DenseViewBuffer final : public Dense::Buffer {
+ private:
+  char* data_;
+  size_t size_;
+
+ public:
+  DenseViewBuffer(char* data, size_t size) : data_(data), size_(size) {}
+
+  size_t size() const override { return size_; }
+  char* data() override { return data_; }
+  const char* data() const override { return data_; }
+};
+
+class DenseViewConstBuffer final : public Dense::Buffer {
+ private:
+  const char* data_;
+  size_t size_;
+
+ public:
+  DenseViewConstBuffer(const char* data, size_t size)
+      : data_(data), size_(size) {}
+
+  size_t size() const override { return size_; }
+  char* data() override {
+    assert(false);  // this should never be user, This dense view should always
+                    // be const
+    return nullptr;
+  };
+  const char* data() const override { return data_; }
+};
+
+class DenseImpl : public Dense {
+ public:
+  DenseImpl(TensorType type, const Shape& dims,
+            Dense::Layout layout = Dense::Layout::ColMajor)
+      : type_(type),
+        dims_(dims),
+        layout_(layout),
+        size_(shape_size(dims)),
+        strides_(eval_strides(dims, layout)) {
+    // Evaluate element size
+    auto elem_size = tensor_type_size(type_);
+    // Allocate the tensor memory (zero-initialized)
+    bytes_ = DenseBufferPtr(new DenseOwnedBuffer(size_ * elem_size));
+  }
+
+  DenseImpl(const char* bytes, size_t bytes_size, TensorType type,
+            const Shape& dims, Dense::Layout layout = Dense::Layout::ColMajor)
+      : DenseImpl(type, dims, layout) {
+    // Sanity check: verify that the bytes size matches the tensor size
+    if (bytes_size != bytes_->size()) {
+      if (dims.size() == 1) {
+        panic(
+            "Size mismatch for tensor of shape (%d), expected %d but got "
+            "%d bytes.",
+            dims[0], bytes_->size(), bytes_size);
+      } else if (dims.size() == 3) {
+        panic(
+            "Size mismatch for tensor of shape (%d,%d,%d), expected %d but "
+            "got "
+            "%d bytes.",
+            dims[0], dims[1], dims[2], bytes_->size(), bytes_size);
+      } else if (dims.size() > 1) {
+        panic(
+            "Size mismatch for tensor of shape (%d,...,%d), expected %d but "
+            "got "
+            "%d bytes.",
+            dims.front(), dims.back(), bytes_->size(), bytes_size);
+      } else {
+        panic("Error in tensor shape, it can't be empty.");
+      }
+    }
+    // Copy data
+    std::copy(bytes, bytes + bytes_size, bytes_->data());
+  }
+
+  // Contstructor from DenseBuffer
+  DenseImpl(DenseBufferPtr&& buffer, TensorType type, const Shape& dims,
+            Dense::Layout layout = Dense::Layout::ColMajor)
+      : type_(type),
+        dims_(dims),
+        layout_(layout),
+        size_(shape_size(dims)),
+        strides_(eval_strides(dims, layout)),
+        bytes_(std::move(buffer)) {}
+
+  TensorType type() const override { return type_; }
+
+  size_t size() const override { return size_; }
+
+  Shape dimensions() const override { return dims_; }
+
+  Buffer* buffer() override { return bytes_.get(); }
+
+  const Buffer* buffer() const override { return bytes_.get(); }
+
+  Layout layout() const override { return layout_; }
+
+  const std::vector<uint32_t>& strides() const override { return strides_; }
+
+  void reshape(const Shape& new_shape) override {
+    auto dims_size = dims_.size();
+    if (new_shape.size() != dims_size) {
+      panic("Shape size mismatch for dense");
+    }
+    Index product1 = 1, product2 = 1;
+    for (Index i = 0; i < dims_size; i++) {
+      product1 *= dims_[i];
+      product2 *= new_shape[i];
+    }
+    if (product1 != product2) {
+      panic("Cannot reshape with incompatible shape");
+    }
+    dims_ = new_shape;
+  }
+
+ protected:
+  TensorType type_;
+  Shape dims_;
+  Layout layout_;
+  size_t size_;
+  std::vector<uint32_t> strides_;
+  DenseBufferPtr bytes_;
+};
+
+bool Dense::operator==(const Tensor& ref) const {
+  // Try to downcast to a Dense pointer
+  auto dense = dynamic_cast<const Dense*>(&ref);
+  // If downcast was successful, return Dense comparison
+  if (dense) {
+    return *this == *dense;
+  } else {
+    // Try to downcast to a Sparse pointer
+    auto sparse = dynamic_cast<const Sparse*>(&ref);
+    if (sparse) {
+      // If downcast was successful, convert the sparse to a Dense
+      auto dense_clone = from_sparse(*sparse, layout());
+      // Return dense comparison
+      return *this == *dense_clone;
+    }
+  }
+  return false;
+}
+
+DenseUniquePtr Dense::create(TensorType type, const Shape& dims,
+                             Dense::Layout layout) {
+  return DenseUniquePtr(new DenseImpl(type, dims, layout));
+}
+
+DenseUniquePtr Dense::copy(const char* array, size_t size, TensorType type,
+                           const Shape& dims, Dense::Layout layout) {
+  return DenseUniquePtr(new DenseImpl(array, size, type, dims, layout));
+}
+
+DenseUniquePtr Dense::from_sparse(const Sparse& sparse, Dense::Layout layout) {
+  const auto& shape = sparse.dimensions();
+  auto dense = DenseUniquePtr(new DenseImpl(sparse.type(), shape, layout));
+  size_t v_size = tensor_type_size(sparse.type());
+  auto strides = dense->strides();
+  // Iterate over the sparse coordinates and values
+  auto sparse_it = sparse.begin();
+  auto dense_bytes = dense->buffer()->data();
+  while (!sparse_it->end()) {
+    // Evaluate the linear index for these coordinates
+    auto index = sparse_it->unravel(strides);
+    // Multiply by the size of each value to get a bytes offset
+    size_t offset = index * v_size;
+    // Copy bytes values
+    std::memcpy(dense_bytes + offset, sparse_it->bytes(), v_size);
+    sparse_it->next();
+  }
+  return dense;
+}
+
+DenseUniquePtr Dense::create_view(const char* array, TensorType type,
+                                  const Shape& dims, Dense::Layout layout) {
+  // Evaluate element size
+  auto elem_size = tensor_type_size(type);
+  auto size = shape_size(dims);
+  auto buffer =
+      DenseBufferPtr(new DenseViewConstBuffer(array, size * elem_size));
+  return DenseUniquePtr(new DenseImpl(std::move(buffer), type, dims, layout));
+}
+
+std::vector<uint32_t> Dense::eval_strides(const Shape& shape, Layout layout) {
+  auto ndims = shape.size();
+  std::vector<uint32_t> strides(ndims);
+  uint64_t cur_stride = 1;
+  for (size_t i = 0; i < ndims; ++i) {
+    if (layout == Layout::ColMajor) {
+      strides[i] = static_cast<uint32_t>(cur_stride);
+      cur_stride *= shape[i];
+    } else {
+      strides[ndims - 1 - i] = static_cast<uint32_t>(cur_stride);
+      cur_stride *= shape[ndims - 1 - i];
+    }
+    constexpr size_t max_stride = std::numeric_limits<Index>::max();
+    if (cur_stride > max_stride) {
+      panic("Stride of dimension %i out of range (%lu/%u)", i, cur_stride,
+            max_stride);
+    }
+  }
+  return strides;
+}
+
+std::vector<TensorConstPtr> Dense::split(const Dense& dense) {
+  auto initial_shape = dense.dimensions();
+  auto n_dims = initial_shape.size();
+  if (n_dims == 1) {
+    panic("Cannot split a one-dimension tensor");
+  }
+  // We only support splitting a Dense along its highest stride dimension, as
+  // it is a buffer copy
+  auto layout = dense.layout();
+
+  // we split along the dimension with the biggest stride
+  bool is_row_major = layout == Dense::Layout::RowMajor;
+  Index dim = static_cast<Index>(is_row_major ? 0 : (initial_shape.size() - 1));
+  // Sub-tensors have one dimension less
+  Shape shape;
+  if (is_row_major) {
+    shape = Shape(initial_shape.data() + 1, initial_shape.size() - 1);
+  } else {
+    shape = Shape(initial_shape.data(), initial_shape.size() - 1);
+  }
+
+  // Prepare empty Dense sub-tensors
+  auto n_dense = initial_shape[dim];
+  std::vector<TensorConstPtr> denses;
+  denses.reserve(n_dense);
+
+  // Create sub-denses from underlying contiguous buffers
+  auto bytes = dense.buffer()->data();
+  auto byte_size = dense.buffer()->size() / n_dense;
+  auto type = dense.type();
+  for (size_t n = 0; n < n_dense; ++n) {
+    denses.push_back(Dense::create_view(bytes, type, shape, layout));
+    bytes += byte_size;
+  }
+  return denses;
+}
+
+}  // namespace akida
```

## akida/engine/src/device_memory.h

 * *Ordering differences only*

```diff
@@ -1,12 +1,12 @@
-#pragma once
-
-#include "engine/dma.h"
-
-namespace akida {
-
-struct InferenceMemory {
-  dma::addr inputs_base_address = 0;
-  dma::addr outputs_base_address = 0;
-};
-
-}  // namespace akida
+#pragma once
+
+#include "engine/dma.h"
+
+namespace akida {
+
+struct InferenceMemory {
+  dma::addr inputs_base_address = 0;
+  dma::addr outputs_base_address = 0;
+};
+
+}  // namespace akida
```

## akida/engine/src/dma_cnp_events.h

 * *Ordering differences only*

```diff
@@ -1,88 +1,88 @@
-#pragma once
-
-#include <memory>
-
-#include "akida/shape.h"
-#include "akida/sparse.h"
-
-#include "infra/int_ops.h"
-
-#include "engine/dma.h"
-
-#include "dma_events.h"
-
-namespace akida {
-
-class DmaCnpEvents final : public DmaEvents {
- public:
-  DmaCnpEvents(const Shape& shape, const dma::wbuffer&& dma_words)
-      : DmaEvents(shape, std::move(dma_words)) {}
-
-  class Iterator final : public sparse::Iterator {
-   public:
-    // The events are stored contiguously using two dma words
-    static constexpr size_t kEventsStride = 2 * sizeof(dma::w32);
-    explicit Iterator(const DmaCnpEvents& events)
-        :  // Coords and values strides are deduced form the event stride
-          coords_stride_(kEventsStride / sizeof(*coords_)),
-          bytes_stride_(kEventsStride / sizeof(*bytes_)),
-          max_index_(shape_size(events.shape_) - 1),
-          shape_(events.shape_) {
-      // The coords are aligned on 16-bit starting from the first event word
-      coords_ = reinterpret_cast<const uint16_t*>(events.buffer_.data());
-      // Coordinates end is deduced from the number of events
-      coords_end_ = coords_ + events.size() * coords_stride_;
-      // Values are represented using a bytes pointer
-      bytes_ = reinterpret_cast<const char*>(events.buffer_.data());
-      // The values start after the three coordinates
-      bytes_ += 3 * sizeof(uint16_t);
-    }
-    // Iterator public API
-    std::vector<Index> coords() const override {
-      for (size_t i = 0; i < 3; ++i) {
-        if (coords_[i] > shape_[i] - 1) {
-          panic(
-              "CNP: coordinates (%d, %d, %d) are out-of-range: Shape is (%d, "
-              "%d, %d)",
-              coords_[0], coords_[1], coords_[2], shape_[0], shape_[1],
-              shape_[2]);
-        }
-      }
-      return std::vector<Index>(coords_, coords_ + 3);
-    }
-    const char* bytes() const override { return bytes_; }
-    void next() override {
-      coords_ += coords_stride_;
-      bytes_ += bytes_stride_;
-    }
-    bool end() const override { return (coords_ == coords_end_); }
-    // Iterator internal API
-    size_t unravel(const std::vector<uint32_t>& strides) const override {
-      auto index = linear_index(coords_, strides);
-      if (index > max_index_) {
-        panic(
-            "CNP: coordinates (%d, %d, %d) are out-of-range: Shape is (%d, %d, "
-            "%d)",
-            coords_[0], coords_[1], coords_[2], shape_[0], shape_[1],
-            shape_[2]);
-      }
-      return index;
-    }
-
-   private:
-    const uint16_t* coords_;
-    const size_t coords_stride_;
-    const uint16_t* coords_end_;
-    const char* bytes_;
-    const size_t bytes_stride_;
-    const size_t max_index_;
-    const Shape shape_;
-  };
-
-  sparse::IteratorPtr begin() const override {
-    return std::make_shared<Iterator>(*this);
-  }
-};
-
-using DmaCnpEventsPtr = std::shared_ptr<DmaCnpEvents>;
-}  // namespace akida
+#pragma once
+
+#include <memory>
+
+#include "akida/shape.h"
+#include "akida/sparse.h"
+
+#include "infra/int_ops.h"
+
+#include "engine/dma.h"
+
+#include "dma_events.h"
+
+namespace akida {
+
+class DmaCnpEvents final : public DmaEvents {
+ public:
+  DmaCnpEvents(const Shape& shape, const dma::wbuffer&& dma_words)
+      : DmaEvents(shape, std::move(dma_words)) {}
+
+  class Iterator final : public sparse::Iterator {
+   public:
+    // The events are stored contiguously using two dma words
+    static constexpr size_t kEventsStride = 2 * sizeof(dma::w32);
+    explicit Iterator(const DmaCnpEvents& events)
+        :  // Coords and values strides are deduced form the event stride
+          coords_stride_(kEventsStride / sizeof(*coords_)),
+          bytes_stride_(kEventsStride / sizeof(*bytes_)),
+          max_index_(shape_size(events.shape_) - 1),
+          shape_(events.shape_) {
+      // The coords are aligned on 16-bit starting from the first event word
+      coords_ = reinterpret_cast<const uint16_t*>(events.buffer_.data());
+      // Coordinates end is deduced from the number of events
+      coords_end_ = coords_ + events.size() * coords_stride_;
+      // Values are represented using a bytes pointer
+      bytes_ = reinterpret_cast<const char*>(events.buffer_.data());
+      // The values start after the three coordinates
+      bytes_ += 3 * sizeof(uint16_t);
+    }
+    // Iterator public API
+    std::vector<Index> coords() const override {
+      for (size_t i = 0; i < 3; ++i) {
+        if (coords_[i] > shape_[i] - 1) {
+          panic(
+              "CNP: coordinates (%d, %d, %d) are out-of-range: Shape is (%d, "
+              "%d, %d)",
+              coords_[0], coords_[1], coords_[2], shape_[0], shape_[1],
+              shape_[2]);
+        }
+      }
+      return std::vector<Index>(coords_, coords_ + 3);
+    }
+    const char* bytes() const override { return bytes_; }
+    void next() override {
+      coords_ += coords_stride_;
+      bytes_ += bytes_stride_;
+    }
+    bool end() const override { return (coords_ == coords_end_); }
+    // Iterator internal API
+    size_t unravel(const std::vector<uint32_t>& strides) const override {
+      auto index = linear_index(coords_, strides);
+      if (index > max_index_) {
+        panic(
+            "CNP: coordinates (%d, %d, %d) are out-of-range: Shape is (%d, %d, "
+            "%d)",
+            coords_[0], coords_[1], coords_[2], shape_[0], shape_[1],
+            shape_[2]);
+      }
+      return index;
+    }
+
+   private:
+    const uint16_t* coords_;
+    const size_t coords_stride_;
+    const uint16_t* coords_end_;
+    const char* bytes_;
+    const size_t bytes_stride_;
+    const size_t max_index_;
+    const Shape shape_;
+  };
+
+  sparse::IteratorPtr begin() const override {
+    return std::make_shared<Iterator>(*this);
+  }
+};
+
+using DmaCnpEventsPtr = std::shared_ptr<DmaCnpEvents>;
+}  // namespace akida
```

## akida/engine/src/dma_config_format.h

 * *Ordering differences only*

```diff
@@ -1,35 +1,35 @@
-#pragma once
-
-#include <cstdint>
-#include "infra/registers_common.h"
-
-namespace akida {
-namespace dma {
-
-// DMA header format
-static constexpr uint32_t HDR_WORD1 = 0x0;
-static constexpr RegDetail HDR_NP_COL(24, 31);
-static constexpr RegDetail HDR_NP_ROW(16, 23);
-static constexpr RegDetail HDR_NP_DST(8, 11);
-static constexpr RegDetail HDR_HRC_EN(7);
-static constexpr RegDetail HDR_UID(0, 3);
-
-static constexpr uint32_t HDR_WORD2 = 0x1;
-static constexpr RegDetail HDR_XL(31);
-static constexpr RegDetail HDR_BLOCK_LEN(16, 29);
-static constexpr RegDetail HDR_START_ADDR(0, 15);
-
-static constexpr uint8_t HDR_UID_CNP_FILTER = 0;
-static constexpr uint8_t HDR_UID_CNP_LEARN_THRES = 2;
-static constexpr uint8_t HDR_UID_CNP_THRES_FIRE = 4;
-static constexpr uint8_t HDR_UID_FNP_WEIGHT = 6;
-static constexpr uint8_t HDR_UID_NP_REGS = 8;
-static constexpr uint8_t HDR_UID_HRC_SRAM = 0;
-static constexpr uint8_t HDR_UID_HRC_REGS = 8;
-
-// Read word
-static constexpr uint32_t HDR_READ_WORD1 = 0x0;
-static constexpr RegDetail HDR_READ_PACKET_SZ(0, 15);
-
-}  // namespace dma
-}  // namespace akida
+#pragma once
+
+#include <cstdint>
+#include "infra/registers_common.h"
+
+namespace akida {
+namespace dma {
+
+// DMA header format
+static constexpr uint32_t HDR_WORD1 = 0x0;
+static constexpr RegDetail HDR_NP_COL(24, 31);
+static constexpr RegDetail HDR_NP_ROW(16, 23);
+static constexpr RegDetail HDR_NP_DST(8, 11);
+static constexpr RegDetail HDR_HRC_EN(7);
+static constexpr RegDetail HDR_UID(0, 3);
+
+static constexpr uint32_t HDR_WORD2 = 0x1;
+static constexpr RegDetail HDR_XL(31);
+static constexpr RegDetail HDR_BLOCK_LEN(16, 29);
+static constexpr RegDetail HDR_START_ADDR(0, 15);
+
+static constexpr uint8_t HDR_UID_CNP_FILTER = 0;
+static constexpr uint8_t HDR_UID_CNP_LEARN_THRES = 2;
+static constexpr uint8_t HDR_UID_CNP_THRES_FIRE = 4;
+static constexpr uint8_t HDR_UID_FNP_WEIGHT = 6;
+static constexpr uint8_t HDR_UID_NP_REGS = 8;
+static constexpr uint8_t HDR_UID_HRC_SRAM = 0;
+static constexpr uint8_t HDR_UID_HRC_REGS = 8;
+
+// Read word
+static constexpr uint32_t HDR_READ_WORD1 = 0x0;
+static constexpr RegDetail HDR_READ_PACKET_SZ(0, 15);
+
+}  // namespace dma
+}  // namespace akida
```

## akida/engine/src/dma_config_ops.cpp

 * *Ordering differences only*

```diff
@@ -1,77 +1,77 @@
-#include "engine/dma_config_ops.h"
-
-#include <cassert>
-
-#include "akida/np.h"
-#include "dma_config_format.h"
-#include "infra/int_ops.h"
-#include "infra/registers_common.h"
-
-namespace akida {
-namespace dma {
-
-static uint8_t target_to_uid(const Target& target) {
-  switch (target) {
-    case Target::CnpFilter:
-      return HDR_UID_CNP_FILTER;
-      break;
-    case Target::CnpLearnThres:
-      return HDR_UID_CNP_LEARN_THRES;
-    case Target::CnpFireThres:
-      return HDR_UID_CNP_THRES_FIRE;
-    case Target::FnpWeights:
-      return HDR_UID_FNP_WEIGHT;
-    case Target::NpRegisters:
-      return HDR_UID_NP_REGS;
-    case Target::HrcRegisters:
-      return HDR_UID_HRC_REGS;
-    case Target::HrcSram:
-      return HDR_UID_HRC_SRAM;
-    default:
-      break;
-  }
-  // this should never be reached
-  assert(false);
-  return 0;
-}
-
-wbuffer format_config_header(const struct np::Ident& np, Target target,
-                             uint32_t size, uint16_t dest_addr, bool xl) {
-  wbuffer header(kConfigWriteHdrWordLen, 0);
-
-  bool hrc_en = (np == np::HRC_IDENT);
-
-  assert(size > 0);
-  assert(!xl || (xl && ((size & 0xF) == 0) &&
-                 "size must be a multiple of 16 when using XL mode"));
-  assert(xl || (!xl && !config_block_size_needs_xl(size) &&
-                "DMA transfer size cannot fit without using XL mode"));
-
-  set_field(&header[HDR_WORD1], HDR_NP_COL, np.col);
-  set_field(&header[HDR_WORD1], HDR_NP_ROW, np.row);
-  set_field(&header[HDR_WORD1], HDR_NP_DST, 1 << np.id);
-  set_field(&header[HDR_WORD1], HDR_HRC_EN, hrc_en ? 1 : 0);
-  set_field(&header[HDR_WORD1], HDR_UID, target_to_uid(target));
-
-  set_field(&header[HDR_WORD2], HDR_XL, xl ? 1 : 0);
-  // if xl increment is set, calculate buffer size in 16 words
-  if (xl) {
-    size = size / kXlIncrementSz;
-  }
-  set_field(&header[HDR_WORD2], HDR_BLOCK_LEN, size);
-  set_field(&header[HDR_WORD2], HDR_START_ADDR, dest_addr);
-
-  return header;
-}
-
-uint32_t parse_config_read_size(const dma::wbuffer& read_header) {
-  return get_field(read_header[HDR_READ_WORD1], HDR_READ_PACKET_SZ);
-}
-
-bool config_block_size_needs_xl(uint32_t block_size) {
-  return (block_size >= ((1u << HDR_BLOCK_LEN.nb_bits) - 1));
-}
-
-}  // namespace dma
-
-}  // namespace akida
+#include "engine/dma_config_ops.h"
+
+#include <cassert>
+
+#include "akida/np.h"
+#include "dma_config_format.h"
+#include "infra/int_ops.h"
+#include "infra/registers_common.h"
+
+namespace akida {
+namespace dma {
+
+static uint8_t target_to_uid(const Target& target) {
+  switch (target) {
+    case Target::CnpFilter:
+      return HDR_UID_CNP_FILTER;
+      break;
+    case Target::CnpLearnThres:
+      return HDR_UID_CNP_LEARN_THRES;
+    case Target::CnpFireThres:
+      return HDR_UID_CNP_THRES_FIRE;
+    case Target::FnpWeights:
+      return HDR_UID_FNP_WEIGHT;
+    case Target::NpRegisters:
+      return HDR_UID_NP_REGS;
+    case Target::HrcRegisters:
+      return HDR_UID_HRC_REGS;
+    case Target::HrcSram:
+      return HDR_UID_HRC_SRAM;
+    default:
+      break;
+  }
+  // this should never be reached
+  assert(false);
+  return 0;
+}
+
+wbuffer format_config_header(const struct np::Ident& np, Target target,
+                             uint32_t size, uint16_t dest_addr, bool xl) {
+  wbuffer header(kConfigWriteHdrWordLen, 0);
+
+  bool hrc_en = (np == np::HRC_IDENT);
+
+  assert(size > 0);
+  assert(!xl || (xl && ((size & 0xF) == 0) &&
+                 "size must be a multiple of 16 when using XL mode"));
+  assert(xl || (!xl && !config_block_size_needs_xl(size) &&
+                "DMA transfer size cannot fit without using XL mode"));
+
+  set_field(&header[HDR_WORD1], HDR_NP_COL, np.col);
+  set_field(&header[HDR_WORD1], HDR_NP_ROW, np.row);
+  set_field(&header[HDR_WORD1], HDR_NP_DST, 1 << np.id);
+  set_field(&header[HDR_WORD1], HDR_HRC_EN, hrc_en ? 1 : 0);
+  set_field(&header[HDR_WORD1], HDR_UID, target_to_uid(target));
+
+  set_field(&header[HDR_WORD2], HDR_XL, xl ? 1 : 0);
+  // if xl increment is set, calculate buffer size in 16 words
+  if (xl) {
+    size = size / kXlIncrementSz;
+  }
+  set_field(&header[HDR_WORD2], HDR_BLOCK_LEN, size);
+  set_field(&header[HDR_WORD2], HDR_START_ADDR, dest_addr);
+
+  return header;
+}
+
+uint32_t parse_config_read_size(const dma::wbuffer& read_header) {
+  return get_field(read_header[HDR_READ_WORD1], HDR_READ_PACKET_SZ);
+}
+
+bool config_block_size_needs_xl(uint32_t block_size) {
+  return (block_size >= ((1u << HDR_BLOCK_LEN.nb_bits) - 1));
+}
+
+}  // namespace dma
+
+}  // namespace akida
```

## akida/engine/src/dma_desc_format.h

 * *Ordering differences only*

```diff
@@ -1,137 +1,137 @@
-#pragma once
-
-#include <cstdint>
-#include "infra/int_ops.h"
-#include "infra/registers_common.h"
-
-namespace akida {
-namespace dma {
-
-namespace config {
-// Config descriptor format
-
-// word1
-static constexpr uint32_t DESC_WORD1 = 0x0;
-static constexpr RegDetail DESC_DIRECTION(28, 31);
-static constexpr RegDetail DESC_INT_DISABLE(12, 15);
-static constexpr RegDetail DESC_INT_DISABLE_OB(12);
-static constexpr RegDetail DESC_INT_DISABLE_IB(13);
-static constexpr RegDetail DESC_NNID(4, 7);
-static constexpr RegDetail DESC_VERSION(0, 3);
-
-// word2: input address (32 bits)
-static constexpr uint32_t DESC_WORD2 = 0x1;
-static constexpr uint32_t DESC_INPUT_ADDR = DESC_WORD2;
-
-// word3
-static constexpr uint32_t DESC_WORD3 = 0x2;
-static constexpr RegDetail DESC_DATA_BUF_SZ(0, 15);
-
-// word4: output address (32 bit)
-static constexpr uint32_t DESC_WORD4 = 0x3;
-static constexpr uint32_t DESC_CONF_OUTPUT_SIZE = DESC_WORD4;
-
-// number of 32 bit words
-static constexpr uint32_t DESC_LEN = 8;
-static constexpr uint32_t DESC_BYTE_SIZE = DESC_LEN * sizeof(uint32_t);
-
-}  // namespace config
-
-namespace event {
-// Event descriptor format
-
-// word1
-static constexpr uint32_t DESC_WORD1 = 0x0;
-static constexpr RegDetail DESC_JOBID(16, 31);
-static constexpr RegDetail DESC_INT_DISABLE(12, 15);
-static constexpr RegDetail DESC_INT_DISABLE_OB(12);
-static constexpr RegDetail DESC_INT_DISABLE_IB(13);
-static constexpr RegDetail DESC_NNID(4, 7);
-static constexpr RegDetail DESC_VERSION(0, 3);
-
-// word2: input address (32 bits)
-static constexpr uint32_t DESC_WORD2 = 0x1;
-static constexpr uint32_t DESC_INPUT_ADDR = 0x1;
-
-// word3
-static constexpr uint32_t DESC_WORD3 = 0x2;
-static constexpr RegDetail DESC_DATA_BUF_SZ(0, 23);
-
-// word4: output address (32 bit)
-static constexpr uint32_t DESC_WORD4 = 0x3;
-static constexpr uint32_t DESC_EV_OUTPUT_SIZE = 0x3;
-
-// word5
-static constexpr uint32_t DESC_WORD5 = 0x4;
-static constexpr RegDetail DESC_LEARN_CLASS(16, 25);
-static constexpr RegDetail DESC_MD(30, 31);
-
-// number of 32 bit words
-static constexpr uint32_t DESC_LEN = 8;
-static constexpr uint32_t DESC_BYTE_SIZE = DESC_LEN * sizeof(uint32_t);
-
-}  // namespace event
-
-namespace hrc {
-// HRC (spike conversion complex) descriptor format
-
-// word1
-static constexpr uint32_t DESC_WORD1 = 0x0;
-static constexpr RegDetail DESC_JOBID(16, 31);
-static constexpr RegDetail DESC_INT_DISABLE(12, 15);
-static constexpr RegDetail DESC_INT_DISABLE_OB(12);
-static constexpr RegDetail DESC_INT_DISABLE_IB(13);
-static constexpr RegDetail DESC_NNID(4, 7);
-static constexpr RegDetail DESC_VERSION(0, 3);
-
-// word2: input address (32 bits)
-static constexpr uint32_t DESC_WORD2 = 0x1;
-static constexpr uint32_t DESC_INPUT_ADDR = 0x1;
-
-// word3
-static constexpr uint32_t DESC_WORD3 = 0x2;
-static constexpr RegDetail DESC_ROW_BYTESZ(0, 15);
-static constexpr RegDetail DESC_COL_HEIGHT(16, 31);
-
-// word4
-static constexpr uint32_t DESC_WORD4 = 0x3;
-static constexpr RegDetail DESC_NEXT_ROW_BYTESZ_OFFSET(0, 15);
-static constexpr RegDetail DESC_ROW_BYTESZ_EXT(16, 31);
-
-// word5
-static constexpr uint32_t DESC_WORD5 = 0x4;
-static constexpr RegDetail DESC_WIN_ROW_BYTESZ(0, 15);
-static constexpr RegDetail DESC_WIN_COL_HEIGHT(16, 31);
-
-// word6
-static constexpr uint32_t DESC_WORD6 = 0x5;
-static constexpr RegDetail DESC_WIN_OVERLAP_LR(0, 15);
-static constexpr RegDetail DESC_PADDING_DISABLE_LR(16, 31);
-
-// word7
-static constexpr uint32_t DESC_WORD7 = 0x6;
-static constexpr RegDetail DESC_X_OFFSET(0, 15);
-static constexpr RegDetail DESC_Y_OFFSET(16, 31);
-
-// word8: output address (32 bit)
-static constexpr uint32_t DESC_WORD8 = 0x7;
-static constexpr uint32_t DESC_HRC_OUTPUT_SIZE = 0x7;
-
-// word9
-static constexpr uint32_t DESC_WORD9 = 0x8;
-static constexpr RegDetail DESC_OB_CONT_SZ(0, 15);  // optional
-static constexpr RegDetail DESC_LEARN_GROUP(16, 25);
-
-// number of 32 bit words. In reality this is 9, but it must be aligned to 8
-// bytes, so 16 is used.
-static constexpr uint32_t DESC_REAL_LEN = 9;
-static constexpr uint32_t DESC_LEN = align_up(DESC_REAL_LEN, 8);
-static constexpr uint32_t DESC_BYTE_SIZE = DESC_LEN * sizeof(uint32_t);
-
-}  // namespace hrc
-
-// constants used for descriptor formatting
-static constexpr uint32_t DESC_VERSION_VALUE = 1;
-
-}  // namespace dma
-}  // namespace akida
+#pragma once
+
+#include <cstdint>
+#include "infra/int_ops.h"
+#include "infra/registers_common.h"
+
+namespace akida {
+namespace dma {
+
+namespace config {
+// Config descriptor format
+
+// word1
+static constexpr uint32_t DESC_WORD1 = 0x0;
+static constexpr RegDetail DESC_DIRECTION(28, 31);
+static constexpr RegDetail DESC_INT_DISABLE(12, 15);
+static constexpr RegDetail DESC_INT_DISABLE_OB(12);
+static constexpr RegDetail DESC_INT_DISABLE_IB(13);
+static constexpr RegDetail DESC_NNID(4, 7);
+static constexpr RegDetail DESC_VERSION(0, 3);
+
+// word2: input address (32 bits)
+static constexpr uint32_t DESC_WORD2 = 0x1;
+static constexpr uint32_t DESC_INPUT_ADDR = DESC_WORD2;
+
+// word3
+static constexpr uint32_t DESC_WORD3 = 0x2;
+static constexpr RegDetail DESC_DATA_BUF_SZ(0, 15);
+
+// word4: output address (32 bit)
+static constexpr uint32_t DESC_WORD4 = 0x3;
+static constexpr uint32_t DESC_CONF_OUTPUT_SIZE = DESC_WORD4;
+
+// number of 32 bit words
+static constexpr uint32_t DESC_LEN = 8;
+static constexpr uint32_t DESC_BYTE_SIZE = DESC_LEN * sizeof(uint32_t);
+
+}  // namespace config
+
+namespace event {
+// Event descriptor format
+
+// word1
+static constexpr uint32_t DESC_WORD1 = 0x0;
+static constexpr RegDetail DESC_JOBID(16, 31);
+static constexpr RegDetail DESC_INT_DISABLE(12, 15);
+static constexpr RegDetail DESC_INT_DISABLE_OB(12);
+static constexpr RegDetail DESC_INT_DISABLE_IB(13);
+static constexpr RegDetail DESC_NNID(4, 7);
+static constexpr RegDetail DESC_VERSION(0, 3);
+
+// word2: input address (32 bits)
+static constexpr uint32_t DESC_WORD2 = 0x1;
+static constexpr uint32_t DESC_INPUT_ADDR = 0x1;
+
+// word3
+static constexpr uint32_t DESC_WORD3 = 0x2;
+static constexpr RegDetail DESC_DATA_BUF_SZ(0, 23);
+
+// word4: output address (32 bit)
+static constexpr uint32_t DESC_WORD4 = 0x3;
+static constexpr uint32_t DESC_EV_OUTPUT_SIZE = 0x3;
+
+// word5
+static constexpr uint32_t DESC_WORD5 = 0x4;
+static constexpr RegDetail DESC_LEARN_CLASS(16, 25);
+static constexpr RegDetail DESC_MD(30, 31);
+
+// number of 32 bit words
+static constexpr uint32_t DESC_LEN = 8;
+static constexpr uint32_t DESC_BYTE_SIZE = DESC_LEN * sizeof(uint32_t);
+
+}  // namespace event
+
+namespace hrc {
+// HRC (spike conversion complex) descriptor format
+
+// word1
+static constexpr uint32_t DESC_WORD1 = 0x0;
+static constexpr RegDetail DESC_JOBID(16, 31);
+static constexpr RegDetail DESC_INT_DISABLE(12, 15);
+static constexpr RegDetail DESC_INT_DISABLE_OB(12);
+static constexpr RegDetail DESC_INT_DISABLE_IB(13);
+static constexpr RegDetail DESC_NNID(4, 7);
+static constexpr RegDetail DESC_VERSION(0, 3);
+
+// word2: input address (32 bits)
+static constexpr uint32_t DESC_WORD2 = 0x1;
+static constexpr uint32_t DESC_INPUT_ADDR = 0x1;
+
+// word3
+static constexpr uint32_t DESC_WORD3 = 0x2;
+static constexpr RegDetail DESC_ROW_BYTESZ(0, 15);
+static constexpr RegDetail DESC_COL_HEIGHT(16, 31);
+
+// word4
+static constexpr uint32_t DESC_WORD4 = 0x3;
+static constexpr RegDetail DESC_NEXT_ROW_BYTESZ_OFFSET(0, 15);
+static constexpr RegDetail DESC_ROW_BYTESZ_EXT(16, 31);
+
+// word5
+static constexpr uint32_t DESC_WORD5 = 0x4;
+static constexpr RegDetail DESC_WIN_ROW_BYTESZ(0, 15);
+static constexpr RegDetail DESC_WIN_COL_HEIGHT(16, 31);
+
+// word6
+static constexpr uint32_t DESC_WORD6 = 0x5;
+static constexpr RegDetail DESC_WIN_OVERLAP_LR(0, 15);
+static constexpr RegDetail DESC_PADDING_DISABLE_LR(16, 31);
+
+// word7
+static constexpr uint32_t DESC_WORD7 = 0x6;
+static constexpr RegDetail DESC_X_OFFSET(0, 15);
+static constexpr RegDetail DESC_Y_OFFSET(16, 31);
+
+// word8: output address (32 bit)
+static constexpr uint32_t DESC_WORD8 = 0x7;
+static constexpr uint32_t DESC_HRC_OUTPUT_SIZE = 0x7;
+
+// word9
+static constexpr uint32_t DESC_WORD9 = 0x8;
+static constexpr RegDetail DESC_OB_CONT_SZ(0, 15);  // optional
+static constexpr RegDetail DESC_LEARN_GROUP(16, 25);
+
+// number of 32 bit words. In reality this is 9, but it must be aligned to 8
+// bytes, so 16 is used.
+static constexpr uint32_t DESC_REAL_LEN = 9;
+static constexpr uint32_t DESC_LEN = align_up(DESC_REAL_LEN, 8);
+static constexpr uint32_t DESC_BYTE_SIZE = DESC_LEN * sizeof(uint32_t);
+
+}  // namespace hrc
+
+// constants used for descriptor formatting
+static constexpr uint32_t DESC_VERSION_VALUE = 1;
+
+}  // namespace dma
+}  // namespace akida
```

## akida/engine/src/dma_desc_ops.cpp

 * *Ordering differences only*

```diff
@@ -1,109 +1,109 @@
-#include "dma_desc_ops.h"
-
-#include <cstdint>
-
-#include "dma_desc_format.h"
-#include "infra/registers_common.h"
-
-namespace akida {
-namespace dma {
-
-uint32_t max_dma_events() {
-  // Max number of events passed to the DMA engine (2^24 -1)
-  static constexpr uint32_t kWordsPerDmaEvent = 2;
-  static constexpr uint32_t kMaxEvents =
-      ((1 << dma::event::DESC_DATA_BUF_SZ.nb_bits) - 1) / kWordsPerDmaEvent;
-  return kMaxEvents;
-}
-
-Descriptor format_config_desc(bool direction, uint32_t input_addr,
-                              uint32_t output_addr, uint32_t buf_sz) {
-  assert(buf_sz > 0 && "Cannot generate a config descriptor for empty buffer");
-  Descriptor descriptor(config::DESC_LEN, 0);
-
-  set_field(&descriptor[config::DESC_WORD1], config::DESC_DIRECTION,
-            direction ? 1 : 0);
-  set_field(&descriptor[config::DESC_WORD1], config::DESC_VERSION,
-            DESC_VERSION_VALUE);
-  descriptor[config::DESC_WORD2] = input_addr;
-  set_field(&descriptor[config::DESC_WORD3], config::DESC_DATA_BUF_SZ, buf_sz);
-  descriptor[config::DESC_WORD4] = output_addr;
-
-  return descriptor;
-}
-
-Descriptor format_event_desc(uint32_t job_id, uint32_t input_addr,
-                             uint32_t output_addr, uint32_t buf_sz,
-                             uint32_t learning_class) {
-  assert(buf_sz > 0 && "Cannot generate an event descriptor for empty buffer");
-  Descriptor descriptor(event::DESC_LEN, 0);
-
-  set_field(&descriptor[event::DESC_WORD1], event::DESC_VERSION,
-            DESC_VERSION_VALUE);
-  set_field(&descriptor[event::DESC_WORD1], event::DESC_JOBID, job_id);
-  // disable inbound interrupt to avoid getting an interrupt too early
-  set_field(&descriptor[event::DESC_WORD1], event::DESC_INT_DISABLE_IB, 1);
-  descriptor[event::DESC_WORD2] = input_addr;
-  set_field(&descriptor[event::DESC_WORD3], event::DESC_DATA_BUF_SZ, buf_sz);
-  descriptor[event::DESC_WORD4] = output_addr;
-
-  set_field(&descriptor[event::DESC_WORD5], event::DESC_LEARN_CLASS,
-            learning_class);
-
-  return descriptor;
-}
-
-Descriptor format_hrc_desc(uint32_t job_id, uint32_t input_addr,
-                           uint32_t output_addr, uint32_t row_bytesize,
-                           uint32_t height, uint32_t next_row_offset,
-                           uint32_t window_row_bytesize, uint32_t window_height,
-                           uint32_t overlap_bytesize, uint32_t y_offset,
-                           uint32_t x_offset, uint32_t learning_class) {
-  assert((row_bytesize > 0) && (height > 0) &&
-         "Cannot generate an HRC descriptor for empty buffer");
-  Descriptor descriptor(hrc::DESC_LEN, 0);
-
-  set_field(&descriptor[hrc::DESC_WORD1], hrc::DESC_VERSION,
-            DESC_VERSION_VALUE);
-  set_field(&descriptor[hrc::DESC_WORD1], hrc::DESC_JOBID, job_id);
-  // disable inbound interrupt to avoid getting an interrupt too early
-  set_field(&descriptor[hrc::DESC_WORD1], hrc::DESC_INT_DISABLE_IB, 1);
-  descriptor[hrc::DESC_WORD2] = input_addr;
-
-  // Mask row_bytesize, if it is too big, the remainder will be written
-  // in row_bytesize_ext
-  constexpr uint32_t row_bytesize_max_value =
-      ((1 << hrc::DESC_ROW_BYTESZ.nb_bits) - 1);
-  set_field(&descriptor[hrc::DESC_WORD3], hrc::DESC_ROW_BYTESZ,
-            (row_bytesize & row_bytesize_max_value));
-
-  set_field(&descriptor[hrc::DESC_WORD3], hrc::DESC_COL_HEIGHT, height);
-
-  set_field(&descriptor[hrc::DESC_WORD4], hrc::DESC_NEXT_ROW_BYTESZ_OFFSET,
-            next_row_offset);
-  // If row_bytesize is too big, the remainder is written in DESC_ROW_BYTESZ_EXT
-  uint32_t row_bytesize_ext = row_bytesize >> hrc::DESC_ROW_BYTESZ.nb_bits;
-  set_field(&descriptor[hrc::DESC_WORD4], hrc::DESC_ROW_BYTESZ_EXT,
-            row_bytesize_ext);
-
-  set_field(&descriptor[hrc::DESC_WORD5], hrc::DESC_WIN_ROW_BYTESZ,
-            window_row_bytesize);
-  set_field(&descriptor[hrc::DESC_WORD5], hrc::DESC_WIN_COL_HEIGHT,
-            window_height);
-
-  set_field(&descriptor[hrc::DESC_WORD6], hrc::DESC_WIN_OVERLAP_LR,
-            overlap_bytesize);
-
-  set_field(&descriptor[hrc::DESC_WORD7], hrc::DESC_Y_OFFSET, y_offset);
-  set_field(&descriptor[hrc::DESC_WORD7], hrc::DESC_X_OFFSET, x_offset);
-
-  descriptor[hrc::DESC_WORD8] = output_addr;
-
-  set_field(&descriptor[hrc::DESC_WORD9], hrc::DESC_LEARN_GROUP,
-            learning_class);
-
-  return descriptor;
-}
-
-}  // namespace dma
-}  // namespace akida
+#include "dma_desc_ops.h"
+
+#include <cstdint>
+
+#include "dma_desc_format.h"
+#include "infra/registers_common.h"
+
+namespace akida {
+namespace dma {
+
+uint32_t max_dma_events() {
+  // Max number of events passed to the DMA engine (2^24 -1)
+  static constexpr uint32_t kWordsPerDmaEvent = 2;
+  static constexpr uint32_t kMaxEvents =
+      ((1 << dma::event::DESC_DATA_BUF_SZ.nb_bits) - 1) / kWordsPerDmaEvent;
+  return kMaxEvents;
+}
+
+Descriptor format_config_desc(bool direction, uint32_t input_addr,
+                              uint32_t output_addr, uint32_t buf_sz) {
+  assert(buf_sz > 0 && "Cannot generate a config descriptor for empty buffer");
+  Descriptor descriptor(config::DESC_LEN, 0);
+
+  set_field(&descriptor[config::DESC_WORD1], config::DESC_DIRECTION,
+            direction ? 1 : 0);
+  set_field(&descriptor[config::DESC_WORD1], config::DESC_VERSION,
+            DESC_VERSION_VALUE);
+  descriptor[config::DESC_WORD2] = input_addr;
+  set_field(&descriptor[config::DESC_WORD3], config::DESC_DATA_BUF_SZ, buf_sz);
+  descriptor[config::DESC_WORD4] = output_addr;
+
+  return descriptor;
+}
+
+Descriptor format_event_desc(uint32_t job_id, uint32_t input_addr,
+                             uint32_t output_addr, uint32_t buf_sz,
+                             uint32_t learning_class) {
+  assert(buf_sz > 0 && "Cannot generate an event descriptor for empty buffer");
+  Descriptor descriptor(event::DESC_LEN, 0);
+
+  set_field(&descriptor[event::DESC_WORD1], event::DESC_VERSION,
+            DESC_VERSION_VALUE);
+  set_field(&descriptor[event::DESC_WORD1], event::DESC_JOBID, job_id);
+  // disable inbound interrupt to avoid getting an interrupt too early
+  set_field(&descriptor[event::DESC_WORD1], event::DESC_INT_DISABLE_IB, 1);
+  descriptor[event::DESC_WORD2] = input_addr;
+  set_field(&descriptor[event::DESC_WORD3], event::DESC_DATA_BUF_SZ, buf_sz);
+  descriptor[event::DESC_WORD4] = output_addr;
+
+  set_field(&descriptor[event::DESC_WORD5], event::DESC_LEARN_CLASS,
+            learning_class);
+
+  return descriptor;
+}
+
+Descriptor format_hrc_desc(uint32_t job_id, uint32_t input_addr,
+                           uint32_t output_addr, uint32_t row_bytesize,
+                           uint32_t height, uint32_t next_row_offset,
+                           uint32_t window_row_bytesize, uint32_t window_height,
+                           uint32_t overlap_bytesize, uint32_t y_offset,
+                           uint32_t x_offset, uint32_t learning_class) {
+  assert((row_bytesize > 0) && (height > 0) &&
+         "Cannot generate an HRC descriptor for empty buffer");
+  Descriptor descriptor(hrc::DESC_LEN, 0);
+
+  set_field(&descriptor[hrc::DESC_WORD1], hrc::DESC_VERSION,
+            DESC_VERSION_VALUE);
+  set_field(&descriptor[hrc::DESC_WORD1], hrc::DESC_JOBID, job_id);
+  // disable inbound interrupt to avoid getting an interrupt too early
+  set_field(&descriptor[hrc::DESC_WORD1], hrc::DESC_INT_DISABLE_IB, 1);
+  descriptor[hrc::DESC_WORD2] = input_addr;
+
+  // Mask row_bytesize, if it is too big, the remainder will be written
+  // in row_bytesize_ext
+  constexpr uint32_t row_bytesize_max_value =
+      ((1 << hrc::DESC_ROW_BYTESZ.nb_bits) - 1);
+  set_field(&descriptor[hrc::DESC_WORD3], hrc::DESC_ROW_BYTESZ,
+            (row_bytesize & row_bytesize_max_value));
+
+  set_field(&descriptor[hrc::DESC_WORD3], hrc::DESC_COL_HEIGHT, height);
+
+  set_field(&descriptor[hrc::DESC_WORD4], hrc::DESC_NEXT_ROW_BYTESZ_OFFSET,
+            next_row_offset);
+  // If row_bytesize is too big, the remainder is written in DESC_ROW_BYTESZ_EXT
+  uint32_t row_bytesize_ext = row_bytesize >> hrc::DESC_ROW_BYTESZ.nb_bits;
+  set_field(&descriptor[hrc::DESC_WORD4], hrc::DESC_ROW_BYTESZ_EXT,
+            row_bytesize_ext);
+
+  set_field(&descriptor[hrc::DESC_WORD5], hrc::DESC_WIN_ROW_BYTESZ,
+            window_row_bytesize);
+  set_field(&descriptor[hrc::DESC_WORD5], hrc::DESC_WIN_COL_HEIGHT,
+            window_height);
+
+  set_field(&descriptor[hrc::DESC_WORD6], hrc::DESC_WIN_OVERLAP_LR,
+            overlap_bytesize);
+
+  set_field(&descriptor[hrc::DESC_WORD7], hrc::DESC_Y_OFFSET, y_offset);
+  set_field(&descriptor[hrc::DESC_WORD7], hrc::DESC_X_OFFSET, x_offset);
+
+  descriptor[hrc::DESC_WORD8] = output_addr;
+
+  set_field(&descriptor[hrc::DESC_WORD9], hrc::DESC_LEARN_GROUP,
+            learning_class);
+
+  return descriptor;
+}
+
+}  // namespace dma
+}  // namespace akida
```

## akida/engine/src/dma_desc_ops.h

 * *Ordering differences only*

```diff
@@ -1,31 +1,31 @@
-#pragma once
-#include <cstdint>
-#include <vector>
-
-namespace akida {
-namespace dma {
-
-using Descriptor = std::vector<uint32_t>;
-
-Descriptor format_config_desc(bool direction, uint32_t input_addr,
-                              uint32_t output_addr, uint32_t buf_sz);
-// constants used for config formatting
-static constexpr bool kDescConfigDirectionWrite = true;
-static constexpr bool kDescConfigDirectionRead = false;
-
-Descriptor format_event_desc(uint32_t job_id, uint32_t input_addr,
-                             uint32_t output_addr, uint32_t buf_sz,
-                             uint32_t learning_class = 0);
-
-Descriptor format_hrc_desc(uint32_t job_id, uint32_t input_addr,
-                           uint32_t output_addr, uint32_t row_bytesize,
-                           uint32_t height, uint32_t next_row_offset,
-                           uint32_t window_row_bytesize, uint32_t window_height,
-                           uint32_t overlap_bytesize, uint32_t y_offset,
-                           uint32_t x_offset, uint32_t learning_class = 0);
-
-// Max number of events passed to the DMA engine
-uint32_t max_dma_events();
-
-}  // namespace dma
-}  // namespace akida
+#pragma once
+#include <cstdint>
+#include <vector>
+
+namespace akida {
+namespace dma {
+
+using Descriptor = std::vector<uint32_t>;
+
+Descriptor format_config_desc(bool direction, uint32_t input_addr,
+                              uint32_t output_addr, uint32_t buf_sz);
+// constants used for config formatting
+static constexpr bool kDescConfigDirectionWrite = true;
+static constexpr bool kDescConfigDirectionRead = false;
+
+Descriptor format_event_desc(uint32_t job_id, uint32_t input_addr,
+                             uint32_t output_addr, uint32_t buf_sz,
+                             uint32_t learning_class = 0);
+
+Descriptor format_hrc_desc(uint32_t job_id, uint32_t input_addr,
+                           uint32_t output_addr, uint32_t row_bytesize,
+                           uint32_t height, uint32_t next_row_offset,
+                           uint32_t window_row_bytesize, uint32_t window_height,
+                           uint32_t overlap_bytesize, uint32_t y_offset,
+                           uint32_t x_offset, uint32_t learning_class = 0);
+
+// Max number of events passed to the DMA engine
+uint32_t max_dma_events();
+
+}  // namespace dma
+}  // namespace akida
```

## akida/engine/src/dma_engine.cpp

 * *Ordering differences only*

```diff
@@ -1,409 +1,409 @@
-#include "dma_engine.h"
-#include "dma_engine_ops.h"
-
-#include <cassert>
-
-#include "akida/hw_version.h"
-#include "engine/dma.h"
-#include "infra/hardware_driver.h"
-#include "infra/int_ops.h"
-#include "infra/registers_common.h"
-#include "infra/system.h"
-
-#include "dma_desc_format.h"
-#include "dma_desc_ops.h"
-#include "program_play.h"
-#include "registers_dma_engine.h"
-
-namespace akida {
-namespace dma {
-
-static void mask_interrupts(HardwareDriver* driver, uint32_t reg_base_addr,
-                            bool is_multi_pass);
-
-Engine::Engine(uint32_t reg_base, uint32_t desc_bytes_size)
-    : descriptor_base_addr(0),
-      descriptor_bytes_size(desc_bytes_size),
-      reg_base_addr(reg_base) {
-  assert(descriptor_bytes_size % 32 == 0 &&
-         "descriptor_bytes_size should be multiple of 32 (HW unit is 32 "
-         "bytes/256 bits)");
-}
-
-static void reset_engine(HardwareDriver* driver, uint32_t reg_base_addr) {
-  uint32_t reg = 0;
-  // perform a soft reset
-  set_field(&reg, DMA_CTRL_SOFT_RESET, 1);
-  driver->write32(reg_base_addr + DMA_CTRL_REG, reg);
-}
-
-void configure_descriptors_buffer(HardwareDriver* driver, const Engine& dma,
-                                  uint32_t num_descriptors) {
-  assert(dma.descriptor_base_addr != 0);
-
-  // update base address register
-  driver->write32(dma.reg_base_addr + DMA_CONT_ADDR_REG,
-                  dma.descriptor_base_addr);
-
-  // update container size register
-  uint32_t reg = 0;
-  // Set container size (constant in 32 bytes (256 bits) unit)
-  set_field(&reg, DMA_DESC_CONT_SIZE, dma.descriptor_bytes_size / 32);
-  // Set maximum number of descriptors. Note this corresponds to the maximum
-  // value the descriptor index can take, so that will go from 0 to
-  // num_descriptors -1.
-  set_field(&reg, DMA_MAX_DESC_CONTS, num_descriptors - 1);
-  driver->write32(dma.reg_base_addr + DMA_CONT_SIZE_REG, reg);
-}
-
-static bool wait_for_interrupt_ext(HardwareDriver* driver, const Engine& dma,
-                                   const RegDetail& flag) {
-  // This is a manual polling with a timeout of few ms
-  constexpr int64_t kTimeout = 5000;
-  auto start = time_ms();
-
-  // Busy loop until timeout is reached or interrupt is generated.
-  while (true) {
-    // check outbound interrupt
-    if (check_for_interrupt(driver, dma, flag)) {
-      // interrupt received, clear status and interrupt
-      clear_interrupts(driver, dma);
-      return true;
-    }
-
-    auto end = time_ms();
-    if ((end - start) > kTimeout) {
-      return false;
-    }
-  }
-}
-
-static void init_config_dma_multipass(HardwareDriver* driver, const Config& dma,
-                                      const uint8_t* program) {
-  const uint32_t& reg_base_addr = dma.engine.reg_base_addr;
-  const uint32_t num_descriptors =
-      program::number_of_program_descriptors_required(program);
-  const uint32_t num_extra_descs =
-      program::number_of_extra_program_descriptors_required(program);
-  const uint32_t total_num_descs = num_descriptors + num_extra_descs;
-
-  // Set extra descriptors
-  uint32_t reg = 0;
-  set_field(&reg, DMA_LAST_EXTRA_DESCRIPTOR, total_num_descs - 1);
-  // at init, learn is disabled so extra descriptor is disabled too
-  set_field(&reg, DMA_EXTRA_DESC_ENABLE, 0);
-  driver->write32(reg_base_addr + DMA_EXTRA_DESC_CTRL_REG, reg);
-
-  // Set outbound container size to 2 (as inbound)
-  reg = 0;
-  set_field(&reg, DMA_REPLAY_MAX_OB_DESC_BUFFERS, 2);
-  driver->write32(reg_base_addr + DMA_REPLAY_MAX_OB_BUFFERS_REG, reg);
-}
-
-static void configure_output_header(HardwareDriver* driver,
-                                    const dma::addr reg_base_addr,
-                                    bool enable) {
-  const auto reg_addr = reg_base_addr + DMA_CTRL_REG;
-  uint32_t reg = driver->read32(reg_addr);
-  set_field(&reg, DMA_CTRL_WR_INFO_EN, enable ? 1 : 0);
-  set_field(&reg, DMA_CTRL_WR_INFO_HDR, 1);
-  constexpr uint32_t header_byte_size = 32;
-  set_field(&reg, DMA_CTRL_WR_INFO_HDR_SZ, header_byte_size);
-  driver->write32(reg_addr, reg);
-}
-
-void init_config_dma(HardwareDriver* driver, const Config& dma,
-                     const uint8_t* program) {
-  // soft reset engine
-  reset_engine(driver, dma.engine.reg_base_addr);
-
-  // set descriptors buffer & number of descriptors
-  configure_descriptors_buffer(
-      driver, dma.engine,
-      program::number_of_program_descriptors_required(program));
-
-  const bool is_multipass = program::is_multi_pass(program);
-
-  // mask interrupts
-  mask_interrupts(driver, dma.engine.reg_base_addr, is_multipass);
-
-  // Enable/disable replay mode by setting max desc burst mode
-  uint32_t reg = 0;
-  set_field(&reg, DMA_REPLAY_MAX_DESC_BURST_MODE, is_multipass ? 1 : 0);
-  driver->write32(dma.engine.reg_base_addr + DMA_REPLAY_BUF_CTRL_REG, reg);
-
-  // Set number of descriptors per burst
-  reg = 0;
-  set_field(&reg, DMA_REPLAY_MAX_DESC_BURST_VALUE,
-            is_multipass ? program::max_num_desc(program) : 0);
-  driver->write32(dma.engine.reg_base_addr + DMA_REPLAY_BURST_VAL_REG, reg);
-
-  // Set delay start
-  reg = 0;
-  set_field(&reg, DMA_DESC_START_DELAY, is_multipass ? 0x60 : 0);
-  driver->write32(dma.engine.reg_base_addr + DMA_DESC_START_DELAYS_REG, reg);
-
-  // Configure output header (it is disabled for multipass)
-  configure_output_header(driver, dma.engine.reg_base_addr, !is_multipass);
-
-  // enable clock count for DMA config
-  toggle_buffer_timer(driver, dma.engine, true);
-
-  if (is_multipass) {
-    // in multipass we have extra configuration to set
-    init_config_dma_multipass(driver, dma, program);
-  }
-}
-
-void toggle_engine(HardwareDriver* driver, uint32_t reg_base_addr,
-                   bool enabled) {
-  auto reg = driver->read32(reg_base_addr + DMA_CTRL_REG);
-  // Set control register to run
-  set_field(&reg, DMA_CTRL_RUN, enabled ? 1 : 0);
-  set_field(&reg, DMA_CTRL_INT_EN, 1);
-  driver->write32(reg_base_addr + DMA_CTRL_REG, reg);
-}
-
-void set_output_buffer_clear(HardwareDriver* driver, const Inputs& dma,
-                             uint32_t clear_size) {
-  const uint32_t& reg_base_addr = dma.engine.reg_base_addr;
-  // Configure the output buffer clearing size
-  dma::w32 reg = 0;
-  // The output buffer clear size is expressed in 32-bit words
-  set_field(&reg, DMA_OB_PLD_CLR_SIZE, div_round_up(clear_size, 4));
-  set_field(&reg, DMA_OB_PLD_CLR_EN, clear_size > 0 ? 1 : 0);
-  driver->write32(reg_base_addr + DMA_OB_PLD_CLEAR_SIZE_REG, reg);
-}
-
-void prepare_engine_multi_pass(HardwareDriver* driver, const Inputs& dma,
-                               dma::addr hw_desc_addr,
-                               dma::addr hw_payload_addr, uint32_t num_loops) {
-  const uint32_t& reg_base_addr = dma.engine.reg_base_addr;
-  // Leave the controller disabled before setting the registers.
-  toggle_engine(driver, reg_base_addr, false);
-
-  // mask interrupts
-  mask_interrupts(driver, reg_base_addr, true);
-
-  // Enable replay mode by setting hw generated descriptors mode, dynamic size,
-  // set replay timer
-  // TODO: consider disabling this as small power enhancement.
-  uint32_t reg = 0;
-  set_field(&reg, DMA_REPLAY_MAX_DESC_BURST_MODE, 1);
-  set_field(&reg, DMA_REPLAY_HW_OB_ADDR_GEN_MODE, 1);
-  set_field(&reg, DMA_REPLAY_HW_OB_ADDR_DYN_MODE, 1);
-  set_field(&reg, DMA_REPLAY_BUFFER_MODE, 1);
-  set_field(&reg, DMA_REPLAY_TIMER_MODE, 0);
-  driver->write32(reg_base_addr + DMA_REPLAY_BUF_CTRL_REG, reg);
-
-  // Set number of loops
-  reg = 0;
-  set_field(&reg, DMA_REPLAY_LOOPS, num_loops);
-  // DMA_REPLAY_LOOPS_LAYER_PR is set with same value as num_loops, used for
-  // layer partial reconfig but mandatory to make it work nevertheless.
-  set_field(&reg, DMA_REPLAY_LOOPS_LAYER_PR, num_loops);
-  driver->write32(reg_base_addr + DMA_REPLAY_BURST_VAL_REG, reg);
-
-  // Address of the main buffer space for the HW generated Descriptors, (only
-  // one is going to be used)
-  uint32_t addr_main_desc = hw_desc_addr;
-  // scratch descriptors container can be set at the same address than main desc
-  uint32_t addr_scratch_desc = addr_main_desc;
-  // scratch payload base address has 1 as size, it only contains header (i.e.
-  // size) because DMA controller does not know that partial reconfiguration is
-  // using internal buffer.
-  uint32_t addr_scratch_payload_base_addr = hw_payload_addr;
-  driver->write32(reg_base_addr + DMA_REPLAY_DESC_MAIN_BUF_ADDR_REG,
-                  addr_main_desc);
-  driver->write32(reg_base_addr + DMA_REPLAY_DESC_SCRATCH_BUF_ADDR_REG,
-                  addr_scratch_desc);
-  driver->write32(reg_base_addr + DMA_REPLAY_OB_EVENT_SCRATCH_ADDR_REG,
-                  addr_scratch_payload_base_addr);
-}
-
-dma::addr enqueue_descriptor(HardwareDriver* driver, const Engine& dma,
-                             const dma::Descriptor& descriptor) {
-  // get number of descriptors
-  auto reg = driver->read32(dma.reg_base_addr + DMA_CONT_SIZE_REG);
-  const auto num_descriptors = get_field(reg, DMA_MAX_DESC_CONTS);
-
-  // get last descriptor id
-  const auto desc_container_reg_addr = dma.reg_base_addr + DMA_DESC_CONT_REG;
-  reg = driver->read32(desc_container_reg_addr);
-  auto last_descriptor_id = get_field(reg, DMA_LAST_DESC_CONT);
-
-  // increment last_descriptor_id and make it loop between [0; num_descriptors]
-  // we cannot use modulo operator because after reset the field will 0xFF, so
-  // the next value should be 0 but 0xFF modulo num_descriptors could lead to
-  // non zero value
-  ++last_descriptor_id;
-  if (last_descriptor_id > num_descriptors) {
-    last_descriptor_id = 0;
-  }
-  // calculate the address where we have to write the descriptor
-  auto last_descriptor_addr = dma.descriptor_base_addr +
-                              (dma.descriptor_bytes_size * last_descriptor_id);
-  // copy descriptor in scratch buffer
-  driver->write(last_descriptor_addr, descriptor.data(),
-                descriptor.size() * sizeof(Descriptor::value_type));
-
-  // then write the incremented value in field DMA_LAST_DESC_CONT.
-  // DMA will then process descriptors from DMA_DESC_CONT_REG to
-  // DMA_LAST_DESC_CONT
-  set_field(&reg, DMA_LAST_DESC_CONT, last_descriptor_id);
-  driver->write32(desc_container_reg_addr, reg);
-
-  return last_descriptor_addr;
-}
-
-void process(HardwareDriver* driver, const Config& dma,
-             const dma::Descriptor& descriptor) {
-  // enqueue descriptor
-  enqueue_descriptor(driver, dma.engine, descriptor);
-  // then wait for completion
-  wait_config_dma_descriptor_complete(driver, dma);
-}
-
-uint16_t get_last_job_id_processed(HardwareDriver* driver, const Inputs& dma) {
-  auto reg = driver->read32(dma.engine.reg_base_addr + DMA_DESC_STATUS_REG);
-  // JOB_ID is 16 bits, so we can cast to uint16_t
-  static_assert(DMA_JOB_ID.nb_bits == sizeof(uint16_t) * 8,
-                "DMA_JOB_ID field should be 16 bits");
-  return static_cast<uint16_t>(get_field(reg, DMA_JOB_ID));
-}
-
-void toggle_buffer_timer(HardwareDriver* driver, const Engine& dma,
-                         bool enabled) {
-  auto reg = driver->read32(dma.reg_base_addr + DMA_IB_BUF_MON_CTRL_REG);
-  set_field(&reg, DMA_BUF_TIMER_EN, enabled ? 1 : 0);
-  driver->write32(dma.reg_base_addr + DMA_IB_BUF_MON_CTRL_REG, reg);
-}
-
-uint32_t read_buffer_timer(HardwareDriver* driver, const Engine& dma) {
-  return driver->read32(dma.reg_base_addr + DMA_BUFFER_TIMER_STATUS_REG);
-}
-
-bool is_buffer_timer_enabled(const HardwareDriver& driver, const Inputs& dma) {
-  auto reg = driver.read32(dma.engine.reg_base_addr + DMA_IB_BUF_MON_CTRL_REG);
-  auto enabled = get_field(reg, DMA_BUF_TIMER_EN);
-  return enabled != 0;
-}
-
-void toggle_pipeline(HardwareDriver* driver, const Inputs& dma, bool enabled) {
-  auto reg = driver->read32(dma.engine.reg_base_addr + DMA_IB_BUF_MON_CTRL_REG);
-  set_field(&reg, DMA_BUF_END_SELECT, enabled ? 1 : 0);
-  // Note: The settings supported are:
-  // 0x0: DMA Outbound Buffer End -> No pipelining
-  // 0x1: DMA Inbound Buffer End -> Full pipelining
-  // 0x2: External Buffer End (from other DMA) -> Used only for debugging HRC:
-  // No HRC pipeling, Mesh pipelining
-  // Value 0x2 should probably not be used.
-
-  driver->write32(dma.engine.reg_base_addr + DMA_IB_BUF_MON_CTRL_REG, reg);
-}
-
-static void mask_interrupts(HardwareDriver* driver, uint32_t reg_base_addr,
-                            bool is_multipass) {
-  auto reg = driver->read32(reg_base_addr + DMA_IB_BUF_MON_CTRL_REG);
-
-  // Mask all interrupts except OB when single pass, or Descriptor Burst when
-  // multipass
-  set_field(&reg, DMA_BUFFER_END_MASK_OB_END, is_multipass ? 1 : 0);
-  set_field(&reg, DMA_BUFFER_END_MASK_IB_END, 1);
-  set_field(&reg, DMA_BUFFER_END_MASK_EXT_DMA_END, 1);
-  set_field(&reg, DMA_BUFFER_END_MASK_DESC_BURST_END, is_multipass ? 0 : 1);
-
-  driver->write32(reg_base_addr + DMA_IB_BUF_MON_CTRL_REG, reg);
-}
-
-bool check_for_interrupt(HardwareDriver* driver, const Engine& dma,
-                         const RegDetail& flag) {
-  uint32_t reg = driver->read32(dma.reg_base_addr + DMA_BUF_MON_STATUS_REG);
-  return get_field(reg, flag) != 0;
-}
-
-void clear_interrupts(HardwareDriver* driver, const Engine& dma) {
-  auto reg = driver->read32(dma.reg_base_addr + DMA_IB_BUF_MON_CTRL_REG);
-  set_field(&reg, DMA_STATUS_CLEAR, 1);
-  driver->write32(dma.reg_base_addr + DMA_IB_BUF_MON_CTRL_REG, reg);
-}
-
-void toggle_extra_descriptors(HardwareDriver* driver, const Config& dma,
-                              bool enable) {
-  const auto reg_addr = dma.engine.reg_base_addr + DMA_EXTRA_DESC_CTRL_REG;
-  auto reg = driver->read32(reg_addr);
-  set_field(&reg, DMA_EXTRA_DESC_ENABLE, enable ? 1 : 0);
-  driver->write32(reg_addr, reg);
-}
-
-void init_default_dma(HardwareDriver* driver, const Engine& dma,
-                      uint32_t number_of_descriptors) {
-  // soft reset
-  reset_engine(driver, dma.reg_base_addr);
-
-  // set descriptor base address & number of descriptors
-  configure_descriptors_buffer(driver, dma, number_of_descriptors);
-
-  // configure output header (enabled by default)
-  configure_output_header(driver, dma.reg_base_addr, true);
-
-  // mask interrupts
-  mask_interrupts(driver, dma.reg_base_addr, false);
-
-  // toggle engine off by default (must be called to enable interrupts)
-  toggle_engine(driver, dma.reg_base_addr, false);
-}
-
-void enable_config_dma_multipass(HardwareDriver* driver, const Config& dma) {
-  // Set last valid to a value > max descriptors, e.g +1 to loop forever. Note
-  // that the value cannot exceed 0xff (maximum value for descriptors id).
-  constexpr uint32_t cur_desc_default_val = 0xff;
-  uint32_t reg = driver->read32(dma.engine.reg_base_addr + DMA_DESC_CONT_REG);
-  uint32_t last_valid_descriptor = get_field(reg, DMA_LAST_DESC_CONT) + 1;
-  assert(last_valid_descriptor < cur_desc_default_val);
-  set_field(&reg, DMA_LAST_DESC_CONT, last_valid_descriptor);
-  set_field(&reg, DMA_CUR_DESC_CONT, cur_desc_default_val);
-  driver->write32(dma.engine.reg_base_addr + DMA_DESC_CONT_REG, reg);
-
-  // turn DMA on
-  toggle_engine(driver, dma.engine.reg_base_addr, true);
-
-  // When turning config dma on after it has been configured for multipass,
-  // it will start configuring NPs with 1st pass, so we need to wait for it
-  // to complete
-  if (!wait_for_interrupt_ext(driver, dma.engine,
-                              DMA_BUFFER_END_INTS_DESC_BURST_DONE)) {
-    panic("Timed out while dma was configuring NPs for multipass");
-  }
-}
-
-void wait_config_dma_descriptor_complete(HardwareDriver* driver,
-                                         const Config& dma) {
-  // turn dma on
-  toggle_engine(driver, dma.engine.reg_base_addr, true);
-  // then wait for interrupt
-  if (!wait_for_interrupt_ext(driver, dma.engine, DMA_BUFFER_END_INTS_OB)) {
-    panic("Timed out while processing dma configuration request");
-  }
-  // turn dma off
-  toggle_engine(driver, dma.engine.reg_base_addr, false);
-}
-
-void enqueue_extra_descriptor(HardwareDriver* driver, const Config& dma,
-                              const Descriptor& descriptor) {
-  // get number of descriptors
-  auto reg = driver->read32(dma.engine.reg_base_addr + DMA_CONT_SIZE_REG);
-  const auto num_descriptors = get_field(reg, DMA_MAX_DESC_CONTS) + 1;
-
-  // Extra descriptor will be stored after all "standard" descriptors
-  auto extra_descriptor_addr =
-      dma.engine.descriptor_base_addr +
-      (dma.engine.descriptor_bytes_size * num_descriptors);
-
-  // copy descriptor in scratch buffer
-  driver->write(extra_descriptor_addr, descriptor.data(),
-                descriptor.size() * sizeof(Descriptor::value_type));
-}
-
-}  // namespace dma
-}  // namespace akida
+#include "dma_engine.h"
+#include "dma_engine_ops.h"
+
+#include <cassert>
+
+#include "akida/hw_version.h"
+#include "engine/dma.h"
+#include "infra/hardware_driver.h"
+#include "infra/int_ops.h"
+#include "infra/registers_common.h"
+#include "infra/system.h"
+
+#include "dma_desc_format.h"
+#include "dma_desc_ops.h"
+#include "program_play.h"
+#include "registers_dma_engine.h"
+
+namespace akida {
+namespace dma {
+
+static void mask_interrupts(HardwareDriver* driver, uint32_t reg_base_addr,
+                            bool is_multi_pass);
+
+Engine::Engine(uint32_t reg_base, uint32_t desc_bytes_size)
+    : descriptor_base_addr(0),
+      descriptor_bytes_size(desc_bytes_size),
+      reg_base_addr(reg_base) {
+  assert(descriptor_bytes_size % 32 == 0 &&
+         "descriptor_bytes_size should be multiple of 32 (HW unit is 32 "
+         "bytes/256 bits)");
+}
+
+static void reset_engine(HardwareDriver* driver, uint32_t reg_base_addr) {
+  uint32_t reg = 0;
+  // perform a soft reset
+  set_field(&reg, DMA_CTRL_SOFT_RESET, 1);
+  driver->write32(reg_base_addr + DMA_CTRL_REG, reg);
+}
+
+void configure_descriptors_buffer(HardwareDriver* driver, const Engine& dma,
+                                  uint32_t num_descriptors) {
+  assert(dma.descriptor_base_addr != 0);
+
+  // update base address register
+  driver->write32(dma.reg_base_addr + DMA_CONT_ADDR_REG,
+                  dma.descriptor_base_addr);
+
+  // update container size register
+  uint32_t reg = 0;
+  // Set container size (constant in 32 bytes (256 bits) unit)
+  set_field(&reg, DMA_DESC_CONT_SIZE, dma.descriptor_bytes_size / 32);
+  // Set maximum number of descriptors. Note this corresponds to the maximum
+  // value the descriptor index can take, so that will go from 0 to
+  // num_descriptors -1.
+  set_field(&reg, DMA_MAX_DESC_CONTS, num_descriptors - 1);
+  driver->write32(dma.reg_base_addr + DMA_CONT_SIZE_REG, reg);
+}
+
+static bool wait_for_interrupt_ext(HardwareDriver* driver, const Engine& dma,
+                                   const RegDetail& flag) {
+  // This is a manual polling with a timeout of few ms
+  constexpr int64_t kTimeout = 5000;
+  auto start = time_ms();
+
+  // Busy loop until timeout is reached or interrupt is generated.
+  while (true) {
+    // check outbound interrupt
+    if (check_for_interrupt(driver, dma, flag)) {
+      // interrupt received, clear status and interrupt
+      clear_interrupts(driver, dma);
+      return true;
+    }
+
+    auto end = time_ms();
+    if ((end - start) > kTimeout) {
+      return false;
+    }
+  }
+}
+
+static void init_config_dma_multipass(HardwareDriver* driver, const Config& dma,
+                                      const uint8_t* program) {
+  const uint32_t& reg_base_addr = dma.engine.reg_base_addr;
+  const uint32_t num_descriptors =
+      program::number_of_program_descriptors_required(program);
+  const uint32_t num_extra_descs =
+      program::number_of_extra_program_descriptors_required(program);
+  const uint32_t total_num_descs = num_descriptors + num_extra_descs;
+
+  // Set extra descriptors
+  uint32_t reg = 0;
+  set_field(&reg, DMA_LAST_EXTRA_DESCRIPTOR, total_num_descs - 1);
+  // at init, learn is disabled so extra descriptor is disabled too
+  set_field(&reg, DMA_EXTRA_DESC_ENABLE, 0);
+  driver->write32(reg_base_addr + DMA_EXTRA_DESC_CTRL_REG, reg);
+
+  // Set outbound container size to 2 (as inbound)
+  reg = 0;
+  set_field(&reg, DMA_REPLAY_MAX_OB_DESC_BUFFERS, 2);
+  driver->write32(reg_base_addr + DMA_REPLAY_MAX_OB_BUFFERS_REG, reg);
+}
+
+static void configure_output_header(HardwareDriver* driver,
+                                    const dma::addr reg_base_addr,
+                                    bool enable) {
+  const auto reg_addr = reg_base_addr + DMA_CTRL_REG;
+  uint32_t reg = driver->read32(reg_addr);
+  set_field(&reg, DMA_CTRL_WR_INFO_EN, enable ? 1 : 0);
+  set_field(&reg, DMA_CTRL_WR_INFO_HDR, 1);
+  constexpr uint32_t header_byte_size = 32;
+  set_field(&reg, DMA_CTRL_WR_INFO_HDR_SZ, header_byte_size);
+  driver->write32(reg_addr, reg);
+}
+
+void init_config_dma(HardwareDriver* driver, const Config& dma,
+                     const uint8_t* program) {
+  // soft reset engine
+  reset_engine(driver, dma.engine.reg_base_addr);
+
+  // set descriptors buffer & number of descriptors
+  configure_descriptors_buffer(
+      driver, dma.engine,
+      program::number_of_program_descriptors_required(program));
+
+  const bool is_multipass = program::is_multi_pass(program);
+
+  // mask interrupts
+  mask_interrupts(driver, dma.engine.reg_base_addr, is_multipass);
+
+  // Enable/disable replay mode by setting max desc burst mode
+  uint32_t reg = 0;
+  set_field(&reg, DMA_REPLAY_MAX_DESC_BURST_MODE, is_multipass ? 1 : 0);
+  driver->write32(dma.engine.reg_base_addr + DMA_REPLAY_BUF_CTRL_REG, reg);
+
+  // Set number of descriptors per burst
+  reg = 0;
+  set_field(&reg, DMA_REPLAY_MAX_DESC_BURST_VALUE,
+            is_multipass ? program::max_num_desc(program) : 0);
+  driver->write32(dma.engine.reg_base_addr + DMA_REPLAY_BURST_VAL_REG, reg);
+
+  // Set delay start
+  reg = 0;
+  set_field(&reg, DMA_DESC_START_DELAY, is_multipass ? 0x60 : 0);
+  driver->write32(dma.engine.reg_base_addr + DMA_DESC_START_DELAYS_REG, reg);
+
+  // Configure output header (it is disabled for multipass)
+  configure_output_header(driver, dma.engine.reg_base_addr, !is_multipass);
+
+  // enable clock count for DMA config
+  toggle_buffer_timer(driver, dma.engine, true);
+
+  if (is_multipass) {
+    // in multipass we have extra configuration to set
+    init_config_dma_multipass(driver, dma, program);
+  }
+}
+
+void toggle_engine(HardwareDriver* driver, uint32_t reg_base_addr,
+                   bool enabled) {
+  auto reg = driver->read32(reg_base_addr + DMA_CTRL_REG);
+  // Set control register to run
+  set_field(&reg, DMA_CTRL_RUN, enabled ? 1 : 0);
+  set_field(&reg, DMA_CTRL_INT_EN, 1);
+  driver->write32(reg_base_addr + DMA_CTRL_REG, reg);
+}
+
+void set_output_buffer_clear(HardwareDriver* driver, const Inputs& dma,
+                             uint32_t clear_size) {
+  const uint32_t& reg_base_addr = dma.engine.reg_base_addr;
+  // Configure the output buffer clearing size
+  dma::w32 reg = 0;
+  // The output buffer clear size is expressed in 32-bit words
+  set_field(&reg, DMA_OB_PLD_CLR_SIZE, div_round_up(clear_size, 4));
+  set_field(&reg, DMA_OB_PLD_CLR_EN, clear_size > 0 ? 1 : 0);
+  driver->write32(reg_base_addr + DMA_OB_PLD_CLEAR_SIZE_REG, reg);
+}
+
+void prepare_engine_multi_pass(HardwareDriver* driver, const Inputs& dma,
+                               dma::addr hw_desc_addr,
+                               dma::addr hw_payload_addr, uint32_t num_loops) {
+  const uint32_t& reg_base_addr = dma.engine.reg_base_addr;
+  // Leave the controller disabled before setting the registers.
+  toggle_engine(driver, reg_base_addr, false);
+
+  // mask interrupts
+  mask_interrupts(driver, reg_base_addr, true);
+
+  // Enable replay mode by setting hw generated descriptors mode, dynamic size,
+  // set replay timer
+  // TODO: consider disabling this as small power enhancement.
+  uint32_t reg = 0;
+  set_field(&reg, DMA_REPLAY_MAX_DESC_BURST_MODE, 1);
+  set_field(&reg, DMA_REPLAY_HW_OB_ADDR_GEN_MODE, 1);
+  set_field(&reg, DMA_REPLAY_HW_OB_ADDR_DYN_MODE, 1);
+  set_field(&reg, DMA_REPLAY_BUFFER_MODE, 1);
+  set_field(&reg, DMA_REPLAY_TIMER_MODE, 0);
+  driver->write32(reg_base_addr + DMA_REPLAY_BUF_CTRL_REG, reg);
+
+  // Set number of loops
+  reg = 0;
+  set_field(&reg, DMA_REPLAY_LOOPS, num_loops);
+  // DMA_REPLAY_LOOPS_LAYER_PR is set with same value as num_loops, used for
+  // layer partial reconfig but mandatory to make it work nevertheless.
+  set_field(&reg, DMA_REPLAY_LOOPS_LAYER_PR, num_loops);
+  driver->write32(reg_base_addr + DMA_REPLAY_BURST_VAL_REG, reg);
+
+  // Address of the main buffer space for the HW generated Descriptors, (only
+  // one is going to be used)
+  uint32_t addr_main_desc = hw_desc_addr;
+  // scratch descriptors container can be set at the same address than main desc
+  uint32_t addr_scratch_desc = addr_main_desc;
+  // scratch payload base address has 1 as size, it only contains header (i.e.
+  // size) because DMA controller does not know that partial reconfiguration is
+  // using internal buffer.
+  uint32_t addr_scratch_payload_base_addr = hw_payload_addr;
+  driver->write32(reg_base_addr + DMA_REPLAY_DESC_MAIN_BUF_ADDR_REG,
+                  addr_main_desc);
+  driver->write32(reg_base_addr + DMA_REPLAY_DESC_SCRATCH_BUF_ADDR_REG,
+                  addr_scratch_desc);
+  driver->write32(reg_base_addr + DMA_REPLAY_OB_EVENT_SCRATCH_ADDR_REG,
+                  addr_scratch_payload_base_addr);
+}
+
+dma::addr enqueue_descriptor(HardwareDriver* driver, const Engine& dma,
+                             const dma::Descriptor& descriptor) {
+  // get number of descriptors
+  auto reg = driver->read32(dma.reg_base_addr + DMA_CONT_SIZE_REG);
+  const auto num_descriptors = get_field(reg, DMA_MAX_DESC_CONTS);
+
+  // get last descriptor id
+  const auto desc_container_reg_addr = dma.reg_base_addr + DMA_DESC_CONT_REG;
+  reg = driver->read32(desc_container_reg_addr);
+  auto last_descriptor_id = get_field(reg, DMA_LAST_DESC_CONT);
+
+  // increment last_descriptor_id and make it loop between [0; num_descriptors]
+  // we cannot use modulo operator because after reset the field will 0xFF, so
+  // the next value should be 0 but 0xFF modulo num_descriptors could lead to
+  // non zero value
+  ++last_descriptor_id;
+  if (last_descriptor_id > num_descriptors) {
+    last_descriptor_id = 0;
+  }
+  // calculate the address where we have to write the descriptor
+  auto last_descriptor_addr = dma.descriptor_base_addr +
+                              (dma.descriptor_bytes_size * last_descriptor_id);
+  // copy descriptor in scratch buffer
+  driver->write(last_descriptor_addr, descriptor.data(),
+                descriptor.size() * sizeof(Descriptor::value_type));
+
+  // then write the incremented value in field DMA_LAST_DESC_CONT.
+  // DMA will then process descriptors from DMA_DESC_CONT_REG to
+  // DMA_LAST_DESC_CONT
+  set_field(&reg, DMA_LAST_DESC_CONT, last_descriptor_id);
+  driver->write32(desc_container_reg_addr, reg);
+
+  return last_descriptor_addr;
+}
+
+void process(HardwareDriver* driver, const Config& dma,
+             const dma::Descriptor& descriptor) {
+  // enqueue descriptor
+  enqueue_descriptor(driver, dma.engine, descriptor);
+  // then wait for completion
+  wait_config_dma_descriptor_complete(driver, dma);
+}
+
+uint16_t get_last_job_id_processed(HardwareDriver* driver, const Inputs& dma) {
+  auto reg = driver->read32(dma.engine.reg_base_addr + DMA_DESC_STATUS_REG);
+  // JOB_ID is 16 bits, so we can cast to uint16_t
+  static_assert(DMA_JOB_ID.nb_bits == sizeof(uint16_t) * 8,
+                "DMA_JOB_ID field should be 16 bits");
+  return static_cast<uint16_t>(get_field(reg, DMA_JOB_ID));
+}
+
+void toggle_buffer_timer(HardwareDriver* driver, const Engine& dma,
+                         bool enabled) {
+  auto reg = driver->read32(dma.reg_base_addr + DMA_IB_BUF_MON_CTRL_REG);
+  set_field(&reg, DMA_BUF_TIMER_EN, enabled ? 1 : 0);
+  driver->write32(dma.reg_base_addr + DMA_IB_BUF_MON_CTRL_REG, reg);
+}
+
+uint32_t read_buffer_timer(HardwareDriver* driver, const Engine& dma) {
+  return driver->read32(dma.reg_base_addr + DMA_BUFFER_TIMER_STATUS_REG);
+}
+
+bool is_buffer_timer_enabled(const HardwareDriver& driver, const Inputs& dma) {
+  auto reg = driver.read32(dma.engine.reg_base_addr + DMA_IB_BUF_MON_CTRL_REG);
+  auto enabled = get_field(reg, DMA_BUF_TIMER_EN);
+  return enabled != 0;
+}
+
+void toggle_pipeline(HardwareDriver* driver, const Inputs& dma, bool enabled) {
+  auto reg = driver->read32(dma.engine.reg_base_addr + DMA_IB_BUF_MON_CTRL_REG);
+  set_field(&reg, DMA_BUF_END_SELECT, enabled ? 1 : 0);
+  // Note: The settings supported are:
+  // 0x0: DMA Outbound Buffer End -> No pipelining
+  // 0x1: DMA Inbound Buffer End -> Full pipelining
+  // 0x2: External Buffer End (from other DMA) -> Used only for debugging HRC:
+  // No HRC pipeling, Mesh pipelining
+  // Value 0x2 should probably not be used.
+
+  driver->write32(dma.engine.reg_base_addr + DMA_IB_BUF_MON_CTRL_REG, reg);
+}
+
+static void mask_interrupts(HardwareDriver* driver, uint32_t reg_base_addr,
+                            bool is_multipass) {
+  auto reg = driver->read32(reg_base_addr + DMA_IB_BUF_MON_CTRL_REG);
+
+  // Mask all interrupts except OB when single pass, or Descriptor Burst when
+  // multipass
+  set_field(&reg, DMA_BUFFER_END_MASK_OB_END, is_multipass ? 1 : 0);
+  set_field(&reg, DMA_BUFFER_END_MASK_IB_END, 1);
+  set_field(&reg, DMA_BUFFER_END_MASK_EXT_DMA_END, 1);
+  set_field(&reg, DMA_BUFFER_END_MASK_DESC_BURST_END, is_multipass ? 0 : 1);
+
+  driver->write32(reg_base_addr + DMA_IB_BUF_MON_CTRL_REG, reg);
+}
+
+bool check_for_interrupt(HardwareDriver* driver, const Engine& dma,
+                         const RegDetail& flag) {
+  uint32_t reg = driver->read32(dma.reg_base_addr + DMA_BUF_MON_STATUS_REG);
+  return get_field(reg, flag) != 0;
+}
+
+void clear_interrupts(HardwareDriver* driver, const Engine& dma) {
+  auto reg = driver->read32(dma.reg_base_addr + DMA_IB_BUF_MON_CTRL_REG);
+  set_field(&reg, DMA_STATUS_CLEAR, 1);
+  driver->write32(dma.reg_base_addr + DMA_IB_BUF_MON_CTRL_REG, reg);
+}
+
+void toggle_extra_descriptors(HardwareDriver* driver, const Config& dma,
+                              bool enable) {
+  const auto reg_addr = dma.engine.reg_base_addr + DMA_EXTRA_DESC_CTRL_REG;
+  auto reg = driver->read32(reg_addr);
+  set_field(&reg, DMA_EXTRA_DESC_ENABLE, enable ? 1 : 0);
+  driver->write32(reg_addr, reg);
+}
+
+void init_default_dma(HardwareDriver* driver, const Engine& dma,
+                      uint32_t number_of_descriptors) {
+  // soft reset
+  reset_engine(driver, dma.reg_base_addr);
+
+  // set descriptor base address & number of descriptors
+  configure_descriptors_buffer(driver, dma, number_of_descriptors);
+
+  // configure output header (enabled by default)
+  configure_output_header(driver, dma.reg_base_addr, true);
+
+  // mask interrupts
+  mask_interrupts(driver, dma.reg_base_addr, false);
+
+  // toggle engine off by default (must be called to enable interrupts)
+  toggle_engine(driver, dma.reg_base_addr, false);
+}
+
+void enable_config_dma_multipass(HardwareDriver* driver, const Config& dma) {
+  // Set last valid to a value > max descriptors, e.g +1 to loop forever. Note
+  // that the value cannot exceed 0xff (maximum value for descriptors id).
+  constexpr uint32_t cur_desc_default_val = 0xff;
+  uint32_t reg = driver->read32(dma.engine.reg_base_addr + DMA_DESC_CONT_REG);
+  uint32_t last_valid_descriptor = get_field(reg, DMA_LAST_DESC_CONT) + 1;
+  assert(last_valid_descriptor < cur_desc_default_val);
+  set_field(&reg, DMA_LAST_DESC_CONT, last_valid_descriptor);
+  set_field(&reg, DMA_CUR_DESC_CONT, cur_desc_default_val);
+  driver->write32(dma.engine.reg_base_addr + DMA_DESC_CONT_REG, reg);
+
+  // turn DMA on
+  toggle_engine(driver, dma.engine.reg_base_addr, true);
+
+  // When turning config dma on after it has been configured for multipass,
+  // it will start configuring NPs with 1st pass, so we need to wait for it
+  // to complete
+  if (!wait_for_interrupt_ext(driver, dma.engine,
+                              DMA_BUFFER_END_INTS_DESC_BURST_DONE)) {
+    panic("Timed out while dma was configuring NPs for multipass");
+  }
+}
+
+void wait_config_dma_descriptor_complete(HardwareDriver* driver,
+                                         const Config& dma) {
+  // turn dma on
+  toggle_engine(driver, dma.engine.reg_base_addr, true);
+  // then wait for interrupt
+  if (!wait_for_interrupt_ext(driver, dma.engine, DMA_BUFFER_END_INTS_OB)) {
+    panic("Timed out while processing dma configuration request");
+  }
+  // turn dma off
+  toggle_engine(driver, dma.engine.reg_base_addr, false);
+}
+
+void enqueue_extra_descriptor(HardwareDriver* driver, const Config& dma,
+                              const Descriptor& descriptor) {
+  // get number of descriptors
+  auto reg = driver->read32(dma.engine.reg_base_addr + DMA_CONT_SIZE_REG);
+  const auto num_descriptors = get_field(reg, DMA_MAX_DESC_CONTS) + 1;
+
+  // Extra descriptor will be stored after all "standard" descriptors
+  auto extra_descriptor_addr =
+      dma.engine.descriptor_base_addr +
+      (dma.engine.descriptor_bytes_size * num_descriptors);
+
+  // copy descriptor in scratch buffer
+  driver->write(extra_descriptor_addr, descriptor.data(),
+                descriptor.size() * sizeof(Descriptor::value_type));
+}
+
+}  // namespace dma
+}  // namespace akida
```

## akida/engine/src/dma_engine.h

 * *Ordering differences only*

```diff
@@ -1,30 +1,30 @@
-#pragma once
-
-#include <cstdint>
-
-#include "engine/dma.h"
-
-namespace akida {
-
-namespace dma {
-
-struct Engine {
- public:
-  explicit Engine(uint32_t reg_base_addr, uint32_t desc_bytes_size);
-
-  dma::addr descriptor_base_addr;
-  const uint32_t descriptor_bytes_size;
-  const uint32_t reg_base_addr;
-};
-
-struct Config {
-  Engine engine;
-};
-
-struct Inputs {
-  Engine engine;
-};
-
-}  // namespace dma
-
-}  // namespace akida
+#pragma once
+
+#include <cstdint>
+
+#include "engine/dma.h"
+
+namespace akida {
+
+namespace dma {
+
+struct Engine {
+ public:
+  explicit Engine(uint32_t reg_base_addr, uint32_t desc_bytes_size);
+
+  dma::addr descriptor_base_addr;
+  const uint32_t descriptor_bytes_size;
+  const uint32_t reg_base_addr;
+};
+
+struct Config {
+  Engine engine;
+};
+
+struct Inputs {
+  Engine engine;
+};
+
+}  // namespace dma
+
+}  // namespace akida
```

## akida/engine/src/dma_engine_ops.h

 * *Ordering differences only*

```diff
@@ -1,134 +1,134 @@
-#pragma once
-
-#include "dma_engine.h"
-
-#include <cstdint>
-#include <vector>
-
-#include "akida/hw_version.h"
-#include "engine/dma.h"
-#include "infra/hardware_driver.h"
-#include "infra/registers_common.h"
-
-#include "dma_desc_ops.h"
-#include "hardware_device_impl.h"
-
-namespace akida {
-
-namespace dma {
-static constexpr uint32_t MAX_PIPELINE_SIZE = (kMaxNbDescriptors - 1);
-
-// Configure DMA descriptors buffer and number of descriptors.
-// It tells the DMA where to look at for descriptors, and how many descriptors
-// it will loop on
-void configure_descriptors_buffer(HardwareDriver* driver, const Engine& dma,
-                                  uint32_t num_descriptors);
-
-// Configure control register and enable/disable DMA engine
-void toggle_engine(HardwareDriver* driver, uint32_t reg_base_addr,
-                   bool enabled);
-
-// Enqueue a descriptor to be processed by DMA. It does the following:
-// - Copies descriptor to descriptors buffer in scratch buffer at the next
-// available index
-// - Programs DMA to process it without waiting for completion, by incrementing
-// DMA_LAST_DESC_CONT field from register DMA_DESC_CONT_REG
-// DMA_LAST_DESC_CONT is a circular counter from 0 to the max number of
-// descriptors - 1
-// Return the address where the descriptor was written
-dma::addr enqueue_descriptor(HardwareDriver* driver, const Engine& dma,
-                             const dma::Descriptor& descriptor);
-
-// Tell config DMA engine to process a given descriptor. It does the following:
-// - Turns DMA on
-// - Calls enqueue_descriptor function (see its comment to know what this
-// function is doing).
-// - Waits for descriptor to be processed
-// - Turns DMA off
-void process(HardwareDriver* driver, const Config& dma,
-             const Descriptor& descriptor);
-
-// Used in single pass: return ID of last processed job
-uint16_t get_last_job_id_processed(HardwareDriver* driver, const Inputs& dma);
-
-// Turn clock counter measures on or off
-void toggle_buffer_timer(HardwareDriver* driver, const Engine& dma,
-                         bool enabled);
-
-// Retrieve clock counter measures
-uint32_t read_buffer_timer(HardwareDriver* driver, const Engine& dma);
-
-// Tell if clock counter is enabled
-bool is_buffer_timer_enabled(const HardwareDriver& driver, const Inputs& dma);
-
-// Enable or disable pipeline. When enabled, it will be kept enable on best
-// effort, disabled in multi pass and when learning is enabled.
-void toggle_pipeline(HardwareDriver* driver, const Inputs& dma, bool enabled);
-
-// Configure inputs controller to generate descriptors and process the multiple
-// passes necessary to process events.
-void prepare_engine_multi_pass(HardwareDriver* driver, const Inputs& dma,
-                               dma::addr hw_desc_addr,
-                               dma::addr hw_payload_addr, uint32_t num_loops);
-
-// Configure output buffer clearing policy
-void set_output_buffer_clear(HardwareDriver* driver, const Inputs& dma,
-                             uint32_t clear_size);
-
-// Check if the given interrupt (flag) is set on the DMA
-bool check_for_interrupt(HardwareDriver* driver, const Engine& dma,
-                         const RegDetail& flag);
-
-// Clear all interrupts from the DMA
-void clear_interrupts(HardwareDriver* driver, const Engine& dma);
-
-// Configure config dma depending on the given program. It does the following:
-// - Soft reset dma
-// - Configures descriptors buffer & number by calling
-// configure_descriptors_buffer function
-// - Masks some interrupts depending on multipass mode or not
-// - Toggles multipass mode, if on, set the number of descriptors per pass
-// - Toggles output header (on for single pass, off for multipass)
-// If the program is multi pass, it also:
-// - Configures the number of extra descriptor required (extra descriptors are
-// needed to learn using FNP3, to update weights from NP to program after each
-// input)
-// - Configures outbound container size
-void init_config_dma(HardwareDriver* driver, const Config& dma,
-                     const uint8_t* program);
-
-// Toggle extra descriptors. Extra descriptors are used by learning using FNP3,
-// to update weights from NP to program after each input
-void toggle_extra_descriptors(HardwareDriver* driver, const Config& dma,
-                              bool enable);
-
-// Configure DMA with default values
-// FIXME: this should not be called on config DMA, unless to scan mesh. When
-// scanning mesh will be out of the engine, this could be renamed to
-// init_input_dma and take const dma::Input& as argument
-// It does the following:
-// - Soft reset dma
-// - Configures descriptors buffer & number by calling
-// configure_descriptors_buffer function
-// - Turns output header on
-// - Toggles dma off
-void init_default_dma(HardwareDriver* driver, const Engine& dma,
-                      uint32_t number_of_descriptors);
-
-// Turn config DMA on and running, and wait for config DMA to configure NPs
-void enable_config_dma_multipass(HardwareDriver* driver, const Config& dma);
-
-// wait for config DMA to process a descriptor
-void wait_config_dma_descriptor_complete(HardwareDriver* driver,
-                                         const Config& dma);
-
-// Enqueue descriptor at "extra descriptors" location.
-// This works if we have only 1 total extra descriptor, which is our use case,
-// because it is only used by learn multipass, and learning can't be split
-// accross NPs so there will be a single extra descriptor in a program
-void enqueue_extra_descriptor(HardwareDriver* driver, const Config& dma,
-                              const Descriptor& descriptor);
-
-}  // namespace dma
-
-}  // namespace akida
+#pragma once
+
+#include "dma_engine.h"
+
+#include <cstdint>
+#include <vector>
+
+#include "akida/hw_version.h"
+#include "engine/dma.h"
+#include "infra/hardware_driver.h"
+#include "infra/registers_common.h"
+
+#include "dma_desc_ops.h"
+#include "hardware_device_impl.h"
+
+namespace akida {
+
+namespace dma {
+static constexpr uint32_t MAX_PIPELINE_SIZE = (kMaxNbDescriptors - 1);
+
+// Configure DMA descriptors buffer and number of descriptors.
+// It tells the DMA where to look at for descriptors, and how many descriptors
+// it will loop on
+void configure_descriptors_buffer(HardwareDriver* driver, const Engine& dma,
+                                  uint32_t num_descriptors);
+
+// Configure control register and enable/disable DMA engine
+void toggle_engine(HardwareDriver* driver, uint32_t reg_base_addr,
+                   bool enabled);
+
+// Enqueue a descriptor to be processed by DMA. It does the following:
+// - Copies descriptor to descriptors buffer in scratch buffer at the next
+// available index
+// - Programs DMA to process it without waiting for completion, by incrementing
+// DMA_LAST_DESC_CONT field from register DMA_DESC_CONT_REG
+// DMA_LAST_DESC_CONT is a circular counter from 0 to the max number of
+// descriptors - 1
+// Return the address where the descriptor was written
+dma::addr enqueue_descriptor(HardwareDriver* driver, const Engine& dma,
+                             const dma::Descriptor& descriptor);
+
+// Tell config DMA engine to process a given descriptor. It does the following:
+// - Turns DMA on
+// - Calls enqueue_descriptor function (see its comment to know what this
+// function is doing).
+// - Waits for descriptor to be processed
+// - Turns DMA off
+void process(HardwareDriver* driver, const Config& dma,
+             const Descriptor& descriptor);
+
+// Used in single pass: return ID of last processed job
+uint16_t get_last_job_id_processed(HardwareDriver* driver, const Inputs& dma);
+
+// Turn clock counter measures on or off
+void toggle_buffer_timer(HardwareDriver* driver, const Engine& dma,
+                         bool enabled);
+
+// Retrieve clock counter measures
+uint32_t read_buffer_timer(HardwareDriver* driver, const Engine& dma);
+
+// Tell if clock counter is enabled
+bool is_buffer_timer_enabled(const HardwareDriver& driver, const Inputs& dma);
+
+// Enable or disable pipeline. When enabled, it will be kept enable on best
+// effort, disabled in multi pass and when learning is enabled.
+void toggle_pipeline(HardwareDriver* driver, const Inputs& dma, bool enabled);
+
+// Configure inputs controller to generate descriptors and process the multiple
+// passes necessary to process events.
+void prepare_engine_multi_pass(HardwareDriver* driver, const Inputs& dma,
+                               dma::addr hw_desc_addr,
+                               dma::addr hw_payload_addr, uint32_t num_loops);
+
+// Configure output buffer clearing policy
+void set_output_buffer_clear(HardwareDriver* driver, const Inputs& dma,
+                             uint32_t clear_size);
+
+// Check if the given interrupt (flag) is set on the DMA
+bool check_for_interrupt(HardwareDriver* driver, const Engine& dma,
+                         const RegDetail& flag);
+
+// Clear all interrupts from the DMA
+void clear_interrupts(HardwareDriver* driver, const Engine& dma);
+
+// Configure config dma depending on the given program. It does the following:
+// - Soft reset dma
+// - Configures descriptors buffer & number by calling
+// configure_descriptors_buffer function
+// - Masks some interrupts depending on multipass mode or not
+// - Toggles multipass mode, if on, set the number of descriptors per pass
+// - Toggles output header (on for single pass, off for multipass)
+// If the program is multi pass, it also:
+// - Configures the number of extra descriptor required (extra descriptors are
+// needed to learn using FNP3, to update weights from NP to program after each
+// input)
+// - Configures outbound container size
+void init_config_dma(HardwareDriver* driver, const Config& dma,
+                     const uint8_t* program);
+
+// Toggle extra descriptors. Extra descriptors are used by learning using FNP3,
+// to update weights from NP to program after each input
+void toggle_extra_descriptors(HardwareDriver* driver, const Config& dma,
+                              bool enable);
+
+// Configure DMA with default values
+// FIXME: this should not be called on config DMA, unless to scan mesh. When
+// scanning mesh will be out of the engine, this could be renamed to
+// init_input_dma and take const dma::Input& as argument
+// It does the following:
+// - Soft reset dma
+// - Configures descriptors buffer & number by calling
+// configure_descriptors_buffer function
+// - Turns output header on
+// - Toggles dma off
+void init_default_dma(HardwareDriver* driver, const Engine& dma,
+                      uint32_t number_of_descriptors);
+
+// Turn config DMA on and running, and wait for config DMA to configure NPs
+void enable_config_dma_multipass(HardwareDriver* driver, const Config& dma);
+
+// wait for config DMA to process a descriptor
+void wait_config_dma_descriptor_complete(HardwareDriver* driver,
+                                         const Config& dma);
+
+// Enqueue descriptor at "extra descriptors" location.
+// This works if we have only 1 total extra descriptor, which is our use case,
+// because it is only used by learn multipass, and learning can't be split
+// accross NPs so there will be a single extra descriptor in a program
+void enqueue_extra_descriptor(HardwareDriver* driver, const Config& dma,
+                              const Descriptor& descriptor);
+
+}  // namespace dma
+
+}  // namespace akida
```

## akida/engine/src/dma_events.h

 * *Ordering differences only*

```diff
@@ -1,54 +1,54 @@
-#pragma once
-
-#include "akida/shape.h"
-#include "akida/sparse.h"
-
-#include "engine/dma.h"
-
-namespace akida {
-
-class DmaEvents : public Sparse {
- public:
-  DmaEvents(const Shape& shape, const dma::wbuffer&& dma_words)
-      : shape_(shape), buffer_(std::move(dma_words)) {}
-
-  TensorType type() const override { return TensorType::uint8; }
-
-  size_t size() const override {
-    // Each event is stored in two DMA words
-    return buffer_.dma_words_.size() / 2;
-  }
-
-  Shape dimensions() const override { return shape_; }
-
-  Tensor::Buffer* buffer() override { return &buffer_; }
-
-  const Tensor::Buffer* buffer() const override { return &buffer_; }
-
-  const dma::wbuffer& data() const { return buffer_.dma_words_; }
-
- protected:
-  class Buffer : public Tensor::Buffer {
-    friend DmaEvents;
-
-   public:
-    explicit Buffer(const dma::wbuffer&& dma_words)
-        : dma_words_(std::move(dma_words)) {}
-    size_t size() const override {
-      return dma_words_.size() * sizeof(dma::w32);
-    }
-    char* data() override { return reinterpret_cast<char*>(dma_words_.data()); }
-    const char* data() const override {
-      return reinterpret_cast<const char*>(dma_words_.data());
-    }
-
-   protected:
-    dma::wbuffer dma_words_;
-  };
-  Shape shape_;
-  Buffer buffer_;
-};
-
-using DmaEventsPtr = std::unique_ptr<DmaEvents>;
-
-}  // namespace akida
+#pragma once
+
+#include "akida/shape.h"
+#include "akida/sparse.h"
+
+#include "engine/dma.h"
+
+namespace akida {
+
+class DmaEvents : public Sparse {
+ public:
+  DmaEvents(const Shape& shape, const dma::wbuffer&& dma_words)
+      : shape_(shape), buffer_(std::move(dma_words)) {}
+
+  TensorType type() const override { return TensorType::uint8; }
+
+  size_t size() const override {
+    // Each event is stored in two DMA words
+    return buffer_.dma_words_.size() / 2;
+  }
+
+  Shape dimensions() const override { return shape_; }
+
+  Tensor::Buffer* buffer() override { return &buffer_; }
+
+  const Tensor::Buffer* buffer() const override { return &buffer_; }
+
+  const dma::wbuffer& data() const { return buffer_.dma_words_; }
+
+ protected:
+  class Buffer : public Tensor::Buffer {
+    friend DmaEvents;
+
+   public:
+    explicit Buffer(const dma::wbuffer&& dma_words)
+        : dma_words_(std::move(dma_words)) {}
+    size_t size() const override {
+      return dma_words_.size() * sizeof(dma::w32);
+    }
+    char* data() override { return reinterpret_cast<char*>(dma_words_.data()); }
+    const char* data() const override {
+      return reinterpret_cast<const char*>(dma_words_.data());
+    }
+
+   protected:
+    dma::wbuffer dma_words_;
+  };
+  Shape shape_;
+  Buffer buffer_;
+};
+
+using DmaEventsPtr = std::unique_ptr<DmaEvents>;
+
+}  // namespace akida
```

## akida/engine/src/dma_events_format.h

 * *Ordering differences only*

```diff
@@ -1,26 +1,26 @@
-#pragma once
-
-#include <cstdint>
-
-#include "infra/registers_common.h"
-
-namespace akida {
-
-// fields for word 1 cnp
-static constexpr RegDetail CONV_X(0, 11);
-static constexpr RegDetail CONV_Y(16, 27);
-static constexpr RegDetail CONV_POTENTIAL_MSB(28, 31);
-// fields for word 2 cnp
-static constexpr RegDetail CONV_F(0, 10);
-static constexpr RegDetail CONV_ACTIVATION(16, 23);
-static constexpr RegDetail CONV_POTENTIAL_LSB(12, 31);
-// fields for word 1 fnp
-static constexpr RegDetail FC_F(0, 19);
-// fields for word 2 fnp
-static constexpr RegDetail FC_ACTIVATION(0, 19);  // potential is the same
-static constexpr RegDetail FC_POLARITY(31);       // should be set to 1
-
-// field for size
-static constexpr RegDetail OUTPUT_WORD_SIZE(0, 27);
-
-}  // namespace akida
+#pragma once
+
+#include <cstdint>
+
+#include "infra/registers_common.h"
+
+namespace akida {
+
+// fields for word 1 cnp
+static constexpr RegDetail CONV_X(0, 11);
+static constexpr RegDetail CONV_Y(16, 27);
+static constexpr RegDetail CONV_POTENTIAL_MSB(28, 31);
+// fields for word 2 cnp
+static constexpr RegDetail CONV_F(0, 10);
+static constexpr RegDetail CONV_ACTIVATION(16, 23);
+static constexpr RegDetail CONV_POTENTIAL_LSB(12, 31);
+// fields for word 1 fnp
+static constexpr RegDetail FC_F(0, 19);
+// fields for word 2 fnp
+static constexpr RegDetail FC_ACTIVATION(0, 19);  // potential is the same
+static constexpr RegDetail FC_POLARITY(31);       // should be set to 1
+
+// field for size
+static constexpr RegDetail OUTPUT_WORD_SIZE(0, 27);
+
+}  // namespace akida
```

## akida/engine/src/dma_events_ops.cpp

 * *Ordering differences only*

```diff
@@ -1,373 +1,373 @@
-#include "dma_events_ops.h"
-
-#include <cstddef>
-#include <vector>
-
-#include "akida/dense.h"
-#include "akida/shape.h"
-#include "akida/sparse.h"
-#include "akida/tensor.h"
-#include "dma_cnp_events.h"
-#include "dma_events_format.h"
-#include "dma_fnp_events.h"
-#include "dma_hrc_events.h"
-#include "engine/dma.h"
-#include "engine/int_conversion.h"
-#include "infra/registers_common.h"
-
-#include "dma_desc_ops.h"
-
-namespace akida {
-
-static void set_cnp_event(Index x, Index y, Index z, uint8_t v,
-                          dma::w32* evt_word1, dma::w32* evt_word2) {
-  set_field(evt_word1, CONV_X, x);
-  set_field(evt_word1, CONV_Y, y);
-  set_field(evt_word2, CONV_F, z);
-  set_field(evt_word2, CONV_ACTIVATION, v);
-  // NOTE: polarity is not set
-}
-
-static void set_fnp_event(Index f, uint8_t v, dma::w32* evt_word1,
-                          dma::w32* evt_word2) {
-  set_field(evt_word1, FC_F, f);
-  set_field(evt_word2, FC_ACTIVATION, v);
-  set_field(evt_word2, FC_POLARITY, 1);
-  // NOTE: the "fire" field is always set
-}
-
-static void add_dummy_event_fnp(const Tensor& inputs, dma::wbuffer* events) {
-  // Add a dummy event for fnp, outside of dimensions with value 1. One index
-  // out of the tensor in every dimension
-  // (max coord is (shape[0] -1, shape[1] - 1, shape[2] - 1)
-  // When null event forwarding, a dummy neuron is needed on NSoC-v2 and is
-  // harmless on newer IP versions.
-  const auto& shape = inputs.dimensions();
-  auto strides = Dense::eval_strides(shape, Dense::Layout::ColMajor);
-  auto index = static_cast<Index>(linear_index(shape.data(), strides));
-  dma::w32 evt_words[2] = {0, 0};
-  set_fnp_event(index, 1, &evt_words[0], &evt_words[1]);
-  events->insert(events->end(), evt_words, evt_words + 2);
-}
-
-static void add_dummy_event_cnp(const Tensor& inputs, dma::wbuffer* events) {
-  // Add a dummy event for cnp, outside of dimensions with value 1. One index
-  // out of the tensor in every dimension
-  // (max coord is (shape[0] -1, shape[1] - 1, shape[2] - 1)
-  // When null event forwarding, a dummy neuron is needed on NSoC-v2 and is
-  // harmless on newer IP versions.
-  const auto& shape = inputs.dimensions();
-  dma::w32 evt_words[2] = {0, 0};
-  set_cnp_event(shape[0], shape[1], shape[2], 1, &evt_words[0], &evt_words[1]);
-  events->insert(events->end(), evt_words, evt_words + 2);
-}
-
-static dma::wbuffer format_cnp_events(const Sparse& inputs) {
-  auto nbevents = inputs.size();
-  dma::wbuffer events(dma::kSparseEventWordSize * nbevents, 0);
-
-  auto events_it = inputs.begin();
-  for (size_t i = 0; i < nbevents; i++) {
-    // map event words
-    auto& evt_word1 = events[i * dma::kSparseEventWordSize];
-    auto& evt_word2 = events[i * dma::kSparseEventWordSize + 1];
-
-    auto v = events_it->value<uint8_t>();
-    auto coords = events_it->coords();
-    set_cnp_event(coords[0], coords[1], coords[2], v, &evt_word1, &evt_word2);
-    events_it->next();
-  }
-  return events;
-}
-
-static dma::wbuffer format_fnp_events_3D(const Sparse& inputs) {
-  auto nbevents = inputs.size();
-  dma::wbuffer events(dma::kSparseEventWordSize * nbevents, 0);
-
-  // Evaluate the strides of the inputs following a col-major convention,
-  // because the hardware linearizes the inputs using that convention
-  auto strides =
-      Dense::eval_strides(inputs.dimensions(), Dense::Layout::ColMajor);
-  auto events_it = inputs.begin();
-  for (size_t i = 0; i < nbevents; i++) {
-    // map event words
-    auto& evt_word1 = events[i * dma::kSparseEventWordSize];
-    auto& evt_word2 = events[i * dma::kSparseEventWordSize + 1];
-
-    auto v = events_it->value<uint8_t>();
-    auto c = static_cast<Index>(events_it->unravel(strides));
-    set_fnp_event(c, v, &evt_word1, &evt_word2);
-    events_it->next();
-  }
-  return events;
-}
-
-static dma::wbuffer format_fnp_events(const Sparse& inputs) {
-  auto shape = inputs.dimensions();
-  if ((shape.size() == 3) && ((shape[0] != 1) || (shape[1] != 1))) {
-    // non-flat FNP inputs should be formatted as CNP, but if channel do not fit
-    // on 11 bits, they need to be formatted as FNP and then we need to flatten
-    // them
-    assert(shape[2] > (1u << CONV_F.nb_bits) && "channels should be > 2048");
-    return format_fnp_events_3D(inputs);
-  }
-
-  auto nbevents = inputs.size();
-  dma::wbuffer events(dma::kSparseEventWordSize * nbevents, 0);
-
-  auto events_it = inputs.begin();
-  for (size_t i = 0; i < nbevents; i++) {
-    // map event words
-    auto& evt_word1 = events[i * dma::kSparseEventWordSize];
-    auto& evt_word2 = events[i * dma::kSparseEventWordSize + 1];
-
-    auto v = events_it->value<uint8_t>();
-    // For a flat input, we only care about the last coordinate
-    auto c = events_it->coords()[2];
-    set_fnp_event(c, v, &evt_word1, &evt_word2);
-    events_it->next();
-  }
-  return events;
-}
-
-static dma::wbuffer format_cnp_events(const Dense& inputs) {
-  // We don't know how many events the Dense contains, but assume it is full
-  dma::wbuffer events;
-  events.reserve(dma::kSparseEventWordSize * inputs.size());
-
-  auto shape = inputs.dimensions();
-  auto w = shape[0];
-  auto h = shape[1];
-  auto c = shape[2];
-  auto values_ptr = inputs.data<uint8_t>();
-  for (Index x = 0; x < w; ++x) {
-    for (Index y = 0; y < h; ++y) {
-      for (Index z = 0; z < c; ++z) {
-        // Extract value for the current coordinate
-        auto value = *values_ptr;
-        if (*values_ptr > 0) {
-          dma::w32 evt_words[2] = {0, 0};
-          set_cnp_event(x, y, z, value, &evt_words[0], &evt_words[1]);
-          events.insert(events.end(), evt_words, evt_words + 2);
-        }
-        ++values_ptr;
-      }
-    }
-  }
-  return events;
-}
-
-static dma::wbuffer format_fnp_events(const Dense& inputs) {
-  // We don't know how many events the Dense contains, but assume it is full
-  dma::wbuffer events;
-  events.reserve(dma::kSparseEventWordSize * inputs.size());
-
-  auto values = inputs.data<uint8_t>();
-  // For a flat input, we don't care about the coordinates
-  for (Index i = 0; i < inputs.size(); ++i) {
-    // Extract value for the current index
-    auto value = values[i];
-    if (value > 0) {
-      dma::w32 evt_words[2] = {0, 0};
-      set_fnp_event(i, value, &evt_words[0], &evt_words[1]);
-      events.insert(events.end(), evt_words, evt_words + 2);
-    }
-  }
-  return events;
-}
-
-static dma::wbuffer format_cnp_events(const Tensor& inputs) {
-  // Assume the tensor is Sparse
-  auto sparse = dynamic_cast<const Sparse*>(&inputs);
-  if (sparse) {
-    return format_cnp_events(*sparse);
-  }
-  return format_cnp_events(dynamic_cast<const Dense&>(inputs));
-}
-
-static dma::wbuffer format_fnp_events(const Tensor& inputs) {
-  // Assume the tensor is Sparse
-  auto sparse = dynamic_cast<const Sparse*>(&inputs);
-  if (sparse) {
-    return format_fnp_events(*sparse);
-  }
-  return format_fnp_events(dynamic_cast<const Dense&>(inputs));
-}
-
-using read_data_func = void (*)(int32_t*, const std::vector<uint32_t>&,
-                                dma::w32, dma::w32);
-
-/**
- * Extract the potentials from a DMA word buffer and build an int32 Dense tensor
- */
-static TensorUniquePtr format_output_potentials(
-    const dma::wbuffer& output_words, const Shape& out_dims,
-    dma::OutputFormat output_format) {
-  // We use lambdas to distinguish between the three potentials output formats
-  read_data_func read_data;
-  switch (output_format) {
-    case dma::OutputFormat::FullyPotentials: {
-      read_data = [](int32_t* output, const std::vector<uint32_t>&,
-                     dma::w32 word1, dma::w32 word2) {
-        // with FullyConnected, output is (1, 1, F) so the index is just the
-        // channel
-        const auto index = static_cast<Index>(get_field(word1, FC_F));
-        output[index] = intN_to_int32<20>(get_field(word2, FC_ACTIVATION));
-      };
-    } break;
-    case dma::OutputFormat::ConvHighPotentials: {
-      read_data = [](int32_t* output, const std::vector<uint32_t>& strides,
-                     dma::w32 word1, dma::w32 word2) {
-        // get index from coordinate
-        std::array<Index, 3> coords;
-        coords[0] = static_cast<Index>(get_field(word1, CONV_X));
-        coords[1] = static_cast<Index>(get_field(word1, CONV_Y));
-        coords[2] = static_cast<Index>(get_field(word2, CONV_F));
-        const auto index = linear_index(coords.data(), strides);
-
-        // get potential value
-        // The potential 4 MSB (including the sign) are on word 1
-        auto pot_msb = intN_to_int32<4>(get_field(word1, CONV_POTENTIAL_MSB));
-        // And the 20 LSB (positive number only) are on word 2
-        auto pot_lsb =
-            static_cast<int32_t>(get_field(word2, CONV_POTENTIAL_LSB));
-        // write value
-        output[index] = (pot_msb << CONV_POTENTIAL_LSB.nb_bits) | pot_lsb;
-      };
-    } break;
-    case dma::OutputFormat::ConvPotentials: {
-      read_data = [](int32_t* output, const std::vector<uint32_t>& strides,
-                     dma::w32 word1, dma::w32 word2) {
-        // get index from coordinate
-        std::array<Index, 3> coords;
-        coords[0] = static_cast<Index>(get_field(word1, CONV_X));
-        coords[1] = static_cast<Index>(get_field(word1, CONV_Y));
-        coords[2] = static_cast<Index>(get_field(word2, CONV_F));
-        const auto index = linear_index(coords.data(), strides);
-
-        // write potential value
-        output[index] = intN_to_int32<20>(get_field(word2, CONV_POTENTIAL_LSB));
-      };
-    } break;
-    default: {
-      panic("Unexpected output type");
-    }
-  }
-  // Allocate the output dense buffer
-  auto dense =
-      Dense::create(TensorType::int32, out_dims, Dense::Layout::RowMajor);
-  // Iterate over DMA events to fill the dense buffer
-  uint32_t n_events =
-      static_cast<uint32_t>(output_words.size() / dma::kSparseEventWordSize);
-  // Get pointer to dense, to avoid using set() method that calls data() and
-  // then check_type() method at every iteration
-  auto dense_ptr = dense->data<int32_t>();
-  for (uint32_t i = 0; i < n_events; i++) {
-    // Events are stored in two consecutive words
-    const auto& output_word1 = output_words[i * dma::kSparseEventWordSize];
-    const auto& output_word2 = output_words[i * dma::kSparseEventWordSize + 1];
-    // Read the output coords and value
-    read_data(dense_ptr, dense->strides(), output_word1, output_word2);
-  }
-  return dense;
-}
-
-DmaEventsPtr to_dma_events(const Tensor& inputs, bool input_is_fnp) {
-  dma::wbuffer event_data;
-  DmaEventsPtr result;
-  dma::wbuffer::size_type nb_words;
-  if (input_is_fnp) {
-    event_data = format_fnp_events(inputs);
-    // workaround when null event
-    if (event_data.empty()) {
-      add_dummy_event_fnp(inputs, &event_data);
-    }
-    nb_words = event_data.size();
-    result = DmaEventsPtr(
-        new DmaFnpEvents(inputs.dimensions(), std::move(event_data)));
-  } else {
-    event_data = format_cnp_events(inputs);
-    // workaround when null event
-    if (event_data.empty()) {
-      add_dummy_event_cnp(inputs, &event_data);
-    }
-    nb_words = event_data.size();
-    result = DmaEventsPtr(
-        new DmaCnpEvents(inputs.dimensions(), std::move(event_data)));
-  }
-  // check we are below the maximum number of events
-  auto max_dma_events = dma::max_dma_events();
-  if (nb_words > max_dma_events * dma::kSparseEventWordSize) {
-    panic("%d exceeds the maximum number of events: %d", nb_words,
-          max_dma_events);
-  }
-  return result;
-}
-
-TensorUniquePtr dma_events_read_outputs(HardwareDriver* driver,
-                                        const uint32_t addr_output_events,
-                                        const Shape output_dimensions,
-                                        dma::OutputFormat output_format) {
-  // The header is the first word of the output
-  uint32_t header = driver->read32(addr_output_events);
-  uint32_t output_word_size = get_field(header, OUTPUT_WORD_SIZE);
-  auto output_bytes_size = output_word_size * sizeof(dma::w32);
-
-  // adjust past header to data
-  uint32_t read_offset_addr = addr_output_events + dma::kOutputHeaderByteSize;
-
-  switch (output_format) {
-    case dma::OutputFormat::ConvActivations: {
-      // Read back data aligned on 32-bit words
-      dma::wbuffer output_events(output_word_size);
-      driver->read(read_offset_addr, output_events.data(), output_bytes_size);
-      // We can directly wrap the output events in a DmaCnpEvents sparse tensor
-      return TensorUniquePtr(
-          new DmaCnpEvents(output_dimensions, std::move(output_events)));
-    }
-    case dma::OutputFormat::FullyActivations: {
-      // Read back data aligned on 32-bit words
-      dma::wbuffer output_events(output_word_size);
-      driver->read(read_offset_addr, output_events.data(), output_bytes_size);
-      // We can directly wrap the output events in a DmaFnpEvents sparse tensor
-      return TensorUniquePtr(
-          new DmaFnpEvents(output_dimensions, std::move(output_events)));
-    }
-    case dma::OutputFormat::HrcActivations: {
-      // Read back data aligned on 32-bit words
-      dma::wbuffer output_events(output_word_size);
-      driver->read(read_offset_addr, output_events.data(), output_bytes_size);
-      // We can directly wrap the output events in a DmaCnpEvents sparse tensor
-      return TensorUniquePtr(
-          new DmaHrcEvents(output_dimensions, std::move(output_events)));
-    }
-    case dma::OutputFormat::DenseActivations: {
-      // determine actual byte size
-      auto out_size = shape_size(output_dimensions) * sizeof(uint8_t);
-      // Create an empty tensor
-      auto dense = Dense::create(TensorType::uint8, output_dimensions,
-                                 Dense::Layout::RowMajor);
-      // directly read activations into the tensor, ignoring word size
-      driver->read(read_offset_addr, dense->buffer()->data(), out_size);
-      return dense;
-    }
-    case dma::OutputFormat::DensePotentials: {
-      // Create an empty tensor
-      auto dense = Dense::create(TensorType::int32, output_dimensions,
-                                 Dense::Layout::RowMajor);
-      auto potentials = dense->buffer()->data();
-      driver->read(read_offset_addr, potentials, output_bytes_size);
-      return dense;
-    }
-    default: {
-      // Read back data aligned on 32-bit words
-      dma::wbuffer output_events(output_word_size);
-      driver->read(read_offset_addr, output_events.data(), output_bytes_size);
-      // Extract potentials and create a Dense
-      return format_output_potentials(output_events, output_dimensions,
-                                      output_format);
-    }
-  }
-}
-
-}  // namespace akida
+#include "dma_events_ops.h"
+
+#include <cstddef>
+#include <vector>
+
+#include "akida/dense.h"
+#include "akida/shape.h"
+#include "akida/sparse.h"
+#include "akida/tensor.h"
+#include "dma_cnp_events.h"
+#include "dma_events_format.h"
+#include "dma_fnp_events.h"
+#include "dma_hrc_events.h"
+#include "engine/dma.h"
+#include "engine/int_conversion.h"
+#include "infra/registers_common.h"
+
+#include "dma_desc_ops.h"
+
+namespace akida {
+
+static void set_cnp_event(Index x, Index y, Index z, uint8_t v,
+                          dma::w32* evt_word1, dma::w32* evt_word2) {
+  set_field(evt_word1, CONV_X, x);
+  set_field(evt_word1, CONV_Y, y);
+  set_field(evt_word2, CONV_F, z);
+  set_field(evt_word2, CONV_ACTIVATION, v);
+  // NOTE: polarity is not set
+}
+
+static void set_fnp_event(Index f, uint8_t v, dma::w32* evt_word1,
+                          dma::w32* evt_word2) {
+  set_field(evt_word1, FC_F, f);
+  set_field(evt_word2, FC_ACTIVATION, v);
+  set_field(evt_word2, FC_POLARITY, 1);
+  // NOTE: the "fire" field is always set
+}
+
+static void add_dummy_event_fnp(const Tensor& inputs, dma::wbuffer* events) {
+  // Add a dummy event for fnp, outside of dimensions with value 1. One index
+  // out of the tensor in every dimension
+  // (max coord is (shape[0] -1, shape[1] - 1, shape[2] - 1)
+  // When null event forwarding, a dummy neuron is needed on NSoC-v2 and is
+  // harmless on newer IP versions.
+  const auto& shape = inputs.dimensions();
+  auto strides = Dense::eval_strides(shape, Dense::Layout::ColMajor);
+  auto index = static_cast<Index>(linear_index(shape.data(), strides));
+  dma::w32 evt_words[2] = {0, 0};
+  set_fnp_event(index, 1, &evt_words[0], &evt_words[1]);
+  events->insert(events->end(), evt_words, evt_words + 2);
+}
+
+static void add_dummy_event_cnp(const Tensor& inputs, dma::wbuffer* events) {
+  // Add a dummy event for cnp, outside of dimensions with value 1. One index
+  // out of the tensor in every dimension
+  // (max coord is (shape[0] -1, shape[1] - 1, shape[2] - 1)
+  // When null event forwarding, a dummy neuron is needed on NSoC-v2 and is
+  // harmless on newer IP versions.
+  const auto& shape = inputs.dimensions();
+  dma::w32 evt_words[2] = {0, 0};
+  set_cnp_event(shape[0], shape[1], shape[2], 1, &evt_words[0], &evt_words[1]);
+  events->insert(events->end(), evt_words, evt_words + 2);
+}
+
+static dma::wbuffer format_cnp_events(const Sparse& inputs) {
+  auto nbevents = inputs.size();
+  dma::wbuffer events(dma::kSparseEventWordSize * nbevents, 0);
+
+  auto events_it = inputs.begin();
+  for (size_t i = 0; i < nbevents; i++) {
+    // map event words
+    auto& evt_word1 = events[i * dma::kSparseEventWordSize];
+    auto& evt_word2 = events[i * dma::kSparseEventWordSize + 1];
+
+    auto v = events_it->value<uint8_t>();
+    auto coords = events_it->coords();
+    set_cnp_event(coords[0], coords[1], coords[2], v, &evt_word1, &evt_word2);
+    events_it->next();
+  }
+  return events;
+}
+
+static dma::wbuffer format_fnp_events_3D(const Sparse& inputs) {
+  auto nbevents = inputs.size();
+  dma::wbuffer events(dma::kSparseEventWordSize * nbevents, 0);
+
+  // Evaluate the strides of the inputs following a col-major convention,
+  // because the hardware linearizes the inputs using that convention
+  auto strides =
+      Dense::eval_strides(inputs.dimensions(), Dense::Layout::ColMajor);
+  auto events_it = inputs.begin();
+  for (size_t i = 0; i < nbevents; i++) {
+    // map event words
+    auto& evt_word1 = events[i * dma::kSparseEventWordSize];
+    auto& evt_word2 = events[i * dma::kSparseEventWordSize + 1];
+
+    auto v = events_it->value<uint8_t>();
+    auto c = static_cast<Index>(events_it->unravel(strides));
+    set_fnp_event(c, v, &evt_word1, &evt_word2);
+    events_it->next();
+  }
+  return events;
+}
+
+static dma::wbuffer format_fnp_events(const Sparse& inputs) {
+  auto shape = inputs.dimensions();
+  if ((shape.size() == 3) && ((shape[0] != 1) || (shape[1] != 1))) {
+    // non-flat FNP inputs should be formatted as CNP, but if channel do not fit
+    // on 11 bits, they need to be formatted as FNP and then we need to flatten
+    // them
+    assert(shape[2] > (1u << CONV_F.nb_bits) && "channels should be > 2048");
+    return format_fnp_events_3D(inputs);
+  }
+
+  auto nbevents = inputs.size();
+  dma::wbuffer events(dma::kSparseEventWordSize * nbevents, 0);
+
+  auto events_it = inputs.begin();
+  for (size_t i = 0; i < nbevents; i++) {
+    // map event words
+    auto& evt_word1 = events[i * dma::kSparseEventWordSize];
+    auto& evt_word2 = events[i * dma::kSparseEventWordSize + 1];
+
+    auto v = events_it->value<uint8_t>();
+    // For a flat input, we only care about the last coordinate
+    auto c = events_it->coords()[2];
+    set_fnp_event(c, v, &evt_word1, &evt_word2);
+    events_it->next();
+  }
+  return events;
+}
+
+static dma::wbuffer format_cnp_events(const Dense& inputs) {
+  // We don't know how many events the Dense contains, but assume it is full
+  dma::wbuffer events;
+  events.reserve(dma::kSparseEventWordSize * inputs.size());
+
+  auto shape = inputs.dimensions();
+  auto w = shape[0];
+  auto h = shape[1];
+  auto c = shape[2];
+  auto values_ptr = inputs.data<uint8_t>();
+  for (Index x = 0; x < w; ++x) {
+    for (Index y = 0; y < h; ++y) {
+      for (Index z = 0; z < c; ++z) {
+        // Extract value for the current coordinate
+        auto value = *values_ptr;
+        if (*values_ptr > 0) {
+          dma::w32 evt_words[2] = {0, 0};
+          set_cnp_event(x, y, z, value, &evt_words[0], &evt_words[1]);
+          events.insert(events.end(), evt_words, evt_words + 2);
+        }
+        ++values_ptr;
+      }
+    }
+  }
+  return events;
+}
+
+static dma::wbuffer format_fnp_events(const Dense& inputs) {
+  // We don't know how many events the Dense contains, but assume it is full
+  dma::wbuffer events;
+  events.reserve(dma::kSparseEventWordSize * inputs.size());
+
+  auto values = inputs.data<uint8_t>();
+  // For a flat input, we don't care about the coordinates
+  for (Index i = 0; i < inputs.size(); ++i) {
+    // Extract value for the current index
+    auto value = values[i];
+    if (value > 0) {
+      dma::w32 evt_words[2] = {0, 0};
+      set_fnp_event(i, value, &evt_words[0], &evt_words[1]);
+      events.insert(events.end(), evt_words, evt_words + 2);
+    }
+  }
+  return events;
+}
+
+static dma::wbuffer format_cnp_events(const Tensor& inputs) {
+  // Assume the tensor is Sparse
+  auto sparse = dynamic_cast<const Sparse*>(&inputs);
+  if (sparse) {
+    return format_cnp_events(*sparse);
+  }
+  return format_cnp_events(dynamic_cast<const Dense&>(inputs));
+}
+
+static dma::wbuffer format_fnp_events(const Tensor& inputs) {
+  // Assume the tensor is Sparse
+  auto sparse = dynamic_cast<const Sparse*>(&inputs);
+  if (sparse) {
+    return format_fnp_events(*sparse);
+  }
+  return format_fnp_events(dynamic_cast<const Dense&>(inputs));
+}
+
+using read_data_func = void (*)(int32_t*, const std::vector<uint32_t>&,
+                                dma::w32, dma::w32);
+
+/**
+ * Extract the potentials from a DMA word buffer and build an int32 Dense tensor
+ */
+static TensorUniquePtr format_output_potentials(
+    const dma::wbuffer& output_words, const Shape& out_dims,
+    dma::OutputFormat output_format) {
+  // We use lambdas to distinguish between the three potentials output formats
+  read_data_func read_data;
+  switch (output_format) {
+    case dma::OutputFormat::FullyPotentials: {
+      read_data = [](int32_t* output, const std::vector<uint32_t>&,
+                     dma::w32 word1, dma::w32 word2) {
+        // with FullyConnected, output is (1, 1, F) so the index is just the
+        // channel
+        const auto index = static_cast<Index>(get_field(word1, FC_F));
+        output[index] = intN_to_int32<20>(get_field(word2, FC_ACTIVATION));
+      };
+    } break;
+    case dma::OutputFormat::ConvHighPotentials: {
+      read_data = [](int32_t* output, const std::vector<uint32_t>& strides,
+                     dma::w32 word1, dma::w32 word2) {
+        // get index from coordinate
+        std::array<Index, 3> coords;
+        coords[0] = static_cast<Index>(get_field(word1, CONV_X));
+        coords[1] = static_cast<Index>(get_field(word1, CONV_Y));
+        coords[2] = static_cast<Index>(get_field(word2, CONV_F));
+        const auto index = linear_index(coords.data(), strides);
+
+        // get potential value
+        // The potential 4 MSB (including the sign) are on word 1
+        auto pot_msb = intN_to_int32<4>(get_field(word1, CONV_POTENTIAL_MSB));
+        // And the 20 LSB (positive number only) are on word 2
+        auto pot_lsb =
+            static_cast<int32_t>(get_field(word2, CONV_POTENTIAL_LSB));
+        // write value
+        output[index] = (pot_msb << CONV_POTENTIAL_LSB.nb_bits) | pot_lsb;
+      };
+    } break;
+    case dma::OutputFormat::ConvPotentials: {
+      read_data = [](int32_t* output, const std::vector<uint32_t>& strides,
+                     dma::w32 word1, dma::w32 word2) {
+        // get index from coordinate
+        std::array<Index, 3> coords;
+        coords[0] = static_cast<Index>(get_field(word1, CONV_X));
+        coords[1] = static_cast<Index>(get_field(word1, CONV_Y));
+        coords[2] = static_cast<Index>(get_field(word2, CONV_F));
+        const auto index = linear_index(coords.data(), strides);
+
+        // write potential value
+        output[index] = intN_to_int32<20>(get_field(word2, CONV_POTENTIAL_LSB));
+      };
+    } break;
+    default: {
+      panic("Unexpected output type");
+    }
+  }
+  // Allocate the output dense buffer
+  auto dense =
+      Dense::create(TensorType::int32, out_dims, Dense::Layout::RowMajor);
+  // Iterate over DMA events to fill the dense buffer
+  uint32_t n_events =
+      static_cast<uint32_t>(output_words.size() / dma::kSparseEventWordSize);
+  // Get pointer to dense, to avoid using set() method that calls data() and
+  // then check_type() method at every iteration
+  auto dense_ptr = dense->data<int32_t>();
+  for (uint32_t i = 0; i < n_events; i++) {
+    // Events are stored in two consecutive words
+    const auto& output_word1 = output_words[i * dma::kSparseEventWordSize];
+    const auto& output_word2 = output_words[i * dma::kSparseEventWordSize + 1];
+    // Read the output coords and value
+    read_data(dense_ptr, dense->strides(), output_word1, output_word2);
+  }
+  return dense;
+}
+
+DmaEventsPtr to_dma_events(const Tensor& inputs, bool input_is_fnp) {
+  dma::wbuffer event_data;
+  DmaEventsPtr result;
+  dma::wbuffer::size_type nb_words;
+  if (input_is_fnp) {
+    event_data = format_fnp_events(inputs);
+    // workaround when null event
+    if (event_data.empty()) {
+      add_dummy_event_fnp(inputs, &event_data);
+    }
+    nb_words = event_data.size();
+    result = DmaEventsPtr(
+        new DmaFnpEvents(inputs.dimensions(), std::move(event_data)));
+  } else {
+    event_data = format_cnp_events(inputs);
+    // workaround when null event
+    if (event_data.empty()) {
+      add_dummy_event_cnp(inputs, &event_data);
+    }
+    nb_words = event_data.size();
+    result = DmaEventsPtr(
+        new DmaCnpEvents(inputs.dimensions(), std::move(event_data)));
+  }
+  // check we are below the maximum number of events
+  auto max_dma_events = dma::max_dma_events();
+  if (nb_words > max_dma_events * dma::kSparseEventWordSize) {
+    panic("%d exceeds the maximum number of events: %d", nb_words,
+          max_dma_events);
+  }
+  return result;
+}
+
+TensorUniquePtr dma_events_read_outputs(HardwareDriver* driver,
+                                        const uint32_t addr_output_events,
+                                        const Shape output_dimensions,
+                                        dma::OutputFormat output_format) {
+  // The header is the first word of the output
+  uint32_t header = driver->read32(addr_output_events);
+  uint32_t output_word_size = get_field(header, OUTPUT_WORD_SIZE);
+  auto output_bytes_size = output_word_size * sizeof(dma::w32);
+
+  // adjust past header to data
+  uint32_t read_offset_addr = addr_output_events + dma::kOutputHeaderByteSize;
+
+  switch (output_format) {
+    case dma::OutputFormat::ConvActivations: {
+      // Read back data aligned on 32-bit words
+      dma::wbuffer output_events(output_word_size);
+      driver->read(read_offset_addr, output_events.data(), output_bytes_size);
+      // We can directly wrap the output events in a DmaCnpEvents sparse tensor
+      return TensorUniquePtr(
+          new DmaCnpEvents(output_dimensions, std::move(output_events)));
+    }
+    case dma::OutputFormat::FullyActivations: {
+      // Read back data aligned on 32-bit words
+      dma::wbuffer output_events(output_word_size);
+      driver->read(read_offset_addr, output_events.data(), output_bytes_size);
+      // We can directly wrap the output events in a DmaFnpEvents sparse tensor
+      return TensorUniquePtr(
+          new DmaFnpEvents(output_dimensions, std::move(output_events)));
+    }
+    case dma::OutputFormat::HrcActivations: {
+      // Read back data aligned on 32-bit words
+      dma::wbuffer output_events(output_word_size);
+      driver->read(read_offset_addr, output_events.data(), output_bytes_size);
+      // We can directly wrap the output events in a DmaCnpEvents sparse tensor
+      return TensorUniquePtr(
+          new DmaHrcEvents(output_dimensions, std::move(output_events)));
+    }
+    case dma::OutputFormat::DenseActivations: {
+      // determine actual byte size
+      auto out_size = shape_size(output_dimensions) * sizeof(uint8_t);
+      // Create an empty tensor
+      auto dense = Dense::create(TensorType::uint8, output_dimensions,
+                                 Dense::Layout::RowMajor);
+      // directly read activations into the tensor, ignoring word size
+      driver->read(read_offset_addr, dense->buffer()->data(), out_size);
+      return dense;
+    }
+    case dma::OutputFormat::DensePotentials: {
+      // Create an empty tensor
+      auto dense = Dense::create(TensorType::int32, output_dimensions,
+                                 Dense::Layout::RowMajor);
+      auto potentials = dense->buffer()->data();
+      driver->read(read_offset_addr, potentials, output_bytes_size);
+      return dense;
+    }
+    default: {
+      // Read back data aligned on 32-bit words
+      dma::wbuffer output_events(output_word_size);
+      driver->read(read_offset_addr, output_events.data(), output_bytes_size);
+      // Extract potentials and create a Dense
+      return format_output_potentials(output_events, output_dimensions,
+                                      output_format);
+    }
+  }
+}
+
+}  // namespace akida
```

## akida/engine/src/dma_events_ops.h

 * *Ordering differences only*

```diff
@@ -1,38 +1,38 @@
-#pragma once
-#include <cstdint>
-
-#include "akida/shape.h"
-#include "akida/tensor.h"
-#include "engine/dma.h"
-#include "infra/hardware_driver.h"
-
-#include "dma_desc_ops.h"
-#include "dma_events.h"
-
-namespace akida {
-
-namespace dma {
-
-enum class OutputFormat {
-  ConvActivations /**<Convolution Sparse 4-bit Activations*/,
-  HrcActivations /**<Convolution Sparse 4-bit Activations in HRC*/,
-  ConvPotentials /**<Convolution Sparse 20-bit Potentials*/,
-  ConvHighPotentials /**<Convolution Sparse 24-bit Potentials*/,
-  FullyActivations /**<FullyConnected Sparse 4-bit Activations*/,
-  FullyPotentials /**<FullyConnected Sparse 20-bit Potentials*/,
-  DenseActivations /**<Dense 4-bit Activations*/,
-  DensePotentials /**<Dense 32-bit Potentials*/
-};
-
-}  // namespace dma
-
-// convert tensor to dma events
-DmaEventsPtr to_dma_events(const Tensor& inputs, bool input_is_fnp);
-
-// Read events at the given memory address and reformat them to spikes
-TensorUniquePtr dma_events_read_outputs(HardwareDriver* driver,
-                                        const uint32_t addr_output_events,
-                                        const Shape output_dimensions,
-                                        dma::OutputFormat output_format);
-
-}  // namespace akida
+#pragma once
+#include <cstdint>
+
+#include "akida/shape.h"
+#include "akida/tensor.h"
+#include "engine/dma.h"
+#include "infra/hardware_driver.h"
+
+#include "dma_desc_ops.h"
+#include "dma_events.h"
+
+namespace akida {
+
+namespace dma {
+
+enum class OutputFormat {
+  ConvActivations /**<Convolution Sparse 4-bit Activations*/,
+  HrcActivations /**<Convolution Sparse 4-bit Activations in HRC*/,
+  ConvPotentials /**<Convolution Sparse 20-bit Potentials*/,
+  ConvHighPotentials /**<Convolution Sparse 24-bit Potentials*/,
+  FullyActivations /**<FullyConnected Sparse 4-bit Activations*/,
+  FullyPotentials /**<FullyConnected Sparse 20-bit Potentials*/,
+  DenseActivations /**<Dense 4-bit Activations*/,
+  DensePotentials /**<Dense 32-bit Potentials*/
+};
+
+}  // namespace dma
+
+// convert tensor to dma events
+DmaEventsPtr to_dma_events(const Tensor& inputs, bool input_is_fnp);
+
+// Read events at the given memory address and reformat them to spikes
+TensorUniquePtr dma_events_read_outputs(HardwareDriver* driver,
+                                        const uint32_t addr_output_events,
+                                        const Shape output_dimensions,
+                                        dma::OutputFormat output_format);
+
+}  // namespace akida
```

## akida/engine/src/dma_fnp_events.h

 * *Ordering differences only*

```diff
@@ -1,79 +1,79 @@
-#pragma once
-
-#include <memory>
-
-#include "akida/shape.h"
-#include "akida/sparse.h"
-
-#include "engine/int_conversion.h"
-
-#include "engine/dma.h"
-
-#include "dma_events.h"
-
-namespace akida {
-
-class DmaFnpEvents final : public DmaEvents {
- public:
-  DmaFnpEvents(const Shape& shape, const dma::wbuffer&& dma_words)
-      : DmaEvents(shape, std::move(dma_words)) {}
-
-  class Iterator final : public sparse::Iterator {
-   public:
-    // The events are stored contiguously using two dma words
-    static constexpr size_t kEventsStride = 2 * sizeof(dma::w32);
-    explicit Iterator(const DmaFnpEvents& events)
-        :  // Coords and values strides are deduced from the event stride
-          coords_stride_(kEventsStride / sizeof(*coords_)),
-          bytes_stride_(kEventsStride / sizeof(*bytes_)),
-          max_index_(shape_size(events.shape_) - 1) {
-      // The coords are in the first event word
-      coords_ = reinterpret_cast<const uint32_t*>(events.buffer_.data());
-      // Coordinates end is deduced from the number of events
-      coords_end_ = coords_ + events.size() * coords_stride_;
-      // Values are represented using a bytes pointer
-      bytes_ = reinterpret_cast<const char*>(events.buffer_.data());
-      // The values are in the second event word
-      bytes_ += sizeof(dma::w32);
-    }
-    // Iterator public API
-    std::vector<Index> coords() const override {
-      return std::vector<Index>{0, 0, filter_index()};
-    }
-    const char* bytes() const override { return bytes_; }
-    void next() override {
-      coords_ += coords_stride_;
-      bytes_ += bytes_stride_;
-    }
-    bool end() const override { return (coords_ == coords_end_); }
-    size_t unravel(const std::vector<uint32_t>& strides) const override {
-      // There is a single coordinate, so we only care about the last stride
-      return filter_index() * strides.back();
-    }
-
-   private:
-    const uint32_t* coords_;
-    const size_t coords_stride_;
-    const uint32_t* coords_end_;
-    const char* bytes_;
-    const size_t bytes_stride_;
-    const size_t max_index_;
-    uint32_t filter_index() const {
-      // Extract the F coordinate from the coords word
-      auto f = static_cast<Index>(get_field(*coords_, FC_F));
-      // The hardware may have generated spikes that are out of range
-      if (f > max_index_) {
-        panic("FNP: filter coordinate %d exceeds maximum value %d.", f,
-              max_index_);
-      }
-      return f;
-    }
-  };
-
-  sparse::IteratorPtr begin() const override {
-    return std::make_shared<Iterator>(*this);
-  }
-};
-
-using DmaFnpEventsPtr = std::shared_ptr<DmaFnpEvents>;
-}  // namespace akida
+#pragma once
+
+#include <memory>
+
+#include "akida/shape.h"
+#include "akida/sparse.h"
+
+#include "engine/int_conversion.h"
+
+#include "engine/dma.h"
+
+#include "dma_events.h"
+
+namespace akida {
+
+class DmaFnpEvents final : public DmaEvents {
+ public:
+  DmaFnpEvents(const Shape& shape, const dma::wbuffer&& dma_words)
+      : DmaEvents(shape, std::move(dma_words)) {}
+
+  class Iterator final : public sparse::Iterator {
+   public:
+    // The events are stored contiguously using two dma words
+    static constexpr size_t kEventsStride = 2 * sizeof(dma::w32);
+    explicit Iterator(const DmaFnpEvents& events)
+        :  // Coords and values strides are deduced from the event stride
+          coords_stride_(kEventsStride / sizeof(*coords_)),
+          bytes_stride_(kEventsStride / sizeof(*bytes_)),
+          max_index_(shape_size(events.shape_) - 1) {
+      // The coords are in the first event word
+      coords_ = reinterpret_cast<const uint32_t*>(events.buffer_.data());
+      // Coordinates end is deduced from the number of events
+      coords_end_ = coords_ + events.size() * coords_stride_;
+      // Values are represented using a bytes pointer
+      bytes_ = reinterpret_cast<const char*>(events.buffer_.data());
+      // The values are in the second event word
+      bytes_ += sizeof(dma::w32);
+    }
+    // Iterator public API
+    std::vector<Index> coords() const override {
+      return std::vector<Index>{0, 0, filter_index()};
+    }
+    const char* bytes() const override { return bytes_; }
+    void next() override {
+      coords_ += coords_stride_;
+      bytes_ += bytes_stride_;
+    }
+    bool end() const override { return (coords_ == coords_end_); }
+    size_t unravel(const std::vector<uint32_t>& strides) const override {
+      // There is a single coordinate, so we only care about the last stride
+      return filter_index() * strides.back();
+    }
+
+   private:
+    const uint32_t* coords_;
+    const size_t coords_stride_;
+    const uint32_t* coords_end_;
+    const char* bytes_;
+    const size_t bytes_stride_;
+    const size_t max_index_;
+    uint32_t filter_index() const {
+      // Extract the F coordinate from the coords word
+      auto f = static_cast<Index>(get_field(*coords_, FC_F));
+      // The hardware may have generated spikes that are out of range
+      if (f > max_index_) {
+        panic("FNP: filter coordinate %d exceeds maximum value %d.", f,
+              max_index_);
+      }
+      return f;
+    }
+  };
+
+  sparse::IteratorPtr begin() const override {
+    return std::make_shared<Iterator>(*this);
+  }
+};
+
+using DmaFnpEventsPtr = std::shared_ptr<DmaFnpEvents>;
+}  // namespace akida
```

## akida/engine/src/dma_hrc_events.h

 * *Ordering differences only*

```diff
@@ -1,94 +1,94 @@
-#pragma once
-
-#include <memory>
-
-#include "akida/shape.h"
-#include "akida/sparse.h"
-
-#include "infra/int_ops.h"
-
-#include "engine/dma.h"
-
-#include "dma_events.h"
-
-namespace akida {
-
-class DmaHrcEvents final : public DmaEvents {
- public:
-  DmaHrcEvents(const Shape& shape, const dma::wbuffer&& dma_words)
-      : DmaEvents(shape, std::move(dma_words)) {}
-
-  class Iterator final : public sparse::Iterator {
-   public:
-    // The events are stored contiguously using two dma words
-    static constexpr size_t kEventsStride = 2 * sizeof(dma::w32);
-    explicit Iterator(const DmaHrcEvents& events)
-        :  // Coords and values strides are deduced form the event stride
-          coords_stride_(kEventsStride / sizeof(*coords_)),
-          bytes_stride_(kEventsStride / sizeof(*bytes_)),
-          max_index_(shape_size(events.shape_) - 1),
-          shape_(events.shape_) {
-      // The coords are aligned on 16-bit starting from the first event word
-      coords_ = reinterpret_cast<const uint16_t*>(events.buffer_.data());
-      // Coordinates end is deduced from the number of events
-      coords_end_ = coords_ + events.size() * coords_stride_;
-      // Values are represented using a bytes pointer
-      bytes_ = reinterpret_cast<const char*>(events.buffer_.data());
-      // The values start after the three coordinates
-      bytes_ += 3 * sizeof(uint16_t);
-    }
-    // Iterator public API
-    std::vector<Index> coords() const override {
-      // shape interpretation is inverted in the HRC internal operation
-      if (coords_[0] >= shape_[1] || coords_[1] >= shape_[0] ||
-          coords_[2] >= shape_[2]) {
-        panic(
-            "CNP: coordinates (%d, %d, %d) are out-of-range: Shape is (%d, %d, "
-            "%d)",
-            coords_[1], coords_[0], coords_[2], shape_[0], shape_[1],
-            shape_[2]);
-      }
-      return std::vector<Index>{coords_[1], coords_[0], coords_[2]};
-    }
-    const char* bytes() const override { return bytes_; }
-    void next() override {
-      coords_ += coords_stride_;
-      bytes_ += bytes_stride_;
-    }
-    bool end() const override { return (coords_ == coords_end_); }
-    // Iterator internal API
-    size_t unravel(const std::vector<uint32_t>& strides) const override {
-      size_t index = 0;
-
-      // Invert stride if hrc events
-      index += coords_[0] * strides[1];
-      index += coords_[1] * strides[0];
-      index += coords_[2] * strides[2];
-
-      if (index > max_index_) {
-        panic(
-            "HRC: coordinates (%d, %d, %d) are out-of-range: Shape is (%d, %d, "
-            "%d)",
-            coords_[1], coords_[0], coords_[2], shape_[0], shape_[1],
-            shape_[2]);
-      }
-      return index;
-    }
-
-   private:
-    const uint16_t* coords_;
-    const size_t coords_stride_;
-    const uint16_t* coords_end_;
-    const char* bytes_;
-    const size_t bytes_stride_;
-    const size_t max_index_;
-    const Shape shape_;
-  };
-
-  sparse::IteratorPtr begin() const override {
-    return std::make_shared<Iterator>(*this);
-  }
-};
-
-using DmaHrcEventsPtr = std::shared_ptr<DmaHrcEvents>;
-}  // namespace akida
+#pragma once
+
+#include <memory>
+
+#include "akida/shape.h"
+#include "akida/sparse.h"
+
+#include "infra/int_ops.h"
+
+#include "engine/dma.h"
+
+#include "dma_events.h"
+
+namespace akida {
+
+class DmaHrcEvents final : public DmaEvents {
+ public:
+  DmaHrcEvents(const Shape& shape, const dma::wbuffer&& dma_words)
+      : DmaEvents(shape, std::move(dma_words)) {}
+
+  class Iterator final : public sparse::Iterator {
+   public:
+    // The events are stored contiguously using two dma words
+    static constexpr size_t kEventsStride = 2 * sizeof(dma::w32);
+    explicit Iterator(const DmaHrcEvents& events)
+        :  // Coords and values strides are deduced form the event stride
+          coords_stride_(kEventsStride / sizeof(*coords_)),
+          bytes_stride_(kEventsStride / sizeof(*bytes_)),
+          max_index_(shape_size(events.shape_) - 1),
+          shape_(events.shape_) {
+      // The coords are aligned on 16-bit starting from the first event word
+      coords_ = reinterpret_cast<const uint16_t*>(events.buffer_.data());
+      // Coordinates end is deduced from the number of events
+      coords_end_ = coords_ + events.size() * coords_stride_;
+      // Values are represented using a bytes pointer
+      bytes_ = reinterpret_cast<const char*>(events.buffer_.data());
+      // The values start after the three coordinates
+      bytes_ += 3 * sizeof(uint16_t);
+    }
+    // Iterator public API
+    std::vector<Index> coords() const override {
+      // shape interpretation is inverted in the HRC internal operation
+      if (coords_[0] >= shape_[1] || coords_[1] >= shape_[0] ||
+          coords_[2] >= shape_[2]) {
+        panic(
+            "CNP: coordinates (%d, %d, %d) are out-of-range: Shape is (%d, %d, "
+            "%d)",
+            coords_[1], coords_[0], coords_[2], shape_[0], shape_[1],
+            shape_[2]);
+      }
+      return std::vector<Index>{coords_[1], coords_[0], coords_[2]};
+    }
+    const char* bytes() const override { return bytes_; }
+    void next() override {
+      coords_ += coords_stride_;
+      bytes_ += bytes_stride_;
+    }
+    bool end() const override { return (coords_ == coords_end_); }
+    // Iterator internal API
+    size_t unravel(const std::vector<uint32_t>& strides) const override {
+      size_t index = 0;
+
+      // Invert stride if hrc events
+      index += coords_[0] * strides[1];
+      index += coords_[1] * strides[0];
+      index += coords_[2] * strides[2];
+
+      if (index > max_index_) {
+        panic(
+            "HRC: coordinates (%d, %d, %d) are out-of-range: Shape is (%d, %d, "
+            "%d)",
+            coords_[1], coords_[0], coords_[2], shape_[0], shape_[1],
+            shape_[2]);
+      }
+      return index;
+    }
+
+   private:
+    const uint16_t* coords_;
+    const size_t coords_stride_;
+    const uint16_t* coords_end_;
+    const char* bytes_;
+    const size_t bytes_stride_;
+    const size_t max_index_;
+    const Shape shape_;
+  };
+
+  sparse::IteratorPtr begin() const override {
+    return std::make_shared<Iterator>(*this);
+  }
+};
+
+using DmaHrcEventsPtr = std::shared_ptr<DmaHrcEvents>;
+}  // namespace akida
```

## akida/engine/src/dma_image_ops.cpp

 * *Ordering differences only*

```diff
@@ -1,56 +1,56 @@
-#include "dma_image_ops.h"
-
-#include <algorithm>
-#include <cassert>
-#include <memory>
-#include <vector>
-
-#include "akida/dense.h"
-#include "engine/dma.h"
-#include "infra/hardware_driver.h"
-#include "infra/int_ops.h"
-
-#include "dma_desc_ops.h"
-
-namespace akida {
-
-dma::Descriptor dma_dense_descriptor(uint32_t addr_in, uint32_t addr_out,
-                                     uint32_t job_id, uint32_t learn_class,
-                                     const uint32_t* input_shape,
-                                     uint32_t window_w, uint32_t window_h) {
-  // According to the HRC convention input shape is (h, w, c)
-  const auto im_w = input_shape[1];
-  const auto im_h = input_shape[0];
-  const auto im_c = input_shape[2];
-
-  // Generate input spikes for each image, tagged by image order
-  uint32_t window_row_byte_size = window_w * im_c;
-  uint32_t next_row_offset = im_w * im_c;
-  uint32_t row_byte_size = im_w * im_c;
-  uint32_t col_height = im_h;
-
-  // If the HRC is used (i.e Not in bypass mode) the window sizes should be
-  // different from 0. The HRC descriptor sent to the DMA needs to contain
-  // the window sizes instead of the actual input sizes (they are only different
-  // when using VALID convolutions) because the HRC needs to see only the pixels
-  // it will use.
-  if (window_h != 0 && window_w != 0) {
-    row_byte_size = window_row_byte_size;
-    col_height = window_h;
-  } else {
-    // if the HRC is in bypass mode this value is not needed by the descriptor
-    next_row_offset = 0;
-  }
-  // TODO: window overlap parameters for now not supported
-  const uint32_t window_overlap_bytesize = 0;
-  const uint32_t x_offset = 0;
-  const uint32_t y_offset = 0;
-
-  // generate descriptor
-  return dma::format_hrc_desc(job_id, addr_in, addr_out, row_byte_size,
-                              col_height, next_row_offset, window_row_byte_size,
-                              window_h, window_overlap_bytesize, y_offset,
-                              x_offset, learn_class);
-}
-
-}  // namespace akida
+#include "dma_image_ops.h"
+
+#include <algorithm>
+#include <cassert>
+#include <memory>
+#include <vector>
+
+#include "akida/dense.h"
+#include "engine/dma.h"
+#include "infra/hardware_driver.h"
+#include "infra/int_ops.h"
+
+#include "dma_desc_ops.h"
+
+namespace akida {
+
+dma::Descriptor dma_dense_descriptor(uint32_t addr_in, uint32_t addr_out,
+                                     uint32_t job_id, uint32_t learn_class,
+                                     const uint32_t* input_shape,
+                                     uint32_t window_w, uint32_t window_h) {
+  // According to the HRC convention input shape is (h, w, c)
+  const auto im_w = input_shape[1];
+  const auto im_h = input_shape[0];
+  const auto im_c = input_shape[2];
+
+  // Generate input spikes for each image, tagged by image order
+  uint32_t window_row_byte_size = window_w * im_c;
+  uint32_t next_row_offset = im_w * im_c;
+  uint32_t row_byte_size = im_w * im_c;
+  uint32_t col_height = im_h;
+
+  // If the HRC is used (i.e Not in bypass mode) the window sizes should be
+  // different from 0. The HRC descriptor sent to the DMA needs to contain
+  // the window sizes instead of the actual input sizes (they are only different
+  // when using VALID convolutions) because the HRC needs to see only the pixels
+  // it will use.
+  if (window_h != 0 && window_w != 0) {
+    row_byte_size = window_row_byte_size;
+    col_height = window_h;
+  } else {
+    // if the HRC is in bypass mode this value is not needed by the descriptor
+    next_row_offset = 0;
+  }
+  // TODO: window overlap parameters for now not supported
+  const uint32_t window_overlap_bytesize = 0;
+  const uint32_t x_offset = 0;
+  const uint32_t y_offset = 0;
+
+  // generate descriptor
+  return dma::format_hrc_desc(job_id, addr_in, addr_out, row_byte_size,
+                              col_height, next_row_offset, window_row_byte_size,
+                              window_h, window_overlap_bytesize, y_offset,
+                              x_offset, learn_class);
+}
+
+}  // namespace akida
```

## akida/engine/src/dma_image_ops.h

 * *Ordering differences only*

```diff
@@ -1,16 +1,16 @@
-#pragma once
-
-#include <cstdint>
-
-#include "akida/dense.h"
-#include "infra/hardware_driver.h"
-
-#include "dma_desc_ops.h"
-
-namespace akida {
-
-dma::Descriptor dma_dense_descriptor(uint32_t addr_in, uint32_t addr_out,
-                                     uint32_t job_id, uint32_t learn_class,
-                                     const uint32_t* input_shape,
-                                     uint32_t window_w, uint32_t window_h);
-}  // namespace akida
+#pragma once
+
+#include <cstdint>
+
+#include "akida/dense.h"
+#include "infra/hardware_driver.h"
+
+#include "dma_desc_ops.h"
+
+namespace akida {
+
+dma::Descriptor dma_dense_descriptor(uint32_t addr_in, uint32_t addr_out,
+                                     uint32_t job_id, uint32_t learn_class,
+                                     const uint32_t* input_shape,
+                                     uint32_t window_w, uint32_t window_h);
+}  // namespace akida
```

## akida/engine/src/external_mem_mgr.cpp

 * *Ordering differences only*

```diff
@@ -1,68 +1,68 @@
-#include "external_mem_mgr.h"
-
-#include <cstdint>
-
-#include "infra/system.h"
-
-#include "memory_mgr.h"
-#include "memory_utils.h"
-
-namespace akida {
-
-dma::addr ExternalMemoryMgr::track_and_put_on_device_if_required(
-    AllocId id, size_t byte_size) {
-  // prevent allocating if ledger already contains an entry
-  if (alloc_ledger_.find(id) != alloc_ledger_.end()) {
-    panic("Tracked allocation ID %p already taken", id);
-  }
-
-  // get address
-  dma::addr addr;
-
-  // alloc memory if we need to, and copy data to it
-  if (accessible_from_akida(id, *driver_)) {
-    // we can safely cast because dma::addr and AllocId types have the same
-    // size
-    addr = to_dma_addr(id);
-  } else {
-    addr = mem_mgr_->alloc(byte_size);
-    driver_->write(addr, id, byte_size);
-  }
-
-  // record in ledger
-  alloc_ledger_[id] = addr;
-  return addr;
-}
-
-void ExternalMemoryMgr::release(AllocId id) {
-  auto addr = tracked(id);
-  if (!accessible_from_akida(id, *driver_)) {
-    mem_mgr_->free(addr);
-  }
-  alloc_ledger_.erase(id);
-}
-
-uint32_t ExternalMemoryMgr::tracked(AllocId id) const {
-  auto entry = alloc_ledger_.find(id);
-  // check if item is not in ledger
-  if (entry == alloc_ledger_.end()) {
-    panic("Tracked allocation ID %p not found", id);
-  }
-  auto& addr = entry->second;
-  return addr;
-}
-
-void ExternalMemoryMgr::reset() {
-  // free all elements, in reverse order
-  for (auto iter = alloc_ledger_.rbegin(); iter != alloc_ledger_.rend();
-       ++iter) {
-    // free only memory that has been allocated by us
-    if (!accessible_from_akida(iter->first, *driver_)) {
-      mem_mgr_->free(iter->second);
-    }
-  }
-  // clear up map
-  alloc_ledger_.clear();
-}
-
-}  // namespace akida
+#include "external_mem_mgr.h"
+
+#include <cstdint>
+
+#include "infra/system.h"
+
+#include "memory_mgr.h"
+#include "memory_utils.h"
+
+namespace akida {
+
+dma::addr ExternalMemoryMgr::track_and_put_on_device_if_required(
+    AllocId id, size_t byte_size) {
+  // prevent allocating if ledger already contains an entry
+  if (alloc_ledger_.find(id) != alloc_ledger_.end()) {
+    panic("Tracked allocation ID %p already taken", id);
+  }
+
+  // get address
+  dma::addr addr;
+
+  // alloc memory if we need to, and copy data to it
+  if (accessible_from_akida(id, *driver_)) {
+    // we can safely cast because dma::addr and AllocId types have the same
+    // size
+    addr = to_dma_addr(id);
+  } else {
+    addr = mem_mgr_->alloc(byte_size);
+    driver_->write(addr, id, byte_size);
+  }
+
+  // record in ledger
+  alloc_ledger_[id] = addr;
+  return addr;
+}
+
+void ExternalMemoryMgr::release(AllocId id) {
+  auto addr = tracked(id);
+  if (!accessible_from_akida(id, *driver_)) {
+    mem_mgr_->free(addr);
+  }
+  alloc_ledger_.erase(id);
+}
+
+uint32_t ExternalMemoryMgr::tracked(AllocId id) const {
+  auto entry = alloc_ledger_.find(id);
+  // check if item is not in ledger
+  if (entry == alloc_ledger_.end()) {
+    panic("Tracked allocation ID %p not found", id);
+  }
+  auto& addr = entry->second;
+  return addr;
+}
+
+void ExternalMemoryMgr::reset() {
+  // free all elements, in reverse order
+  for (auto iter = alloc_ledger_.rbegin(); iter != alloc_ledger_.rend();
+       ++iter) {
+    // free only memory that has been allocated by us
+    if (!accessible_from_akida(iter->first, *driver_)) {
+      mem_mgr_->free(iter->second);
+    }
+  }
+  // clear up map
+  alloc_ledger_.clear();
+}
+
+}  // namespace akida
```

## akida/engine/src/external_mem_mgr.h

 * *Ordering differences only*

```diff
@@ -1,43 +1,43 @@
-#pragma once
-#include <cstddef>
-#include <map>
-
-#include "engine/dma.h"
-#include "infra/hardware_driver.h"
-
-#include "memory_mgr.h"
-
-namespace akida {
-
-class ExternalMemoryMgr {
- public:
-  explicit ExternalMemoryMgr(MemoryMgr* mgr, HardwareDriver* driver)
-      : mem_mgr_(mgr), driver_(driver) {}
-
-  using AllocId = const void*;
-  // Track a local address that will also be on the device.
-  // It copies data on device if they are not accessible from akida (if they are
-  // not in HardwareDriver akida_visible_memory range)
-  dma::addr track_and_put_on_device_if_required(AllocId id, size_t byte_size);
-
-  // release (untrack) data from device (if data was copied on it, memory is
-  // freed)
-  void release(AllocId id);
-
-  // get on device address from id
-  dma::addr tracked(AllocId id) const;
-
-  // Untrack all memory, freeing it if they were copied on device, to restore
-  // initial state
-  void reset();
-
- private:
-  // memory manager
-  MemoryMgr* mem_mgr_;
-  // hardware driver
-  HardwareDriver* driver_;
-  // allocation ledger, a map of id:addresss
-  std::map<AllocId, uint32_t> alloc_ledger_;
-};
-
-}  // namespace akida
+#pragma once
+#include <cstddef>
+#include <map>
+
+#include "engine/dma.h"
+#include "infra/hardware_driver.h"
+
+#include "memory_mgr.h"
+
+namespace akida {
+
+class ExternalMemoryMgr {
+ public:
+  explicit ExternalMemoryMgr(MemoryMgr* mgr, HardwareDriver* driver)
+      : mem_mgr_(mgr), driver_(driver) {}
+
+  using AllocId = const void*;
+  // Track a local address that will also be on the device.
+  // It copies data on device if they are not accessible from akida (if they are
+  // not in HardwareDriver akida_visible_memory range)
+  dma::addr track_and_put_on_device_if_required(AllocId id, size_t byte_size);
+
+  // release (untrack) data from device (if data was copied on it, memory is
+  // freed)
+  void release(AllocId id);
+
+  // get on device address from id
+  dma::addr tracked(AllocId id) const;
+
+  // Untrack all memory, freeing it if they were copied on device, to restore
+  // initial state
+  void reset();
+
+ private:
+  // memory manager
+  MemoryMgr* mem_mgr_;
+  // hardware driver
+  HardwareDriver* driver_;
+  // allocation ledger, a map of id:addresss
+  std::map<AllocId, uint32_t> alloc_ledger_;
+};
+
+}  // namespace akida
```

## akida/engine/src/fnp2_mem_conf_reg.h

 * *Ordering differences only*

```diff
@@ -1,20 +1,20 @@
-#pragma once
-
-#include <cassert>
-#include <cstdint>
-
-namespace akida {
-
-// FNP2 DMA address register: used to configure FNP2 address in DDR. This
-// address is an offset after top level register address.
-constexpr uint32_t FNP2_DDR_CONF_REG_BASE = 0x10;
-
-static inline uint32_t fnp2_memory_conf(const uint32_t top_level_reg_base,
-                                        uint8_t np_id) {
-  // Only 4 FNP2 are supported
-  assert(np_id <= 3);
-  return static_cast<uint32_t>(top_level_reg_base + FNP2_DDR_CONF_REG_BASE +
-                               (np_id * sizeof(uint32_t)));
-}
-
+#pragma once
+
+#include <cassert>
+#include <cstdint>
+
+namespace akida {
+
+// FNP2 DMA address register: used to configure FNP2 address in DDR. This
+// address is an offset after top level register address.
+constexpr uint32_t FNP2_DDR_CONF_REG_BASE = 0x10;
+
+static inline uint32_t fnp2_memory_conf(const uint32_t top_level_reg_base,
+                                        uint8_t np_id) {
+  // Only 4 FNP2 are supported
+  assert(np_id <= 3);
+  return static_cast<uint32_t>(top_level_reg_base + FNP2_DDR_CONF_REG_BASE +
+                               (np_id * sizeof(uint32_t)));
+}
+
 }  // namespace akida
```

## akida/engine/src/hardware_device.cpp

 * *Ordering differences only*

```diff
@@ -1,13 +1,13 @@
-#include "akida/hardware_device.h"
-
-#include <memory>
-
-#include "hardware_device_impl.h"
-
-namespace akida {
-
-HardwareDevicePtr HardwareDevice::create(HardwareDriver* driver) {
-  return std::make_shared<HardwareDeviceImpl>(driver);
-}
-
+#include "akida/hardware_device.h"
+
+#include <memory>
+
+#include "hardware_device_impl.h"
+
+namespace akida {
+
+HardwareDevicePtr HardwareDevice::create(HardwareDriver* driver) {
+  return std::make_shared<HardwareDeviceImpl>(driver);
+}
+
 }  // namespace akida
```

## akida/engine/src/hardware_device_impl.cpp

 * *Ordering differences only*

```diff
@@ -1,836 +1,836 @@
-#include "hardware_device_impl.h"
-
-#include <algorithm>
-#include <cassert>
-#include <cstdint>
-#include <cstring>
-#include <memory>
-#include <tuple>
-
-#include "akida/dense.h"
-#include "akida/hw_version.h"
-#include "akida/input_conversion.h"
-#include "akida/np.h"
-#include "akida/program_memory_info.h"
-#include "akida/registers_top_level.h"
-#include "akida/shape.h"
-#include "engine/dma.h"
-#include "engine/dma_config_ops.h"
-
-#include "infra/int_ops.h"
-#include "infra/registers_common.h"
-#include "infra/system.h"
-
-#include "dma_desc_format.h"
-#include "dma_desc_ops.h"
-#include "dma_engine.h"
-#include "dma_engine_ops.h"
-#include "dma_events_ops.h"
-#include "dma_image_ops.h"
-#include "external_mem_mgr.h"
-#include "memory_utils.h"
-#include "program_play.h"
-#include "registers_dma_engine.h"
-#include "reset_nps.h"
-
-namespace akida {
-
-static void toggle_multi_pass(HardwareDeviceImpl* device,
-                              bool enable_multi_pass);
-
-static void alloc_dma_descriptors(dma::Engine* dma, MemoryMgr* mem_mgr,
-                                  uint32_t num_descriptors) {
-  assert(num_descriptors <= dma::kMaxNbDescriptorsMultipass);
-  assert(dma->descriptor_base_addr == 0);
-  // allocate buffer to contain descriptors
-  dma->descriptor_base_addr =
-      mem_mgr->alloc(num_descriptors * dma->descriptor_bytes_size);
-}
-
-static void free_allocated_buffer(MemoryMgr* mem_mgr, dma::addr* ptr) {
-  // check if pointer was allocated
-  if (*ptr) {
-    mem_mgr->free(*ptr);
-    // we have to set to 0 to mark we have correctly freed
-    *ptr = 0;
-  }
-}
-
-HardwareDeviceImpl::HardwareDeviceImpl(HardwareDriver* driver)
-    : driver_(driver),
-      version_(read_hw_version(*driver_)),
-      dma_config_{dma::Engine(dma_config_reg_base(driver_->top_level_reg()),
-                              dma::config::DESC_BYTE_SIZE)},
-      dma_event_{dma::Engine(dma_event_reg_base(driver_->top_level_reg()),
-                             dma::event::DESC_BYTE_SIZE)},
-      dma_hrc_{dma::Engine(dma_hrc_reg_base(driver_->top_level_reg()),
-                           dma::hrc::DESC_BYTE_SIZE)},
-      mem_mgr_(driver->scratch_memory(), driver->scratch_size()),
-      current_program_(nullptr, 0),
-      current_program_learn_en_(false),
-      external_mem_(&mem_mgr_, driver) {
-  if (version_ == akida::NSoC_v1) {
-    panic(
-        "NSoC_v1 is not supported on this version. Please install akida 2.0.5 "
-        "instead.");
-  }
-  init();
-}
-
-HardwareDeviceImpl::~HardwareDeviceImpl() {
-  free_allocated_buffer(&mem_mgr_, &dma_config_.engine.descriptor_base_addr);
-  free_allocated_buffer(&mem_mgr_, &dma_event_.engine.descriptor_base_addr);
-  free_allocated_buffer(&mem_mgr_, &dma_hrc_.engine.descriptor_base_addr);
-}
-
-HwVersion HardwareDeviceImpl::version() const { return version_; }
-
-void HardwareDeviceImpl::dma_config_write(const dma::w32* buffer,
-                                          size_t buf_size) {
-  // put buffer on device, and get its address
-  auto input_addr = external_mem_.track_and_put_on_device_if_required(
-      buffer, buf_size * sizeof(dma::w32));
-  constexpr uint32_t output_addr = 0;  // not used for write
-  // format descriptor
-  auto descriptor =
-      dma::format_config_desc(dma::kDescConfigDirectionWrite, input_addr,
-                              output_addr, static_cast<uint32_t>(buf_size));
-  assert(descriptor.size() == dma::config::DESC_LEN);
-
-  // tell DMA engine to process descriptor
-  dma::process(driver_, dma_config_, descriptor);
-  // now that buffer has been processed, it can be freed from device
-  external_mem_.release(buffer);
-}
-
-void HardwareDeviceImpl::dma_config_read(dma::w32* buffer,
-                                         const struct np::Ident& np,
-                                         dma::Target target,
-                                         uint16_t addr_target_word,
-                                         uint32_t nb_words) {
-  assert(dma_config_.engine.descriptor_base_addr != 0);
-  if (dma::config_block_size_needs_xl(static_cast<uint32_t>(nb_words))) {
-    panic("Unsupported buffer size in config read");
-  }
-
-  // format header
-  auto header =
-      dma::format_config_header(np, target, nb_words, addr_target_word);
-  uint32_t header_size = static_cast<uint32_t>(header.size());
-
-  // Allocate input and output area
-  auto input_addr = mem_mgr_.alloc(header_size * sizeof(dma::w32));
-  // Allocation should include header size
-  auto output_addr = mem_mgr_.alloc(nb_words * sizeof(dma::w32) +
-                                    dma::kConfigReadPacketOffset);
-  // format descriptor
-  auto descriptor = dma::format_config_desc(
-      dma::kDescConfigDirectionRead, input_addr, output_addr, header_size);
-  assert(descriptor.size() == dma::config::DESC_LEN);
-
-  // write header in DDR
-  driver_->write(input_addr, header.data(), header.size() * sizeof(dma::w32));
-
-  // tell DMA engine to process descriptor
-  dma::process(driver_, dma_config_, descriptor);
-
-  // fetch read header in DDR
-  dma::wbuffer read_hdr(dma::kConfigReadPacketHdrSz);
-  driver_->read(output_addr, read_hdr.data(),
-                dma::kConfigReadPacketHdrSz * sizeof(dma::w32));
-
-  // set packet size (nb of 32b words) and address/offset data
-  uint32_t packetsize = dma::parse_config_read_size(read_hdr);
-  uint32_t read_offset_addr = output_addr + dma::kConfigReadPacketOffset;
-
-  if (nb_words == 0 || packetsize != nb_words) {
-    panic("error on dma config read: invalid packet size (%d), expected %d.",
-          packetsize, nb_words);
-  }
-
-  driver_->read(read_offset_addr, buffer, nb_words * sizeof(dma::w32));
-  // now that input and outputs have been processed, it can be freed
-  mem_mgr_.free(output_addr);
-  mem_mgr_.free(input_addr);
-}
-
-void HardwareDeviceImpl::read_np_registers(uint32_t* output,
-                                           const struct np::Ident& np,
-                                           uint32_t nb_registers) {
-  auto has_alloc = false;
-  if (dma_config_.engine.descriptor_base_addr == 0) {
-    alloc_dma_descriptors(&dma_config_.engine, &mem_mgr_,
-                          dma::kMinNbDescriptors);
-    dma::init_default_dma(driver_, dma_config_.engine, dma::kMinNbDescriptors);
-    has_alloc = true;
-  }
-  dma_config_read(output, np, dma::Target::NpRegisters, 0, nb_registers);
-  if (has_alloc) {
-    free_allocated_buffer(&mem_mgr_, &dma_config_.engine.descriptor_base_addr);
-  }
-}
-
-std::vector<TensorUniquePtr> HardwareDeviceImpl::fit(
-    const std::vector<TensorConstPtr>& inputs,
-    const std::vector<int32_t>& input_labels) {
-  // Check the device had been programmed
-  if (!current_program_.first) {
-    panic("Cannot fit without a program");
-  }
-  if (!current_program_learn_en_)
-    panic("Learn must be enabled to call the fit method.");
-
-  return forward_loop(inputs, &input_labels);
-}
-
-std::vector<TensorUniquePtr> HardwareDeviceImpl::forward(
-    const std::vector<TensorConstPtr>& inputs) {
-  // Check the device had been programmed
-  if (!current_program_.first) {
-    panic("Cannot forward without a program");
-  }
-  if (current_program_learn_en_)
-    panic("Learn must be disabled to call the forward method.");
-
-  return forward_loop(inputs, nullptr);
-}
-
-const dma::Inputs& HardwareDeviceImpl::select_dma_engine(bool dense_inputs) {
-  // Only enable the input DMA used by the current network:
-  // HRC DMA if 1st layer is InputConvolutional, Event DMA otherwise
-  dma::toggle_engine(driver_, dma_hrc_.engine.reg_base_addr, dense_inputs);
-  dma::toggle_engine(driver_, dma_event_.engine.reg_base_addr, !dense_inputs);
-
-  return dense_inputs ? dma_hrc_ : dma_event_;
-}
-
-void HardwareDeviceImpl::pipeline(bool enabled) {
-  dma::toggle_pipeline(driver_, dma_event_, enabled);
-  dma::toggle_pipeline(driver_, dma_hrc_, enabled);
-}
-
-void HardwareDeviceImpl::toggle_clock_counter(bool enable) {
-  dma::toggle_buffer_timer(driver_, dma_event_.engine, enable);
-  dma::toggle_buffer_timer(driver_, dma_hrc_.engine, enable);
-}
-
-uint32_t HardwareDeviceImpl::read_clock_counter() {
-  // read clock from HRC DMA or read from events DMA
-  auto hrc_count_number = dma::read_buffer_timer(driver_, dma_hrc_.engine);
-  auto event_count_number = dma::read_buffer_timer(driver_, dma_event_.engine);
-  return std::max(hrc_count_number, event_count_number);
-}
-
-uint32_t HardwareDeviceImpl::read_config_clock_counter() {
-  return dma::read_buffer_timer(driver_, dma_config_.engine);
-}
-
-bool HardwareDeviceImpl::clock_counter_enabled() {
-  return dma::is_buffer_timer_enabled(*driver_, dma_event_);
-}
-
-static void check_input_dims(const Index* program_in_dims,
-                             const Shape& inputs_shape) {
-  bool valid_dims = true;
-  switch (inputs_shape.size()) {
-    case 1:  // fully connected, 1 dimension
-      if (inputs_shape[0] !=
-          program_in_dims[0] * program_in_dims[1] * program_in_dims[2]) {
-        valid_dims = false;
-      }
-      break;
-    case 3:  // other cases (check only that data size is compatible)
-      if (inputs_shape[0] * inputs_shape[1] * inputs_shape[2] !=
-          program_in_dims[0] * program_in_dims[1] * program_in_dims[2]) {
-        valid_dims = false;
-      }
-      break;
-    default:
-      valid_dims = false;
-      break;
-  }
-  if (!valid_dims) {
-    panic("Invalid input dimensions for this program");
-  }
-}
-
-// reset whole akida core, including DMAs
-static void core_reset(HardwareDriver* driver) {
-  const auto top_level_reg_offset = driver->top_level_reg();
-  auto reg_gen_ctrl =
-      driver->read32(top_level_reg_offset + REG_GENERAL_CONTROL);
-  // Reset NP & CORE
-  set_field(&reg_gen_ctrl, AK_CORE_RST, 1);
-  set_field(&reg_gen_ctrl, SCC_CORE_RESET, 1);
-  driver->write32(top_level_reg_offset + REG_GENERAL_CONTROL, reg_gen_ctrl);
-  // 20 cycles should be waited. Waiting 1ms is more than enough.
-  msleep(1);
-  // Fields need to be reset to 0
-  set_field(&reg_gen_ctrl, AK_CORE_RST, 0);
-  set_field(&reg_gen_ctrl, SCC_CORE_RESET, 0);
-  driver->write32(top_level_reg_offset + REG_GENERAL_CONTROL, reg_gen_ctrl);
-  // 40 cycles should be waited. Waiting 1ms is more than enough.
-  msleep(1);
-}
-
-void HardwareDeviceImpl::init() {
-  // this core reset is only available on production chip
-  core_reset(driver_);
-
-  // reset HW mesh
-  reset_nps_logic_and_cfg(driver_);
-}
-
-static inline const int32_t* get_label(const std::vector<int32_t>& labels,
-                                       size_t index) {
-  return labels.size() == 1 ? &labels[0] : &labels[index];
-}
-
-std::vector<TensorUniquePtr> HardwareDeviceImpl::forward_loop(
-    const std::vector<TensorConstPtr>& inputs,
-    const std::vector<int32_t>* labels) {
-  std::vector<TensorUniquePtr> result;
-
-  result.reserve(inputs.size());
-  size_t nb_inputs_queued = 0;
-
-  // used to detect eventual timeout
-  auto last_output_read = time_ms();
-  static constexpr int32_t timeout = 5000;  // 5s timeout
-
-  // store converted inputs that need to be kept alive while they have not been
-  // processed
-  std::vector<TensorUniquePtr> converted_inputs;
-  const Tensor* input_to_queue;
-
-  // loop until all outputs have been read
-  while (result.size() < inputs.size()) {
-    // keep system alive
-    kick_watchdog();
-    // enqueue as many jobs as current pipeline allow us
-    bool pipeline_ready = true;
-    while (nb_inputs_queued < inputs.size() && pipeline_ready) {
-      // get label that could be the same for all inputs
-      const int32_t* label = nullptr;
-      if (labels != nullptr && labels->size() > 0) {
-        label = get_label(*labels, nb_inputs_queued);
-      }
-      const auto& current_input = *inputs[nb_inputs_queued];
-      // convert input if needed
-      if (program::input_is_dense(current_program_.first)) {
-        // dense input
-        input_to_queue = conversion::as_dense(current_input);
-        if (input_to_queue == nullptr) {
-          converted_inputs.push_back(
-              conversion::to_dense(static_cast<const Sparse&>(current_input)));
-          input_to_queue = converted_inputs.back().get();
-        }
-      } else {
-        // sparse input
-        input_to_queue = conversion::as_sparse(current_input);
-        if (input_to_queue == nullptr) {
-          converted_inputs.push_back(
-              conversion::to_sparse(static_cast<const Dense&>(current_input),
-                                    current_program_.first));
-          input_to_queue = converted_inputs.back().get();
-        }
-      }
-      // try to enqueue
-      pipeline_ready = enqueue(*input_to_queue, label);
-      // if input was inserted, increment counter
-      if (pipeline_ready) {
-        ++nb_inputs_queued;
-      }
-    }
-    // then read outputs that are ready
-    bool output_ready = true;
-    while (output_ready) {
-      auto output = fetch();
-      output_ready = output != nullptr;
-      // if an output was ready, increment counter
-      if (output_ready) {
-        result.push_back(std::move(output));
-        last_output_read = time_ms();
-      } else if (time_ms() - last_output_read > timeout) {
-        panic("Fatal error: timed out while fetching output");
-      }
-    }
-  }
-  return result;
-}
-
-static void toggle_multi_pass(HardwareDeviceImpl* device,
-                              bool enable_multi_pass) {
-  auto driver = device->driver();
-  const auto top_level_reg_offset = driver->top_level_reg();
-  auto reg_gen_ctrl =
-      driver->read32(top_level_reg_offset + REG_GENERAL_CONTROL);
-  // toggle partial reconfig bit at top level register
-  set_field(&reg_gen_ctrl, PR_MESH_RST_END, enable_multi_pass ? 1 : 0);
-  driver->write32(top_level_reg_offset + REG_GENERAL_CONTROL, reg_gen_ctrl);
-}
-
-void HardwareDeviceImpl::unprogram() {
-  // free allocated outputs buffer
-  free_allocated_buffer(&mem_mgr_, &inference_memory_.outputs_base_address);
-  // free allocated inputs buffer
-  free_allocated_buffer(&mem_mgr_, &inference_memory_.inputs_base_address);
-
-  // free dmas memory
-  free_allocated_buffer(&mem_mgr_, &dma_hrc_.engine.descriptor_base_addr);
-  free_allocated_buffer(&mem_mgr_, &dma_event_.engine.descriptor_base_addr);
-  // free config dma memory
-  free_allocated_buffer(&mem_mgr_, &dma_config_.engine.descriptor_base_addr);
-  // if there is a current program, rewind it and reset NPs
-  if (current_program_.first != nullptr) {
-    // rewind the whole program
-    program::rewind(this, current_program_.first);
-    // disable partial reconfig and reset DMAs to go back to default
-    if (program::is_multi_pass(current_program_.first)) {
-      toggle_multi_pass(this, false);
-      // free multi pass memory
-      multi_pass_memory_.free_memory(&mem_mgr_);
-      // Core reset is necessary to avoid certains timeouts observed when
-      // switching to single pass. These are probably due to an internal sync
-      // issue between DMAs, but the core reset seems to be enough to fix the
-      // problem.
-      core_reset(driver_);
-    }
-
-    current_program_ = {nullptr, 0};
-    current_program_learn_en_ = false;
-  }
-
-  // Reset the hardware device Mesh
-  // FIXME: currently this is done on each call of unprogram, because program is
-  // not allocated at once, but each track has its own allocation, so we can
-  // have an out of memory in the middle of programming NPs. Once program memory
-  // will be allocated in a single block, we can move this into the `if
-  // (current_program_.first != nullptr)` block
-  reset_nps_logic_and_cfg(driver_);
-
-  // reset pipeline state (set its size to 0)
-  pipeline_state_.reset(0, 0);
-
-  // reset external memory in case of leftovers due to previous exception
-  // it must be reset before MemoryManager or its entries might be already
-  // free'd
-  external_mem_.reset();
-  // reset memory in case of leftovers due to previous exception
-  mem_mgr_.reset();
-}
-
-inline static uint32_t get_pipeline_size(bool multi_pass) {
-  return multi_pass ? 1 : dma::MAX_PIPELINE_SIZE;
-}
-
-static void enable_global_interrupts(HardwareDriver* driver,
-                                     bool dense_inputs) {
-  const auto top_level_registers = driver->top_level_reg();
-
-  // mask all interrupts except input dma (SCC if input are dense else AEDMA)
-  uint32_t reg = 0xFFFFFFFF;
-  set_field(&reg,
-            dense_inputs ? REG_INTERRUPT_CONTROLLER_SOURCE_MASK_SCC_HRC
-                         : REG_INTERRUPT_CONTROLLER_SOURCE_MASK_AEDMA,
-            0);
-  driver->write32(top_level_registers + REG_INTERRUPT_CONTROLLER_SOURCE_MASK,
-                  reg);
-
-  // enable global interrupts
-  reg = 0;
-  set_field(&reg, INTERRUPT_CONTROLLER_GENERAL_CONTROL_GLB_INT_EN, 1);
-  driver->write32(
-      top_level_registers + REG_INTERRUPT_CONTROLLER_GENERAL_CONTROL, reg);
-}
-
-void HardwareDeviceImpl::program(const uint8_t* program, size_t size) {
-  if (!program) {
-    panic("program should not be null");
-  }
-
-  // don't reprogram if the same program is passed
-  if (current_program_.first == program) {
-    return;
-  }
-
-  // verify program validity
-  program::verify(*this, program, size);
-  // Unprogram the previous mapping
-  unprogram();
-
-  // allocate config dma descriptors
-  alloc_dma_descriptors(
-      &dma_config_.engine, &mem_mgr_,
-      program::number_of_program_descriptors_required(program) +
-          program::number_of_extra_program_descriptors_required(program));
-
-  // Set multi pass mode
-  bool multi_pass_en = program::is_multi_pass(program);
-  toggle_multi_pass(this, multi_pass_en);
-  // init config dma
-  dma::init_config_dma(driver_, dma_config_, program);
-
-  if (multi_pass_en) {
-    // alloc required multi pass memory
-    multi_pass_memory_.alloc_memory(&mem_mgr_, program);
-    // Write DMA descriptors for multipass
-    program::play_multi_pass(this, program, &multi_pass_memory_);
-    // Enable dma config for multipass mode
-    dma::enable_config_dma_multipass(driver_, dma_config_);
-  } else {
-    program::play_single_pass(this, program);
-  }
-
-  // enable akida global interrupts
-  enable_global_interrupts(driver_, program::input_is_dense(program));
-
-  current_program_ = {program, size};
-}
-
-size_t HardwareDeviceImpl::set_batch_size(size_t requested_batch_size,
-                                          bool allocate_inputs) {
-  if (current_program_.first == nullptr) {
-    panic("Cannot set batch size if device is not programmed");
-  }
-  if (!pipeline_state_.empty()) {
-    panic("Cannot set batch size while all jobs have not been fetched");
-  }
-
-  const size_t max_batch_size =
-      get_pipeline_size(program::is_multi_pass(current_program_.first));
-  const auto effective_batch_size =
-      std::min(requested_batch_size, max_batch_size);
-
-  // perform action only if batch size has changed
-  if (effective_batch_size != pipeline_state_.max_size()) {
-    // reconfigure pipeline size
-    auto& input_dma =
-        program::input_is_dense(current_program_.first) ? dma_hrc_ : dma_event_;
-    const auto effective_nb_desc = std::max(
-        static_cast<uint32_t>(effective_batch_size), dma::kMinNbDescriptors);
-
-    // free and reallocate input DMA descriptors then configure the input DMA
-    free_allocated_buffer(&mem_mgr_, &input_dma.engine.descriptor_base_addr);
-    alloc_dma_descriptors(&input_dma.engine, &mem_mgr_, effective_nb_desc);
-    bool cc_enabled = dma::is_buffer_timer_enabled(*driver_, input_dma);
-    init_default_dma(driver_, input_dma.engine, effective_nb_desc);
-    // If clock counter was enabled, re enable it (it was reset & turned off by
-    // DMA reset)
-    if (cc_enabled) {
-      toggle_clock_counter(true);
-    }
-    if (version_ != NSoC_v2) {
-      // When using dense/sparse outputs, we need to enable/disable the output
-      // buffer automatic clearing from the input dma
-      uint32_t clear_size =
-          program::output_is_dense(current_program_.first)
-              ? static_cast<uint32_t>(
-                    output_memory_required(current_program_.first) -
-                    dma::kOutputHeaderByteSize)  // we need to substract header
-                                                 // size
-              : 0;
-      set_output_buffer_clear(driver_, input_dma, clear_size);
-    }
-    bool multi_pass_en = program::is_multi_pass(current_program_.first);
-    // pipeline is enabled if program is not multipass
-    pipeline(!multi_pass_en && !current_program_learn_en_);
-    if (multi_pass_en) {
-      // configure inputs DMA for multipass
-      dma::prepare_engine_multi_pass(
-          driver_, input_dma, multi_pass_memory_.hw_generated_descriptor_addr,
-          multi_pass_memory_.hw_generated_descriptor_out_addr,
-          program::num_passes(current_program_.first));
-    }
-    // pipeline state must be reset with the corresponding DMA last job id
-    // processed
-    pipeline_state_.reset(dma::get_last_job_id_processed(driver_, input_dma),
-                          effective_batch_size);
-
-    // free & reallocate outputs memory
-    free_allocated_buffer(&mem_mgr_, &inference_memory_.outputs_base_address);
-    inference_memory_.outputs_base_address = mem_mgr_.alloc(
-        output_memory_required(current_program_.first) * effective_batch_size);
-
-    // free allocated inputs
-    free_allocated_buffer(&mem_mgr_, &inference_memory_.inputs_base_address);
-    if (allocate_inputs) {
-      // if requested, allocate inputs memory. Since inputs are the only buffer
-      // that is not required to be 32b aligned, it is better to allocate it
-      // after every other buffer so we can avoid alignment "holes" that will
-      // occur if we perform allocations later.
-      inference_memory_.inputs_base_address = mem_mgr_.alloc(
-          input_memory_required(current_program_.first) * effective_batch_size);
-    }
-  }
-
-  return effective_batch_size;
-}
-
-void HardwareDeviceImpl::toggle_learn(bool learn_en) {
-  if (current_program_.first == nullptr) {
-    panic("Cannot toggle learn if device is not programmed");
-  }
-  if (!program::can_learn(current_program_.first)) {
-    panic("Cannot toggle learning mode on this program, it cannot learn");
-  }
-
-  // Learning mode is set without reprogramming entirely
-  const auto multi_pass = program::is_multi_pass(current_program_.first);
-  if (multi_pass) {
-    program::configure_learning_mode_multi_pass(this, current_program_.first,
-                                                multi_pass_memory_, learn_en);
-    // toggle extra descriptors if learn is enabled
-    dma::toggle_extra_descriptors(
-        driver_, dma_config_,
-        learn_en && program::number_of_extra_program_descriptors_required(
-                        current_program_.first) > 0);
-  } else {
-    program::configure_learning_mode_single_pass(this, current_program_.first,
-                                                 learn_en);
-  }
-
-  // Pipeline can only be enabled in single pass if learn is disabled
-  this->pipeline(!multi_pass && !learn_en);
-
-  current_program_learn_en_ = learn_en;
-}
-
-std::vector<TensorUniquePtr> HardwareDeviceImpl::predict(
-    const std::vector<TensorConstPtr>& inputs) {
-  // Check the device had been programmed
-  if (!current_program_.first) {
-    panic("Cannot predict without a program");
-  }
-  if (program::activation(current_program_.first)) {
-    panic("Evaluate requires activations to be disabled");
-  }
-  if (current_program_learn_en_) {
-    panic("Learn must be disabled to call the predict method.");
-  }
-
-  // first process all outputs
-  auto outputs = forward_loop(inputs, nullptr);
-
-  // Prepare results vector
-  std::vector<TensorUniquePtr> result;
-  result.reserve(outputs.size());
-  for (Index i = 0; i < outputs.size(); i++) {
-    // Outputs should be dense
-    auto potentials = conversion::as_dense(*outputs[i]);
-    assert(potentials);
-
-    result.push_back(dequantize(*potentials));
-  }
-
-  return result;
-}
-
-bool HardwareDeviceImpl::enqueue(const Tensor& input, const int32_t* label) {
-  if (current_program_.first == nullptr) {
-    panic("Device must be programmed before enqueuing inputs");
-  }
-  if (!current_program_learn_en_ && label != nullptr) {
-    panic("Learn must be enable to call enqueue with a label");
-  }
-  if (pipeline_state_.max_size() == 0) {
-    panic("A batch size must be defined before enqueuing inputs");
-  }
-
-  // in multi pass, we can only enqueue 1 descriptor at a time
-  const auto is_multi_pass = program::is_multi_pass(current_program_.first);
-
-  // check if there is space left in pipeline
-  if (pipeline_state_.full()) {
-    // pipeline is full, return false
-    return false;
-  }
-
-  // check if input is in the correct format
-  const auto input_is_dense = program::input_is_dense(current_program_.first);
-  if (input_is_dense) {
-    const auto* dense_input = conversion::as_dense(input);
-    if (dense_input == nullptr) {
-      panic("Input should be converted to Dense format before calling enqueue");
-    }
-  } else {
-    const auto* sparse_input = conversion::as_sparse(input);
-    if (sparse_input == nullptr) {
-      panic(
-          "Input should be converted to Sparse format before calling "
-          "enqueue");
-    }
-  }
-
-  // check if input dimensions are as expected
-  const auto in_dims = program::input_dims(current_program_.first);
-  check_input_dims(in_dims, input.dimensions());
-
-  // determine which dma controller should be used for inputs
-  const auto& dma_inputs = select_dma_engine(input_is_dense);
-
-  // Job slot is the next job that should be processed.
-  const auto job_slot = pipeline_state_.reserve_job();
-
-  // get input address on device
-  dma::addr address_in;
-  if (!accessible_from_akida(input.buffer()->data(), *driver_)) {
-    if (inference_memory_.inputs_base_address == 0) {
-      panic(
-          "Input is not accessible by akida, but no memory has been allocated "
-          "for it");
-    }
-    // calculate the input address on device
-    const auto input_buffer_size =
-        input_memory_required(current_program_.first);
-    address_in = inference_memory_.inputs_base_address +
-                 static_cast<dma::addr>(input_buffer_size * job_slot.index);
-    // copy input to device
-    driver_->write(address_in, input.buffer()->data(), input.buffer()->size());
-  } else {
-    // input is already accessible by akida, no need to copy it
-    address_in = to_dma_addr(input.buffer()->data());
-  }
-
-  // calculate address where output will be written
-  const auto out_buffer_size = output_memory_required(current_program_.first);
-  const dma::addr address_out =
-      inference_memory_.outputs_base_address +
-      static_cast<dma::addr>(out_buffer_size * job_slot.index);
-
-  // learn class is label + 1, or 0 if no label
-  uint32_t learn_class = label != nullptr ? *label + 1 : 0;
-
-  // generate descriptor
-  const auto descriptor =
-      input_is_dense
-          ? dma_dense_descriptor(
-                address_in, address_out, job_slot.job_id, learn_class, in_dims,
-                program::dense_window_w(current_program_.first),
-                program::dense_window_h(current_program_.first))
-          : dma::format_event_desc(
-                job_slot.job_id, address_in, address_out,
-                static_cast<uint32_t>(input.buffer()->size() /
-                                      sizeof(dma::w32)),
-                learn_class);
-
-  // in multi pass, we have to set output address in the input DMA since we're
-  // using HW generated address
-  if (is_multi_pass) {
-    driver_->write32(
-        dma_inputs.engine.reg_base_addr + DMA_REPLAY_OB_EVENT_BUF_ADDR_REG,
-        address_out);
-  }
-
-  // store job information.
-  pipeline_state_.enqueue_job(job_slot.job_id, address_out,
-                              input.buffer()->data());
-
-  // send descriptor to dma
-  dma::enqueue_descriptor(driver_, dma_inputs.engine, descriptor);
-
-  return true;
-}
-
-TensorUniquePtr HardwareDeviceImpl::fetch() {
-  // if queue is empty, return null
-  if (pipeline_state_.empty()) {
-    return nullptr;
-  }
-
-  // select input dma
-  const auto& input_dma =
-      program::input_is_dense(current_program_.first) ? dma_hrc_ : dma_event_;
-
-  if (program::is_multi_pass(current_program_.first)) {
-    // in multi pass, there is only 1 job at a time so we just check for an
-    // interrupt
-    if (!dma::check_for_interrupt(driver_, input_dma.engine,
-                                  DMA_BUFFER_END_STATUS_DESC_BURST_DONE)) {
-      // no interrupt, output is not ready yet
-      return nullptr;
-    }
-  } else {
-    // in single pass, we need to check that last processed job id changed
-    if (pipeline_state_.last_job_fetched() ==
-        dma::get_last_job_id_processed(driver_, input_dma)) {
-      return nullptr;
-    }
-  }
-  // clear interrupts
-  dma::clear_interrupts(driver_, input_dma.engine);
-
-  // pop job from the queue
-  auto job = pipeline_state_.pop_job();
-
-  // read output
-  auto result = dma_events_read_outputs(
-      driver_, job.output_address, program::output_dims(current_program_.first),
-      program::output_format(current_program_.first));
-
-  return result;
-}
-
-DenseUniquePtr HardwareDeviceImpl::dequantize(const Dense& potentials) {
-  // Get potentials strides and data from program
-  auto shifts = program::shifts(current_program_.first);
-  auto scales = program::scales(current_program_.first);
-  assert(shifts.second == scales.second);
-  const auto& shift = shifts.first;
-  const auto& scale = scales.first;
-
-  // perform sanity checks
-  const auto coords = potentials.dimensions();
-  if (coords.size() != 3) {
-    panic("dequantize expects a 3D Dense");
-  }
-  if (potentials.layout() != Dense::Layout::RowMajor) {
-    panic("dequantize expects a RowMajor Dense");
-  }
-  if (potentials.type() != TensorType::int32) {
-    panic("dequantize expects an int32 Dense");
-  }
-
-  // Get potentials strides and data to access them via linear index
-  const auto pot_strides = potentials.strides();
-  const auto pot_data = potentials.data<int32_t>();
-  // Allocate a dense output in the form of a RowMajor Tensor
-  auto rescaled_outputs =
-      Dense::create(TensorType::float32, coords, Dense::Layout::RowMajor);
-  // Get rescaled outputs data
-  const auto resc_data = rescaled_outputs->data<float>();
-  for (Index x = 0; x < coords[0]; x++) {
-    for (Index y = 0; y < coords[1]; y++) {
-      // move pointer at the beginning of the neuron
-      Index coord_n0[] = {x, y, 0};
-      auto coord_lin_index_n0 = linear_index(coord_n0, pot_strides);
-      auto poti = &pot_data[coord_lin_index_n0];
-      auto resci = &resc_data[coord_lin_index_n0];
-      for (Index n = 0; n < coords[2]; n++) {
-        // Evaluate rescaled output
-        auto value = static_cast<float>(poti[n] - shift[n]) / scale[n];
-        // Set rescaled value at the same index than output
-        resci[n] = value;
-      }
-    }
-  }
-
-  return rescaled_outputs;
-}
-
-size_t HardwareDeviceImpl::learn_mem_size() const {
-  return program::learn_mem_size(current_program_.first);
-}
-
-void HardwareDeviceImpl::learn_mem(uint32_t* output_buffer) {
-  if (!current_program_learn_en_) {
-    panic("learn is not enabled");
-  }
-  program::learn_mem(this, current_program_.first, output_buffer);
-}
-
-void HardwareDeviceImpl::update_learn_mem(const uint32_t* input_buffer) {
-  program::update_learn_mem(this, current_program_.first, input_buffer);
-}
-
-}  // namespace akida
+#include "hardware_device_impl.h"
+
+#include <algorithm>
+#include <cassert>
+#include <cstdint>
+#include <cstring>
+#include <memory>
+#include <tuple>
+
+#include "akida/dense.h"
+#include "akida/hw_version.h"
+#include "akida/input_conversion.h"
+#include "akida/np.h"
+#include "akida/program_memory_info.h"
+#include "akida/registers_top_level.h"
+#include "akida/shape.h"
+#include "engine/dma.h"
+#include "engine/dma_config_ops.h"
+
+#include "infra/int_ops.h"
+#include "infra/registers_common.h"
+#include "infra/system.h"
+
+#include "dma_desc_format.h"
+#include "dma_desc_ops.h"
+#include "dma_engine.h"
+#include "dma_engine_ops.h"
+#include "dma_events_ops.h"
+#include "dma_image_ops.h"
+#include "external_mem_mgr.h"
+#include "memory_utils.h"
+#include "program_play.h"
+#include "registers_dma_engine.h"
+#include "reset_nps.h"
+
+namespace akida {
+
+static void toggle_multi_pass(HardwareDeviceImpl* device,
+                              bool enable_multi_pass);
+
+static void alloc_dma_descriptors(dma::Engine* dma, MemoryMgr* mem_mgr,
+                                  uint32_t num_descriptors) {
+  assert(num_descriptors <= dma::kMaxNbDescriptorsMultipass);
+  assert(dma->descriptor_base_addr == 0);
+  // allocate buffer to contain descriptors
+  dma->descriptor_base_addr =
+      mem_mgr->alloc(num_descriptors * dma->descriptor_bytes_size);
+}
+
+static void free_allocated_buffer(MemoryMgr* mem_mgr, dma::addr* ptr) {
+  // check if pointer was allocated
+  if (*ptr) {
+    mem_mgr->free(*ptr);
+    // we have to set to 0 to mark we have correctly freed
+    *ptr = 0;
+  }
+}
+
+HardwareDeviceImpl::HardwareDeviceImpl(HardwareDriver* driver)
+    : driver_(driver),
+      version_(read_hw_version(*driver_)),
+      dma_config_{dma::Engine(dma_config_reg_base(driver_->top_level_reg()),
+                              dma::config::DESC_BYTE_SIZE)},
+      dma_event_{dma::Engine(dma_event_reg_base(driver_->top_level_reg()),
+                             dma::event::DESC_BYTE_SIZE)},
+      dma_hrc_{dma::Engine(dma_hrc_reg_base(driver_->top_level_reg()),
+                           dma::hrc::DESC_BYTE_SIZE)},
+      mem_mgr_(driver->scratch_memory(), driver->scratch_size()),
+      current_program_(nullptr, 0),
+      current_program_learn_en_(false),
+      external_mem_(&mem_mgr_, driver) {
+  if (version_ == akida::NSoC_v1) {
+    panic(
+        "NSoC_v1 is not supported on this version. Please install akida 2.0.5 "
+        "instead.");
+  }
+  init();
+}
+
+HardwareDeviceImpl::~HardwareDeviceImpl() {
+  free_allocated_buffer(&mem_mgr_, &dma_config_.engine.descriptor_base_addr);
+  free_allocated_buffer(&mem_mgr_, &dma_event_.engine.descriptor_base_addr);
+  free_allocated_buffer(&mem_mgr_, &dma_hrc_.engine.descriptor_base_addr);
+}
+
+HwVersion HardwareDeviceImpl::version() const { return version_; }
+
+void HardwareDeviceImpl::dma_config_write(const dma::w32* buffer,
+                                          size_t buf_size) {
+  // put buffer on device, and get its address
+  auto input_addr = external_mem_.track_and_put_on_device_if_required(
+      buffer, buf_size * sizeof(dma::w32));
+  constexpr uint32_t output_addr = 0;  // not used for write
+  // format descriptor
+  auto descriptor =
+      dma::format_config_desc(dma::kDescConfigDirectionWrite, input_addr,
+                              output_addr, static_cast<uint32_t>(buf_size));
+  assert(descriptor.size() == dma::config::DESC_LEN);
+
+  // tell DMA engine to process descriptor
+  dma::process(driver_, dma_config_, descriptor);
+  // now that buffer has been processed, it can be freed from device
+  external_mem_.release(buffer);
+}
+
+void HardwareDeviceImpl::dma_config_read(dma::w32* buffer,
+                                         const struct np::Ident& np,
+                                         dma::Target target,
+                                         uint16_t addr_target_word,
+                                         uint32_t nb_words) {
+  assert(dma_config_.engine.descriptor_base_addr != 0);
+  if (dma::config_block_size_needs_xl(static_cast<uint32_t>(nb_words))) {
+    panic("Unsupported buffer size in config read");
+  }
+
+  // format header
+  auto header =
+      dma::format_config_header(np, target, nb_words, addr_target_word);
+  uint32_t header_size = static_cast<uint32_t>(header.size());
+
+  // Allocate input and output area
+  auto input_addr = mem_mgr_.alloc(header_size * sizeof(dma::w32));
+  // Allocation should include header size
+  auto output_addr = mem_mgr_.alloc(nb_words * sizeof(dma::w32) +
+                                    dma::kConfigReadPacketOffset);
+  // format descriptor
+  auto descriptor = dma::format_config_desc(
+      dma::kDescConfigDirectionRead, input_addr, output_addr, header_size);
+  assert(descriptor.size() == dma::config::DESC_LEN);
+
+  // write header in DDR
+  driver_->write(input_addr, header.data(), header.size() * sizeof(dma::w32));
+
+  // tell DMA engine to process descriptor
+  dma::process(driver_, dma_config_, descriptor);
+
+  // fetch read header in DDR
+  dma::wbuffer read_hdr(dma::kConfigReadPacketHdrSz);
+  driver_->read(output_addr, read_hdr.data(),
+                dma::kConfigReadPacketHdrSz * sizeof(dma::w32));
+
+  // set packet size (nb of 32b words) and address/offset data
+  uint32_t packetsize = dma::parse_config_read_size(read_hdr);
+  uint32_t read_offset_addr = output_addr + dma::kConfigReadPacketOffset;
+
+  if (nb_words == 0 || packetsize != nb_words) {
+    panic("error on dma config read: invalid packet size (%d), expected %d.",
+          packetsize, nb_words);
+  }
+
+  driver_->read(read_offset_addr, buffer, nb_words * sizeof(dma::w32));
+  // now that input and outputs have been processed, it can be freed
+  mem_mgr_.free(output_addr);
+  mem_mgr_.free(input_addr);
+}
+
+void HardwareDeviceImpl::read_np_registers(uint32_t* output,
+                                           const struct np::Ident& np,
+                                           uint32_t nb_registers) {
+  auto has_alloc = false;
+  if (dma_config_.engine.descriptor_base_addr == 0) {
+    alloc_dma_descriptors(&dma_config_.engine, &mem_mgr_,
+                          dma::kMinNbDescriptors);
+    dma::init_default_dma(driver_, dma_config_.engine, dma::kMinNbDescriptors);
+    has_alloc = true;
+  }
+  dma_config_read(output, np, dma::Target::NpRegisters, 0, nb_registers);
+  if (has_alloc) {
+    free_allocated_buffer(&mem_mgr_, &dma_config_.engine.descriptor_base_addr);
+  }
+}
+
+std::vector<TensorUniquePtr> HardwareDeviceImpl::fit(
+    const std::vector<TensorConstPtr>& inputs,
+    const std::vector<int32_t>& input_labels) {
+  // Check the device had been programmed
+  if (!current_program_.first) {
+    panic("Cannot fit without a program");
+  }
+  if (!current_program_learn_en_)
+    panic("Learn must be enabled to call the fit method.");
+
+  return forward_loop(inputs, &input_labels);
+}
+
+std::vector<TensorUniquePtr> HardwareDeviceImpl::forward(
+    const std::vector<TensorConstPtr>& inputs) {
+  // Check the device had been programmed
+  if (!current_program_.first) {
+    panic("Cannot forward without a program");
+  }
+  if (current_program_learn_en_)
+    panic("Learn must be disabled to call the forward method.");
+
+  return forward_loop(inputs, nullptr);
+}
+
+const dma::Inputs& HardwareDeviceImpl::select_dma_engine(bool dense_inputs) {
+  // Only enable the input DMA used by the current network:
+  // HRC DMA if 1st layer is InputConvolutional, Event DMA otherwise
+  dma::toggle_engine(driver_, dma_hrc_.engine.reg_base_addr, dense_inputs);
+  dma::toggle_engine(driver_, dma_event_.engine.reg_base_addr, !dense_inputs);
+
+  return dense_inputs ? dma_hrc_ : dma_event_;
+}
+
+void HardwareDeviceImpl::pipeline(bool enabled) {
+  dma::toggle_pipeline(driver_, dma_event_, enabled);
+  dma::toggle_pipeline(driver_, dma_hrc_, enabled);
+}
+
+void HardwareDeviceImpl::toggle_clock_counter(bool enable) {
+  dma::toggle_buffer_timer(driver_, dma_event_.engine, enable);
+  dma::toggle_buffer_timer(driver_, dma_hrc_.engine, enable);
+}
+
+uint32_t HardwareDeviceImpl::read_clock_counter() {
+  // read clock from HRC DMA or read from events DMA
+  auto hrc_count_number = dma::read_buffer_timer(driver_, dma_hrc_.engine);
+  auto event_count_number = dma::read_buffer_timer(driver_, dma_event_.engine);
+  return std::max(hrc_count_number, event_count_number);
+}
+
+uint32_t HardwareDeviceImpl::read_config_clock_counter() {
+  return dma::read_buffer_timer(driver_, dma_config_.engine);
+}
+
+bool HardwareDeviceImpl::clock_counter_enabled() {
+  return dma::is_buffer_timer_enabled(*driver_, dma_event_);
+}
+
+static void check_input_dims(const Index* program_in_dims,
+                             const Shape& inputs_shape) {
+  bool valid_dims = true;
+  switch (inputs_shape.size()) {
+    case 1:  // fully connected, 1 dimension
+      if (inputs_shape[0] !=
+          program_in_dims[0] * program_in_dims[1] * program_in_dims[2]) {
+        valid_dims = false;
+      }
+      break;
+    case 3:  // other cases (check only that data size is compatible)
+      if (inputs_shape[0] * inputs_shape[1] * inputs_shape[2] !=
+          program_in_dims[0] * program_in_dims[1] * program_in_dims[2]) {
+        valid_dims = false;
+      }
+      break;
+    default:
+      valid_dims = false;
+      break;
+  }
+  if (!valid_dims) {
+    panic("Invalid input dimensions for this program");
+  }
+}
+
+// reset whole akida core, including DMAs
+static void core_reset(HardwareDriver* driver) {
+  const auto top_level_reg_offset = driver->top_level_reg();
+  auto reg_gen_ctrl =
+      driver->read32(top_level_reg_offset + REG_GENERAL_CONTROL);
+  // Reset NP & CORE
+  set_field(&reg_gen_ctrl, AK_CORE_RST, 1);
+  set_field(&reg_gen_ctrl, SCC_CORE_RESET, 1);
+  driver->write32(top_level_reg_offset + REG_GENERAL_CONTROL, reg_gen_ctrl);
+  // 20 cycles should be waited. Waiting 1ms is more than enough.
+  msleep(1);
+  // Fields need to be reset to 0
+  set_field(&reg_gen_ctrl, AK_CORE_RST, 0);
+  set_field(&reg_gen_ctrl, SCC_CORE_RESET, 0);
+  driver->write32(top_level_reg_offset + REG_GENERAL_CONTROL, reg_gen_ctrl);
+  // 40 cycles should be waited. Waiting 1ms is more than enough.
+  msleep(1);
+}
+
+void HardwareDeviceImpl::init() {
+  // this core reset is only available on production chip
+  core_reset(driver_);
+
+  // reset HW mesh
+  reset_nps_logic_and_cfg(driver_);
+}
+
+static inline const int32_t* get_label(const std::vector<int32_t>& labels,
+                                       size_t index) {
+  return labels.size() == 1 ? &labels[0] : &labels[index];
+}
+
+std::vector<TensorUniquePtr> HardwareDeviceImpl::forward_loop(
+    const std::vector<TensorConstPtr>& inputs,
+    const std::vector<int32_t>* labels) {
+  std::vector<TensorUniquePtr> result;
+
+  result.reserve(inputs.size());
+  size_t nb_inputs_queued = 0;
+
+  // used to detect eventual timeout
+  auto last_output_read = time_ms();
+  static constexpr int32_t timeout = 5000;  // 5s timeout
+
+  // store converted inputs that need to be kept alive while they have not been
+  // processed
+  std::vector<TensorUniquePtr> converted_inputs;
+  const Tensor* input_to_queue;
+
+  // loop until all outputs have been read
+  while (result.size() < inputs.size()) {
+    // keep system alive
+    kick_watchdog();
+    // enqueue as many jobs as current pipeline allow us
+    bool pipeline_ready = true;
+    while (nb_inputs_queued < inputs.size() && pipeline_ready) {
+      // get label that could be the same for all inputs
+      const int32_t* label = nullptr;
+      if (labels != nullptr && labels->size() > 0) {
+        label = get_label(*labels, nb_inputs_queued);
+      }
+      const auto& current_input = *inputs[nb_inputs_queued];
+      // convert input if needed
+      if (program::input_is_dense(current_program_.first)) {
+        // dense input
+        input_to_queue = conversion::as_dense(current_input);
+        if (input_to_queue == nullptr) {
+          converted_inputs.push_back(
+              conversion::to_dense(static_cast<const Sparse&>(current_input)));
+          input_to_queue = converted_inputs.back().get();
+        }
+      } else {
+        // sparse input
+        input_to_queue = conversion::as_sparse(current_input);
+        if (input_to_queue == nullptr) {
+          converted_inputs.push_back(
+              conversion::to_sparse(static_cast<const Dense&>(current_input),
+                                    current_program_.first));
+          input_to_queue = converted_inputs.back().get();
+        }
+      }
+      // try to enqueue
+      pipeline_ready = enqueue(*input_to_queue, label);
+      // if input was inserted, increment counter
+      if (pipeline_ready) {
+        ++nb_inputs_queued;
+      }
+    }
+    // then read outputs that are ready
+    bool output_ready = true;
+    while (output_ready) {
+      auto output = fetch();
+      output_ready = output != nullptr;
+      // if an output was ready, increment counter
+      if (output_ready) {
+        result.push_back(std::move(output));
+        last_output_read = time_ms();
+      } else if (time_ms() - last_output_read > timeout) {
+        panic("Fatal error: timed out while fetching output");
+      }
+    }
+  }
+  return result;
+}
+
+static void toggle_multi_pass(HardwareDeviceImpl* device,
+                              bool enable_multi_pass) {
+  auto driver = device->driver();
+  const auto top_level_reg_offset = driver->top_level_reg();
+  auto reg_gen_ctrl =
+      driver->read32(top_level_reg_offset + REG_GENERAL_CONTROL);
+  // toggle partial reconfig bit at top level register
+  set_field(&reg_gen_ctrl, PR_MESH_RST_END, enable_multi_pass ? 1 : 0);
+  driver->write32(top_level_reg_offset + REG_GENERAL_CONTROL, reg_gen_ctrl);
+}
+
+void HardwareDeviceImpl::unprogram() {
+  // free allocated outputs buffer
+  free_allocated_buffer(&mem_mgr_, &inference_memory_.outputs_base_address);
+  // free allocated inputs buffer
+  free_allocated_buffer(&mem_mgr_, &inference_memory_.inputs_base_address);
+
+  // free dmas memory
+  free_allocated_buffer(&mem_mgr_, &dma_hrc_.engine.descriptor_base_addr);
+  free_allocated_buffer(&mem_mgr_, &dma_event_.engine.descriptor_base_addr);
+  // free config dma memory
+  free_allocated_buffer(&mem_mgr_, &dma_config_.engine.descriptor_base_addr);
+  // if there is a current program, rewind it and reset NPs
+  if (current_program_.first != nullptr) {
+    // rewind the whole program
+    program::rewind(this, current_program_.first);
+    // disable partial reconfig and reset DMAs to go back to default
+    if (program::is_multi_pass(current_program_.first)) {
+      toggle_multi_pass(this, false);
+      // free multi pass memory
+      multi_pass_memory_.free_memory(&mem_mgr_);
+      // Core reset is necessary to avoid certains timeouts observed when
+      // switching to single pass. These are probably due to an internal sync
+      // issue between DMAs, but the core reset seems to be enough to fix the
+      // problem.
+      core_reset(driver_);
+    }
+
+    current_program_ = {nullptr, 0};
+    current_program_learn_en_ = false;
+  }
+
+  // Reset the hardware device Mesh
+  // FIXME: currently this is done on each call of unprogram, because program is
+  // not allocated at once, but each track has its own allocation, so we can
+  // have an out of memory in the middle of programming NPs. Once program memory
+  // will be allocated in a single block, we can move this into the `if
+  // (current_program_.first != nullptr)` block
+  reset_nps_logic_and_cfg(driver_);
+
+  // reset pipeline state (set its size to 0)
+  pipeline_state_.reset(0, 0);
+
+  // reset external memory in case of leftovers due to previous exception
+  // it must be reset before MemoryManager or its entries might be already
+  // free'd
+  external_mem_.reset();
+  // reset memory in case of leftovers due to previous exception
+  mem_mgr_.reset();
+}
+
+inline static uint32_t get_pipeline_size(bool multi_pass) {
+  return multi_pass ? 1 : dma::MAX_PIPELINE_SIZE;
+}
+
+static void enable_global_interrupts(HardwareDriver* driver,
+                                     bool dense_inputs) {
+  const auto top_level_registers = driver->top_level_reg();
+
+  // mask all interrupts except input dma (SCC if input are dense else AEDMA)
+  uint32_t reg = 0xFFFFFFFF;
+  set_field(&reg,
+            dense_inputs ? REG_INTERRUPT_CONTROLLER_SOURCE_MASK_SCC_HRC
+                         : REG_INTERRUPT_CONTROLLER_SOURCE_MASK_AEDMA,
+            0);
+  driver->write32(top_level_registers + REG_INTERRUPT_CONTROLLER_SOURCE_MASK,
+                  reg);
+
+  // enable global interrupts
+  reg = 0;
+  set_field(&reg, INTERRUPT_CONTROLLER_GENERAL_CONTROL_GLB_INT_EN, 1);
+  driver->write32(
+      top_level_registers + REG_INTERRUPT_CONTROLLER_GENERAL_CONTROL, reg);
+}
+
+void HardwareDeviceImpl::program(const uint8_t* program, size_t size) {
+  if (!program) {
+    panic("program should not be null");
+  }
+
+  // don't reprogram if the same program is passed
+  if (current_program_.first == program) {
+    return;
+  }
+
+  // verify program validity
+  program::verify(*this, program, size);
+  // Unprogram the previous mapping
+  unprogram();
+
+  // allocate config dma descriptors
+  alloc_dma_descriptors(
+      &dma_config_.engine, &mem_mgr_,
+      program::number_of_program_descriptors_required(program) +
+          program::number_of_extra_program_descriptors_required(program));
+
+  // Set multi pass mode
+  bool multi_pass_en = program::is_multi_pass(program);
+  toggle_multi_pass(this, multi_pass_en);
+  // init config dma
+  dma::init_config_dma(driver_, dma_config_, program);
+
+  if (multi_pass_en) {
+    // alloc required multi pass memory
+    multi_pass_memory_.alloc_memory(&mem_mgr_, program);
+    // Write DMA descriptors for multipass
+    program::play_multi_pass(this, program, &multi_pass_memory_);
+    // Enable dma config for multipass mode
+    dma::enable_config_dma_multipass(driver_, dma_config_);
+  } else {
+    program::play_single_pass(this, program);
+  }
+
+  // enable akida global interrupts
+  enable_global_interrupts(driver_, program::input_is_dense(program));
+
+  current_program_ = {program, size};
+}
+
+size_t HardwareDeviceImpl::set_batch_size(size_t requested_batch_size,
+                                          bool allocate_inputs) {
+  if (current_program_.first == nullptr) {
+    panic("Cannot set batch size if device is not programmed");
+  }
+  if (!pipeline_state_.empty()) {
+    panic("Cannot set batch size while all jobs have not been fetched");
+  }
+
+  const size_t max_batch_size =
+      get_pipeline_size(program::is_multi_pass(current_program_.first));
+  const auto effective_batch_size =
+      std::min(requested_batch_size, max_batch_size);
+
+  // perform action only if batch size has changed
+  if (effective_batch_size != pipeline_state_.max_size()) {
+    // reconfigure pipeline size
+    auto& input_dma =
+        program::input_is_dense(current_program_.first) ? dma_hrc_ : dma_event_;
+    const auto effective_nb_desc = std::max(
+        static_cast<uint32_t>(effective_batch_size), dma::kMinNbDescriptors);
+
+    // free and reallocate input DMA descriptors then configure the input DMA
+    free_allocated_buffer(&mem_mgr_, &input_dma.engine.descriptor_base_addr);
+    alloc_dma_descriptors(&input_dma.engine, &mem_mgr_, effective_nb_desc);
+    bool cc_enabled = dma::is_buffer_timer_enabled(*driver_, input_dma);
+    init_default_dma(driver_, input_dma.engine, effective_nb_desc);
+    // If clock counter was enabled, re enable it (it was reset & turned off by
+    // DMA reset)
+    if (cc_enabled) {
+      toggle_clock_counter(true);
+    }
+    if (version_ != NSoC_v2) {
+      // When using dense/sparse outputs, we need to enable/disable the output
+      // buffer automatic clearing from the input dma
+      uint32_t clear_size =
+          program::output_is_dense(current_program_.first)
+              ? static_cast<uint32_t>(
+                    output_memory_required(current_program_.first) -
+                    dma::kOutputHeaderByteSize)  // we need to substract header
+                                                 // size
+              : 0;
+      set_output_buffer_clear(driver_, input_dma, clear_size);
+    }
+    bool multi_pass_en = program::is_multi_pass(current_program_.first);
+    // pipeline is enabled if program is not multipass
+    pipeline(!multi_pass_en && !current_program_learn_en_);
+    if (multi_pass_en) {
+      // configure inputs DMA for multipass
+      dma::prepare_engine_multi_pass(
+          driver_, input_dma, multi_pass_memory_.hw_generated_descriptor_addr,
+          multi_pass_memory_.hw_generated_descriptor_out_addr,
+          program::num_passes(current_program_.first));
+    }
+    // pipeline state must be reset with the corresponding DMA last job id
+    // processed
+    pipeline_state_.reset(dma::get_last_job_id_processed(driver_, input_dma),
+                          effective_batch_size);
+
+    // free & reallocate outputs memory
+    free_allocated_buffer(&mem_mgr_, &inference_memory_.outputs_base_address);
+    inference_memory_.outputs_base_address = mem_mgr_.alloc(
+        output_memory_required(current_program_.first) * effective_batch_size);
+
+    // free allocated inputs
+    free_allocated_buffer(&mem_mgr_, &inference_memory_.inputs_base_address);
+    if (allocate_inputs) {
+      // if requested, allocate inputs memory. Since inputs are the only buffer
+      // that is not required to be 32b aligned, it is better to allocate it
+      // after every other buffer so we can avoid alignment "holes" that will
+      // occur if we perform allocations later.
+      inference_memory_.inputs_base_address = mem_mgr_.alloc(
+          input_memory_required(current_program_.first) * effective_batch_size);
+    }
+  }
+
+  return effective_batch_size;
+}
+
+void HardwareDeviceImpl::toggle_learn(bool learn_en) {
+  if (current_program_.first == nullptr) {
+    panic("Cannot toggle learn if device is not programmed");
+  }
+  if (!program::can_learn(current_program_.first)) {
+    panic("Cannot toggle learning mode on this program, it cannot learn");
+  }
+
+  // Learning mode is set without reprogramming entirely
+  const auto multi_pass = program::is_multi_pass(current_program_.first);
+  if (multi_pass) {
+    program::configure_learning_mode_multi_pass(this, current_program_.first,
+                                                multi_pass_memory_, learn_en);
+    // toggle extra descriptors if learn is enabled
+    dma::toggle_extra_descriptors(
+        driver_, dma_config_,
+        learn_en && program::number_of_extra_program_descriptors_required(
+                        current_program_.first) > 0);
+  } else {
+    program::configure_learning_mode_single_pass(this, current_program_.first,
+                                                 learn_en);
+  }
+
+  // Pipeline can only be enabled in single pass if learn is disabled
+  this->pipeline(!multi_pass && !learn_en);
+
+  current_program_learn_en_ = learn_en;
+}
+
+std::vector<TensorUniquePtr> HardwareDeviceImpl::predict(
+    const std::vector<TensorConstPtr>& inputs) {
+  // Check the device had been programmed
+  if (!current_program_.first) {
+    panic("Cannot predict without a program");
+  }
+  if (program::activation(current_program_.first)) {
+    panic("Evaluate requires activations to be disabled");
+  }
+  if (current_program_learn_en_) {
+    panic("Learn must be disabled to call the predict method.");
+  }
+
+  // first process all outputs
+  auto outputs = forward_loop(inputs, nullptr);
+
+  // Prepare results vector
+  std::vector<TensorUniquePtr> result;
+  result.reserve(outputs.size());
+  for (Index i = 0; i < outputs.size(); i++) {
+    // Outputs should be dense
+    auto potentials = conversion::as_dense(*outputs[i]);
+    assert(potentials);
+
+    result.push_back(dequantize(*potentials));
+  }
+
+  return result;
+}
+
+bool HardwareDeviceImpl::enqueue(const Tensor& input, const int32_t* label) {
+  if (current_program_.first == nullptr) {
+    panic("Device must be programmed before enqueuing inputs");
+  }
+  if (!current_program_learn_en_ && label != nullptr) {
+    panic("Learn must be enable to call enqueue with a label");
+  }
+  if (pipeline_state_.max_size() == 0) {
+    panic("A batch size must be defined before enqueuing inputs");
+  }
+
+  // in multi pass, we can only enqueue 1 descriptor at a time
+  const auto is_multi_pass = program::is_multi_pass(current_program_.first);
+
+  // check if there is space left in pipeline
+  if (pipeline_state_.full()) {
+    // pipeline is full, return false
+    return false;
+  }
+
+  // check if input is in the correct format
+  const auto input_is_dense = program::input_is_dense(current_program_.first);
+  if (input_is_dense) {
+    const auto* dense_input = conversion::as_dense(input);
+    if (dense_input == nullptr) {
+      panic("Input should be converted to Dense format before calling enqueue");
+    }
+  } else {
+    const auto* sparse_input = conversion::as_sparse(input);
+    if (sparse_input == nullptr) {
+      panic(
+          "Input should be converted to Sparse format before calling "
+          "enqueue");
+    }
+  }
+
+  // check if input dimensions are as expected
+  const auto in_dims = program::input_dims(current_program_.first);
+  check_input_dims(in_dims, input.dimensions());
+
+  // determine which dma controller should be used for inputs
+  const auto& dma_inputs = select_dma_engine(input_is_dense);
+
+  // Job slot is the next job that should be processed.
+  const auto job_slot = pipeline_state_.reserve_job();
+
+  // get input address on device
+  dma::addr address_in;
+  if (!accessible_from_akida(input.buffer()->data(), *driver_)) {
+    if (inference_memory_.inputs_base_address == 0) {
+      panic(
+          "Input is not accessible by akida, but no memory has been allocated "
+          "for it");
+    }
+    // calculate the input address on device
+    const auto input_buffer_size =
+        input_memory_required(current_program_.first);
+    address_in = inference_memory_.inputs_base_address +
+                 static_cast<dma::addr>(input_buffer_size * job_slot.index);
+    // copy input to device
+    driver_->write(address_in, input.buffer()->data(), input.buffer()->size());
+  } else {
+    // input is already accessible by akida, no need to copy it
+    address_in = to_dma_addr(input.buffer()->data());
+  }
+
+  // calculate address where output will be written
+  const auto out_buffer_size = output_memory_required(current_program_.first);
+  const dma::addr address_out =
+      inference_memory_.outputs_base_address +
+      static_cast<dma::addr>(out_buffer_size * job_slot.index);
+
+  // learn class is label + 1, or 0 if no label
+  uint32_t learn_class = label != nullptr ? *label + 1 : 0;
+
+  // generate descriptor
+  const auto descriptor =
+      input_is_dense
+          ? dma_dense_descriptor(
+                address_in, address_out, job_slot.job_id, learn_class, in_dims,
+                program::dense_window_w(current_program_.first),
+                program::dense_window_h(current_program_.first))
+          : dma::format_event_desc(
+                job_slot.job_id, address_in, address_out,
+                static_cast<uint32_t>(input.buffer()->size() /
+                                      sizeof(dma::w32)),
+                learn_class);
+
+  // in multi pass, we have to set output address in the input DMA since we're
+  // using HW generated address
+  if (is_multi_pass) {
+    driver_->write32(
+        dma_inputs.engine.reg_base_addr + DMA_REPLAY_OB_EVENT_BUF_ADDR_REG,
+        address_out);
+  }
+
+  // store job information.
+  pipeline_state_.enqueue_job(job_slot.job_id, address_out,
+                              input.buffer()->data());
+
+  // send descriptor to dma
+  dma::enqueue_descriptor(driver_, dma_inputs.engine, descriptor);
+
+  return true;
+}
+
+TensorUniquePtr HardwareDeviceImpl::fetch() {
+  // if queue is empty, return null
+  if (pipeline_state_.empty()) {
+    return nullptr;
+  }
+
+  // select input dma
+  const auto& input_dma =
+      program::input_is_dense(current_program_.first) ? dma_hrc_ : dma_event_;
+
+  if (program::is_multi_pass(current_program_.first)) {
+    // in multi pass, there is only 1 job at a time so we just check for an
+    // interrupt
+    if (!dma::check_for_interrupt(driver_, input_dma.engine,
+                                  DMA_BUFFER_END_STATUS_DESC_BURST_DONE)) {
+      // no interrupt, output is not ready yet
+      return nullptr;
+    }
+  } else {
+    // in single pass, we need to check that last processed job id changed
+    if (pipeline_state_.last_job_fetched() ==
+        dma::get_last_job_id_processed(driver_, input_dma)) {
+      return nullptr;
+    }
+  }
+  // clear interrupts
+  dma::clear_interrupts(driver_, input_dma.engine);
+
+  // pop job from the queue
+  auto job = pipeline_state_.pop_job();
+
+  // read output
+  auto result = dma_events_read_outputs(
+      driver_, job.output_address, program::output_dims(current_program_.first),
+      program::output_format(current_program_.first));
+
+  return result;
+}
+
+DenseUniquePtr HardwareDeviceImpl::dequantize(const Dense& potentials) {
+  // Get potentials strides and data from program
+  auto shifts = program::shifts(current_program_.first);
+  auto scales = program::scales(current_program_.first);
+  assert(shifts.second == scales.second);
+  const auto& shift = shifts.first;
+  const auto& scale = scales.first;
+
+  // perform sanity checks
+  const auto coords = potentials.dimensions();
+  if (coords.size() != 3) {
+    panic("dequantize expects a 3D Dense");
+  }
+  if (potentials.layout() != Dense::Layout::RowMajor) {
+    panic("dequantize expects a RowMajor Dense");
+  }
+  if (potentials.type() != TensorType::int32) {
+    panic("dequantize expects an int32 Dense");
+  }
+
+  // Get potentials strides and data to access them via linear index
+  const auto pot_strides = potentials.strides();
+  const auto pot_data = potentials.data<int32_t>();
+  // Allocate a dense output in the form of a RowMajor Tensor
+  auto rescaled_outputs =
+      Dense::create(TensorType::float32, coords, Dense::Layout::RowMajor);
+  // Get rescaled outputs data
+  const auto resc_data = rescaled_outputs->data<float>();
+  for (Index x = 0; x < coords[0]; x++) {
+    for (Index y = 0; y < coords[1]; y++) {
+      // move pointer at the beginning of the neuron
+      Index coord_n0[] = {x, y, 0};
+      auto coord_lin_index_n0 = linear_index(coord_n0, pot_strides);
+      auto poti = &pot_data[coord_lin_index_n0];
+      auto resci = &resc_data[coord_lin_index_n0];
+      for (Index n = 0; n < coords[2]; n++) {
+        // Evaluate rescaled output
+        auto value = static_cast<float>(poti[n] - shift[n]) / scale[n];
+        // Set rescaled value at the same index than output
+        resci[n] = value;
+      }
+    }
+  }
+
+  return rescaled_outputs;
+}
+
+size_t HardwareDeviceImpl::learn_mem_size() const {
+  return program::learn_mem_size(current_program_.first);
+}
+
+void HardwareDeviceImpl::learn_mem(uint32_t* output_buffer) {
+  if (!current_program_learn_en_) {
+    panic("learn is not enabled");
+  }
+  program::learn_mem(this, current_program_.first, output_buffer);
+}
+
+void HardwareDeviceImpl::update_learn_mem(const uint32_t* input_buffer) {
+  program::update_learn_mem(this, current_program_.first, input_buffer);
+}
+
+}  // namespace akida
```

## akida/engine/src/hardware_device_impl.h

 * *Ordering differences only*

```diff
@@ -1,145 +1,145 @@
-#pragma once
-
-#include <cstddef>
-#include <cstdint>
-#include <memory>
-#include <queue>
-#include <vector>
-
-#include "akida/hardware_device.h"
-#include "akida/hw_version.h"
-#include "akida/np.h"
-#include "akida/tensor.h"
-#include "engine/dma.h"
-#include "infra/hardware_driver.h"
-
-#include "device_memory.h"
-#include "dma_engine.h"
-#include "external_mem_mgr.h"
-#include "memory_mgr.h"
-#include "multipass_memory.h"
-#include "pipeline_state.h"
-
-namespace akida {
-
-namespace dma {
-// forward declarations
-enum class Target;
-}  // namespace dma
-
-class HardwareDeviceImpl final : public HardwareDevice {
- public:
-  HardwareDeviceImpl(HardwareDriver* driver);
-
-  ~HardwareDeviceImpl();
-
-  HwVersion version() const override;
-
-  const char* desc() const override { return driver_->desc(); }
-
-  void pipeline(bool enable);
-
-  void toggle_clock_counter(bool enable) override;
-
-  uint32_t read_clock_counter() override;
-
-  uint32_t read_config_clock_counter() override;
-
-  void dma_config_write(const dma::w32* buffer, size_t buffer_size);
-
-  void dma_config_read(dma::w32* buffer, const struct np::Ident& np,
-                       dma::Target target, uint16_t addr_target_word,
-                       uint32_t nb_words);
-
-  void read_np_registers(uint32_t* output, const struct np::Ident& np,
-                         uint32_t nb_registers) override;
-
-  // Device fit
-  std::vector<TensorUniquePtr> fit(
-      const std::vector<TensorConstPtr>& inputs,
-      const std::vector<int32_t>& input_labels) override;
-
-  // Device forward
-  std::vector<TensorUniquePtr> forward(
-      const std::vector<TensorConstPtr>& inputs) override;
-
-  // Device predict
-  std::vector<TensorUniquePtr> predict(
-      const std::vector<TensorConstPtr>& inputs) override;
-
-  // Queue input
-  bool enqueue(const Tensor& input, const int32_t* label = nullptr) override;
-
-  // check for output
-  TensorUniquePtr fetch() override;
-
-  // apply rescale
-  DenseUniquePtr dequantize(const Dense& potentials) override;
-
-  // perform hardware device programming
-  void program(const uint8_t* program, size_t size) override;
-
-  size_t set_batch_size(size_t requested_batch_size,
-                        bool allocate_inputs) override;
-
-  // enable/disable learning mode
-  void toggle_learn(bool learn_en) override;
-
-  // unprogram current program
-  void unprogram() override;
-
-  // Return the memory used currently in the device
-  MemoryInfo memory() const override { return mem_mgr_.report(); }
-
-  void reset_top_memory() override { mem_mgr_.reset_top_usage(); }
-
-  const BytesBuffer& program() const override { return current_program_; }
-
-  bool learn_enabled() const override { return current_program_learn_en_; }
-
-  size_t learn_mem_size() const override;
-
-  void learn_mem(uint32_t* output_buffer) override;
-
-  void update_learn_mem(const uint32_t* input_buffer) override;
-
-  HardwareDriver* driver() const override { return driver_; }
-
-  MemoryMgr* mem() { return &mem_mgr_; }
-
-  ExternalMemoryMgr* external_mem() { return &external_mem_; }
-
-  const dma::Config& dma_config() const { return dma_config_; }
-
- private:
-  HardwareDriver* driver_;
-  HwVersion version_;
-  dma::Config dma_config_;
-  dma::Inputs dma_event_;
-  dma::Inputs dma_hrc_;
-  MemoryMgr mem_mgr_;
-  BytesBuffer current_program_;
-  bool current_program_learn_en_;
-  ExternalMemoryMgr external_mem_;
-
-  // infos on memory that need to be allocated for multi pass program
-  MultiPassMemory multi_pass_memory_;
-  // infos on memory that need to be allocated for inference
-  InferenceMemory inference_memory_;
-
-  PipelineState pipeline_state_;
-
-  // Initialization helpers
-  void init();
-
-  // pipeline helper
-  std::vector<TensorUniquePtr> forward_loop(
-      const std::vector<TensorConstPtr>& inputs,
-      const std::vector<int32_t>* labels);
-
-  // DMA helpers
-  const dma::Inputs& select_dma_engine(bool is_hrc);
-  bool clock_counter_enabled();
-};
-
-}  // namespace akida
+#pragma once
+
+#include <cstddef>
+#include <cstdint>
+#include <memory>
+#include <queue>
+#include <vector>
+
+#include "akida/hardware_device.h"
+#include "akida/hw_version.h"
+#include "akida/np.h"
+#include "akida/tensor.h"
+#include "engine/dma.h"
+#include "infra/hardware_driver.h"
+
+#include "device_memory.h"
+#include "dma_engine.h"
+#include "external_mem_mgr.h"
+#include "memory_mgr.h"
+#include "multipass_memory.h"
+#include "pipeline_state.h"
+
+namespace akida {
+
+namespace dma {
+// forward declarations
+enum class Target;
+}  // namespace dma
+
+class HardwareDeviceImpl final : public HardwareDevice {
+ public:
+  HardwareDeviceImpl(HardwareDriver* driver);
+
+  ~HardwareDeviceImpl();
+
+  HwVersion version() const override;
+
+  const char* desc() const override { return driver_->desc(); }
+
+  void pipeline(bool enable);
+
+  void toggle_clock_counter(bool enable) override;
+
+  uint32_t read_clock_counter() override;
+
+  uint32_t read_config_clock_counter() override;
+
+  void dma_config_write(const dma::w32* buffer, size_t buffer_size);
+
+  void dma_config_read(dma::w32* buffer, const struct np::Ident& np,
+                       dma::Target target, uint16_t addr_target_word,
+                       uint32_t nb_words);
+
+  void read_np_registers(uint32_t* output, const struct np::Ident& np,
+                         uint32_t nb_registers) override;
+
+  // Device fit
+  std::vector<TensorUniquePtr> fit(
+      const std::vector<TensorConstPtr>& inputs,
+      const std::vector<int32_t>& input_labels) override;
+
+  // Device forward
+  std::vector<TensorUniquePtr> forward(
+      const std::vector<TensorConstPtr>& inputs) override;
+
+  // Device predict
+  std::vector<TensorUniquePtr> predict(
+      const std::vector<TensorConstPtr>& inputs) override;
+
+  // Queue input
+  bool enqueue(const Tensor& input, const int32_t* label = nullptr) override;
+
+  // check for output
+  TensorUniquePtr fetch() override;
+
+  // apply rescale
+  DenseUniquePtr dequantize(const Dense& potentials) override;
+
+  // perform hardware device programming
+  void program(const uint8_t* program, size_t size) override;
+
+  size_t set_batch_size(size_t requested_batch_size,
+                        bool allocate_inputs) override;
+
+  // enable/disable learning mode
+  void toggle_learn(bool learn_en) override;
+
+  // unprogram current program
+  void unprogram() override;
+
+  // Return the memory used currently in the device
+  MemoryInfo memory() const override { return mem_mgr_.report(); }
+
+  void reset_top_memory() override { mem_mgr_.reset_top_usage(); }
+
+  const BytesBuffer& program() const override { return current_program_; }
+
+  bool learn_enabled() const override { return current_program_learn_en_; }
+
+  size_t learn_mem_size() const override;
+
+  void learn_mem(uint32_t* output_buffer) override;
+
+  void update_learn_mem(const uint32_t* input_buffer) override;
+
+  HardwareDriver* driver() const override { return driver_; }
+
+  MemoryMgr* mem() { return &mem_mgr_; }
+
+  ExternalMemoryMgr* external_mem() { return &external_mem_; }
+
+  const dma::Config& dma_config() const { return dma_config_; }
+
+ private:
+  HardwareDriver* driver_;
+  HwVersion version_;
+  dma::Config dma_config_;
+  dma::Inputs dma_event_;
+  dma::Inputs dma_hrc_;
+  MemoryMgr mem_mgr_;
+  BytesBuffer current_program_;
+  bool current_program_learn_en_;
+  ExternalMemoryMgr external_mem_;
+
+  // infos on memory that need to be allocated for multi pass program
+  MultiPassMemory multi_pass_memory_;
+  // infos on memory that need to be allocated for inference
+  InferenceMemory inference_memory_;
+
+  PipelineState pipeline_state_;
+
+  // Initialization helpers
+  void init();
+
+  // pipeline helper
+  std::vector<TensorUniquePtr> forward_loop(
+      const std::vector<TensorConstPtr>& inputs,
+      const std::vector<int32_t>* labels);
+
+  // DMA helpers
+  const dma::Inputs& select_dma_engine(bool is_hrc);
+  bool clock_counter_enabled();
+};
+
+}  // namespace akida
```

## akida/engine/src/hw_version.cpp

 * *Ordering differences only*

```diff
@@ -1,32 +1,32 @@
-#include "akida/hw_version.h"
-#include "akida/registers_top_level.h"
-
-#include <cstring>
-
-namespace akida {
-
-HwVersion read_hw_version(const HardwareDriver& driver) {
-  HwVersion version{0, 0, 0, 0};
-  // Try first to read IP revision from device
-  const auto top_level_reg_offset = driver.top_level_reg();
-  auto reg = driver.read32(top_level_reg_offset + REG_IP_VERSION);
-  auto vendor_id = static_cast<uint8_t>(get_field(reg, VENDOR_ID));
-  if (reg != 0) {
-    auto minor_rev = static_cast<uint8_t>(get_field(reg, MINOR_REV));
-    auto major_rev = static_cast<uint8_t>(get_field(reg, MAJOR_REV));
-    auto prod_id = static_cast<uint8_t>(get_field(reg, PROD_ID));
-    version = {vendor_id, prod_id, major_rev, minor_rev};
-  } else {
-    // Legacy device: rely instead on the information provided by the driver
-    auto driver_desc = driver.desc();
-    if (strstr(driver_desc, "NSoC_v2") != nullptr) {
-      version = NSoC_v2;
-
-    } else if (strstr(driver_desc, "NSoC_v1") != nullptr) {
-      version = NSoC_v1;
-    }
-  }
-  return version;
-}
-
-}  // namespace akida
+#include "akida/hw_version.h"
+#include "akida/registers_top_level.h"
+
+#include <cstring>
+
+namespace akida {
+
+HwVersion read_hw_version(const HardwareDriver& driver) {
+  HwVersion version{0, 0, 0, 0};
+  // Try first to read IP revision from device
+  const auto top_level_reg_offset = driver.top_level_reg();
+  auto reg = driver.read32(top_level_reg_offset + REG_IP_VERSION);
+  auto vendor_id = static_cast<uint8_t>(get_field(reg, VENDOR_ID));
+  if (reg != 0) {
+    auto minor_rev = static_cast<uint8_t>(get_field(reg, MINOR_REV));
+    auto major_rev = static_cast<uint8_t>(get_field(reg, MAJOR_REV));
+    auto prod_id = static_cast<uint8_t>(get_field(reg, PROD_ID));
+    version = {vendor_id, prod_id, major_rev, minor_rev};
+  } else {
+    // Legacy device: rely instead on the information provided by the driver
+    auto driver_desc = driver.desc();
+    if (strstr(driver_desc, "NSoC_v2") != nullptr) {
+      version = NSoC_v2;
+
+    } else if (strstr(driver_desc, "NSoC_v1") != nullptr) {
+      version = NSoC_v1;
+    }
+  }
+  return version;
+}
+
+}  // namespace akida
```

## akida/engine/src/input_conversion.cpp

 * *Ordering differences only*

```diff
@@ -1,37 +1,37 @@
-#include "akida/input_conversion.h"
-
-#include "akida/dense.h"
-#include "akida/sparse.h"
-
-#include "dma_events.h"
-#include "program_play.h"
-
-namespace akida {
-
-namespace conversion {
-const Sparse* as_sparse(const Tensor& input) {
-  return dynamic_cast<const DmaEvents*>(&input);
-}
-
-SparseUniquePtr to_sparse(const Dense& input, const uint8_t* program) {
-  const auto nb_dims = input.dimensions().size();
-  if (nb_dims != 3 && nb_dims != 1) {
-    panic("Sparse can only be 1D or 3D");
-  }
-  return to_dma_events(input, program::input_is_fnp(program));
-}
-
-const Dense* as_dense(const Tensor& input) {
-  return dynamic_cast<const Dense*>(&input);
-}
-
-DenseUniquePtr to_dense(const Sparse& input) {
-  return Dense::from_sparse(input, Dense::Layout::RowMajor);
-}
-
-bool dense_input_expected(const uint8_t* program) {
-  return program::input_is_dense(program);
-}
-
-}  // namespace conversion
-}  // namespace akida
+#include "akida/input_conversion.h"
+
+#include "akida/dense.h"
+#include "akida/sparse.h"
+
+#include "dma_events.h"
+#include "program_play.h"
+
+namespace akida {
+
+namespace conversion {
+const Sparse* as_sparse(const Tensor& input) {
+  return dynamic_cast<const DmaEvents*>(&input);
+}
+
+SparseUniquePtr to_sparse(const Dense& input, const uint8_t* program) {
+  const auto nb_dims = input.dimensions().size();
+  if (nb_dims != 3 && nb_dims != 1) {
+    panic("Sparse can only be 1D or 3D");
+  }
+  return to_dma_events(input, program::input_is_fnp(program));
+}
+
+const Dense* as_dense(const Tensor& input) {
+  return dynamic_cast<const Dense*>(&input);
+}
+
+DenseUniquePtr to_dense(const Sparse& input) {
+  return Dense::from_sparse(input, Dense::Layout::RowMajor);
+}
+
+bool dense_input_expected(const uint8_t* program) {
+  return program::input_is_dense(program);
+}
+
+}  // namespace conversion
+}  // namespace akida
```

## akida/engine/src/memory_mgr.cpp

 * *Ordering differences only*

```diff
@@ -1,77 +1,77 @@
-
-#include "memory_mgr.h"
-
-#include <cassert>
-#include <cstdint>
-
-#include "engine/dma.h"
-#include "infra/int_ops.h"
-#include "infra/system.h"
-
-namespace akida {
-
-MemoryMgr::MemoryInfo MemoryMgr::report() const {
-  auto current_memory = mem_offset_ - mem_base_offset_;
-  auto top_memory = mem_top_offset_ - mem_base_offset_;
-  return std::make_pair(current_memory, top_memory);
-}
-
-void MemoryMgr::reset_top_usage() { mem_top_offset_ = mem_offset_; }
-
-dma::addr MemoryMgr::alloc(size_t size) {
-  assert(size > 0 && "Cannot alloc size 0");
-  // base address must be aligned
-  const auto aligned_mem_offset = align_up(mem_offset_, dma::kAlignment);
-  // check that we have enough memory left
-  if (aligned_mem_offset + size > mem_bottom_offset_) {
-    panic("Out of memory (requested %u bytes, currently using %u bytes)",
-          static_cast<uint32_t>(size),
-          static_cast<uint32_t>(mem_offset_ - mem_base_offset_));
-  }
-  scratch_buf_.push_back({aligned_mem_offset, size});
-  mem_offset_ = aligned_mem_offset + static_cast<uint32_t>(size);
-  // update top memory usage if necessary
-  if (mem_offset_ > mem_top_offset_) {
-    mem_top_offset_ = mem_offset_;
-  }
-  return aligned_mem_offset;
-}
-
-void MemoryMgr::free(uint32_t addr) {
-  if (scratch_buf_.empty()) {
-    panic("Cannot free address %x", addr);
-  }
-  // reverse traverse vector, allocations probably happen in reverse order
-  for (auto it = scratch_buf_.rbegin(); it != scratch_buf_.rend(); it++) {
-    auto size = static_cast<uint32_t>(it->size);
-    if (it->addr == addr) {
-      // remove allocation. Note that erase takes a normal iterator, so we get
-      // to the base iterator and point to the previous element.
-      scratch_buf_.erase(it.base() - 1);
-      // if this is the last item, then update the mem_offset_ by checking the
-      // "highest" allocation in the list. This is necessary if the free had
-      // been done in "disorder".
-      if (addr + size == mem_offset_) {
-        // update mem_offset to the highest allocation
-        uint32_t highest_allocation = mem_base_offset_;
-        for (const auto& block : scratch_buf_) {
-          auto block_upper_limit =
-              block.addr + static_cast<uint32_t>(block.size);
-          if (block_upper_limit > highest_allocation) {
-            highest_allocation = block_upper_limit;
-          }
-        }
-        mem_offset_ = highest_allocation;
-      }
-      return;
-    }
-  }
-  panic("Address %x not found: cannot free", addr);
-}
-
-void MemoryMgr::reset() {
-  scratch_buf_.clear();
-  mem_offset_ = mem_base_offset_;
-}
-
-}  // namespace akida
+
+#include "memory_mgr.h"
+
+#include <cassert>
+#include <cstdint>
+
+#include "engine/dma.h"
+#include "infra/int_ops.h"
+#include "infra/system.h"
+
+namespace akida {
+
+MemoryMgr::MemoryInfo MemoryMgr::report() const {
+  auto current_memory = mem_offset_ - mem_base_offset_;
+  auto top_memory = mem_top_offset_ - mem_base_offset_;
+  return std::make_pair(current_memory, top_memory);
+}
+
+void MemoryMgr::reset_top_usage() { mem_top_offset_ = mem_offset_; }
+
+dma::addr MemoryMgr::alloc(size_t size) {
+  assert(size > 0 && "Cannot alloc size 0");
+  // base address must be aligned
+  const auto aligned_mem_offset = align_up(mem_offset_, dma::kAlignment);
+  // check that we have enough memory left
+  if (aligned_mem_offset + size > mem_bottom_offset_) {
+    panic("Out of memory (requested %u bytes, currently using %u bytes)",
+          static_cast<uint32_t>(size),
+          static_cast<uint32_t>(mem_offset_ - mem_base_offset_));
+  }
+  scratch_buf_.push_back({aligned_mem_offset, size});
+  mem_offset_ = aligned_mem_offset + static_cast<uint32_t>(size);
+  // update top memory usage if necessary
+  if (mem_offset_ > mem_top_offset_) {
+    mem_top_offset_ = mem_offset_;
+  }
+  return aligned_mem_offset;
+}
+
+void MemoryMgr::free(uint32_t addr) {
+  if (scratch_buf_.empty()) {
+    panic("Cannot free address %x", addr);
+  }
+  // reverse traverse vector, allocations probably happen in reverse order
+  for (auto it = scratch_buf_.rbegin(); it != scratch_buf_.rend(); it++) {
+    auto size = static_cast<uint32_t>(it->size);
+    if (it->addr == addr) {
+      // remove allocation. Note that erase takes a normal iterator, so we get
+      // to the base iterator and point to the previous element.
+      scratch_buf_.erase(it.base() - 1);
+      // if this is the last item, then update the mem_offset_ by checking the
+      // "highest" allocation in the list. This is necessary if the free had
+      // been done in "disorder".
+      if (addr + size == mem_offset_) {
+        // update mem_offset to the highest allocation
+        uint32_t highest_allocation = mem_base_offset_;
+        for (const auto& block : scratch_buf_) {
+          auto block_upper_limit =
+              block.addr + static_cast<uint32_t>(block.size);
+          if (block_upper_limit > highest_allocation) {
+            highest_allocation = block_upper_limit;
+          }
+        }
+        mem_offset_ = highest_allocation;
+      }
+      return;
+    }
+  }
+  panic("Address %x not found: cannot free", addr);
+}
+
+void MemoryMgr::reset() {
+  scratch_buf_.clear();
+  mem_offset_ = mem_base_offset_;
+}
+
+}  // namespace akida
```

## akida/engine/src/memory_mgr.h

 * *Ordering differences only*

```diff
@@ -1,47 +1,47 @@
-#pragma once
-#include <cstddef>
-#include <utility>
-#include <vector>
-
-#include "engine/dma.h"
-
-namespace akida {
-
-class MemoryMgr {
- public:
-  struct Allocation {
-    dma::addr addr;
-    size_t size;
-  };
-
-  explicit MemoryMgr(const dma::addr base, const dma::addr size)
-      : mem_base_offset_(base),
-        mem_offset_(base),
-        mem_top_offset_(base),
-        mem_bottom_offset_(base + size) {}
-
-  // This will give DDR memory (e.g.: to use for FNP2 filters).
-  dma::addr alloc(size_t byte_size);
-
-  // This will mark previously allocated memory as free
-  void free(dma::addr addr);
-
-  // Return the memory used currently in the device
-  using MemoryInfo = std::pair<uint32_t, uint32_t>;
-  MemoryInfo report() const;
-
-  // reset the top usage to the current usage
-  void reset_top_usage();
-
-  // Free all memory allocations
-  void reset();
-
- private:
-  const dma::addr mem_base_offset_;
-  dma::addr mem_offset_;
-  dma::addr mem_top_offset_;
-  const dma::addr mem_bottom_offset_;
-  std::vector<Allocation> scratch_buf_;
-};
-
-}  // namespace akida
+#pragma once
+#include <cstddef>
+#include <utility>
+#include <vector>
+
+#include "engine/dma.h"
+
+namespace akida {
+
+class MemoryMgr {
+ public:
+  struct Allocation {
+    dma::addr addr;
+    size_t size;
+  };
+
+  explicit MemoryMgr(const dma::addr base, const dma::addr size)
+      : mem_base_offset_(base),
+        mem_offset_(base),
+        mem_top_offset_(base),
+        mem_bottom_offset_(base + size) {}
+
+  // This will give DDR memory (e.g.: to use for FNP2 filters).
+  dma::addr alloc(size_t byte_size);
+
+  // This will mark previously allocated memory as free
+  void free(dma::addr addr);
+
+  // Return the memory used currently in the device
+  using MemoryInfo = std::pair<uint32_t, uint32_t>;
+  MemoryInfo report() const;
+
+  // reset the top usage to the current usage
+  void reset_top_usage();
+
+  // Free all memory allocations
+  void reset();
+
+ private:
+  const dma::addr mem_base_offset_;
+  dma::addr mem_offset_;
+  dma::addr mem_top_offset_;
+  const dma::addr mem_bottom_offset_;
+  std::vector<Allocation> scratch_buf_;
+};
+
+}  // namespace akida
```

## akida/engine/src/memory_utils.cpp

 * *Ordering differences only*

```diff
@@ -1,23 +1,23 @@
-#include "memory_utils.h"
-
-#include <cstddef>
-
-#include "engine/dma.h"
-
-namespace akida {
-
-bool accessible_from_akida(const void* id, const HardwareDriver& driver) {
-  if ((sizeof(id) == sizeof(dma::addr)) &&
-      (driver.akida_visible_memory() != 0)) {
-    // we can safely cast because dma::addr and pointer types have the same size
-    const auto addr32 = to_dma_addr(id);
-    // if the address is in visible data range we can use it directly
-    if (addr32 >= driver.akida_visible_memory() &&
-        addr32 <= (driver.akida_visible_memory() +
-                   driver.akida_visible_memory_size())) {
-      return true;
-    }
-  }
-  return false;
-}
-}  // namespace akida
+#include "memory_utils.h"
+
+#include <cstddef>
+
+#include "engine/dma.h"
+
+namespace akida {
+
+bool accessible_from_akida(const void* id, const HardwareDriver& driver) {
+  if ((sizeof(id) == sizeof(dma::addr)) &&
+      (driver.akida_visible_memory() != 0)) {
+    // we can safely cast because dma::addr and pointer types have the same size
+    const auto addr32 = to_dma_addr(id);
+    // if the address is in visible data range we can use it directly
+    if (addr32 >= driver.akida_visible_memory() &&
+        addr32 <= (driver.akida_visible_memory() +
+                   driver.akida_visible_memory_size())) {
+      return true;
+    }
+  }
+  return false;
+}
+}  // namespace akida
```

## akida/engine/src/memory_utils.h

 * *Ordering differences only*

```diff
@@ -1,18 +1,18 @@
-#pragma once
-
-#include <cassert>
-#include <cstddef>
-
-#include "engine/dma.h"
-#include "infra/hardware_driver.h"
-
-namespace akida {
-
-inline dma::addr to_dma_addr(const void* id) {
-  assert(sizeof(id) == sizeof(dma::addr));
-  return static_cast<dma::addr>(reinterpret_cast<size_t>((id)));
-}
-
-bool accessible_from_akida(const void* id, const HardwareDriver& driver);
-
-}  // namespace akida
+#pragma once
+
+#include <cassert>
+#include <cstddef>
+
+#include "engine/dma.h"
+#include "infra/hardware_driver.h"
+
+namespace akida {
+
+inline dma::addr to_dma_addr(const void* id) {
+  assert(sizeof(id) == sizeof(dma::addr));
+  return static_cast<dma::addr>(reinterpret_cast<size_t>((id)));
+}
+
+bool accessible_from_akida(const void* id, const HardwareDriver& driver);
+
+}  // namespace akida
```

## akida/engine/src/multipass_memory.cpp

 * *Ordering differences only*

```diff
@@ -1,34 +1,34 @@
-#include "multipass_memory.h"
-
-#include "dma_desc_format.h"
-#include "memory_mgr.h"
-#include "program_play.h"
-
-namespace akida {
-
-void MultiPassMemory::alloc_memory(MemoryMgr* memory_mgr,
-                                   const uint8_t* program) {
-  dummy_output_addr = memory_mgr->alloc(sizeof(dma::w32));
-  // descriptor size depend on input DMA used
-  const auto descriptor_size = program::input_is_dense(program)
-                                   ? dma::hrc::DESC_BYTE_SIZE
-                                   : dma::event::DESC_BYTE_SIZE;
-  hw_generated_descriptor_addr = memory_mgr->alloc(descriptor_size);
-  hw_generated_descriptor_out_addr = memory_mgr->alloc(sizeof(dma::w32));
-}
-
-void MultiPassMemory::free_memory(MemoryMgr* memory_mgr) {
-  // we need to free dummy output
-  memory_mgr->free(dummy_output_addr);
-  // we need to free HW generated descriptor
-  memory_mgr->free(hw_generated_descriptor_addr);
-  // we need to free replay OB payload
-  memory_mgr->free(hw_generated_descriptor_out_addr);
-}
-
-void MultiPassMemory::update_learn_descriptor_addr(
-    dma::addr learn_descriptor_address) {
-  learn_descriptor_addr = learn_descriptor_address;
-}
-
-}  // namespace akida
+#include "multipass_memory.h"
+
+#include "dma_desc_format.h"
+#include "memory_mgr.h"
+#include "program_play.h"
+
+namespace akida {
+
+void MultiPassMemory::alloc_memory(MemoryMgr* memory_mgr,
+                                   const uint8_t* program) {
+  dummy_output_addr = memory_mgr->alloc(sizeof(dma::w32));
+  // descriptor size depend on input DMA used
+  const auto descriptor_size = program::input_is_dense(program)
+                                   ? dma::hrc::DESC_BYTE_SIZE
+                                   : dma::event::DESC_BYTE_SIZE;
+  hw_generated_descriptor_addr = memory_mgr->alloc(descriptor_size);
+  hw_generated_descriptor_out_addr = memory_mgr->alloc(sizeof(dma::w32));
+}
+
+void MultiPassMemory::free_memory(MemoryMgr* memory_mgr) {
+  // we need to free dummy output
+  memory_mgr->free(dummy_output_addr);
+  // we need to free HW generated descriptor
+  memory_mgr->free(hw_generated_descriptor_addr);
+  // we need to free replay OB payload
+  memory_mgr->free(hw_generated_descriptor_out_addr);
+}
+
+void MultiPassMemory::update_learn_descriptor_addr(
+    dma::addr learn_descriptor_address) {
+  learn_descriptor_addr = learn_descriptor_address;
+}
+
+}  // namespace akida
```

## akida/engine/src/multipass_memory.h

 * *Ordering differences only*

```diff
@@ -1,25 +1,25 @@
-#pragma once
-
-#include "engine/dma.h"
-
-#include "memory_mgr.h"
-
-namespace akida {
-
-struct MultiPassMemory {
-  // address of one 32bit word needed by multi pass program to write output of
-  // dummy descriptors
-  dma::addr dummy_output_addr;
-  // address of multi pass learning descriptor;
-  dma::addr learn_descriptor_addr;
-  // address of HW generated descriptors when using HW address generation mode
-  dma::addr hw_generated_descriptor_addr;
-  // address used for temporary storage between replay loops for OB events.
-  dma::addr hw_generated_descriptor_out_addr;
-
-  void alloc_memory(MemoryMgr* memory_mgr, const uint8_t* program);
-  void free_memory(MemoryMgr* memory_mgr);
-  void update_learn_descriptor_addr(dma::addr learn_descriptor_address);
-};
-
-}  // namespace akida
+#pragma once
+
+#include "engine/dma.h"
+
+#include "memory_mgr.h"
+
+namespace akida {
+
+struct MultiPassMemory {
+  // address of one 32bit word needed by multi pass program to write output of
+  // dummy descriptors
+  dma::addr dummy_output_addr;
+  // address of multi pass learning descriptor;
+  dma::addr learn_descriptor_addr;
+  // address of HW generated descriptors when using HW address generation mode
+  dma::addr hw_generated_descriptor_addr;
+  // address used for temporary storage between replay loops for OB events.
+  dma::addr hw_generated_descriptor_out_addr;
+
+  void alloc_memory(MemoryMgr* memory_mgr, const uint8_t* program);
+  void free_memory(MemoryMgr* memory_mgr);
+  void update_learn_descriptor_addr(dma::addr learn_descriptor_address);
+};
+
+}  // namespace akida
```

## akida/engine/src/pipeline_state.h

 * *Ordering differences only*

```diff
@@ -1,74 +1,74 @@
-#pragma once
-
-#include <cstddef>
-#include <cstdint>
-#include <queue>
-
-namespace akida {
-
-class PipelineState {
- public:
-  // utility struct to store infos about dma jobs
-  struct dma_job {
-    uint32_t output_address;
-    const void* input;
-    uint16_t id;
-  };
-  // utility struct to store infos about pipeline state
-  struct slot {
-    uint16_t job_id;
-    size_t index;
-  };
-
-  PipelineState()
-      : job_id_generated_(0),
-        last_job_id_fetched_(0),
-        current_index_(0),
-        pipeline_size_(0) {}
-  inline bool full() const { return current_jobs_.size() == pipeline_size_; }
-  inline bool empty() const { return current_jobs_.empty(); }
-  inline size_t max_size() const { return pipeline_size_; }
-  void enqueue_job(uint16_t id, uint32_t output_address,
-                   const void* input_address) {
-    current_jobs_.push(dma_job{output_address, input_address, id});
-  }
-  dma_job pop_job() {
-    auto job = current_jobs_.front();
-    current_jobs_.pop();
-    // update last_job_id_fetched
-    last_job_id_fetched_ = job.id;
-    return job;
-  }
-  slot reserve_job() {
-    // increment job id first because it must start at 1 after reset to
-    // differenciate it from last_job_id_fetched which start at 0
-    ++job_id_generated_;
-    ++current_index_;
-    if (current_index_ >= pipeline_size_) {
-      current_index_ = 0;
-    }
-    return slot{job_id_generated_, current_index_};
-  }
-  uint16_t last_job_fetched() const { return last_job_id_fetched_; }
-  // reset should be called when dma is reset, because last_job_id_fetched may
-  // not correspond to dma last job id processed
-  // It also defines the max number of element in the pipeline
-  void reset(uint16_t last_job_id, size_t pipeline_size) {
-    last_job_id_fetched_ = last_job_id;
-    job_id_generated_ = last_job_id;
-    current_index_ = 0;
-    pipeline_size_ = pipeline_size;
-    // clear the queue
-    current_jobs_ = std::queue<dma_job>();
-  }
-
- protected:
-  std::queue<dma_job> current_jobs_;
-
-  uint16_t job_id_generated_;
-  uint16_t last_job_id_fetched_;
-  size_t current_index_;
-  size_t pipeline_size_;
-};
-
-}  // namespace akida
+#pragma once
+
+#include <cstddef>
+#include <cstdint>
+#include <queue>
+
+namespace akida {
+
+class PipelineState {
+ public:
+  // utility struct to store infos about dma jobs
+  struct dma_job {
+    uint32_t output_address;
+    const void* input;
+    uint16_t id;
+  };
+  // utility struct to store infos about pipeline state
+  struct slot {
+    uint16_t job_id;
+    size_t index;
+  };
+
+  PipelineState()
+      : job_id_generated_(0),
+        last_job_id_fetched_(0),
+        current_index_(0),
+        pipeline_size_(0) {}
+  inline bool full() const { return current_jobs_.size() == pipeline_size_; }
+  inline bool empty() const { return current_jobs_.empty(); }
+  inline size_t max_size() const { return pipeline_size_; }
+  void enqueue_job(uint16_t id, uint32_t output_address,
+                   const void* input_address) {
+    current_jobs_.push(dma_job{output_address, input_address, id});
+  }
+  dma_job pop_job() {
+    auto job = current_jobs_.front();
+    current_jobs_.pop();
+    // update last_job_id_fetched
+    last_job_id_fetched_ = job.id;
+    return job;
+  }
+  slot reserve_job() {
+    // increment job id first because it must start at 1 after reset to
+    // differenciate it from last_job_id_fetched which start at 0
+    ++job_id_generated_;
+    ++current_index_;
+    if (current_index_ >= pipeline_size_) {
+      current_index_ = 0;
+    }
+    return slot{job_id_generated_, current_index_};
+  }
+  uint16_t last_job_fetched() const { return last_job_id_fetched_; }
+  // reset should be called when dma is reset, because last_job_id_fetched may
+  // not correspond to dma last job id processed
+  // It also defines the max number of element in the pipeline
+  void reset(uint16_t last_job_id, size_t pipeline_size) {
+    last_job_id_fetched_ = last_job_id;
+    job_id_generated_ = last_job_id;
+    current_index_ = 0;
+    pipeline_size_ = pipeline_size;
+    // clear the queue
+    current_jobs_ = std::queue<dma_job>();
+  }
+
+ protected:
+  std::queue<dma_job> current_jobs_;
+
+  uint16_t job_id_generated_;
+  uint16_t last_job_id_fetched_;
+  size_t current_index_;
+  size_t pipeline_size_;
+};
+
+}  // namespace akida
```

## akida/engine/src/program_memory_info.cpp

 * *Ordering differences only*

```diff
@@ -1,84 +1,84 @@
-#include <cstddef>
-#include <cstdint>
-
-#include "akida/program_memory_info.h"
-#include "engine/dma.h"
-#include "infra/int_ops.h"
-
-#include "dma_desc_format.h"
-#include "program_play.h"
-
-namespace akida {
-
-size_t input_memory_required(const uint8_t* program) {
-  if (program == nullptr) {
-    return 0;
-  }
-
-  const auto* dimensions = program::input_dims(program);
-  const size_t max_num_elements = dimensions[0] * dimensions[1] * dimensions[2];
-  // With dense inputs, each element uses 1 byte, so it is just the number of
-  // elements. With sparse inputs, each element uses dma::kSparseEventByteSize
-  return program::input_is_dense(program)
-             ? max_num_elements
-             : max_num_elements * dma::kSparseEventByteSize;
-}
-
-size_t output_memory_required(const uint8_t* program) {
-  if (program == nullptr) {
-    return 0;
-  }
-
-  const auto dimensions = program::output_dims(program);
-  const auto nb_items = dimensions[0] * dimensions[1] * dimensions[2];
-
-  const auto format = program::output_format(program);
-
-  uint32_t item_size;
-  if (format == dma::OutputFormat::DenseActivations) {
-    // dense activations use 1 byte per item
-    item_size = 1;
-  } else if (format == dma::OutputFormat::DensePotentials) {
-    // dense potentials use 32 bits per item
-    item_size = sizeof(uint32_t);
-  } else {
-    // other format are sparse, they use kSparseEventByteSize per item
-    item_size = dma::kSparseEventByteSize;
-  }
-
-  // Output requires a DMA header, and must be aligned
-  return align_up(item_size * nb_items + dma::kOutputHeaderByteSize,
-                  dma::kAlignment);
-}
-
-size_t input_descriptor_memory_required(const uint8_t* program) {
-  // dense inputs use HRC dma, sparse use AE (event) dma
-  return program::input_is_dense(program) ? dma::hrc::DESC_BYTE_SIZE
-                                          : dma::event::DESC_BYTE_SIZE;
-}
-
-size_t program_descriptors_memory_required(const uint8_t* program) {
-  return (program::number_of_program_descriptors_required(program) +
-          program::number_of_extra_program_descriptors_required(program)) *
-         dma::config::DESC_BYTE_SIZE;
-}
-
-size_t program_data_memory_required(const uint8_t* program) {
-  return program::program_data_required_memory(program);
-}
-
-size_t extra_program_memory_required(const uint8_t* program) {
-  size_t result = program::fnp2_tracks_byte_size(program);
-  if (program::is_multi_pass(program)) {
-    // extra data are only required for multi pass: 1 word for dummy descriptor
-    // output, 1 extra input descriptor generated by HW and 1 word for this
-    // descriptor output There is an extra word used for dummy descriptors
-    result += sizeof(dma::w32) +
-              (program::input_is_dense(program) ? dma::hrc::DESC_BYTE_SIZE
-                                                : dma::event::DESC_BYTE_SIZE) +
-              sizeof(dma::w32);
-  }
-  return result;
-}
-
-}  // namespace akida
+#include <cstddef>
+#include <cstdint>
+
+#include "akida/program_memory_info.h"
+#include "engine/dma.h"
+#include "infra/int_ops.h"
+
+#include "dma_desc_format.h"
+#include "program_play.h"
+
+namespace akida {
+
+size_t input_memory_required(const uint8_t* program) {
+  if (program == nullptr) {
+    return 0;
+  }
+
+  const auto* dimensions = program::input_dims(program);
+  const size_t max_num_elements = dimensions[0] * dimensions[1] * dimensions[2];
+  // With dense inputs, each element uses 1 byte, so it is just the number of
+  // elements. With sparse inputs, each element uses dma::kSparseEventByteSize
+  return program::input_is_dense(program)
+             ? max_num_elements
+             : max_num_elements * dma::kSparseEventByteSize;
+}
+
+size_t output_memory_required(const uint8_t* program) {
+  if (program == nullptr) {
+    return 0;
+  }
+
+  const auto dimensions = program::output_dims(program);
+  const auto nb_items = dimensions[0] * dimensions[1] * dimensions[2];
+
+  const auto format = program::output_format(program);
+
+  uint32_t item_size;
+  if (format == dma::OutputFormat::DenseActivations) {
+    // dense activations use 1 byte per item
+    item_size = 1;
+  } else if (format == dma::OutputFormat::DensePotentials) {
+    // dense potentials use 32 bits per item
+    item_size = sizeof(uint32_t);
+  } else {
+    // other format are sparse, they use kSparseEventByteSize per item
+    item_size = dma::kSparseEventByteSize;
+  }
+
+  // Output requires a DMA header, and must be aligned
+  return align_up(item_size * nb_items + dma::kOutputHeaderByteSize,
+                  dma::kAlignment);
+}
+
+size_t input_descriptor_memory_required(const uint8_t* program) {
+  // dense inputs use HRC dma, sparse use AE (event) dma
+  return program::input_is_dense(program) ? dma::hrc::DESC_BYTE_SIZE
+                                          : dma::event::DESC_BYTE_SIZE;
+}
+
+size_t program_descriptors_memory_required(const uint8_t* program) {
+  return (program::number_of_program_descriptors_required(program) +
+          program::number_of_extra_program_descriptors_required(program)) *
+         dma::config::DESC_BYTE_SIZE;
+}
+
+size_t program_data_memory_required(const uint8_t* program) {
+  return program::program_data_required_memory(program);
+}
+
+size_t extra_program_memory_required(const uint8_t* program) {
+  size_t result = program::fnp2_tracks_byte_size(program);
+  if (program::is_multi_pass(program)) {
+    // extra data are only required for multi pass: 1 word for dummy descriptor
+    // output, 1 extra input descriptor generated by HW and 1 word for this
+    // descriptor output There is an extra word used for dummy descriptors
+    result += sizeof(dma::w32) +
+              (program::input_is_dense(program) ? dma::hrc::DESC_BYTE_SIZE
+                                                : dma::event::DESC_BYTE_SIZE) +
+              sizeof(dma::w32);
+  }
+  return result;
+}
+
+}  // namespace akida
```

## akida/engine/src/program_play.cpp

 * *Ordering differences only*

```diff
@@ -1,734 +1,734 @@
-#include "program_play.h"
-
-#include <cassert>
-#include <cstring>
-
-#include "akida/hardware_device.h"
-#include "akida/hw_version.h"
-#include "akida/shape.h"
-#include "akida/version.h"
-#include "engine/akida_device_program_fb_generated.h"
-#include "engine/dma_config_ops.h"
-#include "flatbuffers/flatbuffers.h"
-#include "infra/hardware_driver.h"
-
-#include "dma_desc_format.h"
-#include "dma_engine_ops.h"
-#include "external_mem_mgr.h"
-#include "fnp2_mem_conf_reg.h"
-
-namespace akida {
-namespace program {
-
-static void check_device_version(const HardwareDevice& device,
-                                 const fb::Program& program) {
-  // verify device version matches with program
-  auto* prog_dev_version = program.device_version();
-  auto dev_version = device.version();
-
-  bool valid_version =
-      dev_version.vendor_id == prog_dev_version->vendor_id() &&
-      dev_version.product_id == prog_dev_version->product_id() &&
-      dev_version.major_rev == prog_dev_version->major_rev() &&
-      dev_version.minor_rev == prog_dev_version->minor_rev();
-  if (!valid_version) {
-    panic("Program device version and device version are not compatible");
-  }
-}
-
-void verify(const HardwareDeviceImpl& device, const uint8_t* data,
-            size_t size) {
-  if (!data) {
-    panic("Program is null");
-  }
-  // build program and verify it
-  auto* program = fb::GetProgram(data);
-  flatbuffers::Verifier verifier(data, size);
-  if (!program || !program->Verify(verifier)) {
-    panic("Unable to parse program");
-  }
-  // Check that the akida version this program was compiled with matches the
-  // current version.
-  const auto& program_version = program->version()->c_str();
-  const auto& lib_version = version();
-  if (strcmp(program_version, lib_version) != 0) {
-    panic("Program version [%s] does not match library version [%s]",
-          program_version, lib_version);
-  }
-  check_device_version(device, *program);
-}
-
-static bool use_fnp3_for_learning(const fb::Program& program) {
-  return program.learning_layer() != nullptr &&
-         program.learning_layer()->ram()->np_tracks() != nullptr &&
-         program.learning_layer()->ram()->fnp2_track() == nullptr;
-}
-
-static void rewind_fnp2_track(HardwareDeviceImpl* device,
-                              const fb::Fnp2FilterTrack& track) {
-  device->external_mem()->release(track.data()->data());
-}
-
-static void rewind_np_track(HardwareDeviceImpl* device,
-                            const fb::NpTrack& track, bool multi_pass) {
-  if (multi_pass) {
-    // in multi pass, free config header allocated with track as id
-    device->external_mem()->release(track.data()->data());
-  }
-}
-
-static void rewind_record(HardwareDeviceImpl* device, const fb::Record& record,
-                          bool multi_pass) {
-  // rewind fnp2 track if it is there
-  const auto* fnp2_track = record.fnp2_track();
-  if (fnp2_track) {
-    rewind_fnp2_track(device, *fnp2_track);
-  }
-  // rewind all normal tracks
-  const auto* np_tracks = record.np_tracks();
-  uint32_t np_tracks_size = np_tracks->size();
-  for (int i = np_tracks_size - 1; i >= 0; i--) {
-    const auto& np_track = *np_tracks->Get(i);
-    rewind_np_track(device, np_track, multi_pass);
-  }
-}
-
-static void write_np_track_descriptor(HardwareDriver* driver,
-                                      dma::addr track_addr_on_device,
-                                      uint32_t track_word_size,
-                                      dma::addr descriptor_address) {
-  // format descriptor
-  constexpr uint32_t output_addr = 0;  // not used for write
-  auto descriptor = dma::format_config_desc(dma::kDescConfigDirectionWrite,
-                                            track_addr_on_device, output_addr,
-                                            track_word_size);
-  // write descriptor in its place
-  driver->write(descriptor_address, descriptor.data(),
-                descriptor.size() * sizeof(dma::Descriptor::value_type));
-}
-
-static dma::addr write_track_on_device(HardwareDeviceImpl* device,
-                                       const fb::NpTrack& track) {
-  const auto* buffer = track.data();
-  auto buffer_bytes_size = buffer->size() * sizeof(uint32_t);
-  // put buffer on device, and get its address
-  auto buf_in_mem = device->external_mem()->track_and_put_on_device_if_required(
-      buffer->data(), buffer_bytes_size);
-
-  return buf_in_mem;
-}
-
-static void generate_reading_descriptor_from_np_track(
-    HardwareDeviceImpl* device, const fb::NpTrack& track) {
-  const auto track_addr = device->external_mem()->tracked(track.data()->data());
-  const auto input_addr = track_addr;
-  const auto output_addr = track_addr + dma::kConfigWritePacketOffset;
-
-  // format descriptor
-  const auto descriptor =
-      dma::format_config_desc(dma::kDescConfigDirectionRead, input_addr,
-                              output_addr, dma::kConfigWriteHdrWordLen);
-  // enqueue it at extra descriptor location
-  dma::enqueue_extra_descriptor(device->driver(), device->dma_config(),
-                                descriptor);
-}
-
-static void play_fnp2_track(HardwareDeviceImpl* device,
-                            const fb::Fnp2FilterTrack& track) {
-  auto* driver = device->driver();
-  const auto* buffer = track.data();
-  // alloc and write FNP2 filter data
-  uint32_t address =
-      device->external_mem()->track_and_put_on_device_if_required(
-          buffer->data(), buffer->size() * sizeof(uint32_t));
-
-  // Now write DDR address used for this NP in the dedicated conf register
-  // Note that there are 4 registers where the weights adress can be stored.
-  // This works because currently existing mesh designs only contain one node
-  // with 4 FNPs. Each NP will use the content of the register indexed by the ID
-  // of the NP.
-  // If at some point a mesh is created with a different layout, this might
-  // raise an issue.
-  // Also, this means that in multipass this register can only be used once per
-  // program, and the FNP2 cannot be reused later, because that would require
-  // updating the register value with another address, and there is no way to do
-  // that.
-  const auto np_id = track.np()->id();
-  auto fnp2_mem_conf_reg_addr =
-      fnp2_memory_conf(driver->top_level_reg(), np_id);
-  driver->write32(fnp2_mem_conf_reg_addr, address);
-}
-
-static dma::addr play_record(HardwareDeviceImpl* device,
-                             const fb::Record& record, bool single_pass) {
-  dma::addr descriptor_address = 0;
-  // play all np tracks
-  const auto* np_tracks = record.np_tracks();
-  int np_tracks_size = np_tracks->size();
-  for (int i = 0; i < np_tracks_size; i++) {
-    const auto& np_track = *np_tracks->Get(i);
-    assert(np_track.data()->size() > 0);
-
-    // write track data
-    auto track_address = write_track_on_device(device, np_track);
-    // generate descriptor
-    auto descriptor =
-        dma::format_config_desc(dma::kDescConfigDirectionWrite, track_address,
-                                0, np_track.data()->size());
-    // enqueue descriptor
-    descriptor_address = dma::enqueue_descriptor(
-        device->driver(), device->dma_config().engine, descriptor);
-    if (single_pass) {
-      // in single pass, we need to wait for descriptor to complete
-      dma::wait_config_dma_descriptor_complete(device->driver(),
-                                               device->dma_config());
-      // then release track
-      device->external_mem()->release(np_track.data()->data());
-    }
-  }
-
-  // play fnp2 track if there is one
-  const auto* fnp2_track = record.fnp2_track();
-  if (fnp2_track) {
-    play_fnp2_track(device, *fnp2_track);
-  }
-
-  return descriptor_address;
-}
-
-static void generate_reading_descriptor_from_record(HardwareDeviceImpl* device,
-                                                    const fb::Record& record) {
-  if (record.fnp2_track())
-    panic("Cannot use descriptors to read the FNP2 memory.");
-  // play all normal tracks
-  const auto* np_tracks = record.np_tracks();
-  assert(np_tracks->size() == 1 &&
-         "Learning should use a single NP, so there should be a single track");
-  generate_reading_descriptor_from_np_track(device, *np_tracks->Get(0));
-}
-
-static void play_epg_track(HardwareDriver* driver, uint32_t epg_base,
-                           uint32_t address, uint32_t data) {
-  driver->write32(epg_base + address, data);
-}
-
-void rewind(HardwareDeviceImpl* device, const uint8_t* program_data) {
-  auto* program = fb::GetProgram(program_data);
-
-  const auto& passes = *program->passes();
-  int passes_size = passes.size();
-  bool multi_pass = passes_size > 1;
-
-  const auto learn = program->learning_layer();
-  if (learn) {
-    if (multi_pass) {
-      // in multi pass, both learning & inference registers are written to the
-      // device
-      rewind_record(device, *learn->learning_registers(), multi_pass);
-      rewind_record(device, *learn->inference_registers(), multi_pass);
-    } else {
-      if (device->learn_enabled()) {
-        rewind_record(device, *learn->learning_registers(), multi_pass);
-      } else {
-        rewind_record(device, *learn->inference_registers(), multi_pass);
-      }
-    }
-    rewind_record(device, *learn->ram(), multi_pass);
-  }
-
-  // rewind in reverse order
-  for (int i = passes_size - 1; i >= 0; i--) {
-    const auto& layer_records = *passes[i]->records();
-    int cur_pass_size = layer_records.size();
-    for (int j = cur_pass_size - 1; j >= 0; j--) {
-      const auto& record = *layer_records[j];
-      rewind_record(device, record, multi_pass);
-    }
-  }
-
-  if (multi_pass) {
-    // free up dummy config header
-    device->external_mem()->release(program->dummy_desc_hdr());
-  }
-}
-
-static dma::addr dma_config_header_dummy(HardwareDeviceImpl* device,
-                                         const akida::fb::Program* program) {
-  auto dummy_header = program->dummy_desc_hdr();
-  // make sure flatbuffer struct is the same size as header
-  static_assert(sizeof(*dummy_header) == dma::kConfigWritePacketOffset,
-                "DmaConfigHeader should be the same size as "
-                "kConfigWriteHdrWordLen");
-  // put buffer on device, and get its address
-  auto mem = device->external_mem()->track_and_put_on_device_if_required(
-      dummy_header, sizeof(*dummy_header));
-
-  return mem;
-}
-
-static void write_dummy_descs(HardwareDeviceImpl* device, dma::addr dummy_input,
-                              dma::addr dummy_output,
-                              uint32_t num_dummy_descs) {
-  // Dummy descriptor is a read of size 1. Descriptor size is the header size
-  auto dummy_desc =
-      dma::format_config_desc(dma::kDescConfigDirectionRead, dummy_input,
-                              dummy_output, dma::kConfigWriteHdrWordLen);
-  for (uint32_t j = 0; j < num_dummy_descs; j++) {
-    dma::enqueue_descriptor(device->driver(), device->dma_config().engine,
-                            dummy_desc);
-  }
-}
-
-static inline uint32_t epg_reg_base(const uint32_t top_level_reg_base) {
-  constexpr uint32_t EPG_REG_BASE = 0x00040000;
-  return top_level_reg_base + EPG_REG_BASE;
-}
-
-static void play_epg(HardwareDeviceImpl* device, const fb::Program* program) {
-  // Apply EPG program
-  const auto* epg_tracks = program->epg_tracks();
-  if (epg_tracks) {
-    auto epg_tracks_size = epg_tracks->size();
-    auto driver = device->driver();
-    auto epg_base = epg_reg_base(driver->top_level_reg());
-    for (uint32_t i = 0; i < epg_tracks_size; i++) {
-      const auto& epg_track = epg_tracks->Get(i);
-      play_epg_track(driver, epg_base, epg_track->address(), epg_track->data());
-    }
-  }
-}
-
-void play_single_pass(HardwareDeviceImpl* device, const uint8_t* program_data) {
-  auto* program = fb::GetProgram(program_data);
-  const auto& passes = *program->passes();
-  assert(passes.size() == 1);
-  const auto& layer_records = *passes[0]->records();
-  int records_size = layer_records.size();
-
-  // play all records
-  for (int i = 0; i < records_size; i++) {
-    const auto& record = *layer_records[i];
-    play_record(device, record, true);
-  }
-
-  const auto learn = program->learning_layer();
-  if (learn) {
-    const auto& registers = device->learn_enabled()
-                                ? *learn->learning_registers()
-                                : *learn->inference_registers();
-    play_record(device, registers, true);
-    play_record(device, *learn->ram(), true);
-  }
-  play_epg(device, program);
-}
-
-void play_multi_pass(HardwareDeviceImpl* device, const uint8_t* program_data,
-                     MultiPassMemory* multipass_memory) {
-  auto* program = fb::GetProgram(program_data);
-  const auto learn = program->learning_layer();
-  const auto& passes = *program->passes();
-  uint32_t passes_size = passes.size();
-  // In multi pass mode, there will always be at least 2 passes
-  assert(passes_size >= 2);
-
-  // estimate memory required to hold passes descriptors.
-  const auto max_num_desc_pass = program->max_num_desc();
-
-  // use program to allocate dummy config, input and output space
-  auto dummy_input = dma_config_header_dummy(device, program);
-
-  uint32_t np_tracks_played = 0;
-
-  // now that we have the memory, we can fill the descriptors
-  for (uint32_t i = 0; i < passes_size; i++) {
-    const auto& layer_records = *passes[i]->records();
-    uint32_t records_size = layer_records.size();
-    np_tracks_played = 0;
-    for (uint32_t j = 0; j < records_size; j++) {
-      auto* record = layer_records[j];
-
-      // get number of NP tracks (corresponding to number of DMA descriptors).
-      uint32_t np_tracks_size = record->np_tracks()->size();
-      play_record(device, *record, false);
-      np_tracks_played += np_tracks_size;
-    }
-
-    if (i == passes_size - 1 && learn) {
-      const auto& inference_registers = *learn->inference_registers();
-      const auto& learn_registers = *learn->learning_registers();
-      const auto& ram = *learn->ram();
-      assert(inference_registers.np_tracks()->size() == 1 &&
-             learn_registers.np_tracks()->size() == 1 &&
-             "learning layer registers should always be a single track");
-
-      auto np_tracks_size =
-          inference_registers.np_tracks()->size() + ram.np_tracks()->size();
-      auto learn_desc_address = play_record(device, inference_registers, false);
-      // store the address of descriptor that correspond to the learning layer
-      // registers because we will need to edit this descriptor to make it point
-      // to the learning registers or inference registers when enable/disable
-      // learning
-      multipass_memory->update_learn_descriptor_addr(learn_desc_address);
-      write_track_on_device(device, *learn_registers.np_tracks()->Get(0));
-      play_record(device, ram, false);
-      np_tracks_played += np_tracks_size;
-    }
-
-    // fill unused pass descriptors with "dummy" descriptors for this pass
-    assert(max_num_desc_pass >= np_tracks_played);
-    uint32_t num_dummy_descs = max_num_desc_pass - np_tracks_played;
-    write_dummy_descs(device, dummy_input, multipass_memory->dummy_output_addr,
-                      num_dummy_descs);
-  }
-
-  // Add an extra descriptor to copy the learned memory
-  if (use_fnp3_for_learning(*program)) {
-    generate_reading_descriptor_from_record(device, *learn->ram());
-  }
-  play_epg(device, program);
-}
-
-void configure_learning_mode_single_pass(HardwareDeviceImpl* device,
-                                         const uint8_t* program_data,
-                                         bool learn_en) {
-  auto* program = fb::GetProgram(program_data);
-  assert(program->passes()->size() == 1);
-
-  const auto learn = program->learning_layer();
-  assert(learn);
-  const auto& old_registers =
-      !learn_en ? *learn->learning_registers() : *learn->inference_registers();
-  const auto& new_register =
-      learn_en ? *learn->learning_registers() : *learn->inference_registers();
-
-  rewind_record(device, old_registers, false);
-  play_record(device, new_register, true);
-}
-
-void configure_learning_mode_multi_pass(HardwareDeviceImpl* device,
-                                        const uint8_t* program_data,
-                                        const MultiPassMemory& multipass_memory,
-                                        bool learn_en) {
-  auto* program = fb::GetProgram(program_data);
-  assert(program->passes()->size() > 1);
-  // This function will edit the descriptor at learn_descriptor_addr to make it
-  // point to either learning or inference registers
-  assert(multipass_memory.learn_descriptor_addr != 0);
-  const auto learn = program->learning_layer();
-  assert(learn);
-
-  // get the correct track depending on learning
-  const auto& inference_tracks = *learn->inference_registers()->np_tracks();
-  const auto& learning_tracks = *learn->learning_registers()->np_tracks();
-  assert(inference_tracks.size() == 1 && learning_tracks.size() == 1);
-  const auto* registers_track =
-      learn_en ? learning_tracks[0] : inference_tracks[0];
-  const auto registers_address =
-      device->external_mem()->tracked(registers_track->data()->data());
-  // Overwrite the descriptor
-  write_np_track_descriptor(device->driver(), registers_address,
-                            registers_track->data()->size(),
-                            multipass_memory.learn_descriptor_addr);
-}
-
-Shape output_dims(const uint8_t* program_data) {
-  auto* program = fb::GetProgram(program_data);
-  assert(program->output_dims()->size() == 3);
-  const auto* output_dims_data = program->output_dims()->data();
-  Shape ret{output_dims_data[0], output_dims_data[1], output_dims_data[2]};
-  return ret;
-}
-
-const Index* input_dims(const uint8_t* program_data) {
-  auto* program = fb::GetProgram(program_data);
-  assert(program->input_dims()->size() == 3);
-
-  return program->input_dims()->data();
-}
-
-bool input_is_dense(const uint8_t* program_data) {
-  auto* program = fb::GetProgram(program_data);
-  return program->input_type() == fb::IoType_dense;
-}
-
-bool input_is_fnp(const uint8_t* program_data) {
-  auto* program = fb::GetProgram(program_data);
-  return program->input_type() == fb::IoType_fnp_sparse;
-}
-
-bool output_is_dense(const uint8_t* program_data) {
-  auto* program = fb::GetProgram(program_data);
-  return program->output_type() == fb::IoType_dense;
-}
-
-bool output_is_fnp(const uint8_t* program_data) {
-  auto* program = fb::GetProgram(program_data);
-  return program->output_type() == fb::IoType_fnp_sparse;
-}
-
-dma::OutputFormat output_format(const uint8_t* program_data) {
-  auto* program = fb::GetProgram(program_data);
-  switch (program->output_type()) {
-    case fb::IoType_fnp_sparse: {
-      return program->activation() ? dma::OutputFormat::FullyActivations
-                                   : dma::OutputFormat::FullyPotentials;
-    }
-    case fb::IoType_cnp_sparse: {
-      return program->activation() ? dma::OutputFormat::ConvActivations
-                                   : dma::OutputFormat::ConvPotentials;
-    }
-    case fb::IoType_hrc_sparse: {
-      return program->activation() ? dma::OutputFormat::HrcActivations
-                                   : dma::OutputFormat::ConvHighPotentials;
-    }
-    case fb::IoType_dense: {
-      return program->activation() ? dma::OutputFormat::DenseActivations
-                                   : dma::OutputFormat::DensePotentials;
-    }
-    default:
-      panic("Unsupported output type");
-  }
-}
-
-bool activation(const uint8_t* program_data) {
-  auto* program = fb::GetProgram(program_data);
-  return program->activation();
-}
-
-uint32_t dense_window_w(const uint8_t* program_data) {
-  auto* program = fb::GetProgram(program_data);
-  return program->dense_window_w();
-}
-
-uint32_t dense_window_h(const uint8_t* program_data) {
-  auto* program = fb::GetProgram(program_data);
-  return program->dense_window_h();
-}
-
-Buffer<int32_t> shifts(const uint8_t* program_data) {
-  auto* program = fb::GetProgram(program_data);
-  auto shifts = program->shifts();
-  auto data = shifts->data();
-  auto size = shifts->size();
-  return {data, size};
-}
-
-Buffer<float> scales(const uint8_t* program_data) {
-  auto* program = fb::GetProgram(program_data);
-  auto scales = program->scales();
-  auto data = scales->data();
-  auto size = scales->size();
-  return {data, size};
-}
-
-bool can_learn(const uint8_t* program_data) {
-  auto* program = fb::GetProgram(program_data);
-  return program->learning_layer();
-}
-
-uint8_t max_num_desc(const uint8_t* program_data) {
-  auto* program = fb::GetProgram(program_data);
-  return program->max_num_desc();
-}
-
-uint32_t num_passes(const uint8_t* program_data) {
-  auto* program = fb::GetProgram(program_data);
-  return program->passes()->size();
-}
-
-bool is_multi_pass(const uint8_t* program_data) {
-  return num_passes(program_data) > 1;
-}
-
-uint32_t learn_mem_size(const uint8_t* program_data) {
-  auto* program = fb::GetProgram(program_data);
-  auto learning_layer = program->learning_layer();
-  if (!learning_layer) {
-    return 0;
-  }
-  // return mem size, in number of 32bit words
-  return learning_layer->learn_mem_size();
-}
-
-void update_learn_mem(HardwareDeviceImpl* device, const uint8_t* program_data,
-                      const uint32_t* ram_dump) {
-  auto* program = fb::GetProgram(program_data);
-  auto learning_layer = program->learning_layer();
-  if (!learning_layer) {
-    panic(
-        "Learn memory update requires a device programmed with learning "
-        "layers");
-  }
-
-  // detect ram size
-  auto size = learning_layer->learn_mem_size();
-  // detect if FNP2 or FNP3
-  auto record = learning_layer->ram();
-  assert(record);
-  auto fnp2_track = record->fnp2_track();
-  if (fnp2_track) {
-    // get memory address for this track
-    auto mem_addr = device->external_mem()->tracked(fnp2_track->data()->data());
-    // update memory
-    device->driver()->write(mem_addr, ram_dump, size * sizeof(uint32_t));
-  } else {
-    auto update_learn_mem_hdr = learning_layer->update_learn_mem_hdr();
-    // Note: for now a copy is necessary to update learn memory, to have config
-    // header placed just before memory. In the future, a possible optimization
-    // could be returning the header when reading the memory.
-    std::vector<dma::w32> sram(dma::kConfigWriteHdrWordLen + size);
-    // first copy header in vector
-    sram[0] = update_learn_mem_hdr->w1();
-    sram[1] = update_learn_mem_hdr->w2();
-    // then copy ram dump
-    sram.insert(sram.begin() + dma::kConfigWriteHdrWordLen, ram_dump,
-                ram_dump + size);
-    // now do transfer
-    device->dma_config_write(sram.data(), sram.size());
-  }
-}
-
-void learn_mem(HardwareDeviceImpl* device, const uint8_t* program_data,
-               uint32_t* ram_dump) {
-  auto* program = fb::GetProgram(program_data);
-  auto learning_layer = program->learning_layer();
-  if (!learning_layer) {
-    panic("Learn memory retrieval requires a program from learning layers");
-  }
-
-  // detect ram size
-  auto size = learning_layer->learn_mem_size();
-  // detect if FNP2 or FNP3
-  auto record = learning_layer->ram();
-  assert(record);
-  auto fnp2_track = record->fnp2_track();
-  if (fnp2_track) {
-    // get memory address for this track
-    auto mem_addr = device->external_mem()->tracked(fnp2_track->data()->data());
-    device->driver()->read(mem_addr, ram_dump, size * sizeof(dma::w32));
-  } else {
-    // In multi pass we can directly read in the program.
-    if (is_multi_pass(program_data)) {
-      auto* tracks = record->np_tracks();
-      assert(tracks->size() == 1);
-      auto ram_addr =
-          device->external_mem()->tracked(tracks->Get(0)->data()->data());
-      // Skip the 2 words of DMA read header
-      device->driver()->read(ram_addr + dma::kConfigWritePacketOffset, ram_dump,
-                             size * sizeof(dma::w32));
-    } else {
-      // in single pass when record is FNP3: read SRAM
-      auto np = learning_layer->np();
-      np::Ident ident{np->col(), np->row(), np->id()};
-      device->dma_config_read(ram_dump, ident, dma::Target::FnpWeights, 0,
-                              size);
-    }
-  }
-}
-
-uint32_t number_of_program_descriptors_required(const uint8_t* program_data) {
-  auto* program = fb::GetProgram(program_data);
-  const auto nb_passes = program->passes()->size();
-  if (nb_passes > 1) {
-    return nb_passes * program->max_num_desc();
-  } else {
-    return dma::kMinNbDescriptors;
-  }
-}
-
-uint32_t number_of_extra_program_descriptors_required(
-    const uint8_t* program_data) {
-  auto* program = fb::GetProgram(program_data);
-  return (program->passes()->size() > 1) && use_fnp3_for_learning(*program) ? 1
-                                                                            : 0;
-}
-
-static size_t record_np_tracks_byte_size(const fb::Record& record) {
-  size_t result = 0;
-  // get size of np tracks
-  for (const auto* np_track : *record.np_tracks()) {
-    result += np_track->data()->size() * sizeof(uint32_t);
-  }
-  return result;
-}
-
-static size_t largest_np_track_byte_size(const fb::Record& record) {
-  size_t result = 0;
-  for (const auto* np_track : *record.np_tracks()) {
-    result = std::max(result, static_cast<size_t>(np_track->data()->size()) *
-                                  sizeof(uint32_t));
-  }
-  return result;
-}
-
-size_t program_data_required_memory(const uint8_t* program_data) {
-  size_t result = 0;
-  const auto* program = fb::GetProgram(program_data);
-  const auto nb_passes = program->passes()->size();
-  if (nb_passes > 1) {
-    // in multipass, all np tracks must be in memory
-    for (const auto* pass : *program->passes()) {
-      for (const auto* record : *pass->records()) {
-        result += record_np_tracks_byte_size(*record);
-      }
-    }
-    // if there is learning records we need to count it as well
-    const auto* learn = program->learning_layer();
-    if (learn) {
-      // learn have both inference & learning tracks, plus weights
-      result += record_np_tracks_byte_size(*learn->inference_registers());
-      result += record_np_tracks_byte_size(*learn->learning_registers());
-      result += record_np_tracks_byte_size(*learn->ram());
-    }
-  } else {
-    // in single pass, tracks are played once at a time, so the required memory
-    // is the size of the largest one
-    const auto* pass = (*program->passes())[0];
-    for (const auto* record : *pass->records()) {
-      result = std::max(result, largest_np_track_byte_size(*record));
-    }
-    // check if there are learning records
-    const auto learn = program->learning_layer();
-    if (learn) {
-      result = std::max(
-          result, largest_np_track_byte_size(*learn->inference_registers()));
-      result = std::max(
-          result, largest_np_track_byte_size(*learn->learning_registers()));
-      result = std::max(result, largest_np_track_byte_size(*learn->ram()));
-    }
-  }
-  return result;
-}
-
-size_t fnp2_tracks_byte_size(const uint8_t* program_data) {
-  size_t result = 0;
-  const auto* program = fb::GetProgram(program_data);
-
-  // iterate over all records from all passes
-  for (const auto* pass : *program->passes()) {
-    for (const auto* record : *pass->records()) {
-      // check if we have FNP2 weights
-      if (record->fnp2_track()) {
-        result += static_cast<size_t>(record->fnp2_track()->data()->size()) *
-                  sizeof(uint32_t);
-      }
-    }
-  }
-  // check if there is learning records that could contain FNP2
-  const auto learn = program->learning_layer();
-  if (learn) {
-    // learning/inference registers should not contain FNP2 track
-    assert(learn->inference_registers()->fnp2_track() == nullptr);
-    assert(learn->learning_registers()->fnp2_track() == nullptr);
-    // but ram could
-    if (learn->ram()->fnp2_track()) {
-      result +=
-          static_cast<size_t>(learn->ram()->fnp2_track()->data()->size()) *
-          sizeof(uint32_t);
-    }
-  }
-  return result;
-}
-
-}  // namespace program
-}  // namespace akida
+#include "program_play.h"
+
+#include <cassert>
+#include <cstring>
+
+#include "akida/hardware_device.h"
+#include "akida/hw_version.h"
+#include "akida/shape.h"
+#include "akida/version.h"
+#include "engine/akida_device_program_fb_generated.h"
+#include "engine/dma_config_ops.h"
+#include "flatbuffers/flatbuffers.h"
+#include "infra/hardware_driver.h"
+
+#include "dma_desc_format.h"
+#include "dma_engine_ops.h"
+#include "external_mem_mgr.h"
+#include "fnp2_mem_conf_reg.h"
+
+namespace akida {
+namespace program {
+
+static void check_device_version(const HardwareDevice& device,
+                                 const fb::Program& program) {
+  // verify device version matches with program
+  auto* prog_dev_version = program.device_version();
+  auto dev_version = device.version();
+
+  bool valid_version =
+      dev_version.vendor_id == prog_dev_version->vendor_id() &&
+      dev_version.product_id == prog_dev_version->product_id() &&
+      dev_version.major_rev == prog_dev_version->major_rev() &&
+      dev_version.minor_rev == prog_dev_version->minor_rev();
+  if (!valid_version) {
+    panic("Program device version and device version are not compatible");
+  }
+}
+
+void verify(const HardwareDeviceImpl& device, const uint8_t* data,
+            size_t size) {
+  if (!data) {
+    panic("Program is null");
+  }
+  // build program and verify it
+  auto* program = fb::GetProgram(data);
+  flatbuffers::Verifier verifier(data, size);
+  if (!program || !program->Verify(verifier)) {
+    panic("Unable to parse program");
+  }
+  // Check that the akida version this program was compiled with matches the
+  // current version.
+  const auto& program_version = program->version()->c_str();
+  const auto& lib_version = version();
+  if (strcmp(program_version, lib_version) != 0) {
+    panic("Program version [%s] does not match library version [%s]",
+          program_version, lib_version);
+  }
+  check_device_version(device, *program);
+}
+
+static bool use_fnp3_for_learning(const fb::Program& program) {
+  return program.learning_layer() != nullptr &&
+         program.learning_layer()->ram()->np_tracks() != nullptr &&
+         program.learning_layer()->ram()->fnp2_track() == nullptr;
+}
+
+static void rewind_fnp2_track(HardwareDeviceImpl* device,
+                              const fb::Fnp2FilterTrack& track) {
+  device->external_mem()->release(track.data()->data());
+}
+
+static void rewind_np_track(HardwareDeviceImpl* device,
+                            const fb::NpTrack& track, bool multi_pass) {
+  if (multi_pass) {
+    // in multi pass, free config header allocated with track as id
+    device->external_mem()->release(track.data()->data());
+  }
+}
+
+static void rewind_record(HardwareDeviceImpl* device, const fb::Record& record,
+                          bool multi_pass) {
+  // rewind fnp2 track if it is there
+  const auto* fnp2_track = record.fnp2_track();
+  if (fnp2_track) {
+    rewind_fnp2_track(device, *fnp2_track);
+  }
+  // rewind all normal tracks
+  const auto* np_tracks = record.np_tracks();
+  uint32_t np_tracks_size = np_tracks->size();
+  for (int i = np_tracks_size - 1; i >= 0; i--) {
+    const auto& np_track = *np_tracks->Get(i);
+    rewind_np_track(device, np_track, multi_pass);
+  }
+}
+
+static void write_np_track_descriptor(HardwareDriver* driver,
+                                      dma::addr track_addr_on_device,
+                                      uint32_t track_word_size,
+                                      dma::addr descriptor_address) {
+  // format descriptor
+  constexpr uint32_t output_addr = 0;  // not used for write
+  auto descriptor = dma::format_config_desc(dma::kDescConfigDirectionWrite,
+                                            track_addr_on_device, output_addr,
+                                            track_word_size);
+  // write descriptor in its place
+  driver->write(descriptor_address, descriptor.data(),
+                descriptor.size() * sizeof(dma::Descriptor::value_type));
+}
+
+static dma::addr write_track_on_device(HardwareDeviceImpl* device,
+                                       const fb::NpTrack& track) {
+  const auto* buffer = track.data();
+  auto buffer_bytes_size = buffer->size() * sizeof(uint32_t);
+  // put buffer on device, and get its address
+  auto buf_in_mem = device->external_mem()->track_and_put_on_device_if_required(
+      buffer->data(), buffer_bytes_size);
+
+  return buf_in_mem;
+}
+
+static void generate_reading_descriptor_from_np_track(
+    HardwareDeviceImpl* device, const fb::NpTrack& track) {
+  const auto track_addr = device->external_mem()->tracked(track.data()->data());
+  const auto input_addr = track_addr;
+  const auto output_addr = track_addr + dma::kConfigWritePacketOffset;
+
+  // format descriptor
+  const auto descriptor =
+      dma::format_config_desc(dma::kDescConfigDirectionRead, input_addr,
+                              output_addr, dma::kConfigWriteHdrWordLen);
+  // enqueue it at extra descriptor location
+  dma::enqueue_extra_descriptor(device->driver(), device->dma_config(),
+                                descriptor);
+}
+
+static void play_fnp2_track(HardwareDeviceImpl* device,
+                            const fb::Fnp2FilterTrack& track) {
+  auto* driver = device->driver();
+  const auto* buffer = track.data();
+  // alloc and write FNP2 filter data
+  uint32_t address =
+      device->external_mem()->track_and_put_on_device_if_required(
+          buffer->data(), buffer->size() * sizeof(uint32_t));
+
+  // Now write DDR address used for this NP in the dedicated conf register
+  // Note that there are 4 registers where the weights adress can be stored.
+  // This works because currently existing mesh designs only contain one node
+  // with 4 FNPs. Each NP will use the content of the register indexed by the ID
+  // of the NP.
+  // If at some point a mesh is created with a different layout, this might
+  // raise an issue.
+  // Also, this means that in multipass this register can only be used once per
+  // program, and the FNP2 cannot be reused later, because that would require
+  // updating the register value with another address, and there is no way to do
+  // that.
+  const auto np_id = track.np()->id();
+  auto fnp2_mem_conf_reg_addr =
+      fnp2_memory_conf(driver->top_level_reg(), np_id);
+  driver->write32(fnp2_mem_conf_reg_addr, address);
+}
+
+static dma::addr play_record(HardwareDeviceImpl* device,
+                             const fb::Record& record, bool single_pass) {
+  dma::addr descriptor_address = 0;
+  // play all np tracks
+  const auto* np_tracks = record.np_tracks();
+  int np_tracks_size = np_tracks->size();
+  for (int i = 0; i < np_tracks_size; i++) {
+    const auto& np_track = *np_tracks->Get(i);
+    assert(np_track.data()->size() > 0);
+
+    // write track data
+    auto track_address = write_track_on_device(device, np_track);
+    // generate descriptor
+    auto descriptor =
+        dma::format_config_desc(dma::kDescConfigDirectionWrite, track_address,
+                                0, np_track.data()->size());
+    // enqueue descriptor
+    descriptor_address = dma::enqueue_descriptor(
+        device->driver(), device->dma_config().engine, descriptor);
+    if (single_pass) {
+      // in single pass, we need to wait for descriptor to complete
+      dma::wait_config_dma_descriptor_complete(device->driver(),
+                                               device->dma_config());
+      // then release track
+      device->external_mem()->release(np_track.data()->data());
+    }
+  }
+
+  // play fnp2 track if there is one
+  const auto* fnp2_track = record.fnp2_track();
+  if (fnp2_track) {
+    play_fnp2_track(device, *fnp2_track);
+  }
+
+  return descriptor_address;
+}
+
+static void generate_reading_descriptor_from_record(HardwareDeviceImpl* device,
+                                                    const fb::Record& record) {
+  if (record.fnp2_track())
+    panic("Cannot use descriptors to read the FNP2 memory.");
+  // play all normal tracks
+  const auto* np_tracks = record.np_tracks();
+  assert(np_tracks->size() == 1 &&
+         "Learning should use a single NP, so there should be a single track");
+  generate_reading_descriptor_from_np_track(device, *np_tracks->Get(0));
+}
+
+static void play_epg_track(HardwareDriver* driver, uint32_t epg_base,
+                           uint32_t address, uint32_t data) {
+  driver->write32(epg_base + address, data);
+}
+
+void rewind(HardwareDeviceImpl* device, const uint8_t* program_data) {
+  auto* program = fb::GetProgram(program_data);
+
+  const auto& passes = *program->passes();
+  int passes_size = passes.size();
+  bool multi_pass = passes_size > 1;
+
+  const auto learn = program->learning_layer();
+  if (learn) {
+    if (multi_pass) {
+      // in multi pass, both learning & inference registers are written to the
+      // device
+      rewind_record(device, *learn->learning_registers(), multi_pass);
+      rewind_record(device, *learn->inference_registers(), multi_pass);
+    } else {
+      if (device->learn_enabled()) {
+        rewind_record(device, *learn->learning_registers(), multi_pass);
+      } else {
+        rewind_record(device, *learn->inference_registers(), multi_pass);
+      }
+    }
+    rewind_record(device, *learn->ram(), multi_pass);
+  }
+
+  // rewind in reverse order
+  for (int i = passes_size - 1; i >= 0; i--) {
+    const auto& layer_records = *passes[i]->records();
+    int cur_pass_size = layer_records.size();
+    for (int j = cur_pass_size - 1; j >= 0; j--) {
+      const auto& record = *layer_records[j];
+      rewind_record(device, record, multi_pass);
+    }
+  }
+
+  if (multi_pass) {
+    // free up dummy config header
+    device->external_mem()->release(program->dummy_desc_hdr());
+  }
+}
+
+static dma::addr dma_config_header_dummy(HardwareDeviceImpl* device,
+                                         const akida::fb::Program* program) {
+  auto dummy_header = program->dummy_desc_hdr();
+  // make sure flatbuffer struct is the same size as header
+  static_assert(sizeof(*dummy_header) == dma::kConfigWritePacketOffset,
+                "DmaConfigHeader should be the same size as "
+                "kConfigWriteHdrWordLen");
+  // put buffer on device, and get its address
+  auto mem = device->external_mem()->track_and_put_on_device_if_required(
+      dummy_header, sizeof(*dummy_header));
+
+  return mem;
+}
+
+static void write_dummy_descs(HardwareDeviceImpl* device, dma::addr dummy_input,
+                              dma::addr dummy_output,
+                              uint32_t num_dummy_descs) {
+  // Dummy descriptor is a read of size 1. Descriptor size is the header size
+  auto dummy_desc =
+      dma::format_config_desc(dma::kDescConfigDirectionRead, dummy_input,
+                              dummy_output, dma::kConfigWriteHdrWordLen);
+  for (uint32_t j = 0; j < num_dummy_descs; j++) {
+    dma::enqueue_descriptor(device->driver(), device->dma_config().engine,
+                            dummy_desc);
+  }
+}
+
+static inline uint32_t epg_reg_base(const uint32_t top_level_reg_base) {
+  constexpr uint32_t EPG_REG_BASE = 0x00040000;
+  return top_level_reg_base + EPG_REG_BASE;
+}
+
+static void play_epg(HardwareDeviceImpl* device, const fb::Program* program) {
+  // Apply EPG program
+  const auto* epg_tracks = program->epg_tracks();
+  if (epg_tracks) {
+    auto epg_tracks_size = epg_tracks->size();
+    auto driver = device->driver();
+    auto epg_base = epg_reg_base(driver->top_level_reg());
+    for (uint32_t i = 0; i < epg_tracks_size; i++) {
+      const auto& epg_track = epg_tracks->Get(i);
+      play_epg_track(driver, epg_base, epg_track->address(), epg_track->data());
+    }
+  }
+}
+
+void play_single_pass(HardwareDeviceImpl* device, const uint8_t* program_data) {
+  auto* program = fb::GetProgram(program_data);
+  const auto& passes = *program->passes();
+  assert(passes.size() == 1);
+  const auto& layer_records = *passes[0]->records();
+  int records_size = layer_records.size();
+
+  // play all records
+  for (int i = 0; i < records_size; i++) {
+    const auto& record = *layer_records[i];
+    play_record(device, record, true);
+  }
+
+  const auto learn = program->learning_layer();
+  if (learn) {
+    const auto& registers = device->learn_enabled()
+                                ? *learn->learning_registers()
+                                : *learn->inference_registers();
+    play_record(device, registers, true);
+    play_record(device, *learn->ram(), true);
+  }
+  play_epg(device, program);
+}
+
+void play_multi_pass(HardwareDeviceImpl* device, const uint8_t* program_data,
+                     MultiPassMemory* multipass_memory) {
+  auto* program = fb::GetProgram(program_data);
+  const auto learn = program->learning_layer();
+  const auto& passes = *program->passes();
+  uint32_t passes_size = passes.size();
+  // In multi pass mode, there will always be at least 2 passes
+  assert(passes_size >= 2);
+
+  // estimate memory required to hold passes descriptors.
+  const auto max_num_desc_pass = program->max_num_desc();
+
+  // use program to allocate dummy config, input and output space
+  auto dummy_input = dma_config_header_dummy(device, program);
+
+  uint32_t np_tracks_played = 0;
+
+  // now that we have the memory, we can fill the descriptors
+  for (uint32_t i = 0; i < passes_size; i++) {
+    const auto& layer_records = *passes[i]->records();
+    uint32_t records_size = layer_records.size();
+    np_tracks_played = 0;
+    for (uint32_t j = 0; j < records_size; j++) {
+      auto* record = layer_records[j];
+
+      // get number of NP tracks (corresponding to number of DMA descriptors).
+      uint32_t np_tracks_size = record->np_tracks()->size();
+      play_record(device, *record, false);
+      np_tracks_played += np_tracks_size;
+    }
+
+    if (i == passes_size - 1 && learn) {
+      const auto& inference_registers = *learn->inference_registers();
+      const auto& learn_registers = *learn->learning_registers();
+      const auto& ram = *learn->ram();
+      assert(inference_registers.np_tracks()->size() == 1 &&
+             learn_registers.np_tracks()->size() == 1 &&
+             "learning layer registers should always be a single track");
+
+      auto np_tracks_size =
+          inference_registers.np_tracks()->size() + ram.np_tracks()->size();
+      auto learn_desc_address = play_record(device, inference_registers, false);
+      // store the address of descriptor that correspond to the learning layer
+      // registers because we will need to edit this descriptor to make it point
+      // to the learning registers or inference registers when enable/disable
+      // learning
+      multipass_memory->update_learn_descriptor_addr(learn_desc_address);
+      write_track_on_device(device, *learn_registers.np_tracks()->Get(0));
+      play_record(device, ram, false);
+      np_tracks_played += np_tracks_size;
+    }
+
+    // fill unused pass descriptors with "dummy" descriptors for this pass
+    assert(max_num_desc_pass >= np_tracks_played);
+    uint32_t num_dummy_descs = max_num_desc_pass - np_tracks_played;
+    write_dummy_descs(device, dummy_input, multipass_memory->dummy_output_addr,
+                      num_dummy_descs);
+  }
+
+  // Add an extra descriptor to copy the learned memory
+  if (use_fnp3_for_learning(*program)) {
+    generate_reading_descriptor_from_record(device, *learn->ram());
+  }
+  play_epg(device, program);
+}
+
+void configure_learning_mode_single_pass(HardwareDeviceImpl* device,
+                                         const uint8_t* program_data,
+                                         bool learn_en) {
+  auto* program = fb::GetProgram(program_data);
+  assert(program->passes()->size() == 1);
+
+  const auto learn = program->learning_layer();
+  assert(learn);
+  const auto& old_registers =
+      !learn_en ? *learn->learning_registers() : *learn->inference_registers();
+  const auto& new_register =
+      learn_en ? *learn->learning_registers() : *learn->inference_registers();
+
+  rewind_record(device, old_registers, false);
+  play_record(device, new_register, true);
+}
+
+void configure_learning_mode_multi_pass(HardwareDeviceImpl* device,
+                                        const uint8_t* program_data,
+                                        const MultiPassMemory& multipass_memory,
+                                        bool learn_en) {
+  auto* program = fb::GetProgram(program_data);
+  assert(program->passes()->size() > 1);
+  // This function will edit the descriptor at learn_descriptor_addr to make it
+  // point to either learning or inference registers
+  assert(multipass_memory.learn_descriptor_addr != 0);
+  const auto learn = program->learning_layer();
+  assert(learn);
+
+  // get the correct track depending on learning
+  const auto& inference_tracks = *learn->inference_registers()->np_tracks();
+  const auto& learning_tracks = *learn->learning_registers()->np_tracks();
+  assert(inference_tracks.size() == 1 && learning_tracks.size() == 1);
+  const auto* registers_track =
+      learn_en ? learning_tracks[0] : inference_tracks[0];
+  const auto registers_address =
+      device->external_mem()->tracked(registers_track->data()->data());
+  // Overwrite the descriptor
+  write_np_track_descriptor(device->driver(), registers_address,
+                            registers_track->data()->size(),
+                            multipass_memory.learn_descriptor_addr);
+}
+
+Shape output_dims(const uint8_t* program_data) {
+  auto* program = fb::GetProgram(program_data);
+  assert(program->output_dims()->size() == 3);
+  const auto* output_dims_data = program->output_dims()->data();
+  Shape ret{output_dims_data[0], output_dims_data[1], output_dims_data[2]};
+  return ret;
+}
+
+const Index* input_dims(const uint8_t* program_data) {
+  auto* program = fb::GetProgram(program_data);
+  assert(program->input_dims()->size() == 3);
+
+  return program->input_dims()->data();
+}
+
+bool input_is_dense(const uint8_t* program_data) {
+  auto* program = fb::GetProgram(program_data);
+  return program->input_type() == fb::IoType_dense;
+}
+
+bool input_is_fnp(const uint8_t* program_data) {
+  auto* program = fb::GetProgram(program_data);
+  return program->input_type() == fb::IoType_fnp_sparse;
+}
+
+bool output_is_dense(const uint8_t* program_data) {
+  auto* program = fb::GetProgram(program_data);
+  return program->output_type() == fb::IoType_dense;
+}
+
+bool output_is_fnp(const uint8_t* program_data) {
+  auto* program = fb::GetProgram(program_data);
+  return program->output_type() == fb::IoType_fnp_sparse;
+}
+
+dma::OutputFormat output_format(const uint8_t* program_data) {
+  auto* program = fb::GetProgram(program_data);
+  switch (program->output_type()) {
+    case fb::IoType_fnp_sparse: {
+      return program->activation() ? dma::OutputFormat::FullyActivations
+                                   : dma::OutputFormat::FullyPotentials;
+    }
+    case fb::IoType_cnp_sparse: {
+      return program->activation() ? dma::OutputFormat::ConvActivations
+                                   : dma::OutputFormat::ConvPotentials;
+    }
+    case fb::IoType_hrc_sparse: {
+      return program->activation() ? dma::OutputFormat::HrcActivations
+                                   : dma::OutputFormat::ConvHighPotentials;
+    }
+    case fb::IoType_dense: {
+      return program->activation() ? dma::OutputFormat::DenseActivations
+                                   : dma::OutputFormat::DensePotentials;
+    }
+    default:
+      panic("Unsupported output type");
+  }
+}
+
+bool activation(const uint8_t* program_data) {
+  auto* program = fb::GetProgram(program_data);
+  return program->activation();
+}
+
+uint32_t dense_window_w(const uint8_t* program_data) {
+  auto* program = fb::GetProgram(program_data);
+  return program->dense_window_w();
+}
+
+uint32_t dense_window_h(const uint8_t* program_data) {
+  auto* program = fb::GetProgram(program_data);
+  return program->dense_window_h();
+}
+
+Buffer<int32_t> shifts(const uint8_t* program_data) {
+  auto* program = fb::GetProgram(program_data);
+  auto shifts = program->shifts();
+  auto data = shifts->data();
+  auto size = shifts->size();
+  return {data, size};
+}
+
+Buffer<float> scales(const uint8_t* program_data) {
+  auto* program = fb::GetProgram(program_data);
+  auto scales = program->scales();
+  auto data = scales->data();
+  auto size = scales->size();
+  return {data, size};
+}
+
+bool can_learn(const uint8_t* program_data) {
+  auto* program = fb::GetProgram(program_data);
+  return program->learning_layer();
+}
+
+uint8_t max_num_desc(const uint8_t* program_data) {
+  auto* program = fb::GetProgram(program_data);
+  return program->max_num_desc();
+}
+
+uint32_t num_passes(const uint8_t* program_data) {
+  auto* program = fb::GetProgram(program_data);
+  return program->passes()->size();
+}
+
+bool is_multi_pass(const uint8_t* program_data) {
+  return num_passes(program_data) > 1;
+}
+
+uint32_t learn_mem_size(const uint8_t* program_data) {
+  auto* program = fb::GetProgram(program_data);
+  auto learning_layer = program->learning_layer();
+  if (!learning_layer) {
+    return 0;
+  }
+  // return mem size, in number of 32bit words
+  return learning_layer->learn_mem_size();
+}
+
+void update_learn_mem(HardwareDeviceImpl* device, const uint8_t* program_data,
+                      const uint32_t* ram_dump) {
+  auto* program = fb::GetProgram(program_data);
+  auto learning_layer = program->learning_layer();
+  if (!learning_layer) {
+    panic(
+        "Learn memory update requires a device programmed with learning "
+        "layers");
+  }
+
+  // detect ram size
+  auto size = learning_layer->learn_mem_size();
+  // detect if FNP2 or FNP3
+  auto record = learning_layer->ram();
+  assert(record);
+  auto fnp2_track = record->fnp2_track();
+  if (fnp2_track) {
+    // get memory address for this track
+    auto mem_addr = device->external_mem()->tracked(fnp2_track->data()->data());
+    // update memory
+    device->driver()->write(mem_addr, ram_dump, size * sizeof(uint32_t));
+  } else {
+    auto update_learn_mem_hdr = learning_layer->update_learn_mem_hdr();
+    // Note: for now a copy is necessary to update learn memory, to have config
+    // header placed just before memory. In the future, a possible optimization
+    // could be returning the header when reading the memory.
+    std::vector<dma::w32> sram(dma::kConfigWriteHdrWordLen + size);
+    // first copy header in vector
+    sram[0] = update_learn_mem_hdr->w1();
+    sram[1] = update_learn_mem_hdr->w2();
+    // then copy ram dump
+    sram.insert(sram.begin() + dma::kConfigWriteHdrWordLen, ram_dump,
+                ram_dump + size);
+    // now do transfer
+    device->dma_config_write(sram.data(), sram.size());
+  }
+}
+
+void learn_mem(HardwareDeviceImpl* device, const uint8_t* program_data,
+               uint32_t* ram_dump) {
+  auto* program = fb::GetProgram(program_data);
+  auto learning_layer = program->learning_layer();
+  if (!learning_layer) {
+    panic("Learn memory retrieval requires a program from learning layers");
+  }
+
+  // detect ram size
+  auto size = learning_layer->learn_mem_size();
+  // detect if FNP2 or FNP3
+  auto record = learning_layer->ram();
+  assert(record);
+  auto fnp2_track = record->fnp2_track();
+  if (fnp2_track) {
+    // get memory address for this track
+    auto mem_addr = device->external_mem()->tracked(fnp2_track->data()->data());
+    device->driver()->read(mem_addr, ram_dump, size * sizeof(dma::w32));
+  } else {
+    // In multi pass we can directly read in the program.
+    if (is_multi_pass(program_data)) {
+      auto* tracks = record->np_tracks();
+      assert(tracks->size() == 1);
+      auto ram_addr =
+          device->external_mem()->tracked(tracks->Get(0)->data()->data());
+      // Skip the 2 words of DMA read header
+      device->driver()->read(ram_addr + dma::kConfigWritePacketOffset, ram_dump,
+                             size * sizeof(dma::w32));
+    } else {
+      // in single pass when record is FNP3: read SRAM
+      auto np = learning_layer->np();
+      np::Ident ident{np->col(), np->row(), np->id()};
+      device->dma_config_read(ram_dump, ident, dma::Target::FnpWeights, 0,
+                              size);
+    }
+  }
+}
+
+uint32_t number_of_program_descriptors_required(const uint8_t* program_data) {
+  auto* program = fb::GetProgram(program_data);
+  const auto nb_passes = program->passes()->size();
+  if (nb_passes > 1) {
+    return nb_passes * program->max_num_desc();
+  } else {
+    return dma::kMinNbDescriptors;
+  }
+}
+
+uint32_t number_of_extra_program_descriptors_required(
+    const uint8_t* program_data) {
+  auto* program = fb::GetProgram(program_data);
+  return (program->passes()->size() > 1) && use_fnp3_for_learning(*program) ? 1
+                                                                            : 0;
+}
+
+static size_t record_np_tracks_byte_size(const fb::Record& record) {
+  size_t result = 0;
+  // get size of np tracks
+  for (const auto* np_track : *record.np_tracks()) {
+    result += np_track->data()->size() * sizeof(uint32_t);
+  }
+  return result;
+}
+
+static size_t largest_np_track_byte_size(const fb::Record& record) {
+  size_t result = 0;
+  for (const auto* np_track : *record.np_tracks()) {
+    result = std::max(result, static_cast<size_t>(np_track->data()->size()) *
+                                  sizeof(uint32_t));
+  }
+  return result;
+}
+
+size_t program_data_required_memory(const uint8_t* program_data) {
+  size_t result = 0;
+  const auto* program = fb::GetProgram(program_data);
+  const auto nb_passes = program->passes()->size();
+  if (nb_passes > 1) {
+    // in multipass, all np tracks must be in memory
+    for (const auto* pass : *program->passes()) {
+      for (const auto* record : *pass->records()) {
+        result += record_np_tracks_byte_size(*record);
+      }
+    }
+    // if there is learning records we need to count it as well
+    const auto* learn = program->learning_layer();
+    if (learn) {
+      // learn have both inference & learning tracks, plus weights
+      result += record_np_tracks_byte_size(*learn->inference_registers());
+      result += record_np_tracks_byte_size(*learn->learning_registers());
+      result += record_np_tracks_byte_size(*learn->ram());
+    }
+  } else {
+    // in single pass, tracks are played once at a time, so the required memory
+    // is the size of the largest one
+    const auto* pass = (*program->passes())[0];
+    for (const auto* record : *pass->records()) {
+      result = std::max(result, largest_np_track_byte_size(*record));
+    }
+    // check if there are learning records
+    const auto learn = program->learning_layer();
+    if (learn) {
+      result = std::max(
+          result, largest_np_track_byte_size(*learn->inference_registers()));
+      result = std::max(
+          result, largest_np_track_byte_size(*learn->learning_registers()));
+      result = std::max(result, largest_np_track_byte_size(*learn->ram()));
+    }
+  }
+  return result;
+}
+
+size_t fnp2_tracks_byte_size(const uint8_t* program_data) {
+  size_t result = 0;
+  const auto* program = fb::GetProgram(program_data);
+
+  // iterate over all records from all passes
+  for (const auto* pass : *program->passes()) {
+    for (const auto* record : *pass->records()) {
+      // check if we have FNP2 weights
+      if (record->fnp2_track()) {
+        result += static_cast<size_t>(record->fnp2_track()->data()->size()) *
+                  sizeof(uint32_t);
+      }
+    }
+  }
+  // check if there is learning records that could contain FNP2
+  const auto learn = program->learning_layer();
+  if (learn) {
+    // learning/inference registers should not contain FNP2 track
+    assert(learn->inference_registers()->fnp2_track() == nullptr);
+    assert(learn->learning_registers()->fnp2_track() == nullptr);
+    // but ram could
+    if (learn->ram()->fnp2_track()) {
+      result +=
+          static_cast<size_t>(learn->ram()->fnp2_track()->data()->size()) *
+          sizeof(uint32_t);
+    }
+  }
+  return result;
+}
+
+}  // namespace program
+}  // namespace akida
```

## akida/engine/src/program_play.h

 * *Ordering differences only*

```diff
@@ -1,65 +1,65 @@
-#pragma once
-
-#include <cstddef>
-#include <cstdint>
-
-#include "akida/shape.h"
-#include "dma_events_ops.h"
-
-#include "hardware_device_impl.h"
-#include "multipass_memory.h"
-
-namespace akida {
-namespace program {
-
-void rewind(HardwareDeviceImpl* device, const uint8_t* program_data);
-
-void play_single_pass(HardwareDeviceImpl* device, const uint8_t* program_data);
-void play_multi_pass(HardwareDeviceImpl* device, const uint8_t* program_data,
-                     MultiPassMemory* multipass_memory);
-
-void configure_learning_mode_single_pass(HardwareDeviceImpl* device,
-                                         const uint8_t* program_data,
-                                         bool learn_en);
-void configure_learning_mode_multi_pass(HardwareDeviceImpl* device,
-                                        const uint8_t* program_data,
-                                        const MultiPassMemory& multipass_memory,
-                                        bool learn_en);
-
-void verify(const HardwareDeviceImpl& device, const uint8_t* data, size_t size);
-
-// Utility functions to get informations from program
-const Index* input_dims(const uint8_t* program_data);
-Shape output_dims(const uint8_t* program_data);
-bool input_is_dense(const uint8_t* program_data);
-bool input_is_fnp(const uint8_t* program_data);
-bool output_is_dense(const uint8_t* program_data);
-dma::OutputFormat output_format(const uint8_t* program_data);
-bool activation(const uint8_t* program_data);
-uint32_t dense_window_w(const uint8_t* program_data);
-uint32_t dense_window_h(const uint8_t* program_data);
-bool can_learn(const uint8_t* program_data);
-uint32_t learn_mem_size(const uint8_t* program_data);
-void learn_mem(HardwareDeviceImpl* device, const uint8_t* program_data,
-               uint32_t* ram_dump);
-void update_learn_mem(HardwareDeviceImpl* device, const uint8_t* program_data,
-                      const uint32_t* ram_dump);
-uint8_t max_num_desc(const uint8_t* program_data);
-bool is_multi_pass(const uint8_t* program_data);
-uint32_t num_passes(const uint8_t* program_data);
-// returns the number of program descriptors required for a given program (it
-// does not include extra descriptor for learning if any)
-uint32_t number_of_program_descriptors_required(const uint8_t* program_data);
-// returns number of extra descriptors for a multipass program
-uint32_t number_of_extra_program_descriptors_required(
-    const uint8_t* program_data);
-size_t program_data_required_memory(const uint8_t* program_data);
-size_t fnp2_tracks_byte_size(const uint8_t* program_data);
-
-template<typename Type>
-using Buffer = std::pair<const Type*, size_t>;
-Buffer<int32_t> shifts(const uint8_t* program_data);
-Buffer<float> scales(const uint8_t* program_data);
-
-}  // namespace program
-}  // namespace akida
+#pragma once
+
+#include <cstddef>
+#include <cstdint>
+
+#include "akida/shape.h"
+#include "dma_events_ops.h"
+
+#include "hardware_device_impl.h"
+#include "multipass_memory.h"
+
+namespace akida {
+namespace program {
+
+void rewind(HardwareDeviceImpl* device, const uint8_t* program_data);
+
+void play_single_pass(HardwareDeviceImpl* device, const uint8_t* program_data);
+void play_multi_pass(HardwareDeviceImpl* device, const uint8_t* program_data,
+                     MultiPassMemory* multipass_memory);
+
+void configure_learning_mode_single_pass(HardwareDeviceImpl* device,
+                                         const uint8_t* program_data,
+                                         bool learn_en);
+void configure_learning_mode_multi_pass(HardwareDeviceImpl* device,
+                                        const uint8_t* program_data,
+                                        const MultiPassMemory& multipass_memory,
+                                        bool learn_en);
+
+void verify(const HardwareDeviceImpl& device, const uint8_t* data, size_t size);
+
+// Utility functions to get informations from program
+const Index* input_dims(const uint8_t* program_data);
+Shape output_dims(const uint8_t* program_data);
+bool input_is_dense(const uint8_t* program_data);
+bool input_is_fnp(const uint8_t* program_data);
+bool output_is_dense(const uint8_t* program_data);
+dma::OutputFormat output_format(const uint8_t* program_data);
+bool activation(const uint8_t* program_data);
+uint32_t dense_window_w(const uint8_t* program_data);
+uint32_t dense_window_h(const uint8_t* program_data);
+bool can_learn(const uint8_t* program_data);
+uint32_t learn_mem_size(const uint8_t* program_data);
+void learn_mem(HardwareDeviceImpl* device, const uint8_t* program_data,
+               uint32_t* ram_dump);
+void update_learn_mem(HardwareDeviceImpl* device, const uint8_t* program_data,
+                      const uint32_t* ram_dump);
+uint8_t max_num_desc(const uint8_t* program_data);
+bool is_multi_pass(const uint8_t* program_data);
+uint32_t num_passes(const uint8_t* program_data);
+// returns the number of program descriptors required for a given program (it
+// does not include extra descriptor for learning if any)
+uint32_t number_of_program_descriptors_required(const uint8_t* program_data);
+// returns number of extra descriptors for a multipass program
+uint32_t number_of_extra_program_descriptors_required(
+    const uint8_t* program_data);
+size_t program_data_required_memory(const uint8_t* program_data);
+size_t fnp2_tracks_byte_size(const uint8_t* program_data);
+
+template<typename Type>
+using Buffer = std::pair<const Type*, size_t>;
+Buffer<int32_t> shifts(const uint8_t* program_data);
+Buffer<float> scales(const uint8_t* program_data);
+
+}  // namespace program
+}  // namespace akida
```

## akida/engine/src/registers_dma_engine.h

 * *Ordering differences only*

```diff
@@ -1,185 +1,185 @@
-#pragma once
-
-#include <cstdint>
-#include "infra/registers_common.h"
-
-namespace akida {
-
-// DMA controllers offsets, to be applied after top level register base
-static constexpr uint32_t DMA_EVENT_REG_BASE = 0x00020000;
-static constexpr uint32_t DMA_HRC_REG_BASE = 0x00028000;
-static constexpr uint32_t DMA_CONFIG_REG_BASE = 0x00030000;
-static inline uint32_t dma_event_reg_base(const uint32_t top_level_reg_base) {
-  return top_level_reg_base + DMA_EVENT_REG_BASE;
-}
-static inline uint32_t dma_hrc_reg_base(const uint32_t top_level_reg_base) {
-  return top_level_reg_base + DMA_HRC_REG_BASE;
-}
-static inline uint32_t dma_config_reg_base(const uint32_t top_level_reg_base) {
-  return top_level_reg_base + DMA_CONFIG_REG_BASE;
-}
-
-// DMA core register
-static constexpr uint32_t DMA_CTRL_REG = 0x0;
-static constexpr RegDetail DMA_CTRL_VERSION(0, 2);
-static constexpr RegDetail DMA_CTRL_RUN(8);
-static constexpr RegDetail DMA_CTRL_SOFT_RESET(9);
-static constexpr RegDetail DMA_CTRL_INT_EN(10);
-static constexpr RegDetail DMA_CTRL_RUN_HW_EN(11);
-static constexpr RegDetail DMA_CTRL_OB_BIG_ENDIAN(20);
-static constexpr RegDetail DMA_CTRL_IB_BIG_ENDIAN(21);
-static constexpr RegDetail DMA_CTRL_VALID_FIFO_EN(23);
-static constexpr RegDetail DMA_CTRL_WR_INFO_EN(24);
-static constexpr RegDetail DMA_CTRL_WR_INFO_HDR(25);
-static constexpr RegDetail DMA_CTRL_WR_INFO_HDR_SZ(26, 31);
-
-// Descriptor container register
-static constexpr uint32_t DMA_DESC_CONT_REG = 0x4;
-static constexpr RegDetail DMA_CUR_DESC_CONT(0, 7);
-static constexpr RegDetail DMA_LAST_DESC_CONT(16, 23);
-
-// Container address register (32 bit)
-static constexpr uint32_t DMA_CONT_ADDR_REG = 0x8;
-
-// Container size register
-static constexpr uint32_t DMA_CONT_SIZE_REG = 0xc;
-static constexpr RegDetail DMA_DESC_CONT_SIZE(0, 15);
-static constexpr RegDetail DMA_MAX_DESC_CONTS(16, 23);
-
-// Descriptor status register
-static constexpr uint32_t DMA_DESC_STATUS_REG = 0x10;
-static constexpr RegDetail DMA_DESC_VERSION(0, 3);
-static constexpr RegDetail DMA_JOB_ID(16, 31);
-
-// Input payload address register (32 bit)
-static constexpr uint32_t DMA_INPUT_PAYLOAD_REG = 0x14;
-
-// Output payload address register (32 bit)
-static constexpr uint32_t DMA_OUTPUT_PAYLOAD_REG = 0x18;
-
-// Output word count register (32 bit)
-static constexpr uint32_t DMA_OUTPUT_WORD_COUNT_REG = 0x1c;
-
-// Input word count register
-static constexpr uint32_t DMA_INPUT_WORD_COUNT_REG = 0x20;
-static constexpr RegDetail DMA_INPUT_WORD_COUNT(0, 14);
-
-// Inbound buffer monitor control register
-static constexpr uint32_t DMA_IB_BUF_MON_CTRL_REG = 0x24;
-static constexpr RegDetail DMA_STATUS_CLEAR(0);
-static constexpr RegDetail DMA_BUFFER_CNTR_CLEAR(1);
-static constexpr RegDetail DMA_JOB_ID_FIFO_CLEAR(2);
-static constexpr RegDetail DMA_BUF_END_SELECT(8, 9);
-static constexpr RegDetail DMA_BUF_CNTR_EN(12);
-static constexpr RegDetail DMA_BUF_TIMER_EN(13);
-static constexpr RegDetail DMA_JOB_ID_FIFO_EN(14);
-static constexpr RegDetail DMA_BUFFER_END_MASK_OB_END(16);
-static constexpr RegDetail DMA_BUFFER_END_MASK_IB_END(17);
-static constexpr RegDetail DMA_BUFFER_END_MASK_EXT_DMA_END(18);
-static constexpr RegDetail DMA_BUFFER_END_MASK_DESC_BURST_END(19);
-
-// Buffer monitor status register
-static constexpr uint32_t DMA_BUF_MON_STATUS_REG = 0x28;
-static constexpr RegDetail DMA_BUFFER_END_STATUS(0, 3);
-static constexpr RegDetail DMA_BUFFER_END_STATUS_OB(0);
-static constexpr RegDetail DMA_BUFFER_END_STATUS_IB(1);
-static constexpr RegDetail DMA_BUFFER_END_STATUS_EXT_BUF_END(2);
-static constexpr RegDetail DMA_BUFFER_END_STATUS_DESC_BURST_DONE(3);
-static constexpr RegDetail DMA_BUFFER_END_INTS(16, 19);
-static constexpr RegDetail DMA_BUFFER_END_INTS_OB(16);
-static constexpr RegDetail DMA_BUFFER_END_INTS_IB(17);
-static constexpr RegDetail DMA_BUFFER_END_INTS_EXT_BUF_END(18);
-static constexpr RegDetail DMA_BUFFER_END_INTS_DESC_BURST_DONE(19);
-
-// Buffer counter status register (32 bit)
-static constexpr uint32_t DMA_BUFFER_COUNTER_STATUS_REG = 0x2c;
-
-// Buffer timer status register (32 bit)
-static constexpr uint32_t DMA_BUFFER_TIMER_STATUS_REG = 0x30;
-
-// Descriptor Start Delays Register
-static constexpr uint32_t DMA_DESC_START_DELAYS_REG = 0x38;
-static constexpr RegDetail DMA_DESC_START_DELAY(0, 9);
-
-// Extra Descriptors Control
-static constexpr uint32_t DMA_EXTRA_DESC_CTRL_REG = 0x3c;
-static constexpr RegDetail DMA_LAST_EXTRA_DESCRIPTOR(0, 7);
-static constexpr RegDetail DMA_EXTRA_DESC_ENABLE(12);
-
-// Debug registers
-static constexpr uint32_t DMA_DEBUG_CTRL_REG = 0x40;
-static constexpr uint32_t DMA_DEBUG_BUS0_STATUS_REG = 0x60;
-static constexpr uint32_t DMA_DEBUG_BUS1_STATUS_REG = 0x64;
-static constexpr uint32_t DMA_DEBUG_BUS2_STATUS_REG = 0x68;
-
-// Job ID FIFO register
-static constexpr uint32_t DMA_JOB_ID_FIFO_REG = 0x50;
-static constexpr RegDetail DMA_JOB_ID_FIFO_CNT(0, 7);
-static constexpr RegDetail DMA_JOB_ID_VALID(8);
-static constexpr RegDetail DMA_JOB_ID_FIFO_OUT(16, 31);
-
-// Replay Buffer Control register
-static constexpr uint32_t DMA_REPLAY_BUF_CTRL_REG = 0x70;
-static constexpr RegDetail DMA_REPLAY_MAX_DESC_BURST_MODE(0);
-static constexpr RegDetail DMA_REPLAY_HW_OB_ADDR_GEN_MODE(4);
-static constexpr RegDetail DMA_REPLAY_HW_OB_ADDR_DYN_MODE(5);
-static constexpr RegDetail DMA_REPLAY_HW_OB_DESC_WORD5_EN(6);
-static constexpr RegDetail DMA_REPLAY_START_HALT_EN(8);
-static constexpr RegDetail DMA_REPLAY_INITIAL_START_HALT(9);
-static constexpr RegDetail DMA_REPLAY_BUFFER_MODE(16);
-static constexpr RegDetail DMA_REPLAY_TIMER_MODE(24);
-
-// Replay Burst Value register
-static constexpr uint32_t DMA_REPLAY_BURST_VAL_REG = 0x74;
-static constexpr RegDetail DMA_REPLAY_MAX_DESC_BURST_VALUE(0, 7);
-static constexpr RegDetail DMA_REPLAY_LOOPS(16, 23);
-static constexpr RegDetail DMA_REPLAY_LOOPS_LAYER_PR(24, 31);
-
-// Replay Descriptor Buffer Address register (32 bit)
-static constexpr uint32_t DMA_REPLAY_DESC_MAIN_BUF_ADDR_REG = 0x78;
-static constexpr RegDetail DMA_REPLAY_DESC_MAIN_BUF_ADDR(0, 31);
-
-// Replay Descriptor Scratch Buffer Address register (32 bit)
-static constexpr uint32_t DMA_REPLAY_DESC_SCRATCH_BUF_ADDR_REG = 0x7c;
-static constexpr RegDetail DMA_REPLAY_DESC_SCRATCH_BUF_ADDR(0, 31);
-
-// Replay OB Event Buffer Address register (32 bit)
-static constexpr uint32_t DMA_REPLAY_OB_EVENT_BUF_ADDR_REG = 0x80;
-static constexpr RegDetail DMA_REPLAY_OB_EVENT_BUF_ADDR(0, 31);
-
-// Replay OB Event Scratch Address register (32 bit)
-static constexpr uint32_t DMA_REPLAY_OB_EVENT_SCRATCH_ADDR_REG = 0x84;
-static constexpr RegDetail DMA_REPLAY_OB_EVENT_SCRATCH_ADDR(0, 31);
-
-// Replay OB Buffer Offset Address register
-static constexpr uint32_t DMA_REPLAY_OB_BUF_ADDR_REG = 0x88;
-static constexpr RegDetail DMA_REPLAY_OB_DESC_BUF_OFFSET(0, 2);
-static constexpr RegDetail DMA_REPLAY_OB_EVENTS_BUF_OFFSET(16, 31);
-
-// Replay Maximum OB Offset Buffers register
-static constexpr uint32_t DMA_REPLAY_MAX_OB_BUFFERS_REG = 0x8c;
-static constexpr RegDetail DMA_REPLAY_MAX_OB_DESC_BUFFERS(0, 11);
-static constexpr RegDetail DMA_REPLAY_MAX_OB_EVENTS_BUFFERS(16, 31);
-
-// Replay Descriptor Word5 register
-static constexpr uint32_t DMA_REPLAY_DESC_WORD5_REG = 0x90;
-static constexpr RegDetail DMA_REPLAY_DESC_WORD5(0, 31);
-
-// DMA Interrupt Interval register
-static constexpr uint32_t DMA_INTERRUPT_INTERVAL_REG = 0x94;
-static constexpr RegDetail DMA_INBOUND_INTERVAL(0, 7);
-static constexpr RegDetail DMA_OUTBOUND_INTERVAL(16, 23);
-
-// DMA OB PLD Clear Size Register
-static constexpr uint32_t DMA_OB_PLD_CLEAR_SIZE_REG = 0x98;
-static constexpr RegDetail DMA_OB_PLD_CLR_SIZE(0, 27);
-static constexpr RegDetail DMA_OB_PLD_CLR_EN(31);
-
-// DMA Reset Control register
-static constexpr uint32_t DMA_RESET_CTRL_REG = 0xa0;
-static constexpr RegDetail DMA_LOGIC_RESET(0);
-static constexpr RegDetail DMA_IB_RESET(1);
-static constexpr RegDetail DMA_OB_RESET(2);
-static constexpr RegDetail DMA_FORCE_BURST_RESUME(4);
-
-}  // namespace akida
+#pragma once
+
+#include <cstdint>
+#include "infra/registers_common.h"
+
+namespace akida {
+
+// DMA controllers offsets, to be applied after top level register base
+static constexpr uint32_t DMA_EVENT_REG_BASE = 0x00020000;
+static constexpr uint32_t DMA_HRC_REG_BASE = 0x00028000;
+static constexpr uint32_t DMA_CONFIG_REG_BASE = 0x00030000;
+static inline uint32_t dma_event_reg_base(const uint32_t top_level_reg_base) {
+  return top_level_reg_base + DMA_EVENT_REG_BASE;
+}
+static inline uint32_t dma_hrc_reg_base(const uint32_t top_level_reg_base) {
+  return top_level_reg_base + DMA_HRC_REG_BASE;
+}
+static inline uint32_t dma_config_reg_base(const uint32_t top_level_reg_base) {
+  return top_level_reg_base + DMA_CONFIG_REG_BASE;
+}
+
+// DMA core register
+static constexpr uint32_t DMA_CTRL_REG = 0x0;
+static constexpr RegDetail DMA_CTRL_VERSION(0, 2);
+static constexpr RegDetail DMA_CTRL_RUN(8);
+static constexpr RegDetail DMA_CTRL_SOFT_RESET(9);
+static constexpr RegDetail DMA_CTRL_INT_EN(10);
+static constexpr RegDetail DMA_CTRL_RUN_HW_EN(11);
+static constexpr RegDetail DMA_CTRL_OB_BIG_ENDIAN(20);
+static constexpr RegDetail DMA_CTRL_IB_BIG_ENDIAN(21);
+static constexpr RegDetail DMA_CTRL_VALID_FIFO_EN(23);
+static constexpr RegDetail DMA_CTRL_WR_INFO_EN(24);
+static constexpr RegDetail DMA_CTRL_WR_INFO_HDR(25);
+static constexpr RegDetail DMA_CTRL_WR_INFO_HDR_SZ(26, 31);
+
+// Descriptor container register
+static constexpr uint32_t DMA_DESC_CONT_REG = 0x4;
+static constexpr RegDetail DMA_CUR_DESC_CONT(0, 7);
+static constexpr RegDetail DMA_LAST_DESC_CONT(16, 23);
+
+// Container address register (32 bit)
+static constexpr uint32_t DMA_CONT_ADDR_REG = 0x8;
+
+// Container size register
+static constexpr uint32_t DMA_CONT_SIZE_REG = 0xc;
+static constexpr RegDetail DMA_DESC_CONT_SIZE(0, 15);
+static constexpr RegDetail DMA_MAX_DESC_CONTS(16, 23);
+
+// Descriptor status register
+static constexpr uint32_t DMA_DESC_STATUS_REG = 0x10;
+static constexpr RegDetail DMA_DESC_VERSION(0, 3);
+static constexpr RegDetail DMA_JOB_ID(16, 31);
+
+// Input payload address register (32 bit)
+static constexpr uint32_t DMA_INPUT_PAYLOAD_REG = 0x14;
+
+// Output payload address register (32 bit)
+static constexpr uint32_t DMA_OUTPUT_PAYLOAD_REG = 0x18;
+
+// Output word count register (32 bit)
+static constexpr uint32_t DMA_OUTPUT_WORD_COUNT_REG = 0x1c;
+
+// Input word count register
+static constexpr uint32_t DMA_INPUT_WORD_COUNT_REG = 0x20;
+static constexpr RegDetail DMA_INPUT_WORD_COUNT(0, 14);
+
+// Inbound buffer monitor control register
+static constexpr uint32_t DMA_IB_BUF_MON_CTRL_REG = 0x24;
+static constexpr RegDetail DMA_STATUS_CLEAR(0);
+static constexpr RegDetail DMA_BUFFER_CNTR_CLEAR(1);
+static constexpr RegDetail DMA_JOB_ID_FIFO_CLEAR(2);
+static constexpr RegDetail DMA_BUF_END_SELECT(8, 9);
+static constexpr RegDetail DMA_BUF_CNTR_EN(12);
+static constexpr RegDetail DMA_BUF_TIMER_EN(13);
+static constexpr RegDetail DMA_JOB_ID_FIFO_EN(14);
+static constexpr RegDetail DMA_BUFFER_END_MASK_OB_END(16);
+static constexpr RegDetail DMA_BUFFER_END_MASK_IB_END(17);
+static constexpr RegDetail DMA_BUFFER_END_MASK_EXT_DMA_END(18);
+static constexpr RegDetail DMA_BUFFER_END_MASK_DESC_BURST_END(19);
+
+// Buffer monitor status register
+static constexpr uint32_t DMA_BUF_MON_STATUS_REG = 0x28;
+static constexpr RegDetail DMA_BUFFER_END_STATUS(0, 3);
+static constexpr RegDetail DMA_BUFFER_END_STATUS_OB(0);
+static constexpr RegDetail DMA_BUFFER_END_STATUS_IB(1);
+static constexpr RegDetail DMA_BUFFER_END_STATUS_EXT_BUF_END(2);
+static constexpr RegDetail DMA_BUFFER_END_STATUS_DESC_BURST_DONE(3);
+static constexpr RegDetail DMA_BUFFER_END_INTS(16, 19);
+static constexpr RegDetail DMA_BUFFER_END_INTS_OB(16);
+static constexpr RegDetail DMA_BUFFER_END_INTS_IB(17);
+static constexpr RegDetail DMA_BUFFER_END_INTS_EXT_BUF_END(18);
+static constexpr RegDetail DMA_BUFFER_END_INTS_DESC_BURST_DONE(19);
+
+// Buffer counter status register (32 bit)
+static constexpr uint32_t DMA_BUFFER_COUNTER_STATUS_REG = 0x2c;
+
+// Buffer timer status register (32 bit)
+static constexpr uint32_t DMA_BUFFER_TIMER_STATUS_REG = 0x30;
+
+// Descriptor Start Delays Register
+static constexpr uint32_t DMA_DESC_START_DELAYS_REG = 0x38;
+static constexpr RegDetail DMA_DESC_START_DELAY(0, 9);
+
+// Extra Descriptors Control
+static constexpr uint32_t DMA_EXTRA_DESC_CTRL_REG = 0x3c;
+static constexpr RegDetail DMA_LAST_EXTRA_DESCRIPTOR(0, 7);
+static constexpr RegDetail DMA_EXTRA_DESC_ENABLE(12);
+
+// Debug registers
+static constexpr uint32_t DMA_DEBUG_CTRL_REG = 0x40;
+static constexpr uint32_t DMA_DEBUG_BUS0_STATUS_REG = 0x60;
+static constexpr uint32_t DMA_DEBUG_BUS1_STATUS_REG = 0x64;
+static constexpr uint32_t DMA_DEBUG_BUS2_STATUS_REG = 0x68;
+
+// Job ID FIFO register
+static constexpr uint32_t DMA_JOB_ID_FIFO_REG = 0x50;
+static constexpr RegDetail DMA_JOB_ID_FIFO_CNT(0, 7);
+static constexpr RegDetail DMA_JOB_ID_VALID(8);
+static constexpr RegDetail DMA_JOB_ID_FIFO_OUT(16, 31);
+
+// Replay Buffer Control register
+static constexpr uint32_t DMA_REPLAY_BUF_CTRL_REG = 0x70;
+static constexpr RegDetail DMA_REPLAY_MAX_DESC_BURST_MODE(0);
+static constexpr RegDetail DMA_REPLAY_HW_OB_ADDR_GEN_MODE(4);
+static constexpr RegDetail DMA_REPLAY_HW_OB_ADDR_DYN_MODE(5);
+static constexpr RegDetail DMA_REPLAY_HW_OB_DESC_WORD5_EN(6);
+static constexpr RegDetail DMA_REPLAY_START_HALT_EN(8);
+static constexpr RegDetail DMA_REPLAY_INITIAL_START_HALT(9);
+static constexpr RegDetail DMA_REPLAY_BUFFER_MODE(16);
+static constexpr RegDetail DMA_REPLAY_TIMER_MODE(24);
+
+// Replay Burst Value register
+static constexpr uint32_t DMA_REPLAY_BURST_VAL_REG = 0x74;
+static constexpr RegDetail DMA_REPLAY_MAX_DESC_BURST_VALUE(0, 7);
+static constexpr RegDetail DMA_REPLAY_LOOPS(16, 23);
+static constexpr RegDetail DMA_REPLAY_LOOPS_LAYER_PR(24, 31);
+
+// Replay Descriptor Buffer Address register (32 bit)
+static constexpr uint32_t DMA_REPLAY_DESC_MAIN_BUF_ADDR_REG = 0x78;
+static constexpr RegDetail DMA_REPLAY_DESC_MAIN_BUF_ADDR(0, 31);
+
+// Replay Descriptor Scratch Buffer Address register (32 bit)
+static constexpr uint32_t DMA_REPLAY_DESC_SCRATCH_BUF_ADDR_REG = 0x7c;
+static constexpr RegDetail DMA_REPLAY_DESC_SCRATCH_BUF_ADDR(0, 31);
+
+// Replay OB Event Buffer Address register (32 bit)
+static constexpr uint32_t DMA_REPLAY_OB_EVENT_BUF_ADDR_REG = 0x80;
+static constexpr RegDetail DMA_REPLAY_OB_EVENT_BUF_ADDR(0, 31);
+
+// Replay OB Event Scratch Address register (32 bit)
+static constexpr uint32_t DMA_REPLAY_OB_EVENT_SCRATCH_ADDR_REG = 0x84;
+static constexpr RegDetail DMA_REPLAY_OB_EVENT_SCRATCH_ADDR(0, 31);
+
+// Replay OB Buffer Offset Address register
+static constexpr uint32_t DMA_REPLAY_OB_BUF_ADDR_REG = 0x88;
+static constexpr RegDetail DMA_REPLAY_OB_DESC_BUF_OFFSET(0, 2);
+static constexpr RegDetail DMA_REPLAY_OB_EVENTS_BUF_OFFSET(16, 31);
+
+// Replay Maximum OB Offset Buffers register
+static constexpr uint32_t DMA_REPLAY_MAX_OB_BUFFERS_REG = 0x8c;
+static constexpr RegDetail DMA_REPLAY_MAX_OB_DESC_BUFFERS(0, 11);
+static constexpr RegDetail DMA_REPLAY_MAX_OB_EVENTS_BUFFERS(16, 31);
+
+// Replay Descriptor Word5 register
+static constexpr uint32_t DMA_REPLAY_DESC_WORD5_REG = 0x90;
+static constexpr RegDetail DMA_REPLAY_DESC_WORD5(0, 31);
+
+// DMA Interrupt Interval register
+static constexpr uint32_t DMA_INTERRUPT_INTERVAL_REG = 0x94;
+static constexpr RegDetail DMA_INBOUND_INTERVAL(0, 7);
+static constexpr RegDetail DMA_OUTBOUND_INTERVAL(16, 23);
+
+// DMA OB PLD Clear Size Register
+static constexpr uint32_t DMA_OB_PLD_CLEAR_SIZE_REG = 0x98;
+static constexpr RegDetail DMA_OB_PLD_CLR_SIZE(0, 27);
+static constexpr RegDetail DMA_OB_PLD_CLR_EN(31);
+
+// DMA Reset Control register
+static constexpr uint32_t DMA_RESET_CTRL_REG = 0xa0;
+static constexpr RegDetail DMA_LOGIC_RESET(0);
+static constexpr RegDetail DMA_IB_RESET(1);
+static constexpr RegDetail DMA_OB_RESET(2);
+static constexpr RegDetail DMA_FORCE_BURST_RESUME(4);
+
+}  // namespace akida
```

## akida/engine/src/registers_reset.h

 * *Ordering differences only*

```diff
@@ -1,24 +1,24 @@
-#pragma once
-
-#include <cstdint>
-#include "infra/registers_common.h"
-
-namespace akida {
-
-// Reset control registers
-static constexpr uint32_t REG_CLOCK_RESET_CRG11_BASE = 0xf0001000;
-
-// Akida IP reset register (this is not the SoC system reset register, for that
-// look at 0x1020 and 0x1024)
-static constexpr uint32_t REG_AKIDA_CLOCK_RESET_CTRL =
-    REG_CLOCK_RESET_CRG11_BASE + 0x210;
-static constexpr RegDetail AKIDA_NP_RESET(0);
-static constexpr RegDetail AKIDA_NP_LOGIC_RESET(1);
-static constexpr RegDetail AKIDA_SCC_RESET(4);
-static constexpr RegDetail AKIDA_CORE_CLKPD(16);
-static constexpr RegDetail AKIDA_APB_CLKPD(17);
-static constexpr RegDetail AKIDA_SCC_CLKPD(18);
-static constexpr RegDetail AKIDA_RTC_CLKPD(20);
-static constexpr RegDetail AKIDA_LPDDR_CLKPD(21);
-
-}  // namespace akida
+#pragma once
+
+#include <cstdint>
+#include "infra/registers_common.h"
+
+namespace akida {
+
+// Reset control registers
+static constexpr uint32_t REG_CLOCK_RESET_CRG11_BASE = 0xf0001000;
+
+// Akida IP reset register (this is not the SoC system reset register, for that
+// look at 0x1020 and 0x1024)
+static constexpr uint32_t REG_AKIDA_CLOCK_RESET_CTRL =
+    REG_CLOCK_RESET_CRG11_BASE + 0x210;
+static constexpr RegDetail AKIDA_NP_RESET(0);
+static constexpr RegDetail AKIDA_NP_LOGIC_RESET(1);
+static constexpr RegDetail AKIDA_SCC_RESET(4);
+static constexpr RegDetail AKIDA_CORE_CLKPD(16);
+static constexpr RegDetail AKIDA_APB_CLKPD(17);
+static constexpr RegDetail AKIDA_SCC_CLKPD(18);
+static constexpr RegDetail AKIDA_RTC_CLKPD(20);
+static constexpr RegDetail AKIDA_LPDDR_CLKPD(21);
+
+}  // namespace akida
```

## akida/engine/src/reset_nps.cpp

 * *Ordering differences only*

```diff
@@ -1,28 +1,28 @@
-#include "reset_nps.h"
-
-#include "akida/registers_top_level.h"
-#include "infra/system.h"
-
-namespace akida {
-
-// This method resets logic and configuration of all NPs. It is available on
-// versions > nsoc v1
-void reset_nps_logic_and_cfg(HardwareDriver* driver) {
-  const auto top_level_reg_offset = driver->top_level_reg();
-  auto reg_gen_ctrl =
-      driver->read32(top_level_reg_offset + REG_GENERAL_CONTROL);
-  // Reset logic & configuration
-  set_field(&reg_gen_ctrl, AK_LOGIC_RST, 1);
-  set_field(&reg_gen_ctrl, AK_MESH_RST, 1);
-  driver->write32(top_level_reg_offset + REG_GENERAL_CONTROL, reg_gen_ctrl);
-  // 20 cycles should be waited. Waiting 1ms is more than enough.
-  msleep(1);
-  // Fields need to be reset to 0
-  set_field(&reg_gen_ctrl, AK_LOGIC_RST, 0);
-  set_field(&reg_gen_ctrl, AK_MESH_RST, 0);
-  driver->write32(top_level_reg_offset + REG_GENERAL_CONTROL, reg_gen_ctrl);
-  // 40 cycles should be waited. Waiting 1ms is more than enough.
-  msleep(1);
-}
-
-}  // namespace akida
+#include "reset_nps.h"
+
+#include "akida/registers_top_level.h"
+#include "infra/system.h"
+
+namespace akida {
+
+// This method resets logic and configuration of all NPs. It is available on
+// versions > nsoc v1
+void reset_nps_logic_and_cfg(HardwareDriver* driver) {
+  const auto top_level_reg_offset = driver->top_level_reg();
+  auto reg_gen_ctrl =
+      driver->read32(top_level_reg_offset + REG_GENERAL_CONTROL);
+  // Reset logic & configuration
+  set_field(&reg_gen_ctrl, AK_LOGIC_RST, 1);
+  set_field(&reg_gen_ctrl, AK_MESH_RST, 1);
+  driver->write32(top_level_reg_offset + REG_GENERAL_CONTROL, reg_gen_ctrl);
+  // 20 cycles should be waited. Waiting 1ms is more than enough.
+  msleep(1);
+  // Fields need to be reset to 0
+  set_field(&reg_gen_ctrl, AK_LOGIC_RST, 0);
+  set_field(&reg_gen_ctrl, AK_MESH_RST, 0);
+  driver->write32(top_level_reg_offset + REG_GENERAL_CONTROL, reg_gen_ctrl);
+  // 40 cycles should be waited. Waiting 1ms is more than enough.
+  msleep(1);
+}
+
+}  // namespace akida
```

## akida/engine/src/reset_nps.h

 * *Ordering differences only*

```diff
@@ -1,14 +1,14 @@
-#pragma once
-
-#include <vector>
-
-#include "akida/np.h"
-#include "infra/hardware_driver.h"
-
-namespace akida {
-
-// This method resets logic and configuration of all NPs. It is available on
-// versions > nsoc v1
-void reset_nps_logic_and_cfg(HardwareDriver* driver);
-
-}  // namespace akida
+#pragma once
+
+#include <vector>
+
+#include "akida/np.h"
+#include "infra/hardware_driver.h"
+
+namespace akida {
+
+// This method resets logic and configuration of all NPs. It is available on
+// versions > nsoc v1
+void reset_nps_logic_and_cfg(HardwareDriver* driver);
+
+}  // namespace akida
```

## akida/engine/src/sparse.cpp

 * *Ordering differences only*

```diff
@@ -1,24 +1,24 @@
-#include "akida/sparse.h"
-
-#include <algorithm>
-#include <cstdint>
-
-#include "akida/shape.h"
-#include "akida/tensor.h"
-
-namespace akida {
-
-bool Sparse::operator==(const Tensor& ref) const {
-  // We cannot compare Sparse easily, so we first to convert the ref to a dense
-  auto dense = dynamic_cast<const Dense*>(&ref);
-  if (dense) {
-    // We can use the Dense operator directly
-    return *dense == *this;
-  }
-  // As a fallback, we create a ColMajor Dense clone
-  auto dense_clone = Dense::from_sparse(*this, Dense::Layout::ColMajor);
-  // return Dense comparison
-  return *dense_clone == ref;
-}
-
-}  // namespace akida
+#include "akida/sparse.h"
+
+#include <algorithm>
+#include <cstdint>
+
+#include "akida/shape.h"
+#include "akida/tensor.h"
+
+namespace akida {
+
+bool Sparse::operator==(const Tensor& ref) const {
+  // We cannot compare Sparse easily, so we first to convert the ref to a dense
+  auto dense = dynamic_cast<const Dense*>(&ref);
+  if (dense) {
+    // We can use the Dense operator directly
+    return *dense == *this;
+  }
+  // As a fallback, we create a ColMajor Dense clone
+  auto dense_clone = Dense::from_sparse(*this, Dense::Layout::ColMajor);
+  // return Dense comparison
+  return *dense_clone == ref;
+}
+
+}  // namespace akida
```

## akida/engine/src/tensor.cpp

 * *Ordering differences only*

```diff
@@ -1,34 +1,34 @@
-#include "akida/tensor.h"
-
-#include <memory>
-
-#include "akida/dense.h"
-#include "akida/sparse.h"
-
-#include "infra/system.h"
-
-namespace akida {
-
-DenseConstPtr Tensor::as_dense(TensorConstPtr tensor) {
-  return std::dynamic_pointer_cast<const Dense>(tensor);
-}
-
-SparseConstPtr Tensor::as_sparse(TensorConstPtr tensor) {
-  return std::dynamic_pointer_cast<const Sparse>(tensor);
-}
-
-DenseConstPtr Tensor::ensure_dense(TensorConstPtr tensor) {
-  // Assume this is already a Dense
-  auto dense = Tensor::as_dense(tensor);
-  if (dense) {
-    return dense;
-  }
-  // If we were passed a Sparse, convert it to a Dense
-  auto sparse = std::dynamic_pointer_cast<const Sparse>(tensor);
-  if (sparse) {
-    return Dense::from_sparse(*sparse, Dense::Layout::RowMajor);
-  }
-  return nullptr;
-}
-
-}  // namespace akida
+#include "akida/tensor.h"
+
+#include <memory>
+
+#include "akida/dense.h"
+#include "akida/sparse.h"
+
+#include "infra/system.h"
+
+namespace akida {
+
+DenseConstPtr Tensor::as_dense(TensorConstPtr tensor) {
+  return std::dynamic_pointer_cast<const Dense>(tensor);
+}
+
+SparseConstPtr Tensor::as_sparse(TensorConstPtr tensor) {
+  return std::dynamic_pointer_cast<const Sparse>(tensor);
+}
+
+DenseConstPtr Tensor::ensure_dense(TensorConstPtr tensor) {
+  // Assume this is already a Dense
+  auto dense = Tensor::as_dense(tensor);
+  if (dense) {
+    return dense;
+  }
+  // If we were passed a Sparse, convert it to a Dense
+  auto sparse = std::dynamic_pointer_cast<const Sparse>(tensor);
+  if (sparse) {
+    return Dense::from_sparse(*sparse, Dense::Layout::RowMajor);
+  }
+  return nullptr;
+}
+
+}  // namespace akida
```

## akida/engine/src/version.cpp

 * *Ordering differences only*

```diff
@@ -1,10 +1,10 @@
-#include "akida/version.h"
-
-namespace akida {
-
-const char* version() {
-  // Akida version is generated in CMake and added as define to this file
-  return AKIDA_VERSION;
-}
-
-}  // namespace akida
+#include "akida/version.h"
+
+namespace akida {
+
+const char* version() {
+  // Akida version is generated in CMake and added as define to this file
+  return AKIDA_VERSION;
+}
+
+}  // namespace akida
```

## akida/generate/__init__.py

 * *Ordering differences only*

```diff
@@ -1,19 +1,19 @@
-# ******************************************************************************
-# Copyright 2020 Brainchip Holdings Ltd.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ******************************************************************************
-from .array_to_cpp import array_to_cpp
-from .application_generator import generate_files
-from .test.model.test_generator import ModelTestGenerator
-from .test.engine.test_generator import EngineTestGenerator
+# ******************************************************************************
+# Copyright 2020 Brainchip Holdings Ltd.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ******************************************************************************
+from .array_to_cpp import array_to_cpp
+from .application_generator import generate_files
+from .test.model.test_generator import ModelTestGenerator
+from .test.engine.test_generator import EngineTestGenerator
```

## akida/generate/application_generator.py

 * *Ordering differences only*

```diff
@@ -1,66 +1,66 @@
-import os
-import sys
-import importlib.util
-import inspect
-from pathlib import Path
-import glob
-
-
-class ApplicationGenerator():
-    """This class is an interface for `akida generate` cli command
-
-    In order to generate a sample application with `akida generate` commands,
-    one must create a class that inherits from this class, and override methods
-    to return the desired value.
-    """
-    def generate(self, name, dest_path):
-        """Generate application files
-
-        Override this method to generate the application.
-        """
-        raise NotImplementedError()
-
-
-def _get_generator(fixture_file):
-    # Add the fixture file directory to the system path to allow relative
-    # imports form there
-    path = os.path.split(fixture_file)[0]
-    sys.path.insert(0, path)
-    # Dynamically load the module corresponding to the fixture file
-    name = Path(fixture_file).stem
-    spec = importlib.util.spec_from_file_location(name, fixture_file)
-    if spec is None:
-        return None, None
-    module = importlib.util.module_from_spec(spec)
-    spec.loader.exec_module(module)
-    # Loop over module class symbols
-    for _, obj in inspect.getmembers(module, inspect.isclass):
-        # Only check for members that are defined in the current module, and not imported ones
-        if obj.__module__ == name:
-            # Identify children of ApplicationGenerator
-            if ApplicationGenerator in obj.mro():
-                return name, obj()
-
-    print(f"No ApplicationGenerator children class found in {fixture_file}")
-    return None, None
-
-
-def generate_files(fixtures, dest_path, modules_paths=None):
-    # Add additional modules path to the system path to allow the model files
-    # to import their dependencies
-    if modules_paths is not None:
-        for path in modules_paths:
-            sys.path.insert(0, path)
-    # Evaluate fixtures as globbing patterns
-    fixture_files = []
-    for fixture in fixtures:
-        fixture_files.extend(glob.glob(fixture))
-    # Iterate over each fixture file
-    for fixture_file in fixture_files:
-        if not os.path.isfile(fixture_file):
-            continue
-        # Each fixture file should contain an ApplicationGenerator class
-        name, generator = _get_generator(fixture_file)
-        if name is not None:
-            # Generate application
-            generator.generate(name, dest_path)
+import os
+import sys
+import importlib.util
+import inspect
+from pathlib import Path
+import glob
+
+
+class ApplicationGenerator():
+    """This class is an interface for `akida generate` cli command
+
+    In order to generate a sample application with `akida generate` commands,
+    one must create a class that inherits from this class, and override methods
+    to return the desired value.
+    """
+    def generate(self, name, dest_path):
+        """Generate application files
+
+        Override this method to generate the application.
+        """
+        raise NotImplementedError()
+
+
+def _get_generator(fixture_file):
+    # Add the fixture file directory to the system path to allow relative
+    # imports form there
+    path = os.path.split(fixture_file)[0]
+    sys.path.insert(0, path)
+    # Dynamically load the module corresponding to the fixture file
+    name = Path(fixture_file).stem
+    spec = importlib.util.spec_from_file_location(name, fixture_file)
+    if spec is None:
+        return None, None
+    module = importlib.util.module_from_spec(spec)
+    spec.loader.exec_module(module)
+    # Loop over module class symbols
+    for _, obj in inspect.getmembers(module, inspect.isclass):
+        # Only check for members that are defined in the current module, and not imported ones
+        if obj.__module__ == name:
+            # Identify children of ApplicationGenerator
+            if ApplicationGenerator in obj.mro():
+                return name, obj()
+
+    print(f"No ApplicationGenerator children class found in {fixture_file}")
+    return None, None
+
+
+def generate_files(fixtures, dest_path, modules_paths=None):
+    # Add additional modules path to the system path to allow the model files
+    # to import their dependencies
+    if modules_paths is not None:
+        for path in modules_paths:
+            sys.path.insert(0, path)
+    # Evaluate fixtures as globbing patterns
+    fixture_files = []
+    for fixture in fixtures:
+        fixture_files.extend(glob.glob(fixture))
+    # Iterate over each fixture file
+    for fixture_file in fixture_files:
+        if not os.path.isfile(fixture_file):
+            continue
+        # Each fixture file should contain an ApplicationGenerator class
+        name, generator = _get_generator(fixture_file)
+        if name is not None:
+            # Generate application
+            generator.generate(name, dest_path)
```

## akida/generate/array_to_cpp.py

 * *Ordering differences only*

```diff
@@ -1,176 +1,176 @@
-from pathlib import Path
-import numpy as np
-
-# Bytearray serialization code inspired from:
-#  tensorflow/lite/python/util.py
-#  Copyright 2018, Tensorflow Authors, Apache 2.0 license
-
-
-def _bytes_to_hexa(data, max_line_width=80):
-    """Returns the representation of a bytearray as a comma-separated list of
-     hexadecimal values.
-    """
-
-    starting_pad = "  "
-    array_lines = []
-    array_line = starting_pad
-    for value in bytearray(data):
-        if (len(array_line) + 4) > max_line_width:
-            array_lines.append(array_line + "\n")
-            array_line = starting_pad
-        array_line += " 0x%02x," % value
-    if len(array_line) > len(starting_pad):
-        array_lines.append(array_line + "\n")
-    return "".join(array_lines)
-
-
-def _cpp_implementation(name, declarations):
-    """Wrap CPP declarations into a CPP implementation source
-    """
-
-    template = """
-#include "{name}.h"
-
-// Following tflite example, align on 32-bit if possible.
-#ifdef __has_attribute
-#define HAVE_ATTRIBUTE(x) __has_attribute(x)
-#else
-#define HAVE_ATTRIBUTE(x) 0
-#endif
-#if HAVE_ATTRIBUTE(aligned) || (defined(__GNUC__) && !defined(__clang__))
-#define DATA_ALIGN_ATTRIBUTE __attribute__((aligned(4)))
-#else
-#define DATA_ALIGN_ATTRIBUTE
-#endif
-{declarations}
-"""
-    return template.format(name=name, declarations=declarations)
-
-
-def _c_header(name, declarations):
-    """Wrap CPP declarations into a CPP header source
-    """
-
-    include_guard = "AKIDA_" + name.upper() + "_DATA_H_"
-    template = """
-#ifndef {include_guard}
-#define {include_guard}
-{declarations}
-#endif  // {include_guard}
-"""
-    return template.format(name=name,
-                           declarations=declarations,
-                           include_guard=include_guard)
-
-
-def _bytearray_to_cpp(data, name, max_line_width=80):
-    """Returns strings representing a CPP constant array containing `data`
-     implementation and declaration.
-    """
-
-    values = _bytes_to_hexa(data, max_line_width)
-    length = len(data)
-
-    declarations = """
-const unsigned char {name}[] DATA_ALIGN_ATTRIBUTE = {{
-{values}}};
-const int64_t {name}_len = {length};
-""".format(name=name, values=values, length=length)
-
-    source_text = _cpp_implementation(name, declarations)
-
-    declarations = """
-#include <cstdint>
-extern const unsigned char {name}[];
-extern const int64_t {name}_len;
-""".format(name=name)
-
-    header_text = _c_header(name, declarations)
-
-    return source_text, header_text
-
-
-def _np_array_to_cpp(np_array, name, max_line_width=80):
-    """Returns strings representing a CPP constant array containing `data`
-     implementation and declaration.
-    """
-
-    data = np_array.tobytes()
-    values = _bytes_to_hexa(data, max_line_width)
-    length = len(data)
-    shape = str(np_array.shape)[1:-1]
-
-    if np_array.dtype == np.uint8:
-        tensor_type = "akida::TensorType::uint8"
-    elif np_array.dtype == np.int8:
-        tensor_type = "akida::TensorType::int8"
-    elif np_array.dtype == np.int32:
-        tensor_type = "akida::TensorType::int32"
-    elif np_array.dtype == np.float32:
-        tensor_type = "akida::TensorType::float32"
-    else:
-        raise ValueError(f"Unsupported array type {np_array.dtype}")
-
-    declarations = """
-const unsigned char {name}[] DATA_ALIGN_ATTRIBUTE = {{
-{values}}};
-const int64_t {name}_len = {length};
-const akida::Shape {name}_shape{{{shape}}};
-const akida::TensorType {name}_type = {tensor_type};
-""".format(name=name,
-           values=values,
-           length=length,
-           shape=shape,
-           tensor_type=tensor_type)
-
-    source_text = _cpp_implementation(name, declarations)
-
-    declarations = """
-#include <cstdint>
-#include "akida/shape.h"
-#include "akida/tensor.h"
-extern const unsigned char {name}[];
-extern const int64_t {name}_len;
-extern const akida::Shape {name}_shape;
-extern const akida::TensorType {name}_type;
-""".format(name=name)
-
-    header_text = _c_header(name, declarations)
-
-    return source_text, header_text
-
-
-def array_to_cpp(path, array, name):
-    """Generates CPP source files representing a python array
-
-    This creates a pair of header (.h)  and implementation (.cpp) containing
-    the declaration and implementation of a CPP bytes buffer whose content
-    matches the source array content.
-
-    If the source buffer is a bytearray, the following symbols are declared:
-
-    extern const unsigned char {name}[];
-    extern const int64_t {name}_len;
-
-    If the source buffer is an np.ndarray, the following additional symbols are
-    declared:
-
-    extern const akida::Shape {name}_shape;
-    extern const akida::TensorType {name}_type;
-
-    Args:
-        path (str): the path to the generated source files directory
-        array (bytearray or np.ndarray): the source array
-        name: the source files name (without the extension)
-    """
-    if isinstance(array, np.ndarray):
-        source_text, header_text = _np_array_to_cpp(array, name)
-    else:
-        source_text, header_text = _bytearray_to_cpp(array, name)
-    # Create directory if it does not exist
-    Path(path).mkdir(parents=True, exist_ok=True)
-    # Save header and source file
-    with open(path + '/' + name + '.h', 'w') as file:
-        file.write(header_text)
-    with open(path + '/' + name + '.cpp', 'w') as file:
-        file.write(source_text)
+from pathlib import Path
+import numpy as np
+
+# Bytearray serialization code inspired from:
+#  tensorflow/lite/python/util.py
+#  Copyright 2018, Tensorflow Authors, Apache 2.0 license
+
+
+def _bytes_to_hexa(data, max_line_width=80):
+    """Returns the representation of a bytearray as a comma-separated list of
+     hexadecimal values.
+    """
+
+    starting_pad = "  "
+    array_lines = []
+    array_line = starting_pad
+    for value in bytearray(data):
+        if (len(array_line) + 4) > max_line_width:
+            array_lines.append(array_line + "\n")
+            array_line = starting_pad
+        array_line += " 0x%02x," % value
+    if len(array_line) > len(starting_pad):
+        array_lines.append(array_line + "\n")
+    return "".join(array_lines)
+
+
+def _cpp_implementation(name, declarations):
+    """Wrap CPP declarations into a CPP implementation source
+    """
+
+    template = """
+#include "{name}.h"
+
+// Following tflite example, align on 32-bit if possible.
+#ifdef __has_attribute
+#define HAVE_ATTRIBUTE(x) __has_attribute(x)
+#else
+#define HAVE_ATTRIBUTE(x) 0
+#endif
+#if HAVE_ATTRIBUTE(aligned) || (defined(__GNUC__) && !defined(__clang__))
+#define DATA_ALIGN_ATTRIBUTE __attribute__((aligned(4)))
+#else
+#define DATA_ALIGN_ATTRIBUTE
+#endif
+{declarations}
+"""
+    return template.format(name=name, declarations=declarations)
+
+
+def _c_header(name, declarations):
+    """Wrap CPP declarations into a CPP header source
+    """
+
+    include_guard = "AKIDA_" + name.upper() + "_DATA_H_"
+    template = """
+#ifndef {include_guard}
+#define {include_guard}
+{declarations}
+#endif  // {include_guard}
+"""
+    return template.format(name=name,
+                           declarations=declarations,
+                           include_guard=include_guard)
+
+
+def _bytearray_to_cpp(data, name, max_line_width=80):
+    """Returns strings representing a CPP constant array containing `data`
+     implementation and declaration.
+    """
+
+    values = _bytes_to_hexa(data, max_line_width)
+    length = len(data)
+
+    declarations = """
+const unsigned char {name}[] DATA_ALIGN_ATTRIBUTE = {{
+{values}}};
+const int64_t {name}_len = {length};
+""".format(name=name, values=values, length=length)
+
+    source_text = _cpp_implementation(name, declarations)
+
+    declarations = """
+#include <cstdint>
+extern const unsigned char {name}[];
+extern const int64_t {name}_len;
+""".format(name=name)
+
+    header_text = _c_header(name, declarations)
+
+    return source_text, header_text
+
+
+def _np_array_to_cpp(np_array, name, max_line_width=80):
+    """Returns strings representing a CPP constant array containing `data`
+     implementation and declaration.
+    """
+
+    data = np_array.tobytes()
+    values = _bytes_to_hexa(data, max_line_width)
+    length = len(data)
+    shape = str(np_array.shape)[1:-1]
+
+    if np_array.dtype == np.uint8:
+        tensor_type = "akida::TensorType::uint8"
+    elif np_array.dtype == np.int8:
+        tensor_type = "akida::TensorType::int8"
+    elif np_array.dtype == np.int32:
+        tensor_type = "akida::TensorType::int32"
+    elif np_array.dtype == np.float32:
+        tensor_type = "akida::TensorType::float32"
+    else:
+        raise ValueError(f"Unsupported array type {np_array.dtype}")
+
+    declarations = """
+const unsigned char {name}[] DATA_ALIGN_ATTRIBUTE = {{
+{values}}};
+const int64_t {name}_len = {length};
+const akida::Shape {name}_shape{{{shape}}};
+const akida::TensorType {name}_type = {tensor_type};
+""".format(name=name,
+           values=values,
+           length=length,
+           shape=shape,
+           tensor_type=tensor_type)
+
+    source_text = _cpp_implementation(name, declarations)
+
+    declarations = """
+#include <cstdint>
+#include "akida/shape.h"
+#include "akida/tensor.h"
+extern const unsigned char {name}[];
+extern const int64_t {name}_len;
+extern const akida::Shape {name}_shape;
+extern const akida::TensorType {name}_type;
+""".format(name=name)
+
+    header_text = _c_header(name, declarations)
+
+    return source_text, header_text
+
+
+def array_to_cpp(path, array, name):
+    """Generates CPP source files representing a python array
+
+    This creates a pair of header (.h)  and implementation (.cpp) containing
+    the declaration and implementation of a CPP bytes buffer whose content
+    matches the source array content.
+
+    If the source buffer is a bytearray, the following symbols are declared:
+
+    extern const unsigned char {name}[];
+    extern const int64_t {name}_len;
+
+    If the source buffer is an np.ndarray, the following additional symbols are
+    declared:
+
+    extern const akida::Shape {name}_shape;
+    extern const akida::TensorType {name}_type;
+
+    Args:
+        path (str): the path to the generated source files directory
+        array (bytearray or np.ndarray): the source array
+        name: the source files name (without the extension)
+    """
+    if isinstance(array, np.ndarray):
+        source_text, header_text = _np_array_to_cpp(array, name)
+    else:
+        source_text, header_text = _bytearray_to_cpp(array, name)
+    # Create directory if it does not exist
+    Path(path).mkdir(parents=True, exist_ok=True)
+    # Save header and source file
+    with open(path + '/' + name + '.h', 'w') as file:
+        file.write(header_text)
+    with open(path + '/' + name + '.cpp', 'w') as file:
+        file.write(source_text)
```

## akida/generate/model.py

 * *Ordering differences only*

```diff
@@ -1,26 +1,26 @@
-# ******************************************************************************
-# Copyright 2023 Brainchip Holdings Ltd.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ******************************************************************************
-import os
-from pathlib import Path
-import shutil
-
-
-def deploy_cmake(dest_path):
-    cmake_path = os.path.join(Path(__file__).parent, "test/cmake")
-    dest_path = os.path.join(dest_path, "cmake")
-    if os.path.exists(dest_path):
-        shutil.rmtree(dest_path)
-    shutil.copytree(cmake_path, dest_path)
+# ******************************************************************************
+# Copyright 2023 Brainchip Holdings Ltd.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ******************************************************************************
+import os
+from pathlib import Path
+import shutil
+
+
+def deploy_cmake(dest_path):
+    cmake_path = os.path.join(Path(__file__).parent, "test/cmake")
+    dest_path = os.path.join(dest_path, "cmake")
+    if os.path.exists(dest_path):
+        shutil.rmtree(dest_path)
+    shutil.copytree(cmake_path, dest_path)
```

## akida/generate/test/template.py

 * *Ordering differences only*

```diff
@@ -1,10 +1,10 @@
-import os
-
-
-def test_file_from_template(test_name, tpl_path, dest_path, filename):
-    src_path = os.path.join(tpl_path, filename)
-    dst_path = os.path.join(dest_path, test_name, filename)
-    with open(src_path, 'r') as src, open(dst_path, 'w') as dst:
-        for line in src:
-            # Substitute pattern by filename
-            dst.write(line.replace("TEST_NAME", test_name))
+import os
+
+
+def test_file_from_template(test_name, tpl_path, dest_path, filename):
+    src_path = os.path.join(tpl_path, filename)
+    dst_path = os.path.join(dest_path, test_name, filename)
+    with open(src_path, 'r') as src, open(dst_path, 'w') as dst:
+        for line in src:
+            # Substitute pattern by filename
+            dst.write(line.replace("TEST_NAME", test_name))
```

## akida/generate/test/test_tools.py

 * *Ordering differences only*

```diff
@@ -1,94 +1,94 @@
-import numpy as np
-
-
-def _get_cyclic_input(shape, max_value, duplicate_values):
-    input_size = np.prod(shape)
-    inputs = np.zeros((input_size), dtype=np.uint8)
-    for i in range(input_size):
-        inputs[i] = (i // duplicate_values) % (max_value + 1)
-    return inputs.reshape(shape)
-
-
-def get_cyclic_input(model, n=1, duplicate_values=1):
-    """
-    Generate an input with cyclic values.
-
-    Args:
-        model (:obj:`akida.Model`): the model for which we want an input
-        n (int, optional): number of frames or samples. Default to 1.
-        duplicate_values (int, optional): duplication of each value. For
-            example, a "1,2,3" pattern of values with a duplicate_values of 2
-            will be "1,1,2,2,3,3". Default to 1, the values are not duplicated.
-    Returns:
-        :obj:`np.ndarray`: the cyclic inputs
-    """
-    assert duplicate_values >= 1, "duplicate_values must be strictly positive"
-    input_shape = (n,) + tuple(model.input_shape)
-    bitwidth = model.get_layer(0).input_bits
-    return _get_cyclic_input(input_shape, max_value=2**(bitwidth) - 1,
-                             duplicate_values=duplicate_values)
-
-
-def set_cyclic_weights(model):
-    for j in range(model.get_layer_count()):
-        layer = model.get_layer(j)
-        for var_name in "weights", "weights_pw":
-            if var_name in layer.variables.names:
-                weights = layer.get_variable(var_name)
-                bitwidth = layer.parameters.weights_bits
-                # Generate a vector of cycling weights
-                new_weights = np.zeros(weights.size, dtype=np.int8)
-                if bitwidth == 1:
-                    for i in range(weights.size):
-                        new_weights[i] = i % 2
-                else:
-                    max_value = 2**(bitwidth - 1) - 1
-                    n_values = 2 * max_value + 1
-                    for i in range(weights.size):
-                        # We set weights in sequence, each new sequence
-                        # starting at an increasing value in the sequence
-                        new_weights[i] = (i + i % n_values) % n_values - max_value
-                # Weights are WHCF, but we want to cycle in each filter, so we
-                # reshape the vector to FCHW to have weights grouped by filters
-                new_weights = new_weights.reshape(np.flip(weights.shape))
-                # ... then transpose to obtain the WHCF weights
-                new_weights = np.transpose(new_weights)
-                layer.set_variable(var_name, new_weights)
-
-
-def set_constant_weights(model, value):
-    for i in range(model.get_layer_count()):
-        layer = model.get_layer(i)
-        for var_name in "weights", "weights_pw":
-            if var_name in layer.variables.names:
-                weights = layer.get_variable(var_name)
-                weights[:, :, :, :] = value
-                layer.set_variable(var_name, weights)
-
-
-def set_cyclic_threshold(model):
-    for i in range(model.get_layer_count()):
-        layer = model.get_layer(i)
-        # There will be a threshold everywhere but in the input data
-        if 'threshold' in layer.variables.names:
-            num_neurons = layer.variables["threshold"].size
-            # Set deterministic thresholds and steps
-            # Values must fit in 19 bits (i.e. 20 -1 for the sign), so they are
-            # clipped and multiplied by the sign to keep positive and negative
-            # numbers
-            MAX_THRESH = (1 << 19) - 1
-            # Thresholds are set in the range on -num_neurons and num_neurons.
-            threshold = np.arange(num_neurons, dtype=np.int32) % MAX_THRESH
-            threshold -= (num_neurons // 2)
-            layer.variables["threshold"] = threshold
-            # Steps in hardware are represented as fixed point with 4 bit for
-            # decimals. So the minimum step is 1 / 16 = 0.0625
-            MIN_STEP = 1 / 16
-            # Steps are stored in 20 bit, 4 bit for the decimals, so max value
-            # is:
-            MAX_ACT_STEP = (1 << (20 - 4)) - 1
-            # Steps are required to be strictly positive, so minimum value will
-            #  be MIN_STEP and maximum will be MAX_ACT_STEP
-            layer.variables["act_step"] = np.arange(
-                num_neurons * MIN_STEP, step=MIN_STEP,
-                dtype=np.float32) % MAX_ACT_STEP + MIN_STEP
+import numpy as np
+
+
+def _get_cyclic_input(shape, max_value, duplicate_values):
+    input_size = np.prod(shape)
+    inputs = np.zeros((input_size), dtype=np.uint8)
+    for i in range(input_size):
+        inputs[i] = (i // duplicate_values) % (max_value + 1)
+    return inputs.reshape(shape)
+
+
+def get_cyclic_input(model, n=1, duplicate_values=1):
+    """
+    Generate an input with cyclic values.
+
+    Args:
+        model (:obj:`akida.Model`): the model for which we want an input
+        n (int, optional): number of frames or samples. Default to 1.
+        duplicate_values (int, optional): duplication of each value. For
+            example, a "1,2,3" pattern of values with a duplicate_values of 2
+            will be "1,1,2,2,3,3". Default to 1, the values are not duplicated.
+    Returns:
+        :obj:`np.ndarray`: the cyclic inputs
+    """
+    assert duplicate_values >= 1, "duplicate_values must be strictly positive"
+    input_shape = (n,) + tuple(model.input_shape)
+    bitwidth = model.get_layer(0).input_bits
+    return _get_cyclic_input(input_shape, max_value=2**(bitwidth) - 1,
+                             duplicate_values=duplicate_values)
+
+
+def set_cyclic_weights(model):
+    for j in range(model.get_layer_count()):
+        layer = model.get_layer(j)
+        for var_name in "weights", "weights_pw":
+            if var_name in layer.variables.names:
+                weights = layer.get_variable(var_name)
+                bitwidth = layer.parameters.weights_bits
+                # Generate a vector of cycling weights
+                new_weights = np.zeros(weights.size, dtype=np.int8)
+                if bitwidth == 1:
+                    for i in range(weights.size):
+                        new_weights[i] = i % 2
+                else:
+                    max_value = 2**(bitwidth - 1) - 1
+                    n_values = 2 * max_value + 1
+                    for i in range(weights.size):
+                        # We set weights in sequence, each new sequence
+                        # starting at an increasing value in the sequence
+                        new_weights[i] = (i + i % n_values) % n_values - max_value
+                # Weights are WHCF, but we want to cycle in each filter, so we
+                # reshape the vector to FCHW to have weights grouped by filters
+                new_weights = new_weights.reshape(np.flip(weights.shape))
+                # ... then transpose to obtain the WHCF weights
+                new_weights = np.transpose(new_weights)
+                layer.set_variable(var_name, new_weights)
+
+
+def set_constant_weights(model, value):
+    for i in range(model.get_layer_count()):
+        layer = model.get_layer(i)
+        for var_name in "weights", "weights_pw":
+            if var_name in layer.variables.names:
+                weights = layer.get_variable(var_name)
+                weights[:, :, :, :] = value
+                layer.set_variable(var_name, weights)
+
+
+def set_cyclic_threshold(model):
+    for i in range(model.get_layer_count()):
+        layer = model.get_layer(i)
+        # There will be a threshold everywhere but in the input data
+        if 'threshold' in layer.variables.names:
+            num_neurons = layer.variables["threshold"].size
+            # Set deterministic thresholds and steps
+            # Values must fit in 19 bits (i.e. 20 -1 for the sign), so they are
+            # clipped and multiplied by the sign to keep positive and negative
+            # numbers
+            MAX_THRESH = (1 << 19) - 1
+            # Thresholds are set in the range on -num_neurons and num_neurons.
+            threshold = np.arange(num_neurons, dtype=np.int32) % MAX_THRESH
+            threshold -= (num_neurons // 2)
+            layer.variables["threshold"] = threshold
+            # Steps in hardware are represented as fixed point with 4 bit for
+            # decimals. So the minimum step is 1 / 16 = 0.0625
+            MIN_STEP = 1 / 16
+            # Steps are stored in 20 bit, 4 bit for the decimals, so max value
+            # is:
+            MAX_ACT_STEP = (1 << (20 - 4)) - 1
+            # Steps are required to be strictly positive, so minimum value will
+            #  be MIN_STEP and maximum will be MAX_ACT_STEP
+            layer.variables["act_step"] = np.arange(
+                num_neurons * MIN_STEP, step=MIN_STEP,
+                dtype=np.float32) % MAX_ACT_STEP + MIN_STEP
```

## akida/generate/test/cmake/akida-model.cmake

 * *Ordering differences only*

```diff
@@ -1,30 +1,30 @@
-# ============================================================================
-# To use the Akida model API with generated fixtures you have to include the
-# CMake folder and link the library akida to your CMake target as following:
-#
-# set(CMAKE_INCLUDE_CURRENT_DIR ON)
-# set(CMAKE_MODULE_PATH ${CMAKE_CURRENT_LIST_DIR}/cmake)
-# include(akida-model)
-# target_link_libraries(my_program PRIVATE akida)
-# ============================================================================
-
-cmake_minimum_required(VERSION 3.20)
-set(Python_FIND_VIRTUALENV ONLY)
-set(Python_FIND_REGISTRY LAST)
-find_package(Python 3.7...<3.11 REQUIRED COMPONENTS Interpreter)
-cmake_path(CONVERT "${Python_SITELIB}/akida" TO_CMAKE_PATH_LIST AKIDA_PATH)
-
-if (WIN32)
-    install(FILES ${AKIDA_PATH}/akida.dll DESTINATION ${CMAKE_RUNTIME_OUTPUT_DIRECTORY})
-endif()
-
-SET(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE)
-
-find_library(AKIDA_LIB NAMES akida libakida.so.2 PATHS ${AKIDA_PATH} NO_DEFAULT_PATH REQUIRED)
-
-add_library(akida INTERFACE)
-target_link_libraries(akida INTERFACE ${AKIDA_LIB})
-target_include_directories(akida INTERFACE
-    ${AKIDA_PATH}/api
-    ${AKIDA_PATH}/engine/api/
-    ${AKIDA_PATH})
+# ============================================================================
+# To use the Akida model API with generated fixtures you have to include the
+# CMake folder and link the library akida to your CMake target as following:
+#
+# set(CMAKE_INCLUDE_CURRENT_DIR ON)
+# set(CMAKE_MODULE_PATH ${CMAKE_CURRENT_LIST_DIR}/cmake)
+# include(akida-model)
+# target_link_libraries(my_program PRIVATE akida)
+# ============================================================================
+
+cmake_minimum_required(VERSION 3.20)
+set(Python_FIND_VIRTUALENV ONLY)
+set(Python_FIND_REGISTRY LAST)
+find_package(Python 3.7...<3.11 REQUIRED COMPONENTS Interpreter)
+cmake_path(CONVERT "${Python_SITELIB}/akida" TO_CMAKE_PATH_LIST AKIDA_PATH)
+
+if (WIN32)
+    install(FILES ${AKIDA_PATH}/akida.dll DESTINATION ${CMAKE_RUNTIME_OUTPUT_DIRECTORY})
+endif()
+
+SET(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE)
+
+find_library(AKIDA_LIB NAMES akida libakida.so.2 PATHS ${AKIDA_PATH} NO_DEFAULT_PATH REQUIRED)
+
+add_library(akida INTERFACE)
+target_link_libraries(akida INTERFACE ${AKIDA_LIB})
+target_include_directories(akida INTERFACE
+    ${AKIDA_PATH}/api
+    ${AKIDA_PATH}/engine/api/
+    ${AKIDA_PATH})
```

## akida/generate/test/engine/test_generator.py

 * *Ordering differences only*

```diff
@@ -1,37 +1,37 @@
-import os
-from pathlib import Path
-
-from ...application_generator import ApplicationGenerator
-from ...array_to_cpp import array_to_cpp
-from ..template import test_file_from_template
-
-
-class EngineTestGenerator(ApplicationGenerator):
-
-    def program(self):
-        model = self.model()
-        device = self.device()
-        if model is not None and device is not None:
-            model.map(device, hw_only=True)
-            return model.sequences[0].program
-        return None
-
-    def generate(self, test_name, dest_path):
-        """Generate engine test application files
-        """
-        program = self.program()
-        inputs = self.inputs()
-        outputs = self.outputs()
-        dest_dir = os.path.join(dest_path, test_name)
-        # Generate source arrays
-        if program is not None:
-            array_to_cpp(dest_dir, program, "program")
-            tpl_path = os.path.join(Path(__file__).parent, "app_templates")
-            # Write test files
-            test_file_from_template(test_name, tpl_path, dest_path, "test.h")
-            test_file_from_template(
-                test_name, tpl_path, dest_path, "test.cpp")
-        if inputs is not None:
-            array_to_cpp(dest_dir, inputs, "inputs")
-        if outputs is not None:
-            array_to_cpp(dest_dir, outputs, "outputs")
+import os
+from pathlib import Path
+
+from ...application_generator import ApplicationGenerator
+from ...array_to_cpp import array_to_cpp
+from ..template import test_file_from_template
+
+
+class EngineTestGenerator(ApplicationGenerator):
+
+    def program(self):
+        model = self.model()
+        device = self.device()
+        if model is not None and device is not None:
+            model.map(device, hw_only=True)
+            return model.sequences[0].program
+        return None
+
+    def generate(self, test_name, dest_path):
+        """Generate engine test application files
+        """
+        program = self.program()
+        inputs = self.inputs()
+        outputs = self.outputs()
+        dest_dir = os.path.join(dest_path, test_name)
+        # Generate source arrays
+        if program is not None:
+            array_to_cpp(dest_dir, program, "program")
+            tpl_path = os.path.join(Path(__file__).parent, "app_templates")
+            # Write test files
+            test_file_from_template(test_name, tpl_path, dest_path, "test.h")
+            test_file_from_template(
+                test_name, tpl_path, dest_path, "test.cpp")
+        if inputs is not None:
+            array_to_cpp(dest_dir, inputs, "inputs")
+        if outputs is not None:
+            array_to_cpp(dest_dir, outputs, "outputs")
```

## akida/generate/test/engine/app_templates/test.cpp

```diff
@@ -1,120 +1,120 @@
-#include "akida/dense.h"
-#include "akida/hardware_device.h"
-
-#include "akida/input_conversion.h"
-#include "infra/system.h"
-
-#include "TEST_NAME/inputs.h"
-#include "TEST_NAME/outputs.h"
-#include "TEST_NAME/program.h"
-#include "TEST_NAME/test.h"
-
-static std::vector<akida::TensorUniquePtr> to_sparse(
-    const std::vector<akida::TensorConstPtr>& dense_inputs) {
-  std::vector<akida::TensorUniquePtr> result;
-  result.reserve(dense_inputs.size());
-  for (const auto& input : dense_inputs) {
-    result.push_back(akida::conversion::to_sparse(
-        *static_cast<const akida::Dense*>(input.get()), program));
-  }
-  return result;
-}
-
-bool TEST_NAME(akida::HardwareDriver* driver,
-               on_engine_envent_t on_engine_event) {
-  // Instantiate the device for the corresponding driver
-  auto device = akida::HardwareDevice::create(driver);
-  on_engine_event(EngineProgramStart);
-  // Program device
-  device->program(program, program_len);
-  on_engine_event(EngineProgramSuccess);
-  // Wrap inputs inside a Dense
-  auto input_tensor = akida::Dense::create_view(
-      reinterpret_cast<const char*>(inputs), inputs_type, inputs_shape,
-      akida::Dense::Layout::RowMajor);
-  // Split inputs in sub-tensors because generated inputs are 4D tensors, and
-  // engine enqueue only accepts 3D tensor
-  auto input_vector = akida::Dense::split(*input_tensor);
-  // Wrap expected outputs inside a Dense
-  auto output_tensor = akida::Dense::create_view(
-      reinterpret_cast<const char*>(outputs), outputs_type, outputs_shape,
-      akida::Dense::Layout::RowMajor);
-  auto expected_vector = akida::Dense::split(*output_tensor);
-
-  // set batch size to number of inputs (allocate inputs if no memory is visible
-  // from akida)
-  device->set_batch_size(input_vector.size(),
-                         driver->akida_visible_memory() == 0);
-
-  // convert inputs if required
-  std::vector<akida::TensorUniquePtr> sparse_inputs;
-  bool conversion_required = !akida::conversion::dense_input_expected(program);
-  if (conversion_required) {
-    sparse_inputs = to_sparse(input_vector);
-  }
-
-  // used to detect eventual timeout (should not happen)
-  auto last_output_read = time_ms();
-  static constexpr int32_t timeout = 5000;  // 5s timeout
-
-  // loop until we read all outputs
-  std::vector<akida::TensorUniquePtr> obtained_vector;
-  size_t nb_inputs_queued = 0;
-  while (obtained_vector.size() < input_vector.size()) {
-    // keep system alive
-    kick_watchdog();
-
-    // enqueue as many jobs as current pipeline allow us
-    bool pipeline_ready = true;
-    while (nb_inputs_queued < input_vector.size() && pipeline_ready) {
-      const auto& input = conversion_required ? *sparse_inputs[nb_inputs_queued]
-                                              : *input_vector[nb_inputs_queued];
-      // try to enqueue
-      on_engine_event(EngineEnqueueStart);
-      pipeline_ready = device->enqueue(input);
-      // if input was inserted, increment counter
-      if (pipeline_ready) {
-        on_engine_event(EngineEnqueueSuccess);
-        ++nb_inputs_queued;
-      } else {
-        on_engine_event(EngineEnqueueFailed);
-      }
-    }
-
-    // now try to fetch outputs
-    bool output_ready = true;
-    while (output_ready) {
-      on_engine_event(EngineFetchStart);
-      auto output = device->fetch();
-      output_ready = output != nullptr;
-      // if an output was ready, increment counter
-      if (output_ready) {
-        on_engine_event(EngineFetchSuccess);
-        // if expected outputs were float, we need to dequantize potentials
-        if (outputs_type == akida::TensorType::float32) {
-          obtained_vector.push_back(
-              device->dequantize(*akida::conversion::as_dense(*output)));
-        } else {
-          obtained_vector.push_back(std::move(output));
-        }
-        last_output_read = time_ms();
-      } else if (time_ms() - last_output_read > timeout) {
-        panic("Fatal error: timed out while fetching output");
-      } else {
-        on_engine_event(EngineFetchFailed);
-      }
-    }
-  }
-
-  // check the  number of output is as expected
-  if (obtained_vector.size() != expected_vector.size()) {
-    return false;
-  }
-  // Compare each individual output
-  for (size_t i = 0; i < obtained_vector.size(); ++i) {
-    if (!(*obtained_vector[i] == *expected_vector[i])) {
-      return false;
-    }
-  }
-  return true;
-}
+#include "akida/dense.h"
+#include "akida/hardware_device.h"
+
+#include "akida/input_conversion.h"
+#include "infra/system.h"
+
+#include "TEST_NAME/inputs.h"
+#include "TEST_NAME/outputs.h"
+#include "TEST_NAME/program.h"
+#include "TEST_NAME/test.h"
+
+static std::vector<akida::TensorUniquePtr> to_sparse(
+    const std::vector<akida::TensorConstPtr>& dense_inputs) {
+  std::vector<akida::TensorUniquePtr> result;
+  result.reserve(dense_inputs.size());
+  for (const auto& input : dense_inputs) {
+    result.push_back(akida::conversion::to_sparse(
+        *static_cast<const akida::Dense*>(input.get()), program));
+  }
+  return result;
+}
+
+bool TEST_NAME(akida::HardwareDriver* driver,
+               on_engine_event_t on_engine_event) {
+  // Instantiate the device for the corresponding driver
+  auto device = akida::HardwareDevice::create(driver);
+  on_engine_event(EngineProgramStart);
+  // Program device
+  device->program(program, program_len);
+  on_engine_event(EngineProgramSuccess);
+  // Wrap inputs inside a Dense
+  auto input_tensor = akida::Dense::create_view(
+      reinterpret_cast<const char*>(inputs), inputs_type, inputs_shape,
+      akida::Dense::Layout::RowMajor);
+  // Split inputs in sub-tensors because generated inputs are 4D tensors, and
+  // engine enqueue only accepts 3D tensor
+  auto input_vector = akida::Dense::split(*input_tensor);
+  // Wrap expected outputs inside a Dense
+  auto output_tensor = akida::Dense::create_view(
+      reinterpret_cast<const char*>(outputs), outputs_type, outputs_shape,
+      akida::Dense::Layout::RowMajor);
+  auto expected_vector = akida::Dense::split(*output_tensor);
+
+  // set batch size to number of inputs (allocate inputs if no memory is visible
+  // from akida)
+  device->set_batch_size(input_vector.size(),
+                         driver->akida_visible_memory() == 0);
+
+  // convert inputs if required
+  std::vector<akida::TensorUniquePtr> sparse_inputs;
+  bool conversion_required = !akida::conversion::dense_input_expected(program);
+  if (conversion_required) {
+    sparse_inputs = to_sparse(input_vector);
+  }
+
+  // used to detect eventual timeout (should not happen)
+  auto last_output_read = time_ms();
+  static constexpr int32_t timeout = 5000;  // 5s timeout
+
+  // loop until we read all outputs
+  std::vector<akida::TensorUniquePtr> obtained_vector;
+  size_t nb_inputs_queued = 0;
+  while (obtained_vector.size() < input_vector.size()) {
+    // keep system alive
+    kick_watchdog();
+
+    // enqueue as many jobs as current pipeline allow us
+    bool pipeline_ready = true;
+    while (nb_inputs_queued < input_vector.size() && pipeline_ready) {
+      const auto& input = conversion_required ? *sparse_inputs[nb_inputs_queued]
+                                              : *input_vector[nb_inputs_queued];
+      // try to enqueue
+      on_engine_event(EngineEnqueueStart);
+      pipeline_ready = device->enqueue(input);
+      // if input was inserted, increment counter
+      if (pipeline_ready) {
+        on_engine_event(EngineEnqueueSuccess);
+        ++nb_inputs_queued;
+      } else {
+        on_engine_event(EngineEnqueueFailed);
+      }
+    }
+
+    // now try to fetch outputs
+    bool output_ready = true;
+    while (output_ready) {
+      on_engine_event(EngineFetchStart);
+      auto output = device->fetch();
+      output_ready = output != nullptr;
+      // if an output was ready, increment counter
+      if (output_ready) {
+        on_engine_event(EngineFetchSuccess);
+        // if expected outputs were float, we need to dequantize potentials
+        if (outputs_type == akida::TensorType::float32) {
+          obtained_vector.push_back(
+              device->dequantize(*akida::conversion::as_dense(*output)));
+        } else {
+          obtained_vector.push_back(std::move(output));
+        }
+        last_output_read = time_ms();
+      } else if (time_ms() - last_output_read > timeout) {
+        panic("Fatal error: timed out while fetching output");
+      } else {
+        on_engine_event(EngineFetchFailed);
+      }
+    }
+  }
+
+  // check the  number of output is as expected
+  if (obtained_vector.size() != expected_vector.size()) {
+    return false;
+  }
+  // Compare each individual output
+  for (size_t i = 0; i < obtained_vector.size(); ++i) {
+    if (!(*obtained_vector[i] == *expected_vector[i])) {
+      return false;
+    }
+  }
+  return true;
+}
```

## akida/generate/test/engine/app_templates/test.h

```diff
@@ -1,43 +1,43 @@
-#include "akida/hardware_device.h"
-
-enum EngineEvent {
-  EngineProgramStart,
-  EngineProgramSuccess,
-  EngineEnqueueStart,
-  EngineEnqueueSuccess,
-  EngineEnqueueFailed,
-  EngineFetchStart,
-  EngineFetchSuccess,
-  EngineFetchFailed,
-};
-
-using on_engine_envent_t = void (*)(EngineEvent);
-
-/**
- * @brief Akida engine unit test based on TESTNAME configuration
- *
- * This method takes a pointer to a target-specifc HardwareDriver
- * implementation.
- *
- * The driver would typically be instantiated by a target-specific main
- * application calling the method.
- *
- * The implementation of the method relies on three serialized arrays:
- * - the program,
- * - the predefined test inputs,
- * - the expected test outputs.
- *
- * The method first instantiates a device corresponding to the specified driver,
- * then loads the serialized program from the test configuration.
- *
- * It then performs an inference on the predefined inputs and compares the
- * result with the expected outputs.
- *
- * @param driver : an implementation of an akida::HardwareDriver
- * @param on_engine_event : a callback that will be called on several Engine
- * events
- * @return true if the hardware outputs match the expected outputs.
- */
-bool TEST_NAME(
-    akida::HardwareDriver* driver,
-    on_engine_envent_t on_engine_event = [](EngineEvent) {});
+#include "akida/hardware_device.h"
+
+enum EngineEvent {
+  EngineProgramStart,
+  EngineProgramSuccess,
+  EngineEnqueueStart,
+  EngineEnqueueSuccess,
+  EngineEnqueueFailed,
+  EngineFetchStart,
+  EngineFetchSuccess,
+  EngineFetchFailed,
+};
+
+using on_engine_event_t = void (*)(EngineEvent);
+
+/**
+ * @brief Akida engine unit test based on TESTNAME configuration
+ *
+ * This method takes a pointer to a target-specifc HardwareDriver
+ * implementation.
+ *
+ * The driver would typically be instantiated by a target-specific main
+ * application calling the method.
+ *
+ * The implementation of the method relies on three serialized arrays:
+ * - the program,
+ * - the predefined test inputs,
+ * - the expected test outputs.
+ *
+ * The method first instantiates a device corresponding to the specified driver,
+ * then loads the serialized program from the test configuration.
+ *
+ * It then performs an inference on the predefined inputs and compares the
+ * result with the expected outputs.
+ *
+ * @param driver : an implementation of an akida::HardwareDriver
+ * @param on_engine_event : a callback that will be called on several Engine
+ * events
+ * @return true if the hardware outputs match the expected outputs.
+ */
+bool TEST_NAME(
+    akida::HardwareDriver* driver,
+    on_engine_event_t on_engine_event = [](EngineEvent) {});
```

## akida/generate/test/engine/fixtures/simple_conv_evaluate_v2.py

 * *Ordering differences only*

```diff
@@ -1,29 +1,29 @@
-import akida
-
-from akida.generate.test.test_tools import get_cyclic_input, set_cyclic_weights
-
-
-class TestGenerator(akida.generate.EngineTestGenerator):
-
-    def model(self):
-        input_shape = (6, 6, 3)
-        kernel_size = (3, 3)
-        filters = 10
-        model = akida.Model()
-        model.add(akida.InputData(input_shape))
-        model.add(akida.Convolutional(kernel_size, filters, activation=False))
-        # Set cyclic weights
-        set_cyclic_weights(model)
-        return model
-
-    def device(self):
-        return akida.AKD1000()
-
-    def inputs(self):
-        # Generate cyclic inputs
-        return get_cyclic_input(self.model(), 2)
-
-    def outputs(self):
-        # Predict outputs in software
-        self.model().map(None)
-        return self.model().predict(self.inputs())
+import akida
+
+from akida.generate.test.test_tools import get_cyclic_input, set_cyclic_weights
+
+
+class TestGenerator(akida.generate.EngineTestGenerator):
+
+    def model(self):
+        input_shape = (6, 6, 3)
+        kernel_size = (3, 3)
+        filters = 10
+        model = akida.Model()
+        model.add(akida.InputData(input_shape))
+        model.add(akida.Convolutional(kernel_size, filters, activation=False))
+        # Set cyclic weights
+        set_cyclic_weights(model)
+        return model
+
+    def device(self):
+        return akida.AKD1000()
+
+    def inputs(self):
+        # Generate cyclic inputs
+        return get_cyclic_input(self.model(), 2)
+
+    def outputs(self):
+        # Predict outputs in software
+        self.model().map(None)
+        return self.model().predict(self.inputs())
```

## akida/generate/test/engine/fixtures/simple_conv_v2.py

 * *Ordering differences only*

```diff
@@ -1,29 +1,29 @@
-import akida
-
-from akida.generate.test.test_tools import get_cyclic_input, set_cyclic_weights
-
-
-class TestGenerator(akida.generate.EngineTestGenerator):
-
-    def model(self):
-        input_shape = (6, 6, 3)
-        kernel_size = (3, 3)
-        filters = 10
-        model = akida.Model()
-        model.add(akida.InputData(input_shape))
-        model.add(akida.Convolutional(kernel_size, filters, activation=False))
-        # Set cyclic weights
-        set_cyclic_weights(model)
-        return model
-
-    def device(self):
-        return akida.AKD1000()
-
-    def inputs(self):
-        # Generate cyclic inputs
-        return get_cyclic_input(self.model(), 2)
-
-    def outputs(self):
-        # Forward inputs in software
-        self.model().map(None)
-        return self.model().forward(self.inputs())
+import akida
+
+from akida.generate.test.test_tools import get_cyclic_input, set_cyclic_weights
+
+
+class TestGenerator(akida.generate.EngineTestGenerator):
+
+    def model(self):
+        input_shape = (6, 6, 3)
+        kernel_size = (3, 3)
+        filters = 10
+        model = akida.Model()
+        model.add(akida.InputData(input_shape))
+        model.add(akida.Convolutional(kernel_size, filters, activation=False))
+        # Set cyclic weights
+        set_cyclic_weights(model)
+        return model
+
+    def device(self):
+        return akida.AKD1000()
+
+    def inputs(self):
+        # Generate cyclic inputs
+        return get_cyclic_input(self.model(), 2)
+
+    def outputs(self):
+        # Forward inputs in software
+        self.model().map(None)
+        return self.model().forward(self.inputs())
```

## akida/generate/test/engine/fixtures/simple_hrc_v2.py

 * *Ordering differences only*

```diff
@@ -1,47 +1,47 @@
-import numpy as np
-import akida
-
-from akida.generate.test.test_tools import set_constant_weights
-
-
-class TestGenerator(akida.generate.EngineTestGenerator):
-
-    def model(self):
-        """
-        This test hangs the first time it is run on an NSoC_v2 after a cold boot.
-        """
-        filters = 1
-        kernel_size = (3, 3)
-        input_shape = (8, 8, 1)
-        weights_bits = 8
-        act_bits = 1
-        padding = akida.Padding.Same
-        pool_type = akida.PoolType.Max
-        pool_size = (2, 2)
-        kernel_stride = (2, 2)
-        # Build akida model
-        input_layer = akida.InputConvolutional(filters=filters,
-                                               kernel_size=kernel_size,
-                                               input_shape=input_shape,
-                                               padding=padding,
-                                               act_bits=act_bits,
-                                               weights_bits=weights_bits,
-                                               activation=True,
-                                               pool_type=pool_type,
-                                               pool_size=pool_size,
-                                               kernel_stride=kernel_stride)
-        model = akida.Model()
-        model.add(input_layer)
-        set_constant_weights(model, 1)
-        return model
-
-    def device(self):
-        return akida.AKD1000()
-
-    def inputs(self):
-        return np.ones((1, 8, 8, 1), dtype=np.uint8)
-
-    def outputs(self):
-        # Forward inputs in software
-        self.model().map(None)
-        return self.model().forward(self.inputs())
+import numpy as np
+import akida
+
+from akida.generate.test.test_tools import set_constant_weights
+
+
+class TestGenerator(akida.generate.EngineTestGenerator):
+
+    def model(self):
+        """
+        This test hangs the first time it is run on an NSoC_v2 after a cold boot.
+        """
+        filters = 1
+        kernel_size = (3, 3)
+        input_shape = (8, 8, 1)
+        weights_bits = 8
+        act_bits = 1
+        padding = akida.Padding.Same
+        pool_type = akida.PoolType.Max
+        pool_size = (2, 2)
+        kernel_stride = (2, 2)
+        # Build akida model
+        input_layer = akida.InputConvolutional(filters=filters,
+                                               kernel_size=kernel_size,
+                                               input_shape=input_shape,
+                                               padding=padding,
+                                               act_bits=act_bits,
+                                               weights_bits=weights_bits,
+                                               activation=True,
+                                               pool_type=pool_type,
+                                               pool_size=pool_size,
+                                               kernel_stride=kernel_stride)
+        model = akida.Model()
+        model.add(input_layer)
+        set_constant_weights(model, 1)
+        return model
+
+    def device(self):
+        return akida.AKD1000()
+
+    def inputs(self):
+        return np.ones((1, 8, 8, 1), dtype=np.uint8)
+
+    def outputs(self):
+        # Forward inputs in software
+        self.model().map(None)
+        return self.model().forward(self.inputs())
```

## akida/generate/test/engine/fixtures/simple_sep_conv_v2.py

 * *Ordering differences only*

```diff
@@ -1,31 +1,31 @@
-import akida
-
-from akida.generate.test.test_tools import get_cyclic_input, set_cyclic_weights
-
-
-class TestGenerator(akida.generate.EngineTestGenerator):
-
-    def model(self):
-        input_shape = (6, 6, 3)
-        kernel_size = (3, 3)
-        filters = 10
-        model = akida.Model()
-        model.add(akida.InputData(input_shape))
-        model.add(
-            akida.SeparableConvolutional(kernel_size, filters,
-                                         activation=False))
-        # Set cyclic weights
-        set_cyclic_weights(model)
-        return model
-
-    def device(self):
-        return akida.AKD1000()
-
-    def inputs(self):
-        # Generate cyclic inputs
-        return get_cyclic_input(self.model(), 2)
-
-    def outputs(self):
-        # Forward inputs in software
-        self.model().map(None)
-        return self.model().forward(self.inputs())
+import akida
+
+from akida.generate.test.test_tools import get_cyclic_input, set_cyclic_weights
+
+
+class TestGenerator(akida.generate.EngineTestGenerator):
+
+    def model(self):
+        input_shape = (6, 6, 3)
+        kernel_size = (3, 3)
+        filters = 10
+        model = akida.Model()
+        model.add(akida.InputData(input_shape))
+        model.add(
+            akida.SeparableConvolutional(kernel_size, filters,
+                                         activation=False))
+        # Set cyclic weights
+        set_cyclic_weights(model)
+        return model
+
+    def device(self):
+        return akida.AKD1000()
+
+    def inputs(self):
+        # Generate cyclic inputs
+        return get_cyclic_input(self.model(), 2)
+
+    def outputs(self):
+        # Forward inputs in software
+        self.model().map(None)
+        return self.model().forward(self.inputs())
```

## akida/generate/test/engine/fixtures/test_fnp2.py

 * *Ordering differences only*

```diff
@@ -1,28 +1,28 @@
-import akida
-
-from akida.generate.test.test_tools import get_cyclic_input, set_cyclic_weights
-
-
-class TestGenerator(akida.generate.EngineTestGenerator):
-
-    def model(self):
-        model = akida.Model()
-        model.add(akida.InputData((1, 1, 2048), 4))
-        # Add FullyConnected layer
-        fc = akida.FullyConnected(1000, weights_bits=4, activation=False)
-        model.add(fc)
-        # Set cyclic weights
-        set_cyclic_weights(model)
-        return model
-
-    def device(self):
-        return akida.AKD1000()
-
-    def inputs(self):
-        # Generate cyclic inputs
-        return get_cyclic_input(self.model(), 10)
-
-    def outputs(self):
-        # Forward inputs in software
-        self.model().map(None)
-        return self.model().forward(self.inputs())
+import akida
+
+from akida.generate.test.test_tools import get_cyclic_input, set_cyclic_weights
+
+
+class TestGenerator(akida.generate.EngineTestGenerator):
+
+    def model(self):
+        model = akida.Model()
+        model.add(akida.InputData((1, 1, 2048), 4))
+        # Add FullyConnected layer
+        fc = akida.FullyConnected(1000, weights_bits=4, activation=False)
+        model.add(fc)
+        # Set cyclic weights
+        set_cyclic_weights(model)
+        return model
+
+    def device(self):
+        return akida.AKD1000()
+
+    def inputs(self):
+        # Generate cyclic inputs
+        return get_cyclic_input(self.model(), 10)
+
+    def outputs(self):
+        # Forward inputs in software
+        self.model().map(None)
+        return self.model().forward(self.inputs())
```

## akida/generate/test/engine/fixtures/test_multiple_inputs.py

 * *Ordering differences only*

```diff
@@ -1,29 +1,29 @@
-import akida
-
-from akida.generate.test.test_tools import get_cyclic_input, set_cyclic_weights
-
-
-class TestGenerator(akida.generate.EngineTestGenerator):
-
-    def model(self):
-        input_shape = (6, 6, 3)
-        kernel_size = (3, 3)
-        filters = 5
-        model = akida.Model()
-        model.add(akida.InputData(input_shape))
-        model.add(akida.Convolutional(kernel_size, filters, activation=False))
-        # Set cyclic weights
-        set_cyclic_weights(model)
-        return model
-
-    def device(self):
-        return akida.AKD1000()
-
-    def inputs(self):
-        # Generate 40 cyclic inputs, to make sure we can fill the pipeline
-        return get_cyclic_input(self.model(), 40)
-
-    def outputs(self):
-        # Forward inputs in software
-        self.model().map(None)
-        return self.model().forward(self.inputs())
+import akida
+
+from akida.generate.test.test_tools import get_cyclic_input, set_cyclic_weights
+
+
+class TestGenerator(akida.generate.EngineTestGenerator):
+
+    def model(self):
+        input_shape = (6, 6, 3)
+        kernel_size = (3, 3)
+        filters = 5
+        model = akida.Model()
+        model.add(akida.InputData(input_shape))
+        model.add(akida.Convolutional(kernel_size, filters, activation=False))
+        # Set cyclic weights
+        set_cyclic_weights(model)
+        return model
+
+    def device(self):
+        return akida.AKD1000()
+
+    def inputs(self):
+        # Generate 40 cyclic inputs, to make sure we can fill the pipeline
+        return get_cyclic_input(self.model(), 40)
+
+    def outputs(self):
+        # Forward inputs in software
+        self.model().map(None)
+        return self.model().forward(self.inputs())
```

## akida/generate/test/model/test_generator.py

 * *Ordering differences only*

```diff
@@ -1,37 +1,37 @@
-import os
-import json
-from pathlib import Path
-
-from ...application_generator import ApplicationGenerator
-from ...array_to_cpp import array_to_cpp
-from ..template import test_file_from_template
-
-
-class ModelTestGenerator(ApplicationGenerator):
-
-    def generate(self, test_name, dest_path):
-        """Generate model test application files
-        """
-        model = self.model()
-        inputs = self.inputs()
-        outputs = self.outputs()
-        dest_dir = os.path.join(dest_path, test_name)
-        # Generate source arrays
-        if model is not None:
-            model_buffer = model.to_buffer()
-            array_to_cpp(dest_dir, model_buffer, "model")
-            with open(dest_dir + '/model.json', 'w') as file:
-                file.write(model.to_json())
-            tpl_path = os.path.join(Path(__file__).parent, "app_templates")
-            # Write test files
-            test_file_from_template(test_name, tpl_path, dest_path, "test.h")
-            test_file_from_template(
-                test_name, tpl_path, dest_path, "test.cpp")
-        if inputs is not None:
-            array_to_cpp(dest_dir, inputs, "inputs")
-            with open(dest_dir + '/inputs.json', 'w') as file:
-                json.dump(inputs.tolist(), file)
-        if outputs is not None:
-            array_to_cpp(dest_dir, outputs, "outputs")
-            with open(dest_dir + '/outputs.json', 'w') as file:
-                json.dump(outputs.tolist(), file)
+import os
+import json
+from pathlib import Path
+
+from ...application_generator import ApplicationGenerator
+from ...array_to_cpp import array_to_cpp
+from ..template import test_file_from_template
+
+
+class ModelTestGenerator(ApplicationGenerator):
+
+    def generate(self, test_name, dest_path):
+        """Generate model test application files
+        """
+        model = self.model()
+        inputs = self.inputs()
+        outputs = self.outputs()
+        dest_dir = os.path.join(dest_path, test_name)
+        # Generate source arrays
+        if model is not None:
+            model_buffer = model.to_buffer()
+            array_to_cpp(dest_dir, model_buffer, "model")
+            with open(dest_dir + '/model.json', 'w') as file:
+                file.write(model.to_json())
+            tpl_path = os.path.join(Path(__file__).parent, "app_templates")
+            # Write test files
+            test_file_from_template(test_name, tpl_path, dest_path, "test.h")
+            test_file_from_template(
+                test_name, tpl_path, dest_path, "test.cpp")
+        if inputs is not None:
+            array_to_cpp(dest_dir, inputs, "inputs")
+            with open(dest_dir + '/inputs.json', 'w') as file:
+                json.dump(inputs.tolist(), file)
+        if outputs is not None:
+            array_to_cpp(dest_dir, outputs, "outputs")
+            with open(dest_dir + '/outputs.json', 'w') as file:
+                json.dump(outputs.tolist(), file)
```

## akida/generate/test/model/app_templates/test.cpp

 * *Ordering differences only*

```diff
@@ -1,39 +1,39 @@
-#include <iostream>
-#include <memory>
-
-#include "akida/dense.h"
-#include "akida/model.h"
-
-#include "TEST_NAME/inputs.h"
-#include "TEST_NAME/model.h"
-#include "TEST_NAME/outputs.h"
-
-#define SUCCESS true
-#define FAILURE false
-
-akida::DensePtr to_dense_ptr(const unsigned char* array, size_t lenght,
-                             akida::TensorType type, akida::Shape shape) {
-  auto signed_array = reinterpret_cast<const char*>(array);
-  return akida::Dense::copy(signed_array, lenght, type, shape,
-                            akida::Dense::Layout::RowMajor);
-}
-
-bool TEST_NAME() {
-  // Arrange
-  auto model_buffer = reinterpret_cast<const char*>(model);
-  auto ak_model = akida::Model::from_buffer(model_buffer, model_len);
-  auto dense_input =
-      to_dense_ptr(inputs, inputs_len, inputs_type, inputs_shape);
-  auto dense_output =
-      to_dense_ptr(outputs, outputs_len, outputs_type, outputs_shape);
-
-  // Act
-  auto ak_outputs = ak_model->forward(dense_input);
-
-  // Assert
-  if (!(*ak_outputs == *dense_output)) {
-    return FAILURE;
-  }
-
-  return SUCCESS;
-}
+#include <iostream>
+#include <memory>
+
+#include "akida/dense.h"
+#include "akida/model.h"
+
+#include "TEST_NAME/inputs.h"
+#include "TEST_NAME/model.h"
+#include "TEST_NAME/outputs.h"
+
+#define SUCCESS true
+#define FAILURE false
+
+akida::DensePtr to_dense_ptr(const unsigned char* array, size_t lenght,
+                             akida::TensorType type, akida::Shape shape) {
+  auto signed_array = reinterpret_cast<const char*>(array);
+  return akida::Dense::copy(signed_array, lenght, type, shape,
+                            akida::Dense::Layout::RowMajor);
+}
+
+bool TEST_NAME() {
+  // Arrange
+  auto model_buffer = reinterpret_cast<const char*>(model);
+  auto ak_model = akida::Model::from_buffer(model_buffer, model_len);
+  auto dense_input =
+      to_dense_ptr(inputs, inputs_len, inputs_type, inputs_shape);
+  auto dense_output =
+      to_dense_ptr(outputs, outputs_len, outputs_type, outputs_shape);
+
+  // Act
+  auto ak_outputs = ak_model->forward(dense_input);
+
+  // Assert
+  if (!(*ak_outputs == *dense_output)) {
+    return FAILURE;
+  }
+
+  return SUCCESS;
+}
```

## akida/generate/test/model/app_templates/test.h

 * *Ordering differences only*

```diff
@@ -1,19 +1,19 @@
-#include "akida/hardware_device.h"
-
-/**
- * @brief Akida model unit test based on TESTNAME configuration
- *
- * The implementation of the method relies on three serialized arrays:
- * - the model,
- * - the predefined test inputs,
- * - the expected test outputs.
- *
- * This method loads the model, the inputs and the outputs from the serialized
- * arrays.
- *
- * It then performs an inference on the predefined inputs and compares the
- * result with the expected outputs.
- *
- * @return true if the model outputs match the expected outputs.
- */
-bool TEST_NAME();
+#include "akida/hardware_device.h"
+
+/**
+ * @brief Akida model unit test based on TESTNAME configuration
+ *
+ * The implementation of the method relies on three serialized arrays:
+ * - the model,
+ * - the predefined test inputs,
+ * - the expected test outputs.
+ *
+ * This method loads the model, the inputs and the outputs from the serialized
+ * arrays.
+ *
+ * It then performs an inference on the predefined inputs and compares the
+ * result with the expected outputs.
+ *
+ * @return true if the model outputs match the expected outputs.
+ */
+bool TEST_NAME();
```

## akida/layers/__init__.py

 * *Ordering differences only*

```diff
@@ -1,21 +1,21 @@
-from .add import Add
-from .attention import Attention
-from .concatenate import Concatenate
-from .convolutional import Convolutional
-from .dense2d import Dense2D
-from .dense1d import Dense1D
-from .fully_connected import FullyConnected
-from .input_convolutional import InputConvolutional
-from .input_data import InputData
-from .madnorm import MadNorm
-from .separable_convolutional import SeparableConvolutional
-from .shiftmax import Shiftmax
-from .stem import Stem
-from .batch_normalization import BatchNormalization
-from .conv2d import Conv2D
-from .input_conv2d import InputConv2D
-from .depthwise_conv2d import DepthwiseConv2D
-from .conv2d_transpose import Conv2DTranspose
-from .extract_token import ExtractToken
-from .dequantizer import Dequantizer
-from .depthwise_conv2d_transpose import DepthwiseConv2DTranspose
+from .add import Add
+from .attention import Attention
+from .concatenate import Concatenate
+from .convolutional import Convolutional
+from .dense2d import Dense2D
+from .dense1d import Dense1D
+from .fully_connected import FullyConnected
+from .input_convolutional import InputConvolutional
+from .input_data import InputData
+from .madnorm import MadNorm
+from .separable_convolutional import SeparableConvolutional
+from .shiftmax import Shiftmax
+from .stem import Stem
+from .batch_normalization import BatchNormalization
+from .conv2d import Conv2D
+from .input_conv2d import InputConv2D
+from .depthwise_conv2d import DepthwiseConv2D
+from .conv2d_transpose import Conv2DTranspose
+from .extract_token import ExtractToken
+from .dequantizer import Dequantizer
+from .depthwise_conv2d_transpose import DepthwiseConv2DTranspose
```

## akida/layers/add.py

 * *Ordering differences only*

```diff
@@ -1,48 +1,48 @@
-from akida.core import (Layer, LayerParams, LayerType)
-
-
-class Add(Layer):
-    """Layer that adds two inputs from incoming layers.
-
-    It takes as input the output tensors from the input layers, all of the same
-    shape, and returns a single tensor (also of the same shape).
-    Add layers require Incoming input layers to produce output tensors of the
-    same type.
-    The Add layer will create three variables, `a_shift`, `b_shift` and
-    `output_shift`.
-    The operation it will perform on each couple of integer values on input
-    tensors (a, b) is equivalent to:
-
-        >>>  a1 = a << a_shift
-        >>>  b1 = b << b_shift
-        >>>  intermediate_output = a1 + b1
-        >>>  for i, shift in enumerate(output_shift):
-        >>>      if shift>0:
-        >>>          output[i] = intermediate_output[i] << shift
-        >>>      else:
-        >>>          output[i] = intermediate_output[i] >> shift
-
-    Note that output values will be saturated on the range that can be
-    represented with output_bits.
-
-    Args:
-        output_bits (int, optional): output bitwidth. Defaults to 4.
-        buffer_bits (int, optional): internal bitwidth. Defaults to 24.
-        name (str, optional): name of the layer. Defaults to empty string.
-
-    """
-
-    def __init__(self, output_bits=4, buffer_bits=24, name=""):
-        try:
-            params = LayerParams(
-                LayerType.Add, {
-                    "output_bits": output_bits,
-                    "buffer_bits": buffer_bits,
-                })
-            # Call parent constructor to initialize C++ bindings
-            # Note that we invoke directly __init__ instead of using super, as
-            # specified in pybind documentation
-            Layer.__init__(self, params, name)
-        except BaseException:
-            self = None
-            raise
+from akida.core import (Layer, LayerParams, LayerType)
+
+
+class Add(Layer):
+    """Layer that adds two inputs from incoming layers.
+
+    It takes as input the output tensors from the input layers, all of the same
+    shape, and returns a single tensor (also of the same shape).
+    Add layers require Incoming input layers to produce output tensors of the
+    same type.
+    The Add layer will create three variables, `a_shift`, `b_shift` and
+    `output_shift`.
+    The operation it will perform on each couple of integer values on input
+    tensors (a, b) is equivalent to:
+
+        >>>  a1 = a << a_shift
+        >>>  b1 = b << b_shift
+        >>>  intermediate_output = a1 + b1
+        >>>  for i, shift in enumerate(output_shift):
+        >>>      if shift>0:
+        >>>          output[i] = intermediate_output[i] << shift
+        >>>      else:
+        >>>          output[i] = intermediate_output[i] >> shift
+
+    Note that output values will be saturated on the range that can be
+    represented with output_bits.
+
+    Args:
+        output_bits (int, optional): output bitwidth. Defaults to 4.
+        buffer_bits (int, optional): internal bitwidth. Defaults to 24.
+        name (str, optional): name of the layer. Defaults to empty string.
+
+    """
+
+    def __init__(self, output_bits=4, buffer_bits=24, name=""):
+        try:
+            params = LayerParams(
+                LayerType.Add, {
+                    "output_bits": output_bits,
+                    "buffer_bits": buffer_bits,
+                })
+            # Call parent constructor to initialize C++ bindings
+            # Note that we invoke directly __init__ instead of using super, as
+            # specified in pybind documentation
+            Layer.__init__(self, params, name)
+        except BaseException:
+            self = None
+            raise
```

## akida/layers/attention.py

 * *Ordering differences only*

```diff
@@ -1,55 +1,55 @@
-from akida.core import (Layer, LayerParams, LayerType)
-
-
-class Attention(Layer):
-    """Multi-head attention layer.
-
-    From A. Vaswani et al., "Attention is All You Need" (arXiv:1706.03762):
-    "Self-attention, sometimes called intra-attention is an attention mechanism
-    relating different positions of a single sequence in order to compute a
-    representation of the sequence."
-
-    This layer will take three inputs, Query, Key and Value, and perform these
-    actions on each head:
-
-    * Multiply Query and Key to obtain a vector of attention scores expressing
-      how tokens/patches relate to one another.
-    * Divide by a scale factor.
-    * Convert the score to a probability mask using a Softmax function
-      (replaced by a Shiftmax in our implementation).
-    * Multiply the mask by the Values.
-
-    Note that outputs and masks will be saturated on the range that can be
-    represented with output_bits.
-
-    Args:
-        num_heads (int): number of heads.
-        output_bits (int, optional): output bitwidth. Defaults to 8
-        buffer_bits (int, optional): internal bitwidth. Defaults to 23
-        shiftmax_output_bits (int, optional): output bitwidth for shiftmax,
-            must be no more than 1/2 of buffer_bits. Defaults to 10
-        name (str, optional): name of the layer. Defaults to empty string
-
-    """
-
-    def __init__(self,
-                 num_heads,
-                 output_bits=8,
-                 buffer_bits=23,
-                 shiftmax_output_bits=10,
-                 name=""):
-        try:
-            params = LayerParams(
-                LayerType.Attention, {
-                    "num_heads": num_heads,
-                    "output_bits": output_bits,
-                    "buffer_bits": buffer_bits,
-                    "shiftmax_output_bits": shiftmax_output_bits
-                })
-            # Call parent constructor to initialize C++ bindings
-            # Note that we invoke directly __init__ instead of using super, as
-            # specified in pybind documentation
-            Layer.__init__(self, params, name)
-        except BaseException:
-            self = None
-            raise
+from akida.core import (Layer, LayerParams, LayerType)
+
+
+class Attention(Layer):
+    """Multi-head attention layer.
+
+    From A. Vaswani et al., "Attention is All You Need" (arXiv:1706.03762):
+    "Self-attention, sometimes called intra-attention is an attention mechanism
+    relating different positions of a single sequence in order to compute a
+    representation of the sequence."
+
+    This layer will take three inputs, Query, Key and Value, and perform these
+    actions on each head:
+
+    * Multiply Query and Key to obtain a vector of attention scores expressing
+      how tokens/patches relate to one another.
+    * Divide by a scale factor.
+    * Convert the score to a probability mask using a Softmax function
+      (replaced by a Shiftmax in our implementation).
+    * Multiply the mask by the Values.
+
+    Note that outputs and masks will be saturated on the range that can be
+    represented with output_bits.
+
+    Args:
+        num_heads (int): number of heads.
+        output_bits (int, optional): output bitwidth. Defaults to 8
+        buffer_bits (int, optional): internal bitwidth. Defaults to 23
+        shiftmax_output_bits (int, optional): output bitwidth for shiftmax,
+            must be no more than 1/2 of buffer_bits. Defaults to 10
+        name (str, optional): name of the layer. Defaults to empty string
+
+    """
+
+    def __init__(self,
+                 num_heads,
+                 output_bits=8,
+                 buffer_bits=23,
+                 shiftmax_output_bits=10,
+                 name=""):
+        try:
+            params = LayerParams(
+                LayerType.Attention, {
+                    "num_heads": num_heads,
+                    "output_bits": output_bits,
+                    "buffer_bits": buffer_bits,
+                    "shiftmax_output_bits": shiftmax_output_bits
+                })
+            # Call parent constructor to initialize C++ bindings
+            # Note that we invoke directly __init__ instead of using super, as
+            # specified in pybind documentation
+            Layer.__init__(self, params, name)
+        except BaseException:
+            self = None
+            raise
```

## akida/layers/batch_normalization.py

 * *Ordering differences only*

```diff
@@ -1,36 +1,36 @@
-from akida.core import (Layer, LayerParams, LayerType)
-
-
-class BatchNormalization(Layer):
-    """Batch Normalization applied on the last axis.
-
-    The normalization is applied as:
-
-    outputs = a * x + b
-
-    Args:
-        output_bits (int, optional): output bitwidth. Defaults to 8.
-        buffer_bits (int, optional): buffer bitwidth. Defaults to 32.
-        activation (bool, optional): add a ReLU activation. Defaults to False.
-        name (str, optional): name of the layer. Defaults to empty string.
-    """
-
-    def __init__(self,
-                 output_bits=8,
-                 buffer_bits=32,
-                 activation=False,
-                 name=""):
-        try:
-            params = LayerParams(
-                LayerType.BatchNormalization, {
-                    "output_bits": output_bits,
-                    "buffer_bits": buffer_bits,
-                    "activation": activation,
-                })
-            # Call parent constructor to initialize C++ bindings
-            # Note that we invoke directly __init__ instead of using super, as
-            # specified in pybind documentation
-            Layer.__init__(self, params, name)
-        except BaseException:
-            self = None
-            raise
+from akida.core import (Layer, LayerParams, LayerType)
+
+
+class BatchNormalization(Layer):
+    """Batch Normalization applied on the last axis.
+
+    The normalization is applied as:
+
+    outputs = a * x + b
+
+    Args:
+        output_bits (int, optional): output bitwidth. Defaults to 8.
+        buffer_bits (int, optional): buffer bitwidth. Defaults to 32.
+        activation (bool, optional): add a ReLU activation. Defaults to False.
+        name (str, optional): name of the layer. Defaults to empty string.
+    """
+
+    def __init__(self,
+                 output_bits=8,
+                 buffer_bits=32,
+                 activation=False,
+                 name=""):
+        try:
+            params = LayerParams(
+                LayerType.BatchNormalization, {
+                    "output_bits": output_bits,
+                    "buffer_bits": buffer_bits,
+                    "activation": activation,
+                })
+            # Call parent constructor to initialize C++ bindings
+            # Note that we invoke directly __init__ instead of using super, as
+            # specified in pybind documentation
+            Layer.__init__(self, params, name)
+        except BaseException:
+            self = None
+            raise
```

## akida/layers/concatenate.py

 * *Ordering differences only*

```diff
@@ -1,31 +1,31 @@
-from akida.core import (Layer, LayerParams, LayerType)
-
-
-class Concatenate(Layer):
-    """Layer that concatenates two or more inputs from incoming layers, along
-    the last dimensions.
-
-    The operation is equivalent to this numpy operation
-
-        >>>  # Inputs are a and b
-        >>>  output = np.concatenate((a, b), axis=-1)
-
-    All inbound layers should have the same output dimensions on the first two
-    axis. All inbound layers should have the same output bitwidth and output
-    sign.
-
-    Args:
-        name (str, optional): name of the layer. Defaults to empty string.
-    """
-
-    def __init__(self, name=""):
-        try:
-            params = LayerParams(
-                LayerType.Concatenate, {})
-            # Call parent constructor to initialize C++ bindings
-            # Note that we invoke directly __init__ instead of using super, as
-            # specified in pybind documentation
-            Layer.__init__(self, params, name)
-        except BaseException:
-            self = None
-            raise
+from akida.core import (Layer, LayerParams, LayerType)
+
+
+class Concatenate(Layer):
+    """Layer that concatenates two or more inputs from incoming layers, along
+    the last dimensions.
+
+    The operation is equivalent to this numpy operation
+
+        >>>  # Inputs are a and b
+        >>>  output = np.concatenate((a, b), axis=-1)
+
+    All inbound layers should have the same output dimensions on the first two
+    axis. All inbound layers should have the same output bitwidth and output
+    sign.
+
+    Args:
+        name (str, optional): name of the layer. Defaults to empty string.
+    """
+
+    def __init__(self, name=""):
+        try:
+            params = LayerParams(
+                LayerType.Concatenate, {})
+            # Call parent constructor to initialize C++ bindings
+            # Note that we invoke directly __init__ instead of using super, as
+            # specified in pybind documentation
+            Layer.__init__(self, params, name)
+        except BaseException:
+            self = None
+            raise
```

## akida/layers/conv2d.py

 * *Ordering differences only*

```diff
@@ -1,85 +1,85 @@
-from akida.core import (Layer, Padding, PoolType, LayerType, LayerParams)
-
-
-class Conv2D(Layer):
-    """This represents the Akida V2 Conv2D layer.
-
-    It applies a convolution optionally followed by a bias addition, a
-    pooling operation and a ReLU activation.
-    Inputs shape must be in the form (X, Y, C). Being the result of a quantized
-    operation, it is possible to apply some shifts to adjust the inputs/outputs
-    scales to the equivalent operation performed on floats, while maintaining
-    a limited usage of bits and performing the operations on integer values.
-    The order of the input spatial dimensions is preserved, but their values may
-    change according to the convolution and pooling parameters.
-
-    The Conv2D operation can be described as follows:
-
-        >>> inputs = inputs << input_shift
-        >>> prod = conv2d(inputs, weights)
-        >>> output = prod + (bias << bias_shift) (optional)
-        >>> output = pool(output) (optional)
-        >>> output = ReLU(output) (optional)
-        >>> output = output * output_scale >> output_shift
-
-    Note that output values will be saturated on the range that can be represented with
-    output_bits.
-
-    Args:
-        filters (int): number of filters.
-        kernel_size (int): integer value specifying the height and width of the 2D convolution
-            window.
-        padding (:obj:`Padding`, optional): type of convolution rather Padding.Same or
-            Padding.Valid.
-            Defaults to Padding.Same.
-        kernel_stride (int, optional): integer representing the convolution stride across both
-            spatial dimensions.
-            Defaults to 1.
-        pool_type (:obj:`PoolType`, optional): pooling type (NoPooling, Max or Average).
-            Defaults to PoolType.NoPooling.
-        pool_size (int, optional): integer value specifying the height and width of the window
-            over which to take the maximum or the average (depending on pool_type parameter).
-            Defaults to -1.
-        pool_stride (int, optional): integer representing the stride across both dimensions.
-            Defaults to -1.
-        activation (bool, optional): enable or disable activation function.
-            Defaults to True.
-        output_bits (int, optional): output bitwidth. Defaults to 8.
-        buffer_bits (int, optional): buffer bitwidth. Defaults to 23.
-        name (str, optional): name of the layer. Defaults to empty string.
-
-    """
-
-    def __init__(self,
-                 filters,
-                 kernel_size,
-                 padding=Padding.Same,
-                 kernel_stride=1,
-                 pool_type=PoolType.NoPooling,
-                 pool_size=-1,
-                 pool_stride=-1,
-                 activation=True,
-                 output_bits=8,
-                 buffer_bits=23,
-                 name=""):
-        try:
-            params = LayerParams(
-                LayerType.Conv2D, {
-                    "filters": filters,
-                    "kernel_size": kernel_size,
-                    "padding": padding,
-                    "kernel_stride": kernel_stride,
-                    "pool_type": pool_type,
-                    "pool_size": pool_size,
-                    "pool_stride": pool_stride,
-                    "activation": activation,
-                    "output_bits": output_bits,
-                    "buffer_bits": buffer_bits
-                })
-            # Call parent constructor to initialize C++ bindings
-            # Note that we invoke directly __init__ instead of using super, as
-            # specified in pybind documentation
-            Layer.__init__(self, params, name)
-        except BaseException:
-            self = None
-            raise
+from akida.core import (Layer, Padding, PoolType, LayerType, LayerParams)
+
+
+class Conv2D(Layer):
+    """This represents the Akida V2 Conv2D layer.
+
+    It applies a convolution optionally followed by a bias addition, a
+    pooling operation and a ReLU activation.
+    Inputs shape must be in the form (X, Y, C). Being the result of a quantized
+    operation, it is possible to apply some shifts to adjust the inputs/outputs
+    scales to the equivalent operation performed on floats, while maintaining
+    a limited usage of bits and performing the operations on integer values.
+    The order of the input spatial dimensions is preserved, but their values may
+    change according to the convolution and pooling parameters.
+
+    The Conv2D operation can be described as follows:
+
+        >>> inputs = inputs << input_shift
+        >>> prod = conv2d(inputs, weights)
+        >>> output = prod + (bias << bias_shift) (optional)
+        >>> output = pool(output) (optional)
+        >>> output = ReLU(output) (optional)
+        >>> output = output * output_scale >> output_shift
+
+    Note that output values will be saturated on the range that can be represented with
+    output_bits.
+
+    Args:
+        filters (int): number of filters.
+        kernel_size (int): integer value specifying the height and width of the 2D convolution
+            window.
+        padding (:obj:`Padding`, optional): type of convolution rather Padding.Same or
+            Padding.Valid.
+            Defaults to Padding.Same.
+        kernel_stride (int, optional): integer representing the convolution stride across both
+            spatial dimensions.
+            Defaults to 1.
+        pool_type (:obj:`PoolType`, optional): pooling type (NoPooling, Max or Average).
+            Defaults to PoolType.NoPooling.
+        pool_size (int, optional): integer value specifying the height and width of the window
+            over which to take the maximum or the average (depending on pool_type parameter).
+            Defaults to -1.
+        pool_stride (int, optional): integer representing the stride across both dimensions.
+            Defaults to -1.
+        activation (bool, optional): enable or disable activation function.
+            Defaults to True.
+        output_bits (int, optional): output bitwidth. Defaults to 8.
+        buffer_bits (int, optional): buffer bitwidth. Defaults to 23.
+        name (str, optional): name of the layer. Defaults to empty string.
+
+    """
+
+    def __init__(self,
+                 filters,
+                 kernel_size,
+                 padding=Padding.Same,
+                 kernel_stride=1,
+                 pool_type=PoolType.NoPooling,
+                 pool_size=-1,
+                 pool_stride=-1,
+                 activation=True,
+                 output_bits=8,
+                 buffer_bits=23,
+                 name=""):
+        try:
+            params = LayerParams(
+                LayerType.Conv2D, {
+                    "filters": filters,
+                    "kernel_size": kernel_size,
+                    "padding": padding,
+                    "kernel_stride": kernel_stride,
+                    "pool_type": pool_type,
+                    "pool_size": pool_size,
+                    "pool_stride": pool_stride,
+                    "activation": activation,
+                    "output_bits": output_bits,
+                    "buffer_bits": buffer_bits
+                })
+            # Call parent constructor to initialize C++ bindings
+            # Note that we invoke directly __init__ instead of using super, as
+            # specified in pybind documentation
+            Layer.__init__(self, params, name)
+        except BaseException:
+            self = None
+            raise
```

## akida/layers/conv2d_transpose.py

 * *Ordering differences only*

```diff
@@ -1,63 +1,63 @@
-from akida.core import (Layer, LayerType, LayerParams)
-
-
-class Conv2DTranspose(Layer):
-    """This represents the Akida V2 Conv2DTranspose layer.
-
-    It applies a transposed convolution (also called deconvolution) optionally followed by a bias
-    addition and a ReLU activation.
-    Inputs shape must be in the form (X, Y, C). Being the result of a quantized operation, it is
-    possible to apply some shifts to adjust the inputs/outputs scales to the equivalent operation
-    performed on floats, while maintaining a limited usage of bits and performing the operations on
-    integer values.
-    The order of the input spatial dimensions is preserved, but their values may change according
-    to the layer parameters.
-    Note that the layer performs only transpose convolution with a "Same" padding and a kernel
-    stride equal to 2.
-
-    The Conv2DTranspose operation can be described as follows:
-
-        >>> inputs = inputs << input_shift
-        >>> prod = conv2d_transpose(inputs, weights)
-        >>> output = prod + (bias << bias_shift) #optional
-        >>> output = ReLU(output) #optional
-        >>> output = output * output_scale >> output_shift
-
-    Note that output values will be saturated on the range that can be represented with
-    output_bits.
-
-    Args:
-        filters (int): number of filters.
-        kernel_size (int): integer value specifying the height and width of the 2D convolution
-            window.
-        activation (bool, optional): enable or disable activation function.
-            Defaults to True.
-        output_bits (int, optional): output bitwidth. Defaults to 8.
-        buffer_bits (int, optional): buffer bitwidth. Defaults to 32.
-        name (str, optional): name of the layer. Defaults to empty string.
-
-    """
-
-    def __init__(self,
-                 filters,
-                 kernel_size,
-                 activation=True,
-                 output_bits=8,
-                 buffer_bits=32,
-                 name=""):
-        try:
-            params = LayerParams(
-                LayerType.Conv2DTranspose, {
-                    "filters": filters,
-                    "kernel_size": kernel_size,
-                    "activation": activation,
-                    "output_bits": output_bits,
-                    "buffer_bits": buffer_bits
-                })
-            # Call parent constructor to initialize C++ bindings
-            # Note that we invoke directly __init__ instead of using super, as
-            # specified in pybind documentation
-            Layer.__init__(self, params, name)
-        except BaseException:
-            self = None
-            raise
+from akida.core import (Layer, LayerType, LayerParams)
+
+
+class Conv2DTranspose(Layer):
+    """This represents the Akida V2 Conv2DTranspose layer.
+
+    It applies a transposed convolution (also called deconvolution) optionally followed by a bias
+    addition and a ReLU activation.
+    Inputs shape must be in the form (X, Y, C). Being the result of a quantized operation, it is
+    possible to apply some shifts to adjust the inputs/outputs scales to the equivalent operation
+    performed on floats, while maintaining a limited usage of bits and performing the operations on
+    integer values.
+    The order of the input spatial dimensions is preserved, but their values may change according
+    to the layer parameters.
+    Note that the layer performs only transpose convolution with a "Same" padding and a kernel
+    stride equal to 2.
+
+    The Conv2DTranspose operation can be described as follows:
+
+        >>> inputs = inputs << input_shift
+        >>> prod = conv2d_transpose(inputs, weights)
+        >>> output = prod + (bias << bias_shift) #optional
+        >>> output = ReLU(output) #optional
+        >>> output = output * output_scale >> output_shift
+
+    Note that output values will be saturated on the range that can be represented with
+    output_bits.
+
+    Args:
+        filters (int): number of filters.
+        kernel_size (int): integer value specifying the height and width of the 2D convolution
+            window.
+        activation (bool, optional): enable or disable activation function.
+            Defaults to True.
+        output_bits (int, optional): output bitwidth. Defaults to 8.
+        buffer_bits (int, optional): buffer bitwidth. Defaults to 32.
+        name (str, optional): name of the layer. Defaults to empty string.
+
+    """
+
+    def __init__(self,
+                 filters,
+                 kernel_size,
+                 activation=True,
+                 output_bits=8,
+                 buffer_bits=32,
+                 name=""):
+        try:
+            params = LayerParams(
+                LayerType.Conv2DTranspose, {
+                    "filters": filters,
+                    "kernel_size": kernel_size,
+                    "activation": activation,
+                    "output_bits": output_bits,
+                    "buffer_bits": buffer_bits
+                })
+            # Call parent constructor to initialize C++ bindings
+            # Note that we invoke directly __init__ instead of using super, as
+            # specified in pybind documentation
+            Layer.__init__(self, params, name)
+        except BaseException:
+            self = None
+            raise
```

## akida/layers/convolutional.py

 * *Ordering differences only*

```diff
@@ -1,87 +1,87 @@
-from akida.core import (Layer, Padding, PoolType, LayerType, LayerParams)
-
-
-class Convolutional(Layer):
-    """This represents a standard Convolutional layer.
-
-    The Convolutional layer accepts 1-bit, 2-bit or 4-bit 3D input tensors with
-    an arbitrary number of channels.
-    The Convolutional layer can be configured with 1-bit, 2-bit or 4-bit weights.
-    It applies a convolution (not a cross-correlation) optionally followed by a
-    pooling operation to the input tensors.
-    It can optionally apply a step-wise ReLU activation to its outputs.
-    The layer expects a 4D tensor whose first dimension is the sample index
-    as input.
-    It returns a 4D tensor whose first dimension is the sample index and the
-    last dimension is the number of convolution filters.
-    The order of the input spatial dimensions is preserved, but their value may
-    change according to the convolution and pooling parameters.
-
-    Args:
-        kernel_size (list): list of 2 integer representing the spatial
-            dimensions of the convolutional kernel.
-        filters (int): number of filters.
-        name (str, optional): name of the layer. Defaults to empty string
-        padding (:obj:`Padding`, optional): type of convolution. Defaults to
-            Padding.Same.
-        kernel_stride (list, optional): list of 2 integer representing the
-            convolution stride (X, Y). Defaults to (1, 1).
-        weights_bits (int, optional): number of bits used to quantize weights.
-             Defaults to 1.
-        pool_size (list, optional): list of 2 integers, representing the window
-            size over which to take the maximum or the average (depending on
-            pool_type parameter). Defaults to (-1, -1).
-        pool_type (:obj:`PoolType`, optional): pooling type
-            (NoPooling, Max or Average). Defaults to Pooling.NoPooling.
-        pool_stride (list, optional): list of 2 integers representing
-            the stride dimensions. Defaults to (-1, -1).
-        activation (bool, optional): enable or disable activation
-            function. Defaults to True.
-        act_bits (int, optional): number of bits used to quantize
-            the neuron response. Defaults to 1.
-
-    """
-
-    def __init__(self,
-                 kernel_size,
-                 filters,
-                 name="",
-                 padding=Padding.Same,
-                 kernel_stride=(1, 1),
-                 weights_bits=1,
-                 pool_size=(-1, -1),
-                 pool_type=PoolType.NoPooling,
-                 pool_stride=(-1, -1),
-                 activation=True,
-                 act_bits=1):
-        try:
-            pooling_stride_x = pool_stride[0]
-            if pool_stride[0] < 0:
-                pooling_stride_x = pool_size[0]
-            pooling_stride_y = pool_stride[1]
-            if pool_stride[1] < 0:
-                pooling_stride_y = pool_size[1]
-            params = LayerParams(
-                LayerType.Convolutional, {
-                    "kernel_width": kernel_size[0],
-                    "kernel_height": kernel_size[1],
-                    "padding": padding,
-                    "filters": filters,
-                    "stride_x": kernel_stride[0],
-                    "stride_y": kernel_stride[1],
-                    "weights_bits": weights_bits,
-                    "pooling_width": pool_size[0],
-                    "pooling_height": pool_size[1],
-                    "pool_type": pool_type,
-                    "pooling_stride_x": pooling_stride_x,
-                    "pooling_stride_y": pooling_stride_y,
-                    "activation": activation,
-                    "act_bits": act_bits
-                })
-            # Call parent constructor to initialize C++ bindings
-            # Note that we invoke directly __init__ instead of using super, as
-            # specified in pybind documentation
-            Layer.__init__(self, params, name)
-        except BaseException:
-            self = None
-            raise
+from akida.core import (Layer, Padding, PoolType, LayerType, LayerParams)
+
+
+class Convolutional(Layer):
+    """This represents a standard Convolutional layer.
+
+    The Convolutional layer accepts 1-bit, 2-bit or 4-bit 3D input tensors with
+    an arbitrary number of channels.
+    The Convolutional layer can be configured with 1-bit, 2-bit or 4-bit weights.
+    It applies a convolution (not a cross-correlation) optionally followed by a
+    pooling operation to the input tensors.
+    It can optionally apply a step-wise ReLU activation to its outputs.
+    The layer expects a 4D tensor whose first dimension is the sample index
+    as input.
+    It returns a 4D tensor whose first dimension is the sample index and the
+    last dimension is the number of convolution filters.
+    The order of the input spatial dimensions is preserved, but their value may
+    change according to the convolution and pooling parameters.
+
+    Args:
+        kernel_size (list): list of 2 integer representing the spatial
+            dimensions of the convolutional kernel.
+        filters (int): number of filters.
+        name (str, optional): name of the layer. Defaults to empty string
+        padding (:obj:`Padding`, optional): type of convolution. Defaults to
+            Padding.Same.
+        kernel_stride (list, optional): list of 2 integer representing the
+            convolution stride (X, Y). Defaults to (1, 1).
+        weights_bits (int, optional): number of bits used to quantize weights.
+             Defaults to 1.
+        pool_size (list, optional): list of 2 integers, representing the window
+            size over which to take the maximum or the average (depending on
+            pool_type parameter). Defaults to (-1, -1).
+        pool_type (:obj:`PoolType`, optional): pooling type
+            (NoPooling, Max or Average). Defaults to Pooling.NoPooling.
+        pool_stride (list, optional): list of 2 integers representing
+            the stride dimensions. Defaults to (-1, -1).
+        activation (bool, optional): enable or disable activation
+            function. Defaults to True.
+        act_bits (int, optional): number of bits used to quantize
+            the neuron response. Defaults to 1.
+
+    """
+
+    def __init__(self,
+                 kernel_size,
+                 filters,
+                 name="",
+                 padding=Padding.Same,
+                 kernel_stride=(1, 1),
+                 weights_bits=1,
+                 pool_size=(-1, -1),
+                 pool_type=PoolType.NoPooling,
+                 pool_stride=(-1, -1),
+                 activation=True,
+                 act_bits=1):
+        try:
+            pooling_stride_x = pool_stride[0]
+            if pool_stride[0] < 0:
+                pooling_stride_x = pool_size[0]
+            pooling_stride_y = pool_stride[1]
+            if pool_stride[1] < 0:
+                pooling_stride_y = pool_size[1]
+            params = LayerParams(
+                LayerType.Convolutional, {
+                    "kernel_width": kernel_size[0],
+                    "kernel_height": kernel_size[1],
+                    "padding": padding,
+                    "filters": filters,
+                    "stride_x": kernel_stride[0],
+                    "stride_y": kernel_stride[1],
+                    "weights_bits": weights_bits,
+                    "pooling_width": pool_size[0],
+                    "pooling_height": pool_size[1],
+                    "pool_type": pool_type,
+                    "pooling_stride_x": pooling_stride_x,
+                    "pooling_stride_y": pooling_stride_y,
+                    "activation": activation,
+                    "act_bits": act_bits
+                })
+            # Call parent constructor to initialize C++ bindings
+            # Note that we invoke directly __init__ instead of using super, as
+            # specified in pybind documentation
+            Layer.__init__(self, params, name)
+        except BaseException:
+            self = None
+            raise
```

## akida/layers/dense1d.py

 * *Ordering differences only*

```diff
@@ -1,55 +1,55 @@
-from akida.core import (Layer, LayerParams, LayerType)
-
-
-class Dense1D(Layer):
-    """Dense layer capable of working on 1D inputs.
-
-    This is a simple dotproduct between an input of shape (1, 1, X) and a kernel
-    of shape (X, F) to output a tensor of shape (1, 1, F). Being the result of a
-    quantized operation, it is possible to apply some shifts to adjust the
-    inputs/outputs scales to the equivalent operation performed on floats, while
-    maintaining a limited usage of bits and performing the operations on integer
-    values.
-
-    The 1D Dense operation can be described as follows:
-
-        >>> inputs = inputs << input_shift
-        >>> prod = matmul(inputs, weights)
-        >>> output = prod + (bias << bias_shift)
-        >>> output = output * output_scale >> output_shift
-
-    Inputs shape must be (1, 1, X), if not it's reshaped automatically at the
-    beginning. Note that output values will be saturated on the range that can be
-    represented with output_bits.
-
-    Args:
-        units (int): Positive integer, dimensionality of the output space.
-        output_bits (int, optional): output bitwidth. Defaults to 8.
-        buffer_bits (int, optional): buffer bitwidth. Defaults to 23.
-        activation (bool, optional): apply a relu activation. Defaults to False.
-        name (str, optional): name of the layer. Defaults to empty string.
-
-    """
-
-    def __init__(self,
-                 units,
-                 output_bits=8,
-                 buffer_bits=23,
-                 activation=False,
-                 name=""):
-        try:
-            params = LayerParams(
-                LayerType.Dense2D, {
-                    "units": units,
-                    "output_bits": output_bits,
-                    "buffer_bits": buffer_bits,
-                    "activation": activation,
-                    "collapse_dims": True,
-                })
-            # Call parent constructor to initialize C++ bindings
-            # Note that we invoke directly __init__ instead of using super, as
-            # specified in pybind documentation
-            Layer.__init__(self, params, name)
-        except BaseException:
-            self = None
-            raise
+from akida.core import (Layer, LayerParams, LayerType)
+
+
+class Dense1D(Layer):
+    """Dense layer capable of working on 1D inputs.
+
+    This is a simple dotproduct between an input of shape (1, 1, X) and a kernel
+    of shape (X, F) to output a tensor of shape (1, 1, F). Being the result of a
+    quantized operation, it is possible to apply some shifts to adjust the
+    inputs/outputs scales to the equivalent operation performed on floats, while
+    maintaining a limited usage of bits and performing the operations on integer
+    values.
+
+    The 1D Dense operation can be described as follows:
+
+        >>> inputs = inputs << input_shift
+        >>> prod = matmul(inputs, weights)
+        >>> output = prod + (bias << bias_shift)
+        >>> output = output * output_scale >> output_shift
+
+    Inputs shape must be (1, 1, X), if not it's reshaped automatically at the
+    beginning. Note that output values will be saturated on the range that can be
+    represented with output_bits.
+
+    Args:
+        units (int): Positive integer, dimensionality of the output space.
+        output_bits (int, optional): output bitwidth. Defaults to 8.
+        buffer_bits (int, optional): buffer bitwidth. Defaults to 23.
+        activation (bool, optional): apply a relu activation. Defaults to False.
+        name (str, optional): name of the layer. Defaults to empty string.
+
+    """
+
+    def __init__(self,
+                 units,
+                 output_bits=8,
+                 buffer_bits=23,
+                 activation=False,
+                 name=""):
+        try:
+            params = LayerParams(
+                LayerType.Dense2D, {
+                    "units": units,
+                    "output_bits": output_bits,
+                    "buffer_bits": buffer_bits,
+                    "activation": activation,
+                    "collapse_dims": True,
+                })
+            # Call parent constructor to initialize C++ bindings
+            # Note that we invoke directly __init__ instead of using super, as
+            # specified in pybind documentation
+            Layer.__init__(self, params, name)
+        except BaseException:
+            self = None
+            raise
```

## akida/layers/dense2d.py

 * *Ordering differences only*

```diff
@@ -1,56 +1,56 @@
-from akida.core import (Layer, LayerParams, LayerType)
-
-
-class Dense2D(Layer):
-    """Dense layer capable of working on 2D inputs.
-
-    The 2D Dense operation is simply the repetition of a 1D
-    FullyConnected/Dense operation over each input row.
-    Inputs shape mush be in the form (1, X, Y). Being the result of a quantized
-    operation, it is possible to apply some shifts to adjust the inputs/outputs
-    scales to the equivalent operation performed on floats, while maintaining
-    a limited usage of bits and performing the operations on integer values.
-
-    The 2D Dense operation can be described as follows:
-
-        >>> inputs = inputs << input_shift
-        >>> prod = matmul(inputs, weights)
-        >>> output = prod + (bias << bias_shift)
-        >>> output = output * output_scale >> output_shift
-
-    Inputs shape must be (1, X, Y). Note that output values will be saturated
-    on the range that can be represented with output_bits.
-
-    Args:
-        units (int): Positive integer, dimensionality of the output space.
-        weights_bits (int, optional): number of bits used to quantize weights.
-             Defaults to 4.
-        output_bits (int, optional): output bitwidth. Defaults to 8.
-        buffer_bits (int, optional): buffer bitwidth. Defaults to 23.
-        activation (bool, optional): apply a relu activation. Defaults to False.
-        name (str, optional): name of the layer. Defaults to empty string.
-
-    """
-
-    def __init__(self,
-                 units,
-                 output_bits=8,
-                 buffer_bits=23,
-                 activation=False,
-                 name=""):
-        try:
-            params = LayerParams(
-                LayerType.Dense2D, {
-                    "units": units,
-                    "output_bits": output_bits,
-                    "buffer_bits": buffer_bits,
-                    "activation": activation,
-                    "collapse_dims": False,
-                })
-            # Call parent constructor to initialize C++ bindings
-            # Note that we invoke directly __init__ instead of using super, as
-            # specified in pybind documentation
-            Layer.__init__(self, params, name)
-        except BaseException:
-            self = None
-            raise
+from akida.core import (Layer, LayerParams, LayerType)
+
+
+class Dense2D(Layer):
+    """Dense layer capable of working on 2D inputs.
+
+    The 2D Dense operation is simply the repetition of a 1D
+    FullyConnected/Dense operation over each input row.
+    Inputs shape mush be in the form (1, X, Y). Being the result of a quantized
+    operation, it is possible to apply some shifts to adjust the inputs/outputs
+    scales to the equivalent operation performed on floats, while maintaining
+    a limited usage of bits and performing the operations on integer values.
+
+    The 2D Dense operation can be described as follows:
+
+        >>> inputs = inputs << input_shift
+        >>> prod = matmul(inputs, weights)
+        >>> output = prod + (bias << bias_shift)
+        >>> output = output * output_scale >> output_shift
+
+    Inputs shape must be (1, X, Y). Note that output values will be saturated
+    on the range that can be represented with output_bits.
+
+    Args:
+        units (int): Positive integer, dimensionality of the output space.
+        weights_bits (int, optional): number of bits used to quantize weights.
+             Defaults to 4.
+        output_bits (int, optional): output bitwidth. Defaults to 8.
+        buffer_bits (int, optional): buffer bitwidth. Defaults to 23.
+        activation (bool, optional): apply a relu activation. Defaults to False.
+        name (str, optional): name of the layer. Defaults to empty string.
+
+    """
+
+    def __init__(self,
+                 units,
+                 output_bits=8,
+                 buffer_bits=23,
+                 activation=False,
+                 name=""):
+        try:
+            params = LayerParams(
+                LayerType.Dense2D, {
+                    "units": units,
+                    "output_bits": output_bits,
+                    "buffer_bits": buffer_bits,
+                    "activation": activation,
+                    "collapse_dims": False,
+                })
+            # Call parent constructor to initialize C++ bindings
+            # Note that we invoke directly __init__ instead of using super, as
+            # specified in pybind documentation
+            Layer.__init__(self, params, name)
+        except BaseException:
+            self = None
+            raise
```

## akida/layers/depthwise_conv2d.py

 * *Ordering differences only*

```diff
@@ -1,70 +1,70 @@
-from akida.core import (Layer, LayerParams, LayerType, Padding, PoolType)
-
-
-class DepthwiseConv2D(Layer):
-    """This represents a depthwise convolutional layer.
-
-    This is like a standard convolution, except it acts on each input channel separately.
-    There is a single filter per input channel, so weights shape is (X, Y, F).
-    Being the result of a quantized operation, it is possible to apply some shifts to adjust the
-    inputs/outputs scales to the equivalent operation performed on floats, while maintaining a
-    limited usage of bits and performing the operations on integer values.
-
-    Note: this layer applies a real convolution, and not a cross-correlation. It can optionally
-    apply a step-wise ReLU activation to its outputs.
-    The layer expects a 4D tensor whose first dimension is the sample index as input.
-
-    It returns a 4D tensor whose first dimension is the sample index and the last dimension is the
-    number of convolution filters, so the same as input channels.
-    The order of the input spatial dimensions is preserved, but their value may change according to
-    the convolution and pooling parameters.
-
-    Args:
-        kernel_size (int): Integer representing the spatial dimensions of the depthwise kernel.
-        kernel_stride (int, optional): Integer representing the spatial convolution stride.
-            Defaults to 1.
-        padding (:obj:`Padding`, optional): type of convolution. Defaults to Padding.Same.
-        pool_type (:obj:`PoolType`, optional): pooling type (NoPooling, or Max). Defaults to
-            PoolType.NoPooling.
-        pool_size (int, optional): Integer representing the window size over which to take the
-            maximum. Defaults to -1.
-        pool_stride (int, optional): Integer representing the pooling stride dimensions. A value of
-            -1 means same as pool_size. Defaults to -1.
-        output_bits (int, optional): output bitwidth. Defaults to 8.
-        buffer_bits (int, optional): buffer bitwidth. Defaults to 32.
-        activation (bool, optional): enable or disable relu activation function. Defaults to True.
-        name (str, optional): name of the layer. Defaults to empty string.
-
-    """
-
-    def __init__(self,
-                 kernel_size,
-                 kernel_stride=1,
-                 padding=Padding.Same,
-                 pool_type=PoolType.NoPooling,
-                 pool_size=-1,
-                 pool_stride=-1,
-                 output_bits=8,
-                 buffer_bits=32,
-                 activation=True,
-                 name=""):
-        try:
-            params = LayerParams(
-                LayerType.DepthwiseConv2D, {
-                    "kernel_size": kernel_size,
-                    "kernel_stride": kernel_stride,
-                    "padding": padding,
-                    "pool_type": pool_type,
-                    "pool_size": pool_size,
-                    "pool_stride": pool_stride,
-                    "output_bits": output_bits,
-                    "buffer_bits": buffer_bits,
-                    "activation": activation
-                })
-            # Call parent constructor to initialize C++ bindings
-            # Note that we invoke directly __init__ instead of using super, as
-            # specified in pybind documentation
-            Layer.__init__(self, params, name)
-        except BaseException:
-            self = None
-            raise
+from akida.core import (Layer, LayerParams, LayerType, Padding, PoolType)
+
+
+class DepthwiseConv2D(Layer):
+    """This represents a depthwise convolutional layer.
+
+    This is like a standard convolution, except it acts on each input channel separately.
+    There is a single filter per input channel, so weights shape is (X, Y, F).
+    Being the result of a quantized operation, it is possible to apply some shifts to adjust the
+    inputs/outputs scales to the equivalent operation performed on floats, while maintaining a
+    limited usage of bits and performing the operations on integer values.
+
+    Note: this layer applies a real convolution, and not a cross-correlation. It can optionally
+    apply a step-wise ReLU activation to its outputs.
+    The layer expects a 4D tensor whose first dimension is the sample index as input.
+
+    It returns a 4D tensor whose first dimension is the sample index and the last dimension is the
+    number of convolution filters, so the same as input channels.
+    The order of the input spatial dimensions is preserved, but their value may change according to
+    the convolution and pooling parameters.
+
+    Args:
+        kernel_size (int): Integer representing the spatial dimensions of the depthwise kernel.
+        kernel_stride (int, optional): Integer representing the spatial convolution stride.
+            Defaults to 1.
+        padding (:obj:`Padding`, optional): type of convolution. Defaults to Padding.Same.
+        pool_type (:obj:`PoolType`, optional): pooling type (NoPooling, or Max). Defaults to
+            PoolType.NoPooling.
+        pool_size (int, optional): Integer representing the window size over which to take the
+            maximum. Defaults to -1.
+        pool_stride (int, optional): Integer representing the pooling stride dimensions. A value of
+            -1 means same as pool_size. Defaults to -1.
+        output_bits (int, optional): output bitwidth. Defaults to 8.
+        buffer_bits (int, optional): buffer bitwidth. Defaults to 32.
+        activation (bool, optional): enable or disable relu activation function. Defaults to True.
+        name (str, optional): name of the layer. Defaults to empty string.
+
+    """
+
+    def __init__(self,
+                 kernel_size,
+                 kernel_stride=1,
+                 padding=Padding.Same,
+                 pool_type=PoolType.NoPooling,
+                 pool_size=-1,
+                 pool_stride=-1,
+                 output_bits=8,
+                 buffer_bits=32,
+                 activation=True,
+                 name=""):
+        try:
+            params = LayerParams(
+                LayerType.DepthwiseConv2D, {
+                    "kernel_size": kernel_size,
+                    "kernel_stride": kernel_stride,
+                    "padding": padding,
+                    "pool_type": pool_type,
+                    "pool_size": pool_size,
+                    "pool_stride": pool_stride,
+                    "output_bits": output_bits,
+                    "buffer_bits": buffer_bits,
+                    "activation": activation
+                })
+            # Call parent constructor to initialize C++ bindings
+            # Note that we invoke directly __init__ instead of using super, as
+            # specified in pybind documentation
+            Layer.__init__(self, params, name)
+        except BaseException:
+            self = None
+            raise
```

## akida/layers/depthwise_conv2d_transpose.py

 * *Ordering differences only*

```diff
@@ -1,61 +1,61 @@
-from akida.core import (Layer, LayerType, LayerParams)
-
-
-class DepthwiseConv2DTranspose(Layer):
-    """This represents the Akida V2 DepthwiseConv2DTranspose layer.
-
-    It applies a transposed depthwise convolution (also called deconvolution) optionally followed
-    by a bias addition and a ReLU activation.
-    This is like a standard transposed convolution, except it acts on each input channel
-    separately.
-    Inputs shape must be in the form (X, Y, C). Being the result of a quantized operation, it is
-    possible to apply some shifts to adjust the inputs/outputs scales to the equivalent operation
-    performed on floats, while maintaining a limited usage of bits and performing the operations on
-    integer values.
-    The order of the input spatial dimensions is preserved, but their values may change according
-    to the layer parameters.
-    Note that the layer performs only transpose depthwise convolution with a "Same" padding and a
-    kernel stride equal to 2.
-
-    The DepthwiseConv2DTranspose operation can be described as follows:
-
-        >>> inputs = inputs << input_shift
-        >>> prod = depthwise_conv2d_transpose(inputs, weights)
-        >>> output = prod + (bias << bias_shift) #optional
-        >>> output = ReLU(output) #optional
-        >>> output = output * output_scale >> output_shift
-
-    Note that output values will be saturated on the range that can be represented with
-    output_bits.
-
-    Args:
-        kernel_size (int): Integer representing the spatial dimensions of the depthwise kernel.
-        activation (bool, optional): enable or disable activation function.
-            Defaults to True.
-        output_bits (int, optional): output bitwidth. Defaults to 8.
-        buffer_bits (int, optional): buffer bitwidth. Defaults to 32.
-        name (str, optional): name of the layer. Defaults to empty string.
-
-    """
-
-    def __init__(self,
-                 kernel_size,
-                 activation=True,
-                 output_bits=8,
-                 buffer_bits=32,
-                 name=""):
-        try:
-            params = LayerParams(
-                LayerType.DepthwiseConv2DTranspose, {
-                    "kernel_size": kernel_size,
-                    "activation": activation,
-                    "output_bits": output_bits,
-                    "buffer_bits": buffer_bits
-                })
-            # Call parent constructor to initialize C++ bindings
-            # Note that we invoke directly __init__ instead of using super, as
-            # specified in pybind documentation
-            Layer.__init__(self, params, name)
-        except BaseException:
-            self = None
-            raise
+from akida.core import (Layer, LayerType, LayerParams)
+
+
+class DepthwiseConv2DTranspose(Layer):
+    """This represents the Akida V2 DepthwiseConv2DTranspose layer.
+
+    It applies a transposed depthwise convolution (also called deconvolution) optionally followed
+    by a bias addition and a ReLU activation.
+    This is like a standard transposed convolution, except it acts on each input channel
+    separately.
+    Inputs shape must be in the form (X, Y, C). Being the result of a quantized operation, it is
+    possible to apply some shifts to adjust the inputs/outputs scales to the equivalent operation
+    performed on floats, while maintaining a limited usage of bits and performing the operations on
+    integer values.
+    The order of the input spatial dimensions is preserved, but their values may change according
+    to the layer parameters.
+    Note that the layer performs only transpose depthwise convolution with a "Same" padding and a
+    kernel stride equal to 2.
+
+    The DepthwiseConv2DTranspose operation can be described as follows:
+
+        >>> inputs = inputs << input_shift
+        >>> prod = depthwise_conv2d_transpose(inputs, weights)
+        >>> output = prod + (bias << bias_shift) #optional
+        >>> output = ReLU(output) #optional
+        >>> output = output * output_scale >> output_shift
+
+    Note that output values will be saturated on the range that can be represented with
+    output_bits.
+
+    Args:
+        kernel_size (int): Integer representing the spatial dimensions of the depthwise kernel.
+        activation (bool, optional): enable or disable activation function.
+            Defaults to True.
+        output_bits (int, optional): output bitwidth. Defaults to 8.
+        buffer_bits (int, optional): buffer bitwidth. Defaults to 32.
+        name (str, optional): name of the layer. Defaults to empty string.
+
+    """
+
+    def __init__(self,
+                 kernel_size,
+                 activation=True,
+                 output_bits=8,
+                 buffer_bits=32,
+                 name=""):
+        try:
+            params = LayerParams(
+                LayerType.DepthwiseConv2DTranspose, {
+                    "kernel_size": kernel_size,
+                    "activation": activation,
+                    "output_bits": output_bits,
+                    "buffer_bits": buffer_bits
+                })
+            # Call parent constructor to initialize C++ bindings
+            # Note that we invoke directly __init__ instead of using super, as
+            # specified in pybind documentation
+            Layer.__init__(self, params, name)
+        except BaseException:
+            self = None
+            raise
```

## akida/layers/dequantizer.py

 * *Ordering differences only*

```diff
@@ -1,18 +1,18 @@
-from akida.core import Layer, LayerParams, LayerType
-
-
-class Dequantizer(Layer):
-    """A layer capable of dequantizing an input tensor.
-
-    This resolve the scales of an input tensor, following the equantion::
-
-        output = input x scales
-    """
-
-    def __init__(self, name=""):
-        try:
-            params = LayerParams(LayerType.Dequantizer, {})
-            Layer.__init__(self, params, name)
-        except BaseException:
-            self = None
-            raise
+from akida.core import Layer, LayerParams, LayerType
+
+
+class Dequantizer(Layer):
+    """A layer capable of dequantizing an input tensor.
+
+    This resolve the scales of an input tensor, following the equantion::
+
+        output = input x scales
+    """
+
+    def __init__(self, name=""):
+        try:
+            params = LayerParams(LayerType.Dequantizer, {})
+            Layer.__init__(self, params, name)
+        except BaseException:
+            self = None
+            raise
```

## akida/layers/extract_token.py

 * *Ordering differences only*

```diff
@@ -1,36 +1,36 @@
-from akida.core import (Layer, LayerParams, LayerType)
-
-
-class ExtractToken(Layer):
-    """A layer capable of extracting a range from input tensor.
-
-    This is similar to numpy.take_along_axis, where the indices are in the
-    range [begin:end]. Note that reduction axis will be the first axis that
-    is not 1.
-
-    Args:
-        begin (int, optional): beginning of the range to take into account.
-            Defaults to 0.
-        end (int, optional): end of the range to take into account.
-            Defaults to None.
-    """
-
-    def __init__(self,
-                 begin=0,
-                 end=None,
-                 name=""):
-        if end is None:
-            end = begin + 1
-        try:
-            params = LayerParams(
-                LayerType.ExtractToken, {
-                    "begin": begin,
-                    "end": end
-                })
-            # Call parent constructor to initialize C++ bindings
-            # Note that we invoke directly __init__ instead of using super, as
-            # specified in pybind documentation
-            Layer.__init__(self, params, name)
-        except BaseException:
-            self = None
-            raise
+from akida.core import (Layer, LayerParams, LayerType)
+
+
+class ExtractToken(Layer):
+    """A layer capable of extracting a range from input tensor.
+
+    This is similar to numpy.take_along_axis, where the indices are in the
+    range [begin:end]. Note that reduction axis will be the first axis that
+    is not 1.
+
+    Args:
+        begin (int, optional): beginning of the range to take into account.
+            Defaults to 0.
+        end (int, optional): end of the range to take into account.
+            Defaults to None.
+    """
+
+    def __init__(self,
+                 begin=0,
+                 end=None,
+                 name=""):
+        if end is None:
+            end = begin + 1
+        try:
+            params = LayerParams(
+                LayerType.ExtractToken, {
+                    "begin": begin,
+                    "end": end
+                })
+            # Call parent constructor to initialize C++ bindings
+            # Note that we invoke directly __init__ instead of using super, as
+            # specified in pybind documentation
+            Layer.__init__(self, params, name)
+        except BaseException:
+            self = None
+            raise
```

## akida/layers/fully_connected.py

 * *Ordering differences only*

```diff
@@ -1,47 +1,47 @@
-from akida.core import Layer, LayerType, LayerParams
-
-
-class FullyConnected(Layer):
-    """This represents a Dense or Linear neural layer.
-
-    The FullyConnected layer accepts 1-bit, 2-bit or 4-bit input tensors.
-    The FullyConnected can be configured with 1-bit, 2-bit or 4-bit weights.
-    It multiplies the inputs by its internal unit weights, returning a 4D
-    tensor of values whose first dimension is the number of samples and the
-    last dimension represents the number of units.
-    It can optionally apply a step-wise ReLU activation to its outputs.
-
-    Args:
-        units (int): number of units.
-        name (str, optional): name of the layer. Defaults to empty string.
-        weights_bits (int, optional): number of bits used to quantize weights.
-             Defaults to 1.
-        activation (bool, optional): enable or disable activation
-            function. Defaults to True.
-        act_bits (int, optional): number of bits used to quantize the neuron
-            response. Defaults to 1.
-
-    """
-
-    def __init__(self,
-                 units,
-                 name="",
-                 weights_bits=1,
-                 activation=True,
-                 act_bits=1):
-        try:
-            params = LayerParams(
-                LayerType.FullyConnected, {
-                    "units": units,
-                    "weights_bits": weights_bits,
-                    "activation": activation,
-                    "act_bits": act_bits
-                })
-
-            # Call parent constructor to initialize C++ bindings
-            # Note that we invoke directly __init__ instead of using super, as
-            # specified in pybind documentation
-            Layer.__init__(self, params, name)
-        except BaseException:
-            self = None
-            raise
+from akida.core import Layer, LayerType, LayerParams
+
+
+class FullyConnected(Layer):
+    """This represents a Dense or Linear neural layer.
+
+    The FullyConnected layer accepts 1-bit, 2-bit or 4-bit input tensors.
+    The FullyConnected can be configured with 1-bit, 2-bit or 4-bit weights.
+    It multiplies the inputs by its internal unit weights, returning a 4D
+    tensor of values whose first dimension is the number of samples and the
+    last dimension represents the number of units.
+    It can optionally apply a step-wise ReLU activation to its outputs.
+
+    Args:
+        units (int): number of units.
+        name (str, optional): name of the layer. Defaults to empty string.
+        weights_bits (int, optional): number of bits used to quantize weights.
+             Defaults to 1.
+        activation (bool, optional): enable or disable activation
+            function. Defaults to True.
+        act_bits (int, optional): number of bits used to quantize the neuron
+            response. Defaults to 1.
+
+    """
+
+    def __init__(self,
+                 units,
+                 name="",
+                 weights_bits=1,
+                 activation=True,
+                 act_bits=1):
+        try:
+            params = LayerParams(
+                LayerType.FullyConnected, {
+                    "units": units,
+                    "weights_bits": weights_bits,
+                    "activation": activation,
+                    "act_bits": act_bits
+                })
+
+            # Call parent constructor to initialize C++ bindings
+            # Note that we invoke directly __init__ instead of using super, as
+            # specified in pybind documentation
+            Layer.__init__(self, params, name)
+        except BaseException:
+            self = None
+            raise
```

## akida/layers/input_conv2d.py

 * *Ordering differences only*

```diff
@@ -1,96 +1,96 @@
-from akida.core import (Layer, Padding, PoolType, LayerType, LayerParams)
-
-
-class InputConv2D(Layer):
-    """This represents the Akida V2 InputConv2D layer.
-
-    This layer is an image-specific input layer. It only accepts images in 8-bit pixels, either
-    grayscale or RGB.
-    Its kernel weights should be 8-bit.
-    It applies a convolution (actually a cross-correlation) optionally followed by a bias
-    addition, a pooling operation and a ReLU activation.
-    Inputs shape must be in the form (X, Y, C). Being the result of a quantized operation, it is
-    possible to apply some shifts to adjust the output scales to the equivalent operation performed
-    on floats, while maintaining a limited usage of bits and performing the operations on integer
-    values.
-    The order of the input spatial dimensions is preserved, but their values may change according
-    to the convolution and pooling parameters.
-
-    The InputConv2D operation can be described as follows:
-
-        >>> prod = conv2d(inputs, weights)
-        >>> output = prod + (bias << bias_shift) #optional
-        >>> output = pool(output) #optional
-        >>> output = ReLU(output) #optional
-        >>> output = output * output_scale >> output_shift
-
-    Note that output values will be saturated on the range that can be represented with
-    output_bits.
-
-    Args:
-        input_shape (tuple): the 3D input shape.
-        kernel_size (int): integer value specifying the height and width of the 2D convolution
-            window.
-        filters (int): number of filters.
-        padding (:obj:`Padding`, optional): type of convolution rather Padding.Same or
-            Padding.Valid.
-            Defaults to Padding.Same.
-        padding_value (int, optional): value used when padding.
-            Defaults to 0.
-        kernel_stride (int, optional): integer representing the convolution stride across both
-            spatial dimensions.
-            Defaults to 1.
-        pool_type (:obj:`PoolType`, optional): pooling type (NoPooling, Max).
-            Defaults to PoolType.NoPooling.
-        pool_size (int, optional): integer value specifying the height and width of the window
-            over which to take the maximum or the average (depending on pool_type parameter).
-            Defaults to -1.
-        pool_stride (int, optional): integer representing the stride across both dimensions.
-            Defaults to -1.
-        activation (bool, optional): enable or disable activation function.
-            Defaults to True.
-        output_bits (int, optional): output bitwidth. Defaults to 8.
-        buffer_bits (int, optional): buffer bitwidth. Defaults to 32.
-        name (str, optional): name of the layer. Defaults to empty string.
-
-    """
-
-    def __init__(self,
-                 input_shape,
-                 filters,
-                 kernel_size,
-                 padding=Padding.Same,
-                 padding_value=0,
-                 kernel_stride=1,
-                 pool_type=PoolType.NoPooling,
-                 pool_size=-1,
-                 pool_stride=-1,
-                 activation=True,
-                 output_bits=8,
-                 buffer_bits=32,
-                 name=""):
-        try:
-            params = LayerParams(
-                LayerType.InputConv2D, {
-                    "input_x": input_shape[0],
-                    "input_y": input_shape[1],
-                    "input_channels": input_shape[2],
-                    "filters": filters,
-                    "kernel_size": kernel_size,
-                    "padding": padding,
-                    "padding_value": padding_value,
-                    "kernel_stride": kernel_stride,
-                    "pool_type": pool_type,
-                    "pool_size": pool_size,
-                    "pool_stride": pool_stride,
-                    "activation": activation,
-                    "output_bits": output_bits,
-                    "buffer_bits": buffer_bits
-                })
-            # Call parent constructor to initialize C++ bindings
-            # Note that we invoke directly __init__ instead of using super, as
-            # specified in pybind documentation
-            Layer.__init__(self, params, name)
-        except BaseException:
-            self = None
-            raise
+from akida.core import (Layer, Padding, PoolType, LayerType, LayerParams)
+
+
+class InputConv2D(Layer):
+    """This represents the Akida V2 InputConv2D layer.
+
+    This layer is an image-specific input layer. It only accepts images in 8-bit pixels, either
+    grayscale or RGB.
+    Its kernel weights should be 8-bit.
+    It applies a convolution (actually a cross-correlation) optionally followed by a bias
+    addition, a pooling operation and a ReLU activation.
+    Inputs shape must be in the form (X, Y, C). Being the result of a quantized operation, it is
+    possible to apply some shifts to adjust the output scales to the equivalent operation performed
+    on floats, while maintaining a limited usage of bits and performing the operations on integer
+    values.
+    The order of the input spatial dimensions is preserved, but their values may change according
+    to the convolution and pooling parameters.
+
+    The InputConv2D operation can be described as follows:
+
+        >>> prod = conv2d(inputs, weights)
+        >>> output = prod + (bias << bias_shift) #optional
+        >>> output = pool(output) #optional
+        >>> output = ReLU(output) #optional
+        >>> output = output * output_scale >> output_shift
+
+    Note that output values will be saturated on the range that can be represented with
+    output_bits.
+
+    Args:
+        input_shape (tuple): the 3D input shape.
+        kernel_size (int): integer value specifying the height and width of the 2D convolution
+            window.
+        filters (int): number of filters.
+        padding (:obj:`Padding`, optional): type of convolution rather Padding.Same or
+            Padding.Valid.
+            Defaults to Padding.Same.
+        padding_value (int, optional): value used when padding.
+            Defaults to 0.
+        kernel_stride (int, optional): integer representing the convolution stride across both
+            spatial dimensions.
+            Defaults to 1.
+        pool_type (:obj:`PoolType`, optional): pooling type (NoPooling, Max).
+            Defaults to PoolType.NoPooling.
+        pool_size (int, optional): integer value specifying the height and width of the window
+            over which to take the maximum or the average (depending on pool_type parameter).
+            Defaults to -1.
+        pool_stride (int, optional): integer representing the stride across both dimensions.
+            Defaults to -1.
+        activation (bool, optional): enable or disable activation function.
+            Defaults to True.
+        output_bits (int, optional): output bitwidth. Defaults to 8.
+        buffer_bits (int, optional): buffer bitwidth. Defaults to 32.
+        name (str, optional): name of the layer. Defaults to empty string.
+
+    """
+
+    def __init__(self,
+                 input_shape,
+                 filters,
+                 kernel_size,
+                 padding=Padding.Same,
+                 padding_value=0,
+                 kernel_stride=1,
+                 pool_type=PoolType.NoPooling,
+                 pool_size=-1,
+                 pool_stride=-1,
+                 activation=True,
+                 output_bits=8,
+                 buffer_bits=32,
+                 name=""):
+        try:
+            params = LayerParams(
+                LayerType.InputConv2D, {
+                    "input_x": input_shape[0],
+                    "input_y": input_shape[1],
+                    "input_channels": input_shape[2],
+                    "filters": filters,
+                    "kernel_size": kernel_size,
+                    "padding": padding,
+                    "padding_value": padding_value,
+                    "kernel_stride": kernel_stride,
+                    "pool_type": pool_type,
+                    "pool_size": pool_size,
+                    "pool_stride": pool_stride,
+                    "activation": activation,
+                    "output_bits": output_bits,
+                    "buffer_bits": buffer_bits
+                })
+            # Call parent constructor to initialize C++ bindings
+            # Note that we invoke directly __init__ instead of using super, as
+            # specified in pybind documentation
+            Layer.__init__(self, params, name)
+        except BaseException:
+            self = None
+            raise
```

## akida/layers/input_convolutional.py

 * *Ordering differences only*

```diff
@@ -1,95 +1,95 @@
-from akida.core import (Layer, Padding, PoolType, LayerParams, LayerType)
-
-
-class InputConvolutional(Layer):
-    """The ``InputConvolutional`` layer is an image-specific input layer.
-
-    The InputConvolutional layer accepts images in 8-bit pixels, either
-    grayscale or RGB.
-    It is the only akida layer with 8-bit weights.
-    It applies a 'convolution' (actually a cross-correlation) optionally
-    followed by a pooling operation to the input images.
-    It can optionally apply a step-wise ReLU activation to its outputs.
-    The layer expects a 4D tensor whose first dimension is the sample index
-    representing the 8-bit images as input.
-    It returns a 4D tensor whose first dimension is the sample index and the
-    last dimension is the number of convolution filters.
-    The order of the input spatial dimensions is preserved, but their value may
-    change according to the convolution and pooling parameters.
-
-    Args:
-        input_shape (tuple): the 3D input shape.
-        kernel_size (list): list of 2 integer representing the spatial
-            dimensions of the convolutional kernel.
-        filters (int): number of filters.
-        name (str, optional): name of the layer. Defaults to empty string.
-        padding (:obj:`Padding`, optional): type of convolution. Defaults to
-            Padding.Same.
-        kernel_stride (tuple, optional): tuple of integer representing the
-            convolution stride (X, Y). Defaults to (1, 1).
-        weights_bits (int, optional): number of bits used to quantize weights.
-          Defaults to 1.
-        pool_size (list, optional): list of 2 integers, representing the window
-            size over which to take the maximum or the average (depending on
-            pool_type parameter). Defaults to (-1, -1).
-        pool_type (:obj:`PoolType`, optional): pooling type
-            (NoPooling, Max or Average). Defaults to PoolType.NoPooling.
-        pool_stride (list, optional): list of 2 integers representing
-            the stride dimensions. Defaults to (-1, -1)
-        activation (bool, optional): enable or disable activation
-            function. Defaults to True.
-        act_bits (int, optional): number of bits used to quantize
-            the neuron response. Defaults to 1.
-        padding_value (int, optional): value used when padding. Defaults to 0.
-
-    """
-
-    def __init__(self,
-                 input_shape,
-                 kernel_size,
-                 filters,
-                 name="",
-                 padding=Padding.Same,
-                 kernel_stride=(1, 1),
-                 weights_bits=1,
-                 pool_size=(-1, -1),
-                 pool_type=PoolType.NoPooling,
-                 pool_stride=(-1, -1),
-                 activation=True,
-                 act_bits=1,
-                 padding_value=0):
-        try:
-            pooling_stride_x = pool_stride[0]
-            if pool_stride[0] < 0:
-                pooling_stride_x = pool_size[0]
-            pooling_stride_y = pool_stride[1]
-            if pool_stride[1] < 0:
-                pooling_stride_y = pool_size[1]
-            params = LayerParams(
-                LayerType.InputConvolutional, {
-                    "input_width": input_shape[0],
-                    "input_height": input_shape[1],
-                    "input_channels": input_shape[2],
-                    "kernel_width": kernel_size[0],
-                    "kernel_height": kernel_size[1],
-                    "padding": padding,
-                    "filters": filters,
-                    "stride_x": kernel_stride[0],
-                    "stride_y": kernel_stride[1],
-                    "weights_bits": weights_bits,
-                    "pooling_width": pool_size[0],
-                    "pooling_height": pool_size[1],
-                    "pool_type": pool_type,
-                    "pooling_stride_x": pooling_stride_x,
-                    "pooling_stride_y": pooling_stride_y,
-                    "activation": activation,
-                    "act_bits": act_bits,
-                    "padding_value": padding_value
-                })
-            # Call parent constructor to initialize C++ bindings
-            # Note that we invoke directly __init__ instead of using super, as
-            # specified in pybind documentation
-            Layer.__init__(self, params, name)
-        except BaseException:
-            self = None
-            raise
+from akida.core import (Layer, Padding, PoolType, LayerParams, LayerType)
+
+
+class InputConvolutional(Layer):
+    """The ``InputConvolutional`` layer is an image-specific input layer.
+
+    The InputConvolutional layer accepts images in 8-bit pixels, either
+    grayscale or RGB.
+    It is the only akida layer with 8-bit weights.
+    It applies a 'convolution' (actually a cross-correlation) optionally
+    followed by a pooling operation to the input images.
+    It can optionally apply a step-wise ReLU activation to its outputs.
+    The layer expects a 4D tensor whose first dimension is the sample index
+    representing the 8-bit images as input.
+    It returns a 4D tensor whose first dimension is the sample index and the
+    last dimension is the number of convolution filters.
+    The order of the input spatial dimensions is preserved, but their value may
+    change according to the convolution and pooling parameters.
+
+    Args:
+        input_shape (tuple): the 3D input shape.
+        kernel_size (list): list of 2 integer representing the spatial
+            dimensions of the convolutional kernel.
+        filters (int): number of filters.
+        name (str, optional): name of the layer. Defaults to empty string.
+        padding (:obj:`Padding`, optional): type of convolution. Defaults to
+            Padding.Same.
+        kernel_stride (tuple, optional): tuple of integer representing the
+            convolution stride (X, Y). Defaults to (1, 1).
+        weights_bits (int, optional): number of bits used to quantize weights.
+          Defaults to 1.
+        pool_size (list, optional): list of 2 integers, representing the window
+            size over which to take the maximum or the average (depending on
+            pool_type parameter). Defaults to (-1, -1).
+        pool_type (:obj:`PoolType`, optional): pooling type
+            (NoPooling, Max or Average). Defaults to PoolType.NoPooling.
+        pool_stride (list, optional): list of 2 integers representing
+            the stride dimensions. Defaults to (-1, -1)
+        activation (bool, optional): enable or disable activation
+            function. Defaults to True.
+        act_bits (int, optional): number of bits used to quantize
+            the neuron response. Defaults to 1.
+        padding_value (int, optional): value used when padding. Defaults to 0.
+
+    """
+
+    def __init__(self,
+                 input_shape,
+                 kernel_size,
+                 filters,
+                 name="",
+                 padding=Padding.Same,
+                 kernel_stride=(1, 1),
+                 weights_bits=1,
+                 pool_size=(-1, -1),
+                 pool_type=PoolType.NoPooling,
+                 pool_stride=(-1, -1),
+                 activation=True,
+                 act_bits=1,
+                 padding_value=0):
+        try:
+            pooling_stride_x = pool_stride[0]
+            if pool_stride[0] < 0:
+                pooling_stride_x = pool_size[0]
+            pooling_stride_y = pool_stride[1]
+            if pool_stride[1] < 0:
+                pooling_stride_y = pool_size[1]
+            params = LayerParams(
+                LayerType.InputConvolutional, {
+                    "input_width": input_shape[0],
+                    "input_height": input_shape[1],
+                    "input_channels": input_shape[2],
+                    "kernel_width": kernel_size[0],
+                    "kernel_height": kernel_size[1],
+                    "padding": padding,
+                    "filters": filters,
+                    "stride_x": kernel_stride[0],
+                    "stride_y": kernel_stride[1],
+                    "weights_bits": weights_bits,
+                    "pooling_width": pool_size[0],
+                    "pooling_height": pool_size[1],
+                    "pool_type": pool_type,
+                    "pooling_stride_x": pooling_stride_x,
+                    "pooling_stride_y": pooling_stride_y,
+                    "activation": activation,
+                    "act_bits": act_bits,
+                    "padding_value": padding_value
+                })
+            # Call parent constructor to initialize C++ bindings
+            # Note that we invoke directly __init__ instead of using super, as
+            # specified in pybind documentation
+            Layer.__init__(self, params, name)
+        except BaseException:
+            self = None
+            raise
```

## akida/layers/input_data.py

 * *Ordering differences only*

```diff
@@ -1,40 +1,40 @@
-from akida.core import (Layer, LayerParams, LayerType)
-
-
-class InputData(Layer):
-    """This layer is used to specify the input dimensions of a low bitwidth Model.
-
-    Models accepting 8-bit images must start with an InputConvolutional layer,
-    but layers accepting integer inputs with a lower bitwidth (i.e. not images)
-    and layers accepting signed inputs must start instead with an InputData
-    layer.
-    This layer does not modify its inputs: it just allows to define the Model
-    input dimensions and bitwidth.
-
-    Args:
-        input_shape (tuple): the 3D input shape.
-        input_bits (int, optional): input bitwidth. Defaults to 4.
-        input_signed (bool, optional): whether the input is signed or not.
-            Defaults to False.
-        name (str, optional): name of the layer. Defaults to empty string.
-
-    """
-
-    def __init__(self, input_shape, input_bits=4, input_signed=False, name=""):
-        try:
-            params = LayerParams(
-                LayerType.InputData, {
-                    "input_width": input_shape[0],
-                    "input_height": input_shape[1],
-                    "input_channels": input_shape[2],
-                    "input_signed": 1 if input_signed else 0,
-                    "input_bits": input_bits
-                })
-
-            # Call parent constructor to initialize C++ bindings
-            # Note that we invoke directly __init__ instead of using super, as
-            # specified in pybind documentation
-            Layer.__init__(self, params, name)
-        except BaseException:
-            self = None
-            raise
+from akida.core import (Layer, LayerParams, LayerType)
+
+
+class InputData(Layer):
+    """This layer is used to specify the input dimensions of a low bitwidth Model.
+
+    Models accepting 8-bit images must start with an InputConvolutional layer,
+    but layers accepting integer inputs with a lower bitwidth (i.e. not images)
+    and layers accepting signed inputs must start instead with an InputData
+    layer.
+    This layer does not modify its inputs: it just allows to define the Model
+    input dimensions and bitwidth.
+
+    Args:
+        input_shape (tuple): the 3D input shape.
+        input_bits (int, optional): input bitwidth. Defaults to 4.
+        input_signed (bool, optional): whether the input is signed or not.
+            Defaults to False.
+        name (str, optional): name of the layer. Defaults to empty string.
+
+    """
+
+    def __init__(self, input_shape, input_bits=4, input_signed=False, name=""):
+        try:
+            params = LayerParams(
+                LayerType.InputData, {
+                    "input_width": input_shape[0],
+                    "input_height": input_shape[1],
+                    "input_channels": input_shape[2],
+                    "input_signed": 1 if input_signed else 0,
+                    "input_bits": input_bits
+                })
+
+            # Call parent constructor to initialize C++ bindings
+            # Note that we invoke directly __init__ instead of using super, as
+            # specified in pybind documentation
+            Layer.__init__(self, params, name)
+        except BaseException:
+            self = None
+            raise
```

## akida/layers/madnorm.py

 * *Ordering differences only*

```diff
@@ -1,38 +1,38 @@
-from akida.core import (Layer, LayerParams, LayerType)
-
-
-class MadNorm(Layer):
-    """A function similar to the MAD normalization layer presented
-    in quantizeml. (Note that the normalization is only available over
-    the last dimension)
-
-    Instead of using the standard deviation (std) during the normalization
-    division, the sum of absolute values is used.
-    The normalization is performed in this way:
-
-        MadNorm(x) = x * gamma / sum(abs(x)) + beta
-
-    Args:
-        output_bits (int, optional): output bitwidth. Defaults to 8
-        buffer_bits (int, optional): buffer bitwidth. Defaults to 32.
-        name (str, optional): name of the layer. Defaults to empty string.
-
-    """
-
-    def __init__(self,
-                 output_bits=8,
-                 buffer_bits=32,
-                 name=""):
-        try:
-            params = LayerParams(
-                LayerType.MadNorm, {
-                    "output_bits": output_bits,
-                    "buffer_bits": buffer_bits,
-                })
-            # Call parent constructor to initialize C++ bindings
-            # Note that we invoke directly __init__ instead of using super, as
-            # specified in pybind documentation
-            Layer.__init__(self, params, name)
-        except BaseException:
-            self = None
-            raise
+from akida.core import (Layer, LayerParams, LayerType)
+
+
+class MadNorm(Layer):
+    """A function similar to the MAD normalization layer presented
+    in quantizeml. (Note that the normalization is only available over
+    the last dimension)
+
+    Instead of using the standard deviation (std) during the normalization
+    division, the sum of absolute values is used.
+    The normalization is performed in this way:
+
+        MadNorm(x) = x * gamma / sum(abs(x)) + beta
+
+    Args:
+        output_bits (int, optional): output bitwidth. Defaults to 8
+        buffer_bits (int, optional): buffer bitwidth. Defaults to 32.
+        name (str, optional): name of the layer. Defaults to empty string.
+
+    """
+
+    def __init__(self,
+                 output_bits=8,
+                 buffer_bits=32,
+                 name=""):
+        try:
+            params = LayerParams(
+                LayerType.MadNorm, {
+                    "output_bits": output_bits,
+                    "buffer_bits": buffer_bits,
+                })
+            # Call parent constructor to initialize C++ bindings
+            # Note that we invoke directly __init__ instead of using super, as
+            # specified in pybind documentation
+            Layer.__init__(self, params, name)
+        except BaseException:
+            self = None
+            raise
```

## akida/layers/separable_convolutional.py

 * *Ordering differences only*

```diff
@@ -1,90 +1,90 @@
-from akida.core import (Layer, Padding, PoolType, LayerType, LayerParams)
-
-
-class SeparableConvolutional(Layer):
-    """This represents a separable convolution layer.
-
-    This layer accepts 1-bit, 2-bit or 4-bit 3D input tensors with an arbitrary
-    number of channels.
-    It can be configured with 1-bit, 2-bit or 4-bit weights.
-    Separable convolutions consist in first performing a depthwise spatial
-    convolution (which acts on each input channel separately) followed by a
-    pointwise convolution which mixes together the resulting output channels.
-    Note: this layer applies a real convolution, and not a cross-correlation.
-    It can optionally apply a step-wise ReLU activation to its outputs.
-    The layer expects a 4D tensor whose first dimension is the sample index
-    as input.
-    It returns a 4D tensor whose first dimension is the sample index and the
-    last dimension is the number of convolution filters.
-    The order of the input spatial dimensions is preserved, but their value may
-    change according to the convolution and pooling parameters.
-
-    Args:
-        kernel_size (list): list of 2 integer representing the spatial
-            dimensions of the convolutional kernel.
-        filters (int): number of pointwise filters.
-        name (str, optional): name of the layer. Defaults to empty string.
-        padding (:obj:`Padding`, optional): type of convolution. Defaults to
-            Padding.Same.
-        kernel_stride (list, optional): list of 2 integer representing the
-            convolution stride (X, Y). Defaults to (1, 1).
-        weights_bits (int, optional): number of bits used to quantize weights.
-            Defaults to 2.
-        pool_size (list, optional): list of 2 integers, representing the window
-            size over which to take the maximum or the average (depending on
-            pool_type parameter). Defaults to (-1, -1).
-        pool_type (:obj:`PoolType`, optional): pooling type (NoPooling, Max or
-            Average). Defaults to PoolType.NoPooling.
-        pool_stride (list, optional): list of 2 integers representing
-            the stride dimensions. Defaults to (-1, -1).
-        activation (bool, optional): enable or disable activation
-            function. Defaults to True.
-        act_bits (int, optional): number of bits used to quantize
-            the neuron response. Defaults to 1.
-
-    """
-
-    def __init__(self,
-                 kernel_size,
-                 filters,
-                 name="",
-                 padding=Padding.Same,
-                 kernel_stride=(1, 1),
-                 weights_bits=2,
-                 pool_size=(-1, -1),
-                 pool_type=PoolType.NoPooling,
-                 pool_stride=(-1, -1),
-                 activation=True,
-                 act_bits=1):
-        try:
-            pooling_stride_x = pool_stride[0]
-            if pool_stride[0] < 0:
-                pooling_stride_x = pool_size[0]
-            pooling_stride_y = pool_stride[1]
-            if pool_stride[1] < 0:
-                pooling_stride_y = pool_size[1]
-            params = LayerParams(
-                LayerType.SeparableConvolutional, {
-                    "kernel_width": kernel_size[0],
-                    "kernel_height": kernel_size[1],
-                    "padding": padding,
-                    "filters": filters,
-                    "stride_x": kernel_stride[0],
-                    "stride_y": kernel_stride[1],
-                    "weights_bits": weights_bits,
-                    "pooling_width": pool_size[0],
-                    "pooling_height": pool_size[1],
-                    "pool_type": pool_type,
-                    "pooling_stride_x": pooling_stride_x,
-                    "pooling_stride_y": pooling_stride_y,
-                    "activation": activation,
-                    "act_bits": act_bits
-                })
-
-            # Call parent constructor to initialize C++ bindings
-            # Note that we invoke directly __init__ instead of using super, as
-            # specified in pybind documentation
-            Layer.__init__(self, params, name)
-        except BaseException:
-            self = None
-            raise
+from akida.core import (Layer, Padding, PoolType, LayerType, LayerParams)
+
+
+class SeparableConvolutional(Layer):
+    """This represents a separable convolution layer.
+
+    This layer accepts 1-bit, 2-bit or 4-bit 3D input tensors with an arbitrary
+    number of channels.
+    It can be configured with 1-bit, 2-bit or 4-bit weights.
+    Separable convolutions consist in first performing a depthwise spatial
+    convolution (which acts on each input channel separately) followed by a
+    pointwise convolution which mixes together the resulting output channels.
+    Note: this layer applies a real convolution, and not a cross-correlation.
+    It can optionally apply a step-wise ReLU activation to its outputs.
+    The layer expects a 4D tensor whose first dimension is the sample index
+    as input.
+    It returns a 4D tensor whose first dimension is the sample index and the
+    last dimension is the number of convolution filters.
+    The order of the input spatial dimensions is preserved, but their value may
+    change according to the convolution and pooling parameters.
+
+    Args:
+        kernel_size (list): list of 2 integer representing the spatial
+            dimensions of the convolutional kernel.
+        filters (int): number of pointwise filters.
+        name (str, optional): name of the layer. Defaults to empty string.
+        padding (:obj:`Padding`, optional): type of convolution. Defaults to
+            Padding.Same.
+        kernel_stride (list, optional): list of 2 integer representing the
+            convolution stride (X, Y). Defaults to (1, 1).
+        weights_bits (int, optional): number of bits used to quantize weights.
+            Defaults to 2.
+        pool_size (list, optional): list of 2 integers, representing the window
+            size over which to take the maximum or the average (depending on
+            pool_type parameter). Defaults to (-1, -1).
+        pool_type (:obj:`PoolType`, optional): pooling type (NoPooling, Max or
+            Average). Defaults to PoolType.NoPooling.
+        pool_stride (list, optional): list of 2 integers representing
+            the stride dimensions. Defaults to (-1, -1).
+        activation (bool, optional): enable or disable activation
+            function. Defaults to True.
+        act_bits (int, optional): number of bits used to quantize
+            the neuron response. Defaults to 1.
+
+    """
+
+    def __init__(self,
+                 kernel_size,
+                 filters,
+                 name="",
+                 padding=Padding.Same,
+                 kernel_stride=(1, 1),
+                 weights_bits=2,
+                 pool_size=(-1, -1),
+                 pool_type=PoolType.NoPooling,
+                 pool_stride=(-1, -1),
+                 activation=True,
+                 act_bits=1):
+        try:
+            pooling_stride_x = pool_stride[0]
+            if pool_stride[0] < 0:
+                pooling_stride_x = pool_size[0]
+            pooling_stride_y = pool_stride[1]
+            if pool_stride[1] < 0:
+                pooling_stride_y = pool_size[1]
+            params = LayerParams(
+                LayerType.SeparableConvolutional, {
+                    "kernel_width": kernel_size[0],
+                    "kernel_height": kernel_size[1],
+                    "padding": padding,
+                    "filters": filters,
+                    "stride_x": kernel_stride[0],
+                    "stride_y": kernel_stride[1],
+                    "weights_bits": weights_bits,
+                    "pooling_width": pool_size[0],
+                    "pooling_height": pool_size[1],
+                    "pool_type": pool_type,
+                    "pooling_stride_x": pooling_stride_x,
+                    "pooling_stride_y": pooling_stride_y,
+                    "activation": activation,
+                    "act_bits": act_bits
+                })
+
+            # Call parent constructor to initialize C++ bindings
+            # Note that we invoke directly __init__ instead of using super, as
+            # specified in pybind documentation
+            Layer.__init__(self, params, name)
+        except BaseException:
+            self = None
+            raise
```

## akida/layers/shiftmax.py

 * *Ordering differences only*

```diff
@@ -1,40 +1,40 @@
-from akida.core import (Layer, LayerParams, LayerType)
-
-
-class Shiftmax(Layer):
-    """A function similar to the softmax.
-
-    Instead of using e as base, it uses 2 and a shift. So we replace
-
-    .. math::
-        softmax(x_i) = \\frac{e^{x_i}}{sum(e^{x_k})}
-
-    with
-
-    .. math::
-        shiftmax(x_i) = \\frac{2^{x_i}}{round(log2(sum(2^{x_k})))}
-
-    This is evaluated with a shift.
-
-    Args:
-        output_bits (int, optional): output bitwidth. Defaults to 8.
-        buffer_bits (int, optional): internal bitwidth. Defaults to 23.
-    """
-
-    def __init__(self,
-                 output_bits=8,
-                 buffer_bits=23,
-                 name=""):
-        try:
-            params = LayerParams(
-                LayerType.Shiftmax, {
-                    "output_bits": output_bits,
-                    "buffer_bits": buffer_bits
-                })
-            # Call parent constructor to initialize C++ bindings
-            # Note that we invoke directly __init__ instead of using super, as
-            # specified in pybind documentation
-            Layer.__init__(self, params, name)
-        except BaseException:
-            self = None
-            raise
+from akida.core import (Layer, LayerParams, LayerType)
+
+
+class Shiftmax(Layer):
+    """A function similar to the softmax.
+
+    Instead of using e as base, it uses 2 and a shift. So we replace
+
+    .. math::
+        softmax(x_i) = \\frac{e^{x_i}}{sum(e^{x_k})}
+
+    with
+
+    .. math::
+        shiftmax(x_i) = \\frac{2^{x_i}}{round(log2(sum(2^{x_k})))}
+
+    This is evaluated with a shift.
+
+    Args:
+        output_bits (int, optional): output bitwidth. Defaults to 8.
+        buffer_bits (int, optional): internal bitwidth. Defaults to 23.
+    """
+
+    def __init__(self,
+                 output_bits=8,
+                 buffer_bits=23,
+                 name=""):
+        try:
+            params = LayerParams(
+                LayerType.Shiftmax, {
+                    "output_bits": output_bits,
+                    "buffer_bits": buffer_bits
+                })
+            # Call parent constructor to initialize C++ bindings
+            # Note that we invoke directly __init__ instead of using super, as
+            # specified in pybind documentation
+            Layer.__init__(self, params, name)
+        except BaseException:
+            self = None
+            raise
```

## akida/layers/stem.py

 * *Ordering differences only*

```diff
@@ -1,74 +1,74 @@
-from akida.core import (Layer, LayerParams, LayerType)
-
-
-class Stem(Layer):
-    """Stem layer corresponding to the Stem block of Transformer models.
-
-    It's composed of the following layers:
-
-        - The Embedding layer
-        - The Reshape layer
-        - The ClassToken (+ DistToken for distilled model) layer(s)
-        - The AddPosEmbedding layer
-
-    This layer covers all the above layers operations.
-
-    Note that final output values will be saturated on the range that can
-    be represented with output_bits.
-
-    Note: For now only the convolution operation of the Embedding layer
-    is covered and the final output quantization.
-
-    Args:
-        input_shape (tuple): the spatially square 3D input shape.
-        filters (int, optional): Positive integer, dimensionality of the output space.
-            Defaults to 192.
-        kernel_size (int, optional): kernel size. Defaults to 16.
-        output_bits (int, optional): output bitwidth. Defaults to 8.
-        buffer_bits (int, optional): buffer bitwidth. Defaults to 31.
-        collapse_spatial_dims (bool, optional): boolean to trigger the output spatial
-            dimensions collapse. Defaults to True.
-        num_non_patch_tokens (int, optional): number of non patch tokens to concatenate
-            with the input along it last axis. Defaults to 0.
-        add_pos_embs_available (bool, optional): boolean to trigger the positional
-            embedding matrix addition. Defaults to False.
-        name (str, optional): name of the layer. Defaults to empty string.
-
-    """
-
-    def __init__(self,
-                 input_shape,
-                 filters=192,
-                 kernel_size=16,
-                 output_bits=8,
-                 buffer_bits=31,
-                 collapse_spatial_dims=True,
-                 num_non_patch_tokens=0,
-                 add_pos_embs_available=False,
-                 name=""):
-        try:
-            if (input_shape[0] != input_shape[1]):
-                raise ValueError(
-                    "input should have square spatial dimensions."
-                    f"Receives x_size={input_shape[0]} and y_size={input_shape[1]}"
-                )
-
-            params = LayerParams(
-                LayerType.Stem, {
-                    "input_spatial_size": input_shape[0],
-                    "input_channels": input_shape[2],
-                    "filters": filters,
-                    "kernel_size": kernel_size,
-                    "output_bits": output_bits,
-                    "buffer_bits": buffer_bits,
-                    "collapse_spatial_dims": collapse_spatial_dims,
-                    "num_non_patch_tokens": num_non_patch_tokens,
-                    "add_pos_embs_available": add_pos_embs_available
-                })
-            # Call parent constructor to initialize C++ bindings
-            # Note that we invoke directly __init__ instead of using super, as
-            # specified in pybind documentation
-            Layer.__init__(self, params, name)
-        except BaseException:
-            self = None
-            raise
+from akida.core import (Layer, LayerParams, LayerType)
+
+
+class Stem(Layer):
+    """Stem layer corresponding to the Stem block of Transformer models.
+
+    It's composed of the following layers:
+
+        - The Embedding layer
+        - The Reshape layer
+        - The ClassToken (+ DistToken for distilled model) layer(s)
+        - The AddPosEmbedding layer
+
+    This layer covers all the above layers operations.
+
+    Note that final output values will be saturated on the range that can
+    be represented with output_bits.
+
+    Note: For now only the convolution operation of the Embedding layer
+    is covered and the final output quantization.
+
+    Args:
+        input_shape (tuple): the spatially square 3D input shape.
+        filters (int, optional): Positive integer, dimensionality of the output space.
+            Defaults to 192.
+        kernel_size (int, optional): kernel size. Defaults to 16.
+        output_bits (int, optional): output bitwidth. Defaults to 8.
+        buffer_bits (int, optional): buffer bitwidth. Defaults to 31.
+        collapse_spatial_dims (bool, optional): boolean to trigger the output spatial
+            dimensions collapse. Defaults to True.
+        num_non_patch_tokens (int, optional): number of non patch tokens to concatenate
+            with the input along it last axis. Defaults to 0.
+        add_pos_embs_available (bool, optional): boolean to trigger the positional
+            embedding matrix addition. Defaults to False.
+        name (str, optional): name of the layer. Defaults to empty string.
+
+    """
+
+    def __init__(self,
+                 input_shape,
+                 filters=192,
+                 kernel_size=16,
+                 output_bits=8,
+                 buffer_bits=31,
+                 collapse_spatial_dims=True,
+                 num_non_patch_tokens=0,
+                 add_pos_embs_available=False,
+                 name=""):
+        try:
+            if (input_shape[0] != input_shape[1]):
+                raise ValueError(
+                    "input should have square spatial dimensions."
+                    f"Receives x_size={input_shape[0]} and y_size={input_shape[1]}"
+                )
+
+            params = LayerParams(
+                LayerType.Stem, {
+                    "input_spatial_size": input_shape[0],
+                    "input_channels": input_shape[2],
+                    "filters": filters,
+                    "kernel_size": kernel_size,
+                    "output_bits": output_bits,
+                    "buffer_bits": buffer_bits,
+                    "collapse_spatial_dims": collapse_spatial_dims,
+                    "num_non_patch_tokens": num_non_patch_tokens,
+                    "add_pos_embs_available": add_pos_embs_available
+                })
+            # Call parent constructor to initialize C++ bindings
+            # Note that we invoke directly __init__ instead of using super, as
+            # specified in pybind documentation
+            Layer.__init__(self, params, name)
+        except BaseException:
+            self = None
+            raise
```

## Comparing `akida-2.3.3.dist-info/LICENSE` & `akida-2.3.4.dist-info/LICENSE`

 * *Ordering differences only*

 * *Files 6% similar despite different names*

```diff
@@ -1,41 +1,41 @@
-BRAINCHIP SOFTWARE END USER LICENSE AGREEMENT
-
-PLEASE READ THIS END USER LICENSE AGREEMENT (LICENSE) CAREFULLY BEFORE DOWNLOADING OR USING THE SOFTWARE. BY DOWNLOADING OR USING THE SOFTWARE, YOU ARE AGREEING TO BE BOUND BY THE TERMS OF THIS LICENSE. IF YOU DO NOT AGREE TO THE TERMS OF THIS LICENSE, PROMPTLY DELETE THE UNUSED SOFTWARE FROM YOUR COMPUTER OR DEVICE.
-
-1. License Granted.
-The application, demonstration, and other software accompanying this License, whether on disk, in read-only memory, or on any other media ("BRAINCHIPs Software") and the related documentation are licensed to you by BRAINCHIP, INC. ("BRAINCHIP") for your personal internal use only (with no rights to resell, transfer, assign, or sublicense), and you shall not use BRAINCHIPs Software to process or permit it be used to process data for any third party, or to be used in the operation of a service bureau. BRAINCHIP is the licensor under this License Agreement. You own, rent, or use free-of-charges the medium on which BRAINCHIP's Software is recorded, but BRAINCHIP and/or BRAINCHIP's Licensor(s) retain title to BRAINCHIP's Software and related documentation. This License allows you to use BRAINCHIP's Software on a single hardware support and only make one copy of BRAINCHIP's Software in machine-readable form for backup purposes only. You must reproduce on such a copy BRAINCHIP's copyright notice and any other proprietary legends that were on the original copy of BRAINCHIP's Software. 
-
-2. Restrictions.
-BRAINCHIP's Software contains copyrighted material, trade secrets, and other proprietary material. In order to protect that proprietary material, you may not decompile, reverse engineer, disassemble, or otherwise reduce BRAINCHIP's Software to a human-perceivable form; copy, modify, network, rent, lease, loan, or distribute BRAINCHIP's Software; or create derivative works based upon BRAINCHIP's Software in whole or in part other than as expressly permitted in writing by BRAINCHIP. You agree that BRAINCHIP shall own all derivative works and improvements to BRAINCHIPs Software. You shall not use BRAINCHIPs Software to program any device other than BRAINCHIP devices. You may not publish or disclose the results of any benchmarking or testing of BRAINCHIPs Software without the prior written permission of BRAINCHIP. You may not sublicense, assign, or transfer BRAINCHIPs Software.
-
-3. Termination.
-This License is effective until terminated. You may terminate this License at any time by destroying BRAINCHIP's Software and related documentation, and all copies thereof. This License will terminate immediately without notice from BRAINCHIP if you fail to comply with any provision of this License. Upon termination you must destroy BRAINCHIP's Software and related documentation and all copies thereof. Upon termination you shall remain subject to the provisions, restrictions, and exclusions in this License Agreement and shall have no right to any refund of any amounts paid for BRAINCHIP's Software. No termination shall release you from liability for any breach of this License Agreement.
-
-4. Export Law Assurances.
-You agree and certify that neither BRAINCHIP's Software, nor any other technical data received from BRAINCHIP, nor the direct product thereof, will be shipped, transferred, or exported, directly or indirectly, to any country in violation of any applicable law. You agree to comply with all restrictions and export controls imposed by the laws and regulations of the United States and any other applicable jurisdiction.
-
-5. Disclaimer of Warranty on BRAINCHIP's Software.
-You expressly acknowledge and agree that the use of BRAINCHIP's Software is at your sole risk. BRAINCHIP's Software and related documentation are provided "AS IS" and without warranty of any kind.
-BRAINCHIP AND BRAINCHIP'S LICENSOR(S) DISCLAIM ALL WARRANTIES, CONDITIONS, AND REPRESENTATIONS (EXPRESS OR IMPLIED, ORAL OR WRITTEN) WITH RESPECT TO BRAINCHIPS SOFTWARE AND RELATED DOCUMENTATION, INCLUDING, BUT NOT LIMITED TO, IMPLIED WARRANTIES AND/OR CONDITIONS OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE OR NON-INFRINGEMENT OR NON-INTERFERENCE. BRAINCHIP DISCLAIMS ALL WARRANTIES THAT FUNCTIONS CONTAINED IN BRAINCHIP'S SOFTWARE WILL MEET YOUR REQUIREMENTS, OR THAT THE OPERATION OF BRAINCHIP'S SOFTWARE WILL BE UNINTERRUPTED OR ERROR-FREE, OR THAT DEFECTS IN BRAINCHIP'S SOFTWARE WILL BE CORRECTED, OR THAT BRAINCHIPS SOFTWARE WILL NOT BE VULNERABLE TO INTRUSION OR ATTACK. FURTHERMORE, BRAINCHIP DOES NOT WARRANT OR MAKE ANY REPRESENTATIONS REGARDING THE USE OR THE RESULTS OF THE USE OF BRAINCHIP'S SOFTWARE OR RELATED DOCUMENTATION IN TERMS OF THEIR CORRECTNESS, ACCURACY, RELIABILITY, OR OTHERWISE. NO COURSE OF DEALING OR ORAL OR WRITTEN INFORMATION OR ADVICE GIVEN BY BRAINCHIP OR A BRAINCHIP-AUTHORIZED REPRESENTATIVE SHALL CREATE A WARRANTY OR REPRESENTATION. SHOULD BRAINCHIP SOFTWARE PROVE DEFECTIVE, YOU (AND NOT BRAINCHIP OR A BRAINCHIP-AUTHORIZED REPRESENTATIVE) SHALL ASSUME THE ENTIRE COST OF ALL NECESSARY SERVICING, REPAIRS, OR CORRECTIONS.
-EXCEPT AS EXPRESSLY PROVIDED TO THE CONTRARY IN THIS LICENSE, ALL CONDITIONS, WARRANTIES, AND REPRESENTATIONS EXPRESSED OR IMPLIED BY STATUTE, LAW, OR OTHERWISE IN RELATION TO BRAINCHIP'S SOFTWARE ARE HEREBY DISCLAIMED AND EXCLUDED, AND BRAINCHIP SHALL BE UNDER NO LIABILITY FOR ANY LOSS, DAMAGE, OR INJURY, DIRECT OR INDIRECT, RESULTING FROM DEFECTIVE MATERIAL, FAULTY WORKMANSHIP, OR OTHERWISE.
-BRAINCHIP MAKES NO WARRANTIES, REPRESENTATIONS, OR GUARANTEES REGARDING USAGE OF BRAINCHIPS SOFTWARE IN COMPLIANCE WITH GDPR REGULATIONS. YOU SHOULD CONSULT WITH YOUR LEGAL COUNSEL TO ENSURE YOUR USE OF BRAINCHIP'S SOFTWARE IS IN COMPLIANCE WITH GDPR REGULATIONS OR ANY OTHER LOCAL LAWS AND REGULATIONS.
-THE TERMS OF THIS DISCLAIMER DO NOT AFFECT OR PREJUDICE THE STATUTORY RIGHTS OF A CONSUMER ACQUIRING BRAINCHIP'S SOFTWARE NOT IN THE COURSE OF BUSINESS, NOR DO THEY LIMIT OR EXCLUDE ANY LIABILITY FOR DEATH OR PERSONNAL INJURY CAUSED BY BRAINCHIP'S NEGLIGENCE.
-
-6. Limitation of Liability.
-UNDER NO CIRCUMSTANCES SHALL BRAINCHIP BE LIABLE FOR ANY INCIDENTAL, SPECIAL,
- INDIRECT, EXEMPLARY, OR CONSEQUENTIAL DAMAGES, LOST PROFITS,  OR DAMAGES DUE TO BUSINESS INTERRUPTION, DISCLOSURE OF PERSONALLY IDENTIFYING INFORMATION, CORRUPTION OF DATA, OR LOSS OF INFORMATION, ARISING OUT OF OR RELATING TO THE USE OR INABILITY TO USE BRAINCHIP'S SOFTWARE OR RELATED DOCUMENTATION, BASED ON ANY THEORY OF LIABILITY, INCLUDING, WITHOUT LIMITATION, CONTRACT, TORT, NEGLIGENCE, STRICT LIABILITY, VIOLATION OF PUBLIC POLICY, OR EQUITY, AND REGARDLESS OF WHETHER BRAINCHIP OR A BRAINCHIP-AUTHORIZED REPRESENTATIVE WAS ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.
- IN NO EVENT SHALL BRAINCHIP'S TOTAL LIABILITY TO YOU FOR ALL DAMAGES, LIABILITIES, LOSSES, EXPENSES, CLAIMS, AND CAUSES OF ACTION BASED ON ANY THEORY OF LIABILITY, INCLUDING, WITHOUT LIMITATION, CONTRACT, TORT, NEGLIGENCE, STRICT LIABILITY, VIOLATION OF PUBLIC POLICY, OR EQUITY, EXCEED THE AMOUNT PAID BY YOU FOR BRAINCHIP'S SOFTWARE.
-
-7. Controlling Law and Severability.
-This License shall be governed by and construed in accordance with the laws of the State of California, U.S.A., applicable to contracts made in and fully performed in the State of California, U.S.A., without giving effect to conflict of law principles that would cause the application of laws of any other jurisdiction. The United Nations Convention on Contracts for the International Sales of Goods shall not apply to this License. If for any reason a court of competent jurisdiction finds any provision of this License, or portion thereof, to be unenforceable, that provision of the License shall be enforced so as to give effect to the original intent of the parties as closely as possible to the fullest extent permitted by applicable law, and all other provisions of this License shall continue in full force and effect.
-
-8. Entire Agreement.
-This License constitutes and contains the entire agreement and understanding between you and BRAINCHIP concerning the subject matter hereof.  This License supersedes and replaces all prior negotiations, representations, promises, understandings, proposals, and agreements, whether written or oral, between you and BRAINCHIP concerning the subject matters addressed herein, and it is a fully integrated document. No amendment to or modification of this License will be binding unless in writing and signed by a duly authorized representative of BRAINCHIP.
-
-9. Acknowledgment.
-You acknowledge that you have read and understand this License and agree to be bound by these terms and conditions.
-
-Should you have any questions concerning this License, please contact:
-Brainchip, Inc., 23041 Avenida De La Carlota, Suite 250 Laguna Hills CA 92653 (United States) or write to sales@brainchipinc.com
-
-Copyright 2022, BrainChip Holdings Ltd. All rights reserved.
+BRAINCHIP SOFTWARE END USER LICENSE AGREEMENT
+
+PLEASE READ THIS END USER LICENSE AGREEMENT (LICENSE) CAREFULLY BEFORE DOWNLOADING OR USING THE SOFTWARE. BY DOWNLOADING OR USING THE SOFTWARE, YOU ARE AGREEING TO BE BOUND BY THE TERMS OF THIS LICENSE. IF YOU DO NOT AGREE TO THE TERMS OF THIS LICENSE, PROMPTLY DELETE THE UNUSED SOFTWARE FROM YOUR COMPUTER OR DEVICE.
+
+1. License Granted.
+The application, demonstration, and other software accompanying this License, whether on disk, in read-only memory, or on any other media ("BRAINCHIPs Software") and the related documentation are licensed to you by BRAINCHIP, INC. ("BRAINCHIP") for your personal internal use only (with no rights to resell, transfer, assign, or sublicense), and you shall not use BRAINCHIPs Software to process or permit it be used to process data for any third party, or to be used in the operation of a service bureau. BRAINCHIP is the licensor under this License Agreement. You own, rent, or use free-of-charges the medium on which BRAINCHIP's Software is recorded, but BRAINCHIP and/or BRAINCHIP's Licensor(s) retain title to BRAINCHIP's Software and related documentation. This License allows you to use BRAINCHIP's Software on a single hardware support and only make one copy of BRAINCHIP's Software in machine-readable form for backup purposes only. You must reproduce on such a copy BRAINCHIP's copyright notice and any other proprietary legends that were on the original copy of BRAINCHIP's Software. 
+
+2. Restrictions.
+BRAINCHIP's Software contains copyrighted material, trade secrets, and other proprietary material. In order to protect that proprietary material, you may not decompile, reverse engineer, disassemble, or otherwise reduce BRAINCHIP's Software to a human-perceivable form; copy, modify, network, rent, lease, loan, or distribute BRAINCHIP's Software; or create derivative works based upon BRAINCHIP's Software in whole or in part other than as expressly permitted in writing by BRAINCHIP. You agree that BRAINCHIP shall own all derivative works and improvements to BRAINCHIPs Software. You shall not use BRAINCHIPs Software to program any device other than BRAINCHIP devices. You may not publish or disclose the results of any benchmarking or testing of BRAINCHIPs Software without the prior written permission of BRAINCHIP. You may not sublicense, assign, or transfer BRAINCHIPs Software.
+
+3. Termination.
+This License is effective until terminated. You may terminate this License at any time by destroying BRAINCHIP's Software and related documentation, and all copies thereof. This License will terminate immediately without notice from BRAINCHIP if you fail to comply with any provision of this License. Upon termination you must destroy BRAINCHIP's Software and related documentation and all copies thereof. Upon termination you shall remain subject to the provisions, restrictions, and exclusions in this License Agreement and shall have no right to any refund of any amounts paid for BRAINCHIP's Software. No termination shall release you from liability for any breach of this License Agreement.
+
+4. Export Law Assurances.
+You agree and certify that neither BRAINCHIP's Software, nor any other technical data received from BRAINCHIP, nor the direct product thereof, will be shipped, transferred, or exported, directly or indirectly, to any country in violation of any applicable law. You agree to comply with all restrictions and export controls imposed by the laws and regulations of the United States and any other applicable jurisdiction.
+
+5. Disclaimer of Warranty on BRAINCHIP's Software.
+You expressly acknowledge and agree that the use of BRAINCHIP's Software is at your sole risk. BRAINCHIP's Software and related documentation are provided "AS IS" and without warranty of any kind.
+BRAINCHIP AND BRAINCHIP'S LICENSOR(S) DISCLAIM ALL WARRANTIES, CONDITIONS, AND REPRESENTATIONS (EXPRESS OR IMPLIED, ORAL OR WRITTEN) WITH RESPECT TO BRAINCHIPS SOFTWARE AND RELATED DOCUMENTATION, INCLUDING, BUT NOT LIMITED TO, IMPLIED WARRANTIES AND/OR CONDITIONS OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE OR NON-INFRINGEMENT OR NON-INTERFERENCE. BRAINCHIP DISCLAIMS ALL WARRANTIES THAT FUNCTIONS CONTAINED IN BRAINCHIP'S SOFTWARE WILL MEET YOUR REQUIREMENTS, OR THAT THE OPERATION OF BRAINCHIP'S SOFTWARE WILL BE UNINTERRUPTED OR ERROR-FREE, OR THAT DEFECTS IN BRAINCHIP'S SOFTWARE WILL BE CORRECTED, OR THAT BRAINCHIPS SOFTWARE WILL NOT BE VULNERABLE TO INTRUSION OR ATTACK. FURTHERMORE, BRAINCHIP DOES NOT WARRANT OR MAKE ANY REPRESENTATIONS REGARDING THE USE OR THE RESULTS OF THE USE OF BRAINCHIP'S SOFTWARE OR RELATED DOCUMENTATION IN TERMS OF THEIR CORRECTNESS, ACCURACY, RELIABILITY, OR OTHERWISE. NO COURSE OF DEALING OR ORAL OR WRITTEN INFORMATION OR ADVICE GIVEN BY BRAINCHIP OR A BRAINCHIP-AUTHORIZED REPRESENTATIVE SHALL CREATE A WARRANTY OR REPRESENTATION. SHOULD BRAINCHIP SOFTWARE PROVE DEFECTIVE, YOU (AND NOT BRAINCHIP OR A BRAINCHIP-AUTHORIZED REPRESENTATIVE) SHALL ASSUME THE ENTIRE COST OF ALL NECESSARY SERVICING, REPAIRS, OR CORRECTIONS.
+EXCEPT AS EXPRESSLY PROVIDED TO THE CONTRARY IN THIS LICENSE, ALL CONDITIONS, WARRANTIES, AND REPRESENTATIONS EXPRESSED OR IMPLIED BY STATUTE, LAW, OR OTHERWISE IN RELATION TO BRAINCHIP'S SOFTWARE ARE HEREBY DISCLAIMED AND EXCLUDED, AND BRAINCHIP SHALL BE UNDER NO LIABILITY FOR ANY LOSS, DAMAGE, OR INJURY, DIRECT OR INDIRECT, RESULTING FROM DEFECTIVE MATERIAL, FAULTY WORKMANSHIP, OR OTHERWISE.
+BRAINCHIP MAKES NO WARRANTIES, REPRESENTATIONS, OR GUARANTEES REGARDING USAGE OF BRAINCHIPS SOFTWARE IN COMPLIANCE WITH GDPR REGULATIONS. YOU SHOULD CONSULT WITH YOUR LEGAL COUNSEL TO ENSURE YOUR USE OF BRAINCHIP'S SOFTWARE IS IN COMPLIANCE WITH GDPR REGULATIONS OR ANY OTHER LOCAL LAWS AND REGULATIONS.
+THE TERMS OF THIS DISCLAIMER DO NOT AFFECT OR PREJUDICE THE STATUTORY RIGHTS OF A CONSUMER ACQUIRING BRAINCHIP'S SOFTWARE NOT IN THE COURSE OF BUSINESS, NOR DO THEY LIMIT OR EXCLUDE ANY LIABILITY FOR DEATH OR PERSONNAL INJURY CAUSED BY BRAINCHIP'S NEGLIGENCE.
+
+6. Limitation of Liability.
+UNDER NO CIRCUMSTANCES SHALL BRAINCHIP BE LIABLE FOR ANY INCIDENTAL, SPECIAL,
+ INDIRECT, EXEMPLARY, OR CONSEQUENTIAL DAMAGES, LOST PROFITS,  OR DAMAGES DUE TO BUSINESS INTERRUPTION, DISCLOSURE OF PERSONALLY IDENTIFYING INFORMATION, CORRUPTION OF DATA, OR LOSS OF INFORMATION, ARISING OUT OF OR RELATING TO THE USE OR INABILITY TO USE BRAINCHIP'S SOFTWARE OR RELATED DOCUMENTATION, BASED ON ANY THEORY OF LIABILITY, INCLUDING, WITHOUT LIMITATION, CONTRACT, TORT, NEGLIGENCE, STRICT LIABILITY, VIOLATION OF PUBLIC POLICY, OR EQUITY, AND REGARDLESS OF WHETHER BRAINCHIP OR A BRAINCHIP-AUTHORIZED REPRESENTATIVE WAS ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.
+ IN NO EVENT SHALL BRAINCHIP'S TOTAL LIABILITY TO YOU FOR ALL DAMAGES, LIABILITIES, LOSSES, EXPENSES, CLAIMS, AND CAUSES OF ACTION BASED ON ANY THEORY OF LIABILITY, INCLUDING, WITHOUT LIMITATION, CONTRACT, TORT, NEGLIGENCE, STRICT LIABILITY, VIOLATION OF PUBLIC POLICY, OR EQUITY, EXCEED THE AMOUNT PAID BY YOU FOR BRAINCHIP'S SOFTWARE.
+
+7. Controlling Law and Severability.
+This License shall be governed by and construed in accordance with the laws of the State of California, U.S.A., applicable to contracts made in and fully performed in the State of California, U.S.A., without giving effect to conflict of law principles that would cause the application of laws of any other jurisdiction. The United Nations Convention on Contracts for the International Sales of Goods shall not apply to this License. If for any reason a court of competent jurisdiction finds any provision of this License, or portion thereof, to be unenforceable, that provision of the License shall be enforced so as to give effect to the original intent of the parties as closely as possible to the fullest extent permitted by applicable law, and all other provisions of this License shall continue in full force and effect.
+
+8. Entire Agreement.
+This License constitutes and contains the entire agreement and understanding between you and BRAINCHIP concerning the subject matter hereof.  This License supersedes and replaces all prior negotiations, representations, promises, understandings, proposals, and agreements, whether written or oral, between you and BRAINCHIP concerning the subject matters addressed herein, and it is a fully integrated document. No amendment to or modification of this License will be binding unless in writing and signed by a duly authorized representative of BRAINCHIP.
+
+9. Acknowledgment.
+You acknowledge that you have read and understand this License and agree to be bound by these terms and conditions.
+
+Should you have any questions concerning this License, please contact:
+Brainchip, Inc., 23041 Avenida De La Carlota, Suite 250 Laguna Hills CA 92653 (United States) or write to sales@brainchipinc.com
+
+Copyright 2022, BrainChip Holdings Ltd. All rights reserved.
```

## Comparing `akida-2.3.3.dist-info/METADATA` & `akida-2.3.4.dist-info/METADATA`

 * *Files 23% similar despite different names*

```diff
@@ -1,23 +1,20 @@
 Metadata-Version: 2.1
 Name: akida
-Version: 2.3.3
+Version: 2.3.4
 Summary: Akida Execution Engine
 Home-page: https://doc.brainchipinc.com
 Author: David Corvoysier
 Author-email: dcorvoysier@brainchip.com
 License: Proprietary
-Platform: UNKNOWN
 Requires-Python: >=3.7
 Description-Content-Type: text/markdown
 License-File: LICENSE
 License-File: LICENSE.3rdparty
 Requires-Dist: numpy
 
 # Akida Execution Engine
 
 The Akida Execution Engine is an interface to the Brainchip Akida Neural
 Processor.
 To allow the development of Akida models without an actual Akida hardware, it
 includes a software backend that simulates the Akida Neural Processor.
-
-
```

## Comparing `akida-2.3.3.dist-info/RECORD` & `akida-2.3.4.dist-info/RECORD`

 * *Files 16% similar despite different names*

```diff
@@ -1,160 +1,160 @@
-akida/__init__.py,sha256=l_aUSeQQsSf6JO3MwLQzDKnE7qsdqlI-BVfXm0XQ71g,1389
-akida/akida.dll,sha256=xgOaAHHD4Gpx4SBP-E-rmK-GYoJ6j_nV9Rx1JGvxIgM,1232896
-akida/akida.lib,sha256=EdfjrSDINuMf9MQ6K6JNF_l4FSgnqxesoK8oqnsEn2o,35182
-akida/cli.py,sha256=p5KbKrFGDf16Xd8SKw6Vx7VxpM2wfSwkOIiEL75enRA,3022
-akida/core.pyd,sha256=xv6jMWu-WScNDNJtRCFow4DuFjYBjTJ9yOJAHCyprZo,1174016
-akida/layer.py,sha256=yBTG_xR5Zu1dFpq92jLLQrOddQ7ky99TSj5hM9Ol3Pk,3971
-akida/libomp.dll,sha256=y8gGtWueNAlxtOyd9HFhXFQPfZJy6GTDt5FPB5GPufc,701440
-akida/model.py,sha256=JO8Asuh6HELwpa6s-YGAMXvfJ6JNdfTQTyh8TFm8MZI,13080
-akida/np.py,sha256=c2VMhw9W3sT98H8opLK8hB7-rXTONWBu5K6sWZ8LBqg,634
-akida/sequence.py,sha256=dv5yNjfgUERQmQ9aw6n9V32kP5f3Ql0krmWViQ-G1SA,474
-akida/sparsity.py,sha256=ev2lW2NgNPSGkR976QVJeZ2qp5LnmZ4120pYXnKGN3g,961
-akida/statistics.py,sha256=z9i4jXTEW2fllyDrpaW3wAKm073in79oP9YnHSEzrH4,4196
-akida/virtual_devices.py,sha256=4RfjAyc-KNjmWShOyYIm5eMksenlI1PfMWtmgvSJesg,6506
-akida/api/akida/backend_type.h,sha256=1mDIFrwxCQGA-O0JJ6OybXlgFB0rzgkfWWHsGh11yqo,439
-akida/api/akida/backends.h,sha256=8PWW6UEa4R-sA8EEQq50LXEmEX0JuHvswtW7xUQhprQ,645
-akida/api/akida/device.h,sha256=8QVzYcZuesrw-WL7osP1JSbyjTpzr-pqz0hBP2dZ4sA,1888
-akida/api/akida/layer.h,sha256=s0M2O80FIts3nRvs046gQTW2y01G6OjxtrTuewD_5HA,3637
-akida/api/akida/layer_params.h,sha256=HKJdCumByAVshEOkLovPFz513S83ZOKzcVX5FXAR_oU,3448
-akida/api/akida/learning_params.h,sha256=wEeT1tiKTuPpjKXHf9vmDr-O4jYyW9rYbwqG0-BS-sY,2218
-akida/api/akida/mesh.h,sha256=XWiIla10FQoMVFx-_yklt4ul8vX1sBUjPF4SgfzHE_A,349
-akida/api/akida/mesh_mapper.h,sha256=nz0CyXGvg6qgo6F2eFpKbqcVhwcCzDhHRONWqE4ouA4,1281
-akida/api/akida/model.h,sha256=moixUFvh2Zm9W97MgApfwCdCXXbl_5xZIjEgKEd0e6c,7396
-akida/api/akida/np_mapping.h,sha256=ifNAlcQqpGsSt9LsXGfzQQFXgR4uTfVeasRaOkQJP-g,1821
-akida/api/akida/sequence.h,sha256=7dnxBNlE1jNP04pfXxxo2YqMUN31IX15-4ivlbAHbdA,2503
-akida/api/akida/variables.h,sha256=cijkdmZkXC9XvoJKO0CC-Vpsu5Kp86OQgisiU4vHpsQ,1379
-akida/api/host/circular_queue.h,sha256=_k8TYpnf8Dwaqvu-Uqp8_mToih-fAGMCoSmDug5jtsU,3168
-akida/api/host/hardware_devices.h,sha256=mTRQwuGJtqeC7Jlbt8GFwapzc0EpjRmyoWC3lM8_Ihw,1109
-akida/api/host/hardware_drivers.h,sha256=8eZ32SLL-1MDSNNeq-dl-wtnfnUKz0uj0MkgY92SIe8,345
-akida/api/host/host_device.h,sha256=46cuagOrKhC4wTIdB6D6P1K1O0OaR9rm7oFQCu_OTmw,876
-akida/api/host/power_meter.h,sha256=XgioJygVFlgh4X81GhpVDBSilOMcQMfHhS-eoUkkmTc,1234
-akida/api/host/soc_clock_mode.h,sha256=pjNe8cEPVVOuoy6zsECVQgFCOBn9t_SU85SEMbkumLc,542
-akida/api/host/soc_driver.h,sha256=0WZo_wjGPsXmBZxaRpcj3djv4n06F4JWx3wfXve5R3s,600
-akida/api/infra/exports.h,sha256=B932QdzrQ5bqs9x-WMDp78eiVMem2caljT8zQiavsag,231
-akida/api/infra/hardware_driver.h,sha256=ZUGP-FqiGGRfyQfdbC-v7v23tdSvt31Vik4rBuOd_Og,2139
-akida/api/infra/int_ops.h,sha256=uI1TPDQ2YsaRQ1Xsl065yCyBFEOt-JhWGPcW6xxjqF4,610
-akida/api/infra/registers_common.h,sha256=69-9LcCVEOXQbCfeqZUBL0d_UaOIULj2knHYy5Y5E8E,1040
-akida/api/infra/system.h,sha256=FQkdOgNNJNnWiLp5L8Dp-EeZNv_UoFoYjw7S5VIWfhE,823
-akida/compatibility/__init__.py,sha256=aWiCOXlWfReDC9ZpX6wK_Za592gdMn69IwyVqdbs1KU,27
-akida/compatibility/conversion.py,sha256=QYCLK5vNBAF7LteMcQtY-0rtfnFcc6jzrVWPUX7bu0g,5989
-akida/deploy/__init__.py,sha256=1NmDKiQttIK_rBDG84OqLlDWhGknrGiAwFCd_xDS4Y0,798
-akida/deploy/engine.py,sha256=6LLvwzUhI-eVQKABUWcPnswq9uzUC330xASQ61zZtr8,1651
-akida/engine/CMakeLists.txt,sha256=3o4GU_jEOJXc3TiaF7_UZxXcZEigGo0anjX7U6OHVVA,139
-akida/engine/README.md,sha256=jNxaco_5qB6E8SowLG9N4OaOUpOCJzmVY4xSuqQ4PG8,12902
-akida/engine/api/akd1000/bare_metal_driver.h,sha256=F_8Y0CRm5d7Xykqy_05fVJMudWE28z2SNIGnMgWaeRk,1502
-akida/engine/api/akd1000/memory_mapping.h,sha256=xJOz--ftFWbM6Yz93lCf4bGMvhqJc4YZqJWN44mw0pc,835
-akida/engine/api/akd1000/registers_soc.h,sha256=cIyjpyCoUGazBzIOiWT7iKrKTpTlU4NxLbzjAOFK_fc,2604
-akida/engine/api/akida/dense.h,sha256=_sRK1H7T-8locHtlHy6_D9Bhub7ACBH-0njXG0iKsjk,6812
-akida/engine/api/akida/hardware_device.h,sha256=ASZ_snEGK6jvDLWK_zb6txkEDgKF-PysVecds8u9YZM,8697
-akida/engine/api/akida/hw_version.h,sha256=w5tDtzwSm-052yHdWCbMHxvCH0eXSUmNCV01-m7ThPs,1083
-akida/engine/api/akida/input_conversion.h,sha256=TUlIjtIDh68PISXv_rAd7QynVVdCjP166uitptHAR70,464
-akida/engine/api/akida/np.h,sha256=ZIu7feaGHFeIOM-1OiSqyIsudgtylwr4KXhnhVwLeDc,1125
-akida/engine/api/akida/program_memory_info.h,sha256=PINNoJ0KL3l6cVZRu_wiM1B_0j6NojTfLgt6dYqy9aM,1620
-akida/engine/api/akida/registers_top_level.h,sha256=bODgYkJ4Jb194YHIt5yxawWtVuLW55PnIkoaDdtc1pk,2960
-akida/engine/api/akida/shape.h,sha256=broRN1xjRmwSO8_vc3gHLHCNQ0z5Yau-lxIYHjAlcH0,4078
-akida/engine/api/akida/sparse.h,sha256=haZMsAZNx5ggwsuUDGGm3kiuORoEDRzJAs6qHN94ALg,2630
-akida/engine/api/akida/tensor.h,sha256=22ZvhvDJXKKAM88-hdKVZZlz62Qm2lMhMGU0a_urrVM,6493
-akida/engine/api/akida/version.h,sha256=phBc3l75-B18m8VealTl8E9ItOezEpeP54YwpEDxLDw,138
-akida/engine/api/infra/exports.h,sha256=B932QdzrQ5bqs9x-WMDp78eiVMem2caljT8zQiavsag,231
-akida/engine/api/infra/hardware_driver.h,sha256=ZUGP-FqiGGRfyQfdbC-v7v23tdSvt31Vik4rBuOd_Og,2139
-akida/engine/api/infra/int_ops.h,sha256=uI1TPDQ2YsaRQ1Xsl065yCyBFEOt-JhWGPcW6xxjqF4,610
-akida/engine/api/infra/registers_common.h,sha256=69-9LcCVEOXQbCfeqZUBL0d_UaOIULj2knHYy5Y5E8E,1040
-akida/engine/api/infra/system.h,sha256=FQkdOgNNJNnWiLp5L8Dp-EeZNv_UoFoYjw7S5VIWfhE,823
-akida/engine/cmake/akida-engine.cmake,sha256=RknqtojJG_SgjI_NYMgiHoSCJKNH7l1tRSvzEXhhXiI,965
-akida/engine/devices/akd1000/bare_metal_driver.cpp,sha256=YLtlUPhnTX9wyqM4D2pE9z3K68z7h5NfV5VGOb8jzUE,1303
-akida/engine/inc/engine/akida_device_program_fb_generated.h,sha256=HoNgt9jZJCmG9sxrW5fJ6qjcP5XlOh8wv7gw-aH20T8,32276
-akida/engine/inc/engine/dma.h,sha256=QGMRc6m1RjFFSN8hDs4Vs0fTWiKoBzuOdhhGd2qF3aU,994
-akida/engine/inc/engine/dma_config_ops.h,sha256=rGdkfrsUuSUly2tkaKaBrCFsrc9I1r0oFxDA2JI8_4A,981
-akida/engine/inc/engine/int_conversion.h,sha256=ecigdTX6khJ9X5VrpqjKMBnnaI4q3UF46Tpxe5vLoLU,1898
-akida/engine/src/dense.cpp,sha256=yWkygiuDOrEr2HhF9I9YfBbATeGQgiEAlDLdpY78EwM,8961
-akida/engine/src/device_memory.h,sha256=IElsCvMHlDzT1FXN_4zJQtafi3k5CCFPPQq3cpIJ7q0,196
-akida/engine/src/dma_cnp_events.h,sha256=2LOJXJdHO_WzvwNKodrVND1BEG_tiGywQs3RxncFk2o,2965
-akida/engine/src/dma_config_format.h,sha256=_BvNQhcOxZ72SMtGUKY6v6ZuSXZlA4B0HVV5aBGQOyQ,1116
-akida/engine/src/dma_config_ops.cpp,sha256=pa53bJqVzC2deqW6sSDWbeqy5h5Hvy8X-iyUcrRFBVY,2303
-akida/engine/src/dma_desc_format.h,sha256=qk3A8gXhpG4sIzF3MMQBY3qm4y_Wy81sBwwOcSGCXEQ,4399
-akida/engine/src/dma_desc_ops.cpp,sha256=e_mISB9wbFnAEsfBvDzxPBLxpRpbAE_OM-i_MdtxYMM,4474
-akida/engine/src/dma_desc_ops.h,sha256=wPnXL7ZcYLXv7er3LIr-NZ_mzNDBYShABd2YTCwEN5s,1202
-akida/engine/src/dma_engine.cpp,sha256=14LKfpBE-lyCATlDD1OkXKI-MjYOksmz_CpSLsvHEd4,16815
-akida/engine/src/dma_engine.h,sha256=92V7sBBI_AbwHycYP6czm1chmaolA5mVqRe8c5veeRg,447
-akida/engine/src/dma_engine_ops.h,sha256=jS4s1pn4vLigkwSyoZNCKS-ET21eVthueLS2_GuH1Ms,5769
-akida/engine/src/dma_events.h,sha256=IbjnOuvrNXk-GCVH47jrNV4AKjARVAGoWovpX8uvh64,1396
-akida/engine/src/dma_events_format.h,sha256=wWNHgIE5kPojqQ5Z4pE0dnsh6fmBFEeKSR-J8LIjB9w,786
-akida/engine/src/dma_events_ops.cpp,sha256=Kk5Z7W4HhWkV7J-rITgemnYh0vG8hJvurW1918yJVUE,14845
-akida/engine/src/dma_events_ops.h,sha256=XxtL9_Gg5ty2JzSnxrGad_NbK1iLcnrqH4W8TlUsMrk,1285
-akida/engine/src/dma_fnp_events.h,sha256=XecGXVkPcvSlEFj1eE1jk84us2THlm2ppUGmnPXbNEA,2705
-akida/engine/src/dma_hrc_events.h,sha256=XWJZ9Gib5pfY2plMlnQ8RUp5RInAvU6oX2DJGT3yU7Y,3189
-akida/engine/src/dma_image_ops.cpp,sha256=3tBt_CKjg9ZtH3U3si84z2Avf-4MBGe_xa7VK6aVbnc,2071
-akida/engine/src/dma_image_ops.h,sha256=b2o0OPv2bNfZe4V01uViugbdC7Gu0P0D9FtdGqq74Mk,471
-akida/engine/src/external_mem_mgr.cpp,sha256=EFppUl8XNtH1K8oQ2tx_Fa4qmpipM30hPVhwy8Q0KWQ,1752
-akida/engine/src/external_mem_mgr.h,sha256=3yVgvcTCqMubrUOIeGTRpFuzGK6DQi1DTAnP15gMABk,1173
-akida/engine/src/fnp2_mem_conf_reg.h,sha256=OeBhGiHBgz0ul2SsQP1J5EUfkiTwYjaNCeHC5DSJgXo,620
-akida/engine/src/hardware_device.cpp,sha256=C2L4AgfUdQOD43cS01SRu8ljPiRtY44jO_MCNYjqNRM,267
-akida/engine/src/hardware_device_impl.cpp,sha256=fHa2IlXOYOmXOnF_k9wVwFvhMPP7A6yMQhjZy2SHaYc,31796
-akida/engine/src/hardware_device_impl.h,sha256=lvUsJn0feFtcxrEMTzsGAWDjOy7V7ceab5rJle8xbNw,4050
-akida/engine/src/hw_version.cpp,sha256=llEu_ZYb7WcsGI1CbKUxXsCRikjiY6reYIajy_LnKak,1099
-akida/engine/src/input_conversion.cpp,sha256=s0d4wfMRn6cpyhScwVtXl5o6ii82DU0JuXxG0YCppaE,941
-akida/engine/src/memory_mgr.cpp,sha256=xpNQHKokFzbR7MEf9vIPGQvUCDTOaXnNYP0HyZHJu9Q,2658
-akida/engine/src/memory_mgr.h,sha256=AL7srNik4OLIxk3Hcs_HfZnieW3XRgYhKv2jntoJu40,1101
-akida/engine/src/memory_utils.cpp,sha256=0fzz8NKIihSPwlBSQisWXwdbQAYCjSJbDrxWuq4Zfow,699
-akida/engine/src/memory_utils.h,sha256=YWyt3hxkDwWlSxu8Z_v4vCv4WpjwZ2ex1tJnYhssjMs,405
-akida/engine/src/multipass_memory.cpp,sha256=2NC9eO2AtkEE6XgxJfciEaNzfCXZG7Q71zpJaTmrplg,1219
-akida/engine/src/multipass_memory.h,sha256=7GnjIwHoCpRr06sujMpKGbLMKuT1wwObCSsXg_6_4h0,803
-akida/engine/src/pipeline_state.h,sha256=Rg3g6Ib7oa4f1--BqZk7sIxFdpuxX-EZZ-qig0GhmSw,2198
-akida/engine/src/program_memory_info.cpp,sha256=Fc6SIBWBt4kNvHYw1s_X-iXETYmAPZsn6EuFobZm_uc,2990
-akida/engine/src/program_play.cpp,sha256=RUKYyNM2Q2suRwoAULDKhXq2l0_MZhMSkunKengrEuM,28397
-akida/engine/src/program_play.h,sha256=NWYt0WaScJKF8NogKqYkuvDeuyXoP_Xq8Ryzk1NAX84,2836
-akida/engine/src/registers_dma_engine.h,sha256=SdDenvds93ekwN08B-5N8BjIFpDCdhZmzRR1HIzkQi8,8060
-akida/engine/src/registers_reset.h,sha256=ZIWhik-nTW9df78z0wkFuf4arSyU9SDsZLUmKlaUtCk,824
-akida/engine/src/reset_nps.cpp,sha256=aD47q9_N8k6kghtLu5SuAfnFRZm7GecVGkMRXiAlNyY,1022
-akida/engine/src/reset_nps.h,sha256=nvJwnKkazZpQM5mnXU8y22rdaS4oLq0y09HvFKz4s-Y,300
-akida/engine/src/sparse.cpp,sha256=c_nYaee9mXOIL_ayFl2RlxOZkaQhVbIc1zGSREG_72o,646
-akida/engine/src/tensor.cpp,sha256=7Mo4xPYaNh_Ngxocr2lBoS6mxWyK1ypWL7m7Mud58Go,835
-akida/engine/src/version.cpp,sha256=Kt_373FbMkr2wRRTL2ATx3XY-Quknt3H68tYsmvCCb4,204
-akida/generate/__init__.py,sha256=NNv9T6CWT65PB9VYbdIhtyGzYcxWBSrLF3AKTSG-O8M,974
-akida/generate/application_generator.py,sha256=FPYpEXetKsBqY92fIfTxEqpwqz3KKmKgHTZ-YKLEuf4,2440
-akida/generate/array_to_cpp.py,sha256=07ZP633TwV5gmfe3yGzvS1_G1Kejl65YWFmUBW0z1qU,5482
-akida/generate/model.py,sha256=4PlPJPiGBd8TMOX52LXX2v5n6kZfDTMWU0vIka1UvN4,1080
-akida/generate/test/template.py,sha256=v37TotXGy0he7LzoPcS-OQlDfArHH34xXB40iJY_y_A,396
-akida/generate/test/test_tools.py,sha256=8Ugfzba5b9NGt73-kgav3W29AlMQt-jFCuFcqZZ3wgk,4459
-akida/generate/test/cmake/akida-model.cmake,sha256=GzXeb-cMFwdHXQRTKccwSw20l-2opkzKlU5GmE4nvnE,1199
-akida/generate/test/engine/test_generator.py,sha256=MgQuzGFxMqGh1qd0TqLCH91lJ921MFASxleHmZt6VYU,1350
-akida/generate/test/engine/app_templates/test.cpp,sha256=gzdfzkI872qPIpSTPnqLk1rpki5uIMKw5CY17KFuFW8,4458
-akida/generate/test/engine/app_templates/test.h,sha256=cqSElmea4FJ7pcMURzz2xhYG8ch8zgTkSIiMLQrMAUI,1360
-akida/generate/test/engine/fixtures/simple_conv_evaluate_v2.py,sha256=etv6IkwbTmQgqS14KUFn2ezzXafuacMJJp9hmJzMWoI,830
-akida/generate/test/engine/fixtures/simple_conv_v2.py,sha256=YKRcXG1BcJpSmbUFQ25W1QWnMA7A-pFT9Yd0EnvK2sw,829
-akida/generate/test/engine/fixtures/simple_hrc_v2.py,sha256=SqJg67UsRRQKzuhKtla4GglleNfP2yu5avyxlzT4Lz0,1702
-akida/generate/test/engine/fixtures/simple_sep_conv_v2.py,sha256=fFoLeh9puYgD80QovpiKtjrhYgI-Sk2HbuhN-BYjLwo,894
-akida/generate/test/engine/fixtures/test_fnp2.py,sha256=ucKAZGGtBZyel0a4-u08N93vj2nhVZO3Gklg_R11vq0,803
-akida/generate/test/engine/fixtures/test_multiple_inputs.py,sha256=4Q9SaWQCiVmNLpClRiJYaFetPjXpjBNEpLw9SokynGc,871
-akida/generate/test/model/test_generator.py,sha256=o8Nlnfo0KXlPd12UrMEwuK1Xtya4rgkEAflipyRd21c,1487
-akida/generate/test/model/app_templates/test.cpp,sha256=LlptqIujUYgV5FIfr4wUdMagphknQV8I5D3hR8JtKSA,1102
-akida/generate/test/model/app_templates/test.h,sha256=TEztiBd9kWMPXV8ZKfdVq6Lq_4S32_pkj2fa_w3XJEs,576
-akida/layers/__init__.py,sha256=JK8nv7CZYz1Jh5jxZPxrOLZf56avlMvIjPEHmATXVGA,835
-akida/layers/add.py,sha256=YY-upNQNf9wwZG70vvZd14ZYx_jTiye0sPCA44qMyhQ,1878
-akida/layers/attention.py,sha256=pI8Ca255-ZO9Kjt5tdVusZNE8qiW0CCW7HwBJ46t5Lk,2207
-akida/layers/batch_normalization.py,sha256=-hTPu8cLsCk2i3r933BxPygrXYPr-jh2ZLyk_pz3yhw,1272
-akida/layers/concatenate.py,sha256=FgQkEz3_G52TvqGhsRfMsHbKpqf7Yi26yEwdtT46Rp4,1059
-akida/layers/conv2d.py,sha256=dHyYQxaQnuV2C8nD41ytfGoIZDcUyEDRUpjTwDte97o,3800
-akida/layers/conv2d_transpose.py,sha256=o4_OAbYrUrinf2NPpt6yTqzwYi4subCM3jcuz3xbUDE,2687
-akida/layers/convolutional.py,sha256=BrDPPPibuezbYL4x3RQXFbnFvb05fcr-MblixIU3o3A,4072
-akida/layers/dense1d.py,sha256=qb0D6-M_CxPRNEAFkQsxI8UMnZmcoRuVcvAawlhz-WE,2245
-akida/layers/dense2d.py,sha256=U-gwwPiz4XinbCsOwoN421l0EnOcRlhQSb4akA145Nk,2317
-akida/layers/depthwise_conv2d.py,sha256=5ik5Hl1Y4k-tTm55DVaDU5PM8-nTYlFRt_8HUmO_gJo,3471
-akida/layers/depthwise_conv2d_transpose.py,sha256=Zz9tmG-9N5MwcRK9bxrAgpr3BsnMvYRPWHbrbWCMOfc,2729
-akida/layers/dequantizer.py,sha256=UKB45ssy0l3sMK2U9yGN0ozjC79z-DL-NbWKc_KgLFs,497
-akida/layers/extract_token.py,sha256=ZNXO2hP0BmkHFoNaV5rB4Sc7VroCE1y3neF8vm8H57k,1212
-akida/layers/fully_connected.py,sha256=l3j0D33DFGvpogm4Uvw2oO7qRVIo9R_6xfZKbjk0GBU,1837
-akida/layers/input_conv2d.py,sha256=133Ajmj4azTJkhf4PE3fQdNc2tweQ9KDxG41HX-AOws,4359
-akida/layers/input_convolutional.py,sha256=z1jlozVetstkb7LvJTKYMXfU3Tuz8tkuQ5XRAFBI1Lg,4490
-akida/layers/input_data.py,sha256=EwWL8l4TC-uG3W_BW6nAbaLWgTmcr-jQgk0M6Xu_-Yk,1643
-akida/layers/madnorm.py,sha256=6kQu2fRW6SaTg72XMRLxy_-Ba3Cphhx1MjHqvijOVao,1365
-akida/layers/separable_convolutional.py,sha256=aXwgG5kViwMeBzXhT8YuHs3y9E19QBdPKbmvORaHELk,4253
-akida/layers/shiftmax.py,sha256=wAYWSlwSdOyipPpl22ZeGGSEEO7TOs9rHH9ef7hOvko,1215
-akida/layers/stem.py,sha256=1nU0Z9J-10vHiCoL08ngBfIWKlMYyAHYfuhcXQNI9JA,3138
-akida-2.3.3.dist-info/LICENSE,sha256=46EA-Qy0_GQKQq5IvaDQ2gXySCYeFd9bO7BHU2bxL6U,8974
-akida-2.3.3.dist-info/LICENSE.3rdparty,sha256=1Uuvkfm1hDH88n7DKAILuZQ6KG7tPAHgU0i6C6jO6no,1980
-akida-2.3.3.dist-info/METADATA,sha256=yRcrDUyN1nPMfVOxeSXgg53iG-4vCmyOs2uQJ4g0x0A,624
-akida-2.3.3.dist-info/WHEEL,sha256=fVcVlLzi8CGi_Ul8vjMdn8gER25dn5GBg9E6k9z41-Y,100
-akida-2.3.3.dist-info/entry_points.txt,sha256=_KRNGDUM4kZBDxEZCcVDUBkCsz8me4G4gBjWGzc4CTA,41
-akida-2.3.3.dist-info/top_level.txt,sha256=RKaGSiprCDSQ8PCjbVbKc4BOOChQcEoNOvqlv8iyY0A,6
-akida-2.3.3.dist-info/RECORD,,
+akida/__init__.py,sha256=oq8YQF4O0qPpJkd7-g7ZiRoHaRjjKIIISP5jhlTHG_A,1368
+akida/cli.py,sha256=3kBkqsDdeuJF2PebLzj57I3a_KrAAXAteOejiLfZSFY,2944
+akida/core.so,sha256=mc31eChAfrf40HVihH5ShO8QvfZcw_kfXdM3UVXuC8I,679536
+akida/layer.py,sha256=raO4vwNLxwvcfG5oX1py5uQrWq7TMZqRvM6yK7qJjlI,3930
+akida/libakida.so.2,sha256=MrhqL5HHjyOQaGMcOVTi10ZxkOEiTSMaOjQDehSLYes,1822120
+akida/model.py,sha256=VV3yzAwOJDyWn7hrVduzzwf3ecb6U5eVt_vr9K5N3BQ,13165
+akida/np.py,sha256=InchiJMNPWqMmCQ7ldlHtMt8XLkmG8vnCqvRlDM-pJQ,613
+akida/sequence.py,sha256=oJVGGVvZdjzfXGAl6Ow3fSbMjOTN1UcfmP1lkn6RAEo,457
+akida/sparsity.py,sha256=9rxAq5WsTl5XHfIv22CTS-oDARaBZk9x21aOwgub6f4,932
+akida/statistics.py,sha256=20TZHOTJL_n0RiyqaOfbmAWG4UQK6QyH3rhX_NMVhrE,4082
+akida/virtual_devices.py,sha256=IvQSFEHmBEEkuh2gAg9lfo0J_16DFF3SoZ0BQ57e5YY,6385
+akida/api/akida/backend_type.h,sha256=D6NeOIeBN5JebJvIW15YfNJIjh-fvu2v-Vh6N2HoMKo,417
+akida/api/akida/backends.h,sha256=sXTmXbQiUlTe0e_q_sh0BOmMhuoyLOHipUGRC6EaJfg,619
+akida/api/akida/device.h,sha256=XNCp6lqi_XUXLr53I7xlqB8FuEpPADlj6HmHWvAq2zs,1815
+akida/api/akida/layer.h,sha256=V6HMHrLm0hlZA47gkN0DqqBjVUYjNj9fK87ypqcCzM4,3494
+akida/api/akida/layer_params.h,sha256=mlFjSc_UwdyRv8s5SVW79DK4VZhIktb_zuS9QGDtMVw,3304
+akida/api/akida/learning_params.h,sha256=2XjnPfjdLDHJOvr7KJc9Uddv4okhi90uWQ4hsBe3954,2133
+akida/api/akida/mesh.h,sha256=FC4KcJs2hxMLS_7S1AT4GivQ93pS5gSgSBkYk9TWmSo,328
+akida/api/akida/mesh_mapper.h,sha256=TWy2wuZfu9Mb8CG8tR_ckHOW2RSS7k4TpGlcDHYrRhI,1227
+akida/api/akida/model.h,sha256=BEMZdRBrP11rSwbyRyQkJE85E3MXY3jzm2E_lpufMKQ,7170
+akida/api/akida/np_mapping.h,sha256=hWMmFgJZ-2JViQ2Gkvdlv4cPlbVKFOgDE433vKy3LUw,1757
+akida/api/akida/sequence.h,sha256=nmI89hMyZLZIBRNmIxLmwyTkMAKHEfFPLOfTEAnb8MQ,2416
+akida/api/akida/variable_helpers.h,sha256=C5jy5gCKIZ21ocPtDF9DEXYPeXCXkJi_IJiSEErMn2g,148
+akida/api/akida/variables.h,sha256=Q8VEY4egvWcWsebTv-HLZ2dgtUy8ie2r9M6cXGTdv5A,1324
+akida/api/host/circular_queue.h,sha256=QYKhv2dsMMaPMZhrJAHKy0EKDjCUPjID6wN3YIit1LQ,3035
+akida/api/host/hardware_devices.h,sha256=8hjq7Jp3uHVRSUdmDWtssEYkRe4_T0i_zXV6N57ujZ0,1076
+akida/api/host/hardware_drivers.h,sha256=fpGM9-Pvjb4_5a4JDqWFG15HFdu4I85b7oTvKigKtPg,328
+akida/api/host/host_device.h,sha256=o0b6V6_JVshb2iXL7P7TITbI3oKlB3Xbzq4trx7F8yw,841
+akida/api/host/power_meter.h,sha256=maQxAMOkcNJl9L0v1TLeAEF9I5UKyuMPOcsKwXBIP74,1186
+akida/api/host/soc_clock_mode.h,sha256=lUH_StGUOblioR8bU8HbdjE5UbPp7C-XhBpabtyBP74,521
+akida/api/host/soc_driver.h,sha256=jjXgBsCouGbAhbnCr7hADJz6xhsPcLCi1Mngmvr5qNY,578
+akida/api/infra/exports.h,sha256=mGtp3lcHBRjXeNsOkLakfOReXp16viGAA6TtQ0UHkyY,222
+akida/api/infra/hardware_driver.h,sha256=aINm-NxTFJsmjwyRYD2jpBSEoHQgK7GQqwAy0yFdF_o,2056
+akida/api/infra/int_ops.h,sha256=dhIKFtmx-OJSd15SZXnu5PUu6hsiRVf2oKSvE-oHuyo,584
+akida/api/infra/registers_common.h,sha256=s7NqiwKljhnTM_z2XzPlhIPzXtlwr46RlTWGRwPaxoI,1002
+akida/api/infra/system.h,sha256=OuKxxTlsBgjUM6hWJqPOu9gu6LOintnh6UBBToQR1_w,788
+akida/compatibility/__init__.py,sha256=Yq64tBf9ijDS0qlSfYjbhiUGclOPhR6km5qopBErTPM,26
+akida/compatibility/conversion.py,sha256=ZdG5_O2WBzAhPWjRq_CWpZG18f6bbXO-VbJ68VZPGr4,5827
+akida/deploy/__init__.py,sha256=Cx6aN0i2mOcftjzyxCjyz0Wl7gRzvznTRHp-jvhE1j4,782
+akida/deploy/engine.py,sha256=z2l7eWA_D1EMofIjgz1ybSgsw2PPa3SHrblbVDEioWs,1611
+akida/engine/CMakeLists.txt,sha256=fK0XaHcCp_1hsxZbLiYR0NaAuj2fUDEttVo6-Ic18rw,132
+akida/engine/README.md,sha256=zbfj7F8-Zk2Fh8twdGPp5YY9rK91pYaPPegPfIimmys,12524
+akida/engine/api/akd1000/bare_metal_driver.h,sha256=Da3jcsdIkWC_UNVD8BnCt9hs4UA55xt_JyJzKLSsMds,1447
+akida/engine/api/akd1000/memory_mapping.h,sha256=VKsdhrZh9i4OR3PSmUzPxze7hxtvvC6OeNXUGtrCbho,808
+akida/engine/api/akd1000/registers_soc.h,sha256=xzgEnXQMsTsHo3o1KT2RW4eW3dsMv918HUn48437NF0,2547
+akida/engine/api/akida/dense.h,sha256=15ONCdIXFYkD9J5ERrF9M3QWKK7xuLBJuEKtYCQnlIY,6605
+akida/engine/api/akida/hardware_device.h,sha256=K-aVN-EpqPpz5R1br37G9RnI0aqY0pBG8-UnFaIOrmw,8444
+akida/engine/api/akida/hw_version.h,sha256=soeZOD-PcFSrTriaj2zK-yLnjqA92a2Eds48suEVWM4,1047
+akida/engine/api/akida/input_conversion.h,sha256=O1J11uIB6T8D68wIxHqAxyP6UGiyhRqr5U-sYmG90jQ,441
+akida/engine/api/akida/np.h,sha256=qX_ns2zAs_3hdepqPYNYwn9TiKyLRyKs_t_4NvuigFk,1077
+akida/engine/api/akida/program_memory_info.h,sha256=xQCiWXpK0oYJnADKuVxzrazLzRJd2mO9iu8KmzA5L-M,1565
+akida/engine/api/akida/registers_top_level.h,sha256=HGZH6MsbWYXuMus_6xLyQ7ETRJx82gSy0eRoHFE6fZ4,2895
+akida/engine/api/akida/shape.h,sha256=xWl3wfSRmG8OZDa-BkNg_zSd8AsMArBT1-TC2SONWaQ,3933
+akida/engine/api/akida/sparse.h,sha256=BFNLAgVcJii6OrF8Y8hsxnvpSTFOZDHEhXLEDnxxXKw,2524
+akida/engine/api/akida/tensor.h,sha256=uFW_YoytxQIcp1-cITsF6xbrRhVrG7q3eMTeVm8JfkQ,6227
+akida/engine/api/akida/version.h,sha256=eyBAcyObbr6B8BFWm2pnOIls0pcwR3m9GeYPowVOZdQ,129
+akida/engine/api/infra/exports.h,sha256=mGtp3lcHBRjXeNsOkLakfOReXp16viGAA6TtQ0UHkyY,222
+akida/engine/api/infra/hardware_driver.h,sha256=aINm-NxTFJsmjwyRYD2jpBSEoHQgK7GQqwAy0yFdF_o,2056
+akida/engine/api/infra/int_ops.h,sha256=dhIKFtmx-OJSd15SZXnu5PUu6hsiRVf2oKSvE-oHuyo,584
+akida/engine/api/infra/registers_common.h,sha256=s7NqiwKljhnTM_z2XzPlhIPzXtlwr46RlTWGRwPaxoI,1002
+akida/engine/api/infra/system.h,sha256=OuKxxTlsBgjUM6hWJqPOu9gu6LOintnh6UBBToQR1_w,788
+akida/engine/cmake/akida-engine.cmake,sha256=XB36m1LWefHHgHX6BVaqKPBExVSPD7zQub5OoOt2YAw,927
+akida/engine/devices/akd1000/bare_metal_driver.cpp,sha256=pZ0gyqfhpiuHofCjwaM72lpL9nCCdNRdeftfj9h462Y,1263
+akida/engine/inc/engine/akida_device_program_fb_generated.h,sha256=TO9IaOcz09C4Q9dNga4xe4WXcbcq7TKrpmz-EO2n0jg,31379
+akida/engine/inc/engine/dma.h,sha256=SRpF0VmXLTjNFcSjqvznJ3XTPGUG_KVmaKEr4vVbqiI,962
+akida/engine/inc/engine/dma_config_ops.h,sha256=5bzSSm67IUZG-lkfK8j2NarMY6sIvHFysETpLMk1MhE,940
+akida/engine/inc/engine/int_conversion.h,sha256=NBSnYfe3ed4R_PkEorMB1l58058xLVYvsskwXumhTTM,1838
+akida/engine/src/dense.cpp,sha256=qh5M5hJ7W_anq7T4I_lX0JB2bNB3e6HhRyk0yXTwQr4,8687
+akida/engine/src/device_memory.h,sha256=nKQu-99DAw2VbLzomfbZrW41ML2RWl6NG0CrwvQCToc,184
+akida/engine/src/dma_cnp_events.h,sha256=kXzvhZ4FyN5uVDOYYedrvX3K2Hs64i07ZP7sgqDX1hQ,2877
+akida/engine/src/dma_config_format.h,sha256=jbyOiT19ZGT-KhTN2y-ST6MqeU8M1A-4ao0JEFrSWkA,1081
+akida/engine/src/dma_config_ops.cpp,sha256=F6D8F-yjfjj-o_y6EdLGQj8u3jK_9tWQkBT3IegOLiI,2226
+akida/engine/src/dma_desc_format.h,sha256=XyJT2ndxen6wPZyAaeh9qB4my97CtnLIBhQcEUrZA00,4262
+akida/engine/src/dma_desc_ops.cpp,sha256=vuT9viTJKEFnPsrLuA1mkIKqz9GRsifqy1ZvNigrgEQ,4365
+akida/engine/src/dma_desc_ops.h,sha256=RtjPLCBLgfuGDZmUpqY8sXMIwKoUCtKt-pNnlF7Ss6Y,1171
+akida/engine/src/dma_engine.cpp,sha256=W-QaRz0I7JwS7PUKggsNGhRAc9NGU0Vumny8rlR-RLg,16406
+akida/engine/src/dma_engine.h,sha256=UfxIBl7Viw14w1G0dkEwnlGvaHHNmAvAbeX7CuCUe4o,417
+akida/engine/src/dma_engine_ops.h,sha256=RWerZuavSg6nd4m6qIBw4n3ReCDVGNGkGvRIRmMQsto,5635
+akida/engine/src/dma_events.h,sha256=0a-9u8kvHOe5eUXZQ5BkTwM-HpWvvJwiZPvw_I_58YU,1342
+akida/engine/src/dma_events_format.h,sha256=SJRy21TspJEJ7RlfZGrYJU3oEMo-8n4-FCWjSSuG3Pk,760
+akida/engine/src/dma_events_ops.cpp,sha256=8YkyOu546bb3dOvLzPxsox7-v0FGQn_zf22N0c0u_pQ,14472
+akida/engine/src/dma_events_ops.h,sha256=NH4D48b25op4RHyE8QIpG6Gmx9lRPWlOp7jKqpy_T2o,1247
+akida/engine/src/dma_fnp_events.h,sha256=JHdjZ-R8EvhvGOy69JUh2uAK39iC4dtrRwC3GwicjC0,2626
+akida/engine/src/dma_hrc_events.h,sha256=EI82tJQY-OR7R3vAsJP7Vm9osq8kuXh36szxdhY5de0,3095
+akida/engine/src/dma_image_ops.cpp,sha256=KeWJ7Ve-vt5sRtNvuTVBMyNrvchzi0CpKlA12eF17mw,2015
+akida/engine/src/dma_image_ops.h,sha256=-TjPDNqN5mzUEItc-wd-PFPLcABN98w3-BpOY8ChGX4,455
+akida/engine/src/external_mem_mgr.cpp,sha256=KfATnTY6P3Xf5SbE8vjzZN04RemcTtGzHIFmUphSUgw,1684
+akida/engine/src/external_mem_mgr.h,sha256=fWDMmZY61ymO1uOEw4WqAq93-WDwG9e1sOuaylXwKFI,1130
+akida/engine/src/fnp2_mem_conf_reg.h,sha256=qJ0eYQZMnrYSnbxuVNUjbEQySHkyOeKKaF9C-98o4X8,601
+akida/engine/src/hardware_device.cpp,sha256=o34dZ_17SB_r3wIUzlpBku-WNrMIXaSmr2FTW1vdObg,255
+akida/engine/src/hardware_device_impl.cpp,sha256=SftfVxg4GH3Impkdu4zYlRB7XFsTrMbB_tHwJTSKOuk,30960
+akida/engine/src/hardware_device_impl.h,sha256=Q9uNOUDypVkerLS0J-R9mNIToMOmA8hnufT1nUfWNcc,3905
+akida/engine/src/hw_version.cpp,sha256=7rYoR9Nurpmt2DFOqfcFV8PwR7Uc2lD5rkyRorw8K98,1067
+akida/engine/src/input_conversion.cpp,sha256=rXq4N8JHX3SJkL6Wxbi5J3tIY2PEkfeo4HPrbjvF8y4,904
+akida/engine/src/memory_mgr.cpp,sha256=mLz4VVBOXVxhL1wrmNSCvEsIuGP82NwS-L4zWVyN2Ig,2581
+akida/engine/src/memory_mgr.h,sha256=IiE8pz6jhpNBLN455ct3_jEizXckQBCGiZZe7WFg5hQ,1054
+akida/engine/src/memory_utils.cpp,sha256=lxAFYf3ncAoFBVEqKDVJeZ04ASdoyPcUdigqttPmVxI,676
+akida/engine/src/memory_utils.h,sha256=wQ6SIT4oUZhmYixG7yIJlTT2heEv-f8FxUZNhzcEqn0,387
+akida/engine/src/multipass_memory.cpp,sha256=xaWqFnb0aRWX8ZT9Q8eL6rz7s-QXPXWC-aezW18mtQM,1185
+akida/engine/src/multipass_memory.h,sha256=h0mIImG6_uNgliS4RcZ06KNZ_m6GskXA5pz_Qsbe9_g,778
+akida/engine/src/pipeline_state.h,sha256=jxDhuhfn35eYxJtUIC84P3nXrhWDoDwUcacNkCG1r1I,2124
+akida/engine/src/program_memory_info.cpp,sha256=RcwxsY1GVHgK-2mW8Vl-zg-qQe_sWhqKvaK8ck6lzhE,2906
+akida/engine/src/program_play.cpp,sha256=2lHor1egS3zh6nytPzNgyLqwe0xnVNbveasJmk82qpk,27663
+akida/engine/src/program_play.h,sha256=Rd-EZMdxz6JkAup7i-ina2rXo9CrzUYHLIfcImbNaMo,2771
+akida/engine/src/registers_dma_engine.h,sha256=aKFCxc-hU0FlKbIF1fxDQ-cleZC4tZiAtJ2OOCDKPA8,7875
+akida/engine/src/registers_reset.h,sha256=XX1PfqXqGGzO5CnfCMKnJaam2i5anDBnOK9CyE0KPkE,800
+akida/engine/src/reset_nps.cpp,sha256=P1Zj4sU_jVp8eoiJliQEtp0fp2At0rmHvuXHESZam4E,994
+akida/engine/src/reset_nps.h,sha256=tPV5v5t1ZhnzItqdMlfQXfIN1JV4e9Xx4f90OOG25qU,286
+akida/engine/src/sparse.cpp,sha256=etoxi0J6ENIvV6goXbT3FzO9iz7jBlaCaxiG7r-7k2Q,622
+akida/engine/src/tensor.cpp,sha256=2Cpw0eyif2Bqkz6kaUawQL9hTfWc1sOfSZ0z6M6XrCw,801
+akida/engine/src/version.cpp,sha256=JT-h8rWhpyIMICJcAdYDmtwMKIQzuciR4IVcvTG_Fkc,194
+akida/generate/__init__.py,sha256=E4m8oNN9D-OBgTUe5vuzMzPHi3eyP_iGqOil9YtS9Fw,955
+akida/generate/application_generator.py,sha256=aVt-bs3pb8Zvxhl2SwHBhy4z1Ta47HXbql_94CTu0Xg,2374
+akida/generate/array_to_cpp.py,sha256=JJl_3aaMaogj_JLIXKN-x7G2H40HM-kvJktLy2MrTe4,5306
+akida/generate/model.py,sha256=ql45VHWjIObkLAOGzsoTvi_nojH91vEo3Ak8bI9DpIk,1054
+akida/generate/test/template.py,sha256=paew18D69h1yImSioMqVDApQQDQ9PH-2uKQwvc4OrU0,386
+akida/generate/test/test_tools.py,sha256=40Blb1Bl1A5vE5wMz9C_a9_NSO86EDjrcBDVaaCbIJ8,4365
+akida/generate/test/cmake/akida-model.cmake,sha256=eayIZiugDpxH_1gyt6bJJoyf3s1y5QDSsYDnGkwMXEE,1169
+akida/generate/test/engine/test_generator.py,sha256=qsFgZyEukRX-TP5KHDeuf6KM1NAamqgvDjjAoHJnTb8,1313
+akida/generate/test/engine/app_templates/test.cpp,sha256=KxPi1io1Z99b8zeOHfjeS8BJ0hmcOZ52uj3XJVHBrAo,4337
+akida/generate/test/engine/app_templates/test.h,sha256=oJYrg2Qg-DZ1Dimrgs2qDX-2ijATSIeCrzMtGqRmyCQ,1315
+akida/generate/test/engine/fixtures/simple_conv_evaluate_v2.py,sha256=EiHXoeV9YvOL5Au0IUD_ouhBgwZzAucnSgjTK84-Mdg,801
+akida/generate/test/engine/fixtures/simple_conv_v2.py,sha256=K5QJ2M2X1W1YzaDDSjwu3CRLDS5YU-y8YeoI5bPqUbc,800
+akida/generate/test/engine/fixtures/simple_hrc_v2.py,sha256=ApAgwNatvGgSk-Tg3uErglqd-Q-WzlQJ9mbma7-I1A0,1655
+akida/generate/test/engine/fixtures/simple_sep_conv_v2.py,sha256=xsTb6J7UOD9bT9VhwlggeFuEPrVjQcfTOu1RrGss4Zw,863
+akida/generate/test/engine/fixtures/test_fnp2.py,sha256=CqkfSfmT8YPcXRiXBHOMinH_Ivs-ZhTBuP2ZIsM1bTI,775
+akida/generate/test/engine/fixtures/test_multiple_inputs.py,sha256=qx968UnElsVF7crneg2T-LziyKmOEna3kSSllzf4vSc,842
+akida/generate/test/model/test_generator.py,sha256=-ps7YKpB-NPaaGQx6kaAZRuhLau3R-JnX0T_SLElwzQ,1450
+akida/generate/test/model/app_templates/test.cpp,sha256=JZNijDGiMlsGnbtkAGWwPF9xK-DXj3k0rYRGUeSmlDU,1063
+akida/generate/test/model/app_templates/test.h,sha256=pSGNJEjmxZyEgQM84S7wxIH56R-btY8kvwt43oE11pc,557
+akida/layers/__init__.py,sha256=HODtQmkUxDo58tP6lqHdIfF8hxEGm-FDs0Z1H5RmP5Q,814
+akida/layers/add.py,sha256=0ywTqUDII3rKYJng6wF8H8TsvvO82tYH2PdUm6PeZhc,1830
+akida/layers/attention.py,sha256=-vzI4N-DH1rQTD0-LKDcBEsnSKytUWqiMe8i_jYjmo8,2152
+akida/layers/batch_normalization.py,sha256=s2GtttHztIbscUGIkLG6CI5wrQpwRLME2791FCLs9T8,1236
+akida/layers/concatenate.py,sha256=pR3cwBNmK1lic3p8S3hDN5I3k5jMaS7MWQJHu0SeHrc,1028
+akida/layers/conv2d.py,sha256=7lxW3IvMNf_SvPDfgf2kMrfzq6qyxsFvGLiQpYrisJk,3715
+akida/layers/conv2d_transpose.py,sha256=sz7hfdMxamTg86UjkRljAckbJyQtsauwXVCsXT6BxtY,2624
+akida/layers/convolutional.py,sha256=Xz2dj6fX5-TJRqXK06V5ZGIDVrEFwv7iFfjFKnDj5oc,3985
+akida/layers/dense1d.py,sha256=jysVN3sOHNefgKEFFOx8Y1S3xByEtJA13Qebk1UIHoQ,2190
+akida/layers/dense2d.py,sha256=0Zm0W2ZnhnkmULKyNyFg2MeoP0sCEqAUujGrsiP86yo,2261
+akida/layers/depthwise_conv2d.py,sha256=LU2mqpZPWF6ZKTnsa1Jsitg29t0YLhcCGmKpuqwxTUY,3401
+akida/layers/depthwise_conv2d_transpose.py,sha256=kNfbqD7tI4S9bL6131qiZKQ4GOOZY3G3gp96_wfME1I,2668
+akida/layers/dequantizer.py,sha256=BuLR26ptoHvg_OwD20YMNhjOcjTirl1Oh3RKWGkjQlU,479
+akida/layers/extract_token.py,sha256=0Lg9VKGsb38tMs3XZ0SfSDKVETQo_kr1sXBguE0cDjQ,1176
+akida/layers/fully_connected.py,sha256=0HkDjuiHlawoEbPiKQonBvjIyK77Fd9zyFDPMeTZTUM,1790
+akida/layers/input_conv2d.py,sha256=ZBAbtJS_UiM4oZh9iheH8ko5zIsFgUPjh9loAfT2qsU,4263
+akida/layers/input_convolutional.py,sha256=2KCktoqPD09ykeKP_71PWP1AuG6B8adX8l3zcF4q1Qc,4395
+akida/layers/input_data.py,sha256=Hr4vbfPwG4TZZVk_p75jRa1vAYScUcv3yxvVyQniDH0,1603
+akida/layers/madnorm.py,sha256=FJ8gsw6irFX7BHsGFWnHAKLotsaV8B79UWH8J1fL2kU,1327
+akida/layers/separable_convolutional.py,sha256=rOWSwwZSlhii6lFdtkfNReSegKgQDAD7sw9fV_hp3hU,4163
+akida/layers/shiftmax.py,sha256=ozFiSkwGqLyGt8sE5hljzku9annt2rmSrl0MfhHVA-s,1175
+akida/layers/stem.py,sha256=ormUsVo5wm8_sxFeVy7VefiOrPdw6zsUEO7Mpe4c00I,3064
+akida-2.3.4.dist-info/LICENSE,sha256=-YbUNt-9_ZAq_v4uRzM-H1K2amslSOlpDQTVyG2Ogw4,8933
+akida-2.3.4.dist-info/LICENSE.3rdparty,sha256=tSolounXR8B0j2Z8yPmBYGrAdA6Zuf9vkVHpyH93NvU,1942
+akida-2.3.4.dist-info/METADATA,sha256=u_j4E83uSzj8mg1kIKxmehLO8uZ8o9H4V8KLe-tEF4A,604
+akida-2.3.4.dist-info/WHEEL,sha256=jqqnqb88opTI21_G0QEz_QTpBhauauJPf5ADtTGkx1k,113
+akida-2.3.4.dist-info/entry_points.txt,sha256=_KRNGDUM4kZBDxEZCcVDUBkCsz8me4G4gBjWGzc4CTA,41
+akida-2.3.4.dist-info/top_level.txt,sha256=RKaGSiprCDSQ8PCjbVbKc4BOOChQcEoNOvqlv8iyY0A,6
+akida-2.3.4.dist-info/RECORD,,
+akida.libs/libgomp-01527a09.so.1.0.0,sha256=-2t3Tuk3STrkxTiUKfw87qbwHpJ-tTo1oavj3s05TuQ,209120
```

