# Comparing `tmp/brainpylib-0.1.7-cp39-cp39-win_amd64.whl.zip` & `tmp/brainpylib-0.1.8-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,11 +1,11 @@
-Zip file size: 149505 bytes, number of entries: 70
--rw-rw-rw-  2.0 fat      549 b- defN 23-Apr-06 07:26 brainpylib/__init__.py
+Zip file size: 149453 bytes, number of entries: 70
+-rw-rw-rw-  2.0 fat      549 b- defN 23-Apr-12 13:51 brainpylib/__init__.py
 -rw-rw-rw-  2.0 fat      384 b- defN 23-Jan-08 11:31 brainpylib/compat.py
--rw-rw-rw-  2.0 fat   156672 b- defN 23-Apr-06 07:40 brainpylib/cpu_ops.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat   156672 b- defN 23-Apr-17 08:41 brainpylib/cpu_ops.cp39-win_amd64.pyd
 -rw-rw-rw-  2.0 fat      190 b- defN 23-Jan-07 13:50 brainpylib/event_ops.py
 -rw-rw-rw-  2.0 fat      756 b- defN 23-Jan-08 11:34 brainpylib/jitconn_ops.py
 -rw-rw-rw-  2.0 fat      193 b- defN 23-Jan-07 13:50 brainpylib/op_register.py
 -rw-rw-rw-  2.0 fat      392 b- defN 23-Jan-07 13:52 brainpylib/sparse_ops.py
 -rw-rw-rw-  2.0 fat       25 b- defN 23-Jan-07 13:45 brainpylib/_src/__init__.py
 -rw-rw-rw-  2.0 fat     1393 b- defN 23-Jan-07 13:58 brainpylib/_src/check.py
 -rw-rw-rw-  2.0 fat      411 b- defN 22-Dec-03 03:05 brainpylib/_src/errors.py
@@ -29,18 +29,18 @@
 -rw-rw-rw-  2.0 fat     5279 b- defN 23-Jan-07 13:50 brainpylib/_src/event_ops/event_info_collection.py
 -rw-rw-rw-  2.0 fat     6301 b- defN 22-Nov-29 13:46 brainpylib/_src/event_ops/tests/event_info_VS_jax_operators.py
 -rw-rw-rw-  2.0 fat    13768 b- defN 23-Apr-05 03:28 brainpylib/_src/event_ops/tests/test_event_csr_matvec.py
 -rw-rw-rw-  2.0 fat      396 b- defN 22-Nov-29 13:46 brainpylib/_src/event_ops/tests/test_event_csr_matvec_gpu.py
 -rw-rw-rw-  2.0 fat     1801 b- defN 23-Jan-08 12:07 brainpylib/_src/event_ops/tests/test_event_info_collection.py
 -rw-rw-rw-  2.0 fat      386 b- defN 22-Nov-29 13:46 brainpylib/_src/event_ops/tests/test_event_info_collection_gpu.py
 -rw-rw-rw-  2.0 fat      195 b- defN 23-Jan-07 13:43 brainpylib/_src/jitconn_ops/__init__.py
--rw-rw-rw-  2.0 fat    24180 b- defN 23-Jan-07 13:50 brainpylib/_src/jitconn_ops/event_matvec.py
+-rw-rw-rw-  2.0 fat    23990 b- defN 23-Apr-12 14:54 brainpylib/_src/jitconn_ops/event_matvec.py
 -rw-rw-rw-  2.0 fat    16888 b- defN 23-Jan-08 10:03 brainpylib/_src/jitconn_ops/matmat.py
--rw-rw-rw-  2.0 fat    28298 b- defN 23-Jan-07 14:07 brainpylib/_src/jitconn_ops/matvec.py
--rw-rw-rw-  2.0 fat     7734 b- defN 23-Jan-07 13:43 brainpylib/_src/jitconn_ops/tests/event_matvec_jitconn_performance.py
+-rw-rw-rw-  2.0 fat    26086 b- defN 23-Apr-12 14:54 brainpylib/_src/jitconn_ops/matvec.py
+-rw-rw-rw-  2.0 fat     7661 b- defN 23-Apr-13 02:28 brainpylib/_src/jitconn_ops/tests/event_matvec_jitconn_performance.py
 -rw-rw-rw-  2.0 fat     1694 b- defN 23-Jan-08 11:25 brainpylib/_src/jitconn_ops/tests/matmat_jitconn_performance.py
 -rw-rw-rw-  2.0 fat     5073 b- defN 23-Jan-08 11:55 brainpylib/_src/jitconn_ops/tests/matmat_testcase.py
 -rw-rw-rw-  2.0 fat     1586 b- defN 23-Jan-08 06:22 brainpylib/_src/jitconn_ops/tests/matvec_jitconn_performance.py
 -rw-rw-rw-  2.0 fat    22837 b- defN 23-Jan-08 12:27 brainpylib/_src/jitconn_ops/tests/test_event_matvec.py
 -rw-rw-rw-  2.0 fat      404 b- defN 23-Jan-07 13:43 brainpylib/_src/jitconn_ops/tests/test_event_matvec_gpu.py
 -rw-rw-rw-  2.0 fat      390 b- defN 23-Jan-08 12:43 brainpylib/_src/jitconn_ops/tests/test_matmat_cpu.py
 -rw-rw-rw-  2.0 fat      384 b- defN 23-Jan-08 12:40 brainpylib/_src/jitconn_ops/tests/test_matmat_gpu.py
@@ -60,13 +60,13 @@
 -rw-rw-rw-  2.0 fat     7885 b- defN 23-Mar-25 08:33 brainpylib/_src/sparse_ops/sparse_csr_matvec.py
 -rw-rw-rw-  2.0 fat     5302 b- defN 23-Jan-07 13:52 brainpylib/_src/sparse_ops/utils.py
 -rw-rw-rw-  2.0 fat     2687 b- defN 22-Nov-29 13:46 brainpylib/_src/sparse_ops/tests/csr_matvec_VS_cusparse_csr_matvec.py
 -rw-rw-rw-  2.0 fat    10148 b- defN 23-Jan-08 12:30 brainpylib/_src/sparse_ops/tests/test_cusparse_matvec.py
 -rw-rw-rw-  2.0 fat      405 b- defN 22-Nov-29 13:46 brainpylib/_src/sparse_ops/tests/test_cusparse_matvec_gpu.py
 -rw-rw-rw-  2.0 fat     2610 b- defN 23-Jan-08 12:30 brainpylib/_src/sparse_ops/tests/test_sparse_csr_matvec.py
 -rw-rw-rw-  2.0 fat      378 b- defN 22-Nov-29 13:46 brainpylib/_src/sparse_ops/tests/test_sparse_csr_matvec_gpu.py
--rw-rw-rw-  2.0 fat    35773 b- defN 23-Apr-06 07:54 brainpylib-0.1.7.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     2898 b- defN 23-Apr-06 07:54 brainpylib-0.1.7.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 23-Apr-06 07:54 brainpylib-0.1.7.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       30 b- defN 23-Apr-06 07:54 brainpylib-0.1.7.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     7051 b- defN 23-Apr-06 07:54 brainpylib-0.1.7.dist-info/RECORD
-70 files, 495541 bytes uncompressed, 137895 bytes compressed:  72.2%
+-rw-rw-rw-  2.0 fat    35773 b- defN 23-Apr-17 08:41 brainpylib-0.1.8.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     2848 b- defN 23-Apr-17 08:41 brainpylib-0.1.8.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 23-Apr-17 08:41 brainpylib-0.1.8.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       30 b- defN 23-Apr-17 08:41 brainpylib-0.1.8.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     7051 b- defN 23-Apr-17 08:41 brainpylib-0.1.8.dist-info/RECORD
+70 files, 493016 bytes uncompressed, 137843 bytes compressed:  72.0%
```

## zipnote {}

```diff
@@ -189,23 +189,23 @@
 
 Filename: brainpylib/_src/sparse_ops/tests/test_sparse_csr_matvec.py
 Comment: 
 
 Filename: brainpylib/_src/sparse_ops/tests/test_sparse_csr_matvec_gpu.py
 Comment: 
 
-Filename: brainpylib-0.1.7.dist-info/LICENSE
+Filename: brainpylib-0.1.8.dist-info/LICENSE
 Comment: 
 
-Filename: brainpylib-0.1.7.dist-info/METADATA
+Filename: brainpylib-0.1.8.dist-info/METADATA
 Comment: 
 
-Filename: brainpylib-0.1.7.dist-info/WHEEL
+Filename: brainpylib-0.1.8.dist-info/WHEEL
 Comment: 
 
-Filename: brainpylib-0.1.7.dist-info/top_level.txt
+Filename: brainpylib-0.1.8.dist-info/top_level.txt
 Comment: 
 
-Filename: brainpylib-0.1.7.dist-info/RECORD
+Filename: brainpylib-0.1.8.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## brainpylib/__init__.py

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-__version__ = "0.1.7"
+__version__ = "0.1.8"
 
 
 # IMPORTANT, must import first
 from ._src import check, register_custom_calls
 del check, register_custom_calls
 
 # operator customization
```

## brainpylib/_src/jitconn_ops/event_matvec.py

```diff
@@ -1,29 +1,30 @@
 # -*- coding: utf-8 -*-
 
-
+import math
 from functools import partial
 from typing import Tuple, Optional
 
 import numpy as np
 from jax import numpy as jnp, dtypes
 from jax.core import ShapedArray, Primitive
 from jax.interpreters import xla, ad
 from jax.lib import xla_client
 
 from brainpylib._src.errors import GPUOperatorNotFound
 from brainpylib._src.op_register import (register_general_batching)
 from brainpylib._src.tools import transform_brainpy_array
-from .matvec import (matvec_prob_homo_p,
-                     matvec_prob_uniform_p,
-                     matvec_prob_normal_p,
-                     matvec_prob_conn_homo_weight,
-                     matvec_prob_conn_uniform_weight,
-                     matvec_prob_conn_normal_weight
-                     )
+from .matvec import (
+  matvec_prob_homo_p,
+  matvec_prob_uniform_p,
+  matvec_prob_normal_p,
+  matvec_prob_conn_homo_weight,
+  matvec_prob_conn_uniform_weight,
+  matvec_prob_conn_normal_weight
+)
 
 try:
   from brainpylib import gpu_ops
 except ImportError:
   gpu_ops = None
 
 __all__ = [
@@ -210,19 +211,18 @@
     out_dtype = dtypes.canonicalize_dtype(float)
     type_name = b'_float' if out_dtype == jnp.float32 else b'_double'
   else:
     out_dtype = event_shape.element_type()
     event_type = b'_float' if out_dtype == jnp.float32 else b'_double'
     type_name = event_type
 
-  p = float(np.log((1 - conn_prob) if conn_prob < 1 else 1e-40))
-  opaque = gpu_ops.build_jitconn_prob_homo_descriptor(shape[1] if transpose else shape[0],
-                                                      shape[0] if transpose else shape[1],
-                                                      seed,
-                                                      p)
+  opaque = gpu_ops.build_jitconn_prob_homo_descriptor2(shape[1] if transpose else shape[0],
+                                                       shape[0] if transpose else shape[1],
+                                                       seed,
+                                                       math.ceil(1 / conn_prob) * 2 - 1)
 
   if outdim_parallel:
     fn = b'gpu_event_matvec_prob_homo_v2' + type_name + event_type
   else:
     fn = b'gpu_event_matvec_atomic_prob_homo_v2' + type_name + event_type
 
   return xla_client.ops.CustomCallWithLayout(
@@ -352,21 +352,20 @@
     out_dtype = dtypes.canonicalize_dtype(float)
     type_name = b'_float' if out_dtype == jnp.float32 else b'_double'
   else:
     out_dtype = event_shape.element_type()
     event_type = b'_float' if out_dtype == jnp.float32 else b'_double'
     type_name = event_type
 
-  p = float(np.log((1 - conn_prob) if conn_prob < 1 else 1e-40))
-  opaque = gpu_ops.build_jitconn_prob_uniform_descriptor(shape[1] if transpose else shape[0],
-                                                         shape[0] if transpose else shape[1],
-                                                         seed,
-                                                         p,
-                                                         w_low,
-                                                         w_high - w_low)
+  opaque = gpu_ops.build_jitconn_prob_uniform_descriptor2(shape[1] if transpose else shape[0],
+                                                          shape[0] if transpose else shape[1],
+                                                          seed,
+                                                          math.ceil(1 / conn_prob) * 2 - 1,
+                                                          w_low,
+                                                          w_high - w_low)
   if outdim_parallel:
     fn = b'gpu_event_matvec_prob_uniform_v2' + type_name + event_type
   else:
     fn = b'gpu_event_matvec_atomic_prob_uniform_v2' + type_name + event_type
   return xla_client.ops.CustomCallWithLayout(
     c,
     fn,
@@ -498,21 +497,20 @@
     event_type = b'_bool'
     out_dtype = dtypes.canonicalize_dtype(float)
     type_name = b'_float' if out_dtype == jnp.float32 else b'_double'
   else:
     out_dtype = event_shape.element_type()
     event_type = b'_float' if out_dtype == jnp.float32 else b'_double'
     type_name = event_type
-  p = float(np.log((1 - conn_prob) if conn_prob < 1 else 1e-40))
-  opaque = gpu_ops.build_jitconn_prob_normal_descriptor(shape[1] if transpose else shape[0],
-                                                        shape[0] if transpose else shape[1],
-                                                        seed,
-                                                        p,
-                                                        w_mu,
-                                                        w_sigma)
+  opaque = gpu_ops.build_jitconn_prob_normal_descriptor2(shape[1] if transpose else shape[0],
+                                                         shape[0] if transpose else shape[1],
+                                                         seed,
+                                                         math.ceil(1 / conn_prob) * 2 - 1,
+                                                         w_mu,
+                                                         w_sigma)
   if outdim_parallel:
     fn = b'gpu_event_matvec_prob_normal_v2' + type_name + event_type
   else:
     fn = b'gpu_event_matvec_atomic_prob_normal_v2' + type_name + event_type
   return xla_client.ops.CustomCallWithLayout(
     c,
     fn,
```

## brainpylib/_src/jitconn_ops/matvec.py

```diff
@@ -1,9 +1,11 @@
 # -*- coding: utf-8 -*-
-import warnings
+
+
+import math
 from functools import partial
 from typing import Tuple, Optional
 
 import numpy as np
 from jax import numpy as jnp, dtypes
 from jax.core import ShapedArray, Primitive
 from jax.interpreters import xla, ad
@@ -14,15 +16,14 @@
 from brainpylib._src.tools import transform_brainpy_array
 
 try:
   from brainpylib import gpu_ops
 except ImportError:
   gpu_ops = None
 
-
 __all__ = [
   'matvec_prob_conn_homo_weight',
   'matvec_prob_conn_uniform_weight',
   'matvec_prob_conn_normal_weight',
 ]
 
 
@@ -31,15 +32,14 @@
     weight: float,
     *,
     conn_prob: float,
     shape: Tuple[int, int],
     seed: Optional[int] = None,
     transpose: bool = False,
     outdim_parallel: bool = True,
-    version: str = 'v2'
 ) -> jnp.ndarray:
   r"""Perform the :math:`y=M@v` operation,
   where :math:`M` is just-in-time randomly generated with a scalar `weight` at each position.
 
   This operator support ``jit()``, ``vmap()``, ``grad()`` and ``pmap()`` etc. transformations
   on CPU and GPU devices.
 
@@ -95,37 +95,35 @@
     if vector.shape[0] != shape[0]:
       raise ValueError(f'Shape mismatch, vec ({vector.shape[0]},) @ mat {shape}.')
   else:
     if vector.shape[0] != shape[1]:
       raise ValueError(f'Shape mismatch, mat {shape} @ vec ({vector.shape[0]},).')
   if seed is None:
     seed = int(np.random.randint(0, int(1e8)))
-  assert version in ['v1', 'v2']
   r = matvec_prob_homo_p.bind(vector,
                               conn_prob=conn_prob,
                               shape=shape,
                               seed=seed,
                               transpose=transpose,
                               outdim_parallel=outdim_parallel,
-                              version=version)[0]
+                              )[0]
   weight = jnp.asarray(weight, dtype=r.dtype)
   return r * weight
 
 
 def matvec_prob_conn_uniform_weight(
     vector: jnp.ndarray,
     *,
     w_low: float,
     w_high: float,
     conn_prob: float,
     shape: Tuple[int, int],
     seed: Optional[int] = None,
     transpose: bool = False,
     outdim_parallel: bool = True,
-    version: str = 'v2'
 ) -> jnp.ndarray:
   r"""Perform the :math:`y=M@v` operation,
   where :math:`M` is just-in-time randomly generated with a uniform distribution for its value.
 
   This operator support ``jit()``, ``vmap()``, ``grad()`` and ``pmap()`` etc. transformations
   on CPU and GPU devices.
 
@@ -191,28 +189,27 @@
                                     w_low=w_low,
                                     w_high=w_high,
                                     conn_prob=conn_prob,
                                     shape=shape,
                                     seed=seed,
                                     transpose=transpose,
                                     outdim_parallel=outdim_parallel,
-                                    version=version)[0]
+                                    )[0]
 
 
 def matvec_prob_conn_normal_weight(
     vector: jnp.ndarray,
     *,
     w_mu: float,
     w_sigma: float,
     conn_prob: float,
     shape: Tuple[int, int],
     seed: Optional[int] = None,
     transpose: bool = False,
     outdim_parallel: bool = True,
-    version: str = 'v2'
 ) -> jnp.ndarray:
   r"""Perform the :math:`y=M@v` operation,
   where :math:`M` is just-in-time randomly generated with a normal distribution for its value.
 
   This operator support ``jit()``, ``vmap()``, ``grad()`` and ``pmap()`` etc. transformations
   on CPU and GPU devices.
 
@@ -277,27 +274,27 @@
                                    w_mu=w_mu,
                                    w_sigma=w_sigma,
                                    conn_prob=conn_prob,
                                    shape=shape,
                                    seed=seed,
                                    transpose=transpose,
                                    outdim_parallel=outdim_parallel,
-                                   version=version)[0]
+                                   )[0]
 
 
 def _matvec_prob_homo_abstract(
-    vector, *, conn_prob, shape, seed, transpose, outdim_parallel, version='v2'
+    vector, *, conn_prob, shape, seed, transpose, outdim_parallel
 ):
   out = ShapedArray(dtype=dtypes.canonicalize_dtype(float),
                     shape=(shape[1] if transpose else shape[0],))
   return [out]
 
 
 def _matvec_prob_homo_cpu_translation(
-    c, vector, *, conn_prob, shape, seed, transpose, outdim_parallel, version
+    c, vector, *, conn_prob, shape, seed, transpose, outdim_parallel
 ):
   log_p = float(np.log((1 - conn_prob) if (conn_prob < 1) else 1e-40))
   n_row, n_col = (shape[1], shape[0]) if transpose else shape
 
   vec_shape = c.get_shape(vector)
   out_dtype = vec_shape.element_type()
   out_type = b'_float' if out_dtype == jnp.float32 else b'_double'
@@ -324,40 +321,30 @@
         xla_client.Shape.array_shape(out_dtype, (shape[1] if transpose else shape[0],), (0,)),
       )
     ),
   )
 
 
 def _matvec_prob_homo_gpu_translation(
-    c, vector, *, conn_prob, shape, seed, transpose, outdim_parallel, version
+    c, vector, *, conn_prob, shape, seed, transpose, outdim_parallel
 ):
   if gpu_ops is None:
     raise GPUOperatorNotFound(matvec_prob_homo_p.name)
 
   out_dtype = dtypes.canonicalize_dtype(float)
-  type_name = b'_float' if out_dtype == jnp.float32 else b'_double'
-  if version == 'v1':
-    version_name = b'_v1'
-    opaque = gpu_ops.build_jitconn_prob_homo_descriptor(shape[1] if transpose else shape[0],
-                                                        shape[0] if transpose else shape[1],
-                                                        seed,
-                                                        conn_prob)
-  elif version == 'v2':
-    version_name = b'_v2'
-    opaque = gpu_ops.build_jitconn_prob_homo_descriptor(shape[1] if transpose else shape[0],
-                                                        shape[0] if transpose else shape[1],
-                                                        seed,
-                                                        float(np.log((1 - conn_prob) if conn_prob < 1 else 1e-40)))
-  else:
-    raise ValueError
+  type_name = _check_out_type(out_dtype)
+  opaque = gpu_ops.build_jitconn_prob_homo_descriptor2(shape[1] if transpose else shape[0],
+                                                       shape[0] if transpose else shape[1],
+                                                       seed,
+                                                       math.ceil(1 / conn_prob) * 2 - 1)
 
   if outdim_parallel:
-    fn = b'gpu_matvec_prob_homo' + version_name + type_name
+    fn = b'gpu_matvec_prob_homo_v2' + type_name
   else:
-    fn = b'gpu_matvec_atomic_prob_homo' + version_name + type_name
+    fn = b'gpu_matvec_atomic_prob_homo_v2' + type_name
   return xla_client.ops.CustomCallWithLayout(
     c,
     fn,
     operands=(vector,),
     operand_shapes_with_layout=(c.get_shape(vector),),
     shape_with_layout=xla_client.Shape.tuple_shape(
       (
@@ -365,68 +352,68 @@
       )
     ),
     opaque=opaque,
   )
 
 
 def _matvec_prob_homo_jvp(
-    primals, tangents, *, conn_prob, shape, seed, transpose, outdim_parallel, version
+    primals, tangents, *, conn_prob, shape, seed, transpose, outdim_parallel
 ):
   vector, = primals
   vector_dot, = tangents
   r = matvec_prob_homo_p.bind(vector,
                               conn_prob=conn_prob,
                               shape=shape,
                               seed=seed,
                               transpose=transpose,
                               outdim_parallel=outdim_parallel,
-                              version=version)
+                              )
   r_dot = matvec_prob_homo_p.bind(vector_dot,
                                   conn_prob=conn_prob,
                                   shape=shape,
                                   seed=seed,
                                   transpose=transpose,
                                   outdim_parallel=outdim_parallel,
-                                  version=version)
+                                  )
   return r, r_dot
 
 
 def _matvec_prob_homo_transpose(
-    ct, vector, *, conn_prob, shape, seed, transpose, outdim_parallel, version
+    ct, vector, *, conn_prob, shape, seed, transpose, outdim_parallel
 ):
   return matvec_prob_homo_p.bind(ct[0],
                                  conn_prob=conn_prob,
                                  seed=seed,
                                  shape=shape,
                                  transpose=not transpose,
                                  outdim_parallel=not outdim_parallel,
-                                 version=version)
+                                 )
 
 
 matvec_prob_homo_p = Primitive('matvec_prob_homo')
 matvec_prob_homo_p.multiple_results = True
 matvec_prob_homo_p.def_abstract_eval(_matvec_prob_homo_abstract)
 matvec_prob_homo_p.def_impl(partial(xla.apply_primitive, matvec_prob_homo_p))
 xla.backend_specific_translations['cpu'][matvec_prob_homo_p] = _matvec_prob_homo_cpu_translation
 xla.backend_specific_translations['gpu'][matvec_prob_homo_p] = _matvec_prob_homo_gpu_translation
 register_general_batching(matvec_prob_homo_p)
 ad.primitive_jvps[matvec_prob_homo_p] = _matvec_prob_homo_jvp
 ad.primitive_transposes[matvec_prob_homo_p] = _matvec_prob_homo_transpose
 
 
 def _matvec_prob_uniform_abstract(
-    vector, *, w_low, w_high, conn_prob, shape, seed, transpose, outdim_parallel, version
+    vector, *, w_low, w_high, conn_prob, shape, seed, transpose, outdim_parallel
 ):
   out = ShapedArray(dtype=dtypes.canonicalize_dtype(float),
                     shape=(shape[1] if transpose else shape[0],))
   return [out]
 
 
 def _matvec_prob_uniform_cpu_translation(
-    c, vector, *, w_low, w_high, conn_prob, shape, seed, transpose, outdim_parallel, version
+    c, vector, *, w_low, w_high, conn_prob, shape, seed, transpose, outdim_parallel
 ):
   log_p = np.log((1 - conn_prob) if (conn_prob < 1) else 1e-40)
   n_row, n_col = (shape[1], shape[0]) if transpose else shape
 
   vec_shape = c.get_shape(vector)
   out_dtype = vec_shape.element_type()
   type_name = b'_float' if out_dtype == jnp.float32 else b'_double'
@@ -460,45 +447,32 @@
         xla_client.Shape.array_shape(out_dtype, (shape[1] if transpose else shape[0],), (0,)),
       )
     ),
   )
 
 
 def _matvec_prob_uniform_gpu_translation(
-    c, vector, *, w_low, w_high, conn_prob, shape, seed, transpose, outdim_parallel, version
+    c, vector, *, w_low, w_high, conn_prob, shape, seed, transpose, outdim_parallel
 ):
   if gpu_ops is None:
     raise GPUOperatorNotFound(matvec_prob_homo_p.name)
 
   out_dtype = dtypes.canonicalize_dtype(float)
-  type_name = b'_float' if out_dtype == jnp.float32 else b'_double'
-
-  if version == 'v1':
-    version_name = b'_v1'
-    opaque = gpu_ops.build_jitconn_prob_uniform_descriptor(shape[1] if transpose else shape[0],
-                                                           shape[0] if transpose else shape[1],
-                                                           seed,
-                                                           conn_prob,
-                                                           w_low,
-                                                           w_high - w_low)
-  elif version == 'v2':
-    version_name = b'_v2'
-    opaque = gpu_ops.build_jitconn_prob_uniform_descriptor(shape[1] if transpose else shape[0],
-                                                           shape[0] if transpose else shape[1],
-                                                           seed,
-                                                           float(np.log((1 - conn_prob) if conn_prob < 1 else 1e-40)),
-                                                           w_low,
-                                                           w_high - w_low)
-  else:
-    raise ValueError
+  type_name = _check_out_type(out_dtype)
+  opaque = gpu_ops.build_jitconn_prob_uniform_descriptor2(shape[1] if transpose else shape[0],
+                                                          shape[0] if transpose else shape[1],
+                                                          seed,
+                                                          math.ceil(1 / conn_prob) * 2 - 1,
+                                                          w_low,
+                                                          w_high - w_low)
 
   if outdim_parallel:
-    fn = b'gpu_matvec_prob_uniform' + version_name + type_name
+    fn = b'gpu_matvec_prob_uniform_v2' + type_name
   else:
-    fn = b'gpu_matvec_atomic_prob_uniform' + version_name + type_name
+    fn = b'gpu_matvec_atomic_prob_uniform_v2' + type_name
 
   return xla_client.ops.CustomCallWithLayout(
     c,
     fn,
     operands=(vector,),
     operand_shapes_with_layout=(c.get_shape(vector),),
     shape_with_layout=xla_client.Shape.tuple_shape(
@@ -507,74 +481,74 @@
       )
     ),
     opaque=opaque,
   )
 
 
 def _matvec_prob_uniform_jvp(
-    primals, tangents, *, w_low, w_high, conn_prob, shape, seed, transpose, outdim_parallel, version
+    primals, tangents, *, w_low, w_high, conn_prob, shape, seed, transpose, outdim_parallel
 ):
   vector, = primals
   vector_dot, = tangents
   r = matvec_prob_uniform_p.bind(vector,
                                  w_low=w_low,
                                  w_high=w_high,
                                  conn_prob=conn_prob,
                                  shape=shape,
                                  seed=seed,
                                  transpose=transpose,
                                  outdim_parallel=outdim_parallel,
-                                 version=version)
+                                 )
   r_dot = matvec_prob_uniform_p.bind(vector_dot,
                                      w_low=w_low,
                                      w_high=w_high,
                                      conn_prob=conn_prob,
                                      shape=shape,
                                      seed=seed,
                                      transpose=transpose,
                                      outdim_parallel=outdim_parallel,
-                                     version=version)
+                                     )
   return r, r_dot
 
 
 def _matvec_prob_uniform_transpose(
-    ct, events, *, w_low, w_high, conn_prob, shape, seed, transpose, outdim_parallel, version
+    ct, events, *, w_low, w_high, conn_prob, shape, seed, transpose, outdim_parallel
 ):
   return matvec_prob_uniform_p.bind(ct[0],
                                     w_low=w_low,
                                     w_high=w_high,
                                     conn_prob=conn_prob,
                                     seed=seed,
                                     shape=shape,
                                     transpose=not transpose,
                                     outdim_parallel=not outdim_parallel,
-                                    version=version)
+                                    )
 
 
 matvec_prob_uniform_p = Primitive('matvec_prob_uniform')
 matvec_prob_uniform_p.multiple_results = True
 matvec_prob_uniform_p.def_abstract_eval(_matvec_prob_uniform_abstract)
 matvec_prob_uniform_p.def_impl(partial(xla.apply_primitive, matvec_prob_uniform_p))
 xla.backend_specific_translations['cpu'][matvec_prob_uniform_p] = _matvec_prob_uniform_cpu_translation
 xla.backend_specific_translations['gpu'][matvec_prob_uniform_p] = _matvec_prob_uniform_gpu_translation
 register_general_batching(matvec_prob_uniform_p)
 ad.primitive_jvps[matvec_prob_uniform_p] = _matvec_prob_uniform_jvp
 ad.primitive_transposes[matvec_prob_uniform_p] = _matvec_prob_uniform_transpose
 
 
 def _matvec_prob_normal_abstract(
-    vector, *, w_mu, w_sigma, conn_prob, shape, seed, transpose, outdim_parallel, version
+    vector, *, w_mu, w_sigma, conn_prob, shape, seed, transpose, outdim_parallel
 ):
   out = ShapedArray(dtype=dtypes.canonicalize_dtype(float),
                     shape=(shape[1] if transpose else shape[0],))
   return [out]
 
 
 def _matvec_prob_normal_cpu_translation(
-    c, events, *, w_mu, w_sigma, conn_prob, shape, seed, transpose, outdim_parallel, version
+    c, events, *, w_mu, w_sigma, conn_prob, shape, seed, transpose, outdim_parallel
 ):
   log_p = np.log((1 - conn_prob) if (conn_prob < 1) else 1e-40)
   n_row, n_col = (shape[1], shape[0]) if transpose else shape
 
   vec_shape = c.get_shape(events)
   out_dtype = vec_shape.element_type()
   type_name = b'_float' if out_dtype == jnp.float32 else b'_double'
@@ -608,45 +582,32 @@
         xla_client.Shape.array_shape(out_dtype, (shape[1] if transpose else shape[0],), (0,)),
       )
     ),
   )
 
 
 def _matvec_prob_normal_gpu_translation(
-    c, vector, *, w_mu, w_sigma, conn_prob, shape, seed, transpose, outdim_parallel, version
+    c, vector, *, w_mu, w_sigma, conn_prob, shape, seed, transpose, outdim_parallel
 ):
   if gpu_ops is None:
     raise GPUOperatorNotFound(matvec_prob_homo_p.name)
 
   out_dtype = dtypes.canonicalize_dtype(float)
-  type_name = b'_float' if out_dtype == jnp.float32 else b'_double'
-
-  if version == 'v1':
-    version_name = b'_v1'
-    opaque = gpu_ops.build_jitconn_prob_normal_descriptor(shape[1] if transpose else shape[0],
+  type_name = _check_out_type(out_dtype)
+  opaque = gpu_ops.build_jitconn_prob_uniform_descriptor2(shape[1] if transpose else shape[0],
                                                           shape[0] if transpose else shape[1],
                                                           seed,
-                                                          conn_prob,
+                                                          math.ceil(1 / conn_prob) * 2 - 1,
                                                           w_mu,
                                                           w_sigma)
-  elif version == 'v2':
-    version_name = b'_v2'
-    opaque = gpu_ops.build_jitconn_prob_uniform_descriptor(shape[1] if transpose else shape[0],
-                                                           shape[0] if transpose else shape[1],
-                                                           seed,
-                                                           float(np.log((1 - conn_prob) if conn_prob < 1 else 1e-40)),
-                                                           w_mu,
-                                                           w_sigma)
-  else:
-    raise ValueError
 
   if outdim_parallel:
-    fn = b'gpu_matvec_prob_normal' + version_name + type_name
+    fn = b'gpu_matvec_prob_normal_v2' + type_name
   else:
-    fn = b'gpu_matvec_atomic_prob_normal' + version_name + type_name
+    fn = b'gpu_matvec_atomic_prob_normal_v2' + type_name
 
   return xla_client.ops.CustomCallWithLayout(
     c,
     fn,
     operands=(vector,),
     operand_shapes_with_layout=(c.get_shape(vector),),
     shape_with_layout=xla_client.Shape.tuple_shape(
@@ -655,55 +616,61 @@
       )
     ),
     opaque=opaque,
   )
 
 
 def _matvec_prob_normal_jvp(
-    primals, tangents, *, w_mu, w_sigma, conn_prob, shape, seed, transpose, outdim_parallel, version
+    primals, tangents, *, w_mu, w_sigma, conn_prob, shape, seed, transpose, outdim_parallel
 ):
   vector, = primals
   vector_dot, = tangents
   r = matvec_prob_normal_p.bind(vector,
                                 w_mu=w_mu,
                                 w_sigma=w_sigma,
                                 conn_prob=conn_prob,
                                 shape=shape,
                                 seed=seed,
                                 transpose=transpose,
-                                outdim_parallel=outdim_parallel,
-                                version=version)
+                                outdim_parallel=outdim_parallel)
   r_dot = matvec_prob_normal_p.bind(vector_dot,
                                     w_mu=w_mu,
                                     w_sigma=w_sigma,
                                     conn_prob=conn_prob,
                                     shape=shape,
                                     seed=seed,
                                     transpose=transpose,
-                                    outdim_parallel=outdim_parallel,
-                                    version=version)
+                                    outdim_parallel=outdim_parallel)
   return r, r_dot
 
 
 def _matvec_prob_normal_transpose(
-    ct, events, *, w_mu, w_sigma, conn_prob, shape, seed, transpose, outdim_parallel, version
+    ct, events, *, w_mu, w_sigma, conn_prob, shape, seed, transpose, outdim_parallel
 ):
   return matvec_prob_normal_p.bind(ct[0],
                                    w_mu=w_mu,
                                    w_sigma=w_sigma,
                                    conn_prob=conn_prob,
                                    seed=seed,
                                    shape=shape,
                                    transpose=not transpose,
-                                   outdim_parallel=not outdim_parallel,
-                                   version=version)
+                                   outdim_parallel=not outdim_parallel)
 
 
 matvec_prob_normal_p = Primitive('matvec_prob_normal')
 matvec_prob_normal_p.multiple_results = True
 matvec_prob_normal_p.def_abstract_eval(_matvec_prob_normal_abstract)
 matvec_prob_normal_p.def_impl(partial(xla.apply_primitive, matvec_prob_normal_p))
 xla.backend_specific_translations['cpu'][matvec_prob_normal_p] = _matvec_prob_normal_cpu_translation
 xla.backend_specific_translations['gpu'][matvec_prob_normal_p] = _matvec_prob_normal_gpu_translation
 register_general_batching(matvec_prob_normal_p)
 ad.primitive_jvps[matvec_prob_normal_p] = _matvec_prob_normal_jvp
 ad.primitive_transposes[matvec_prob_normal_p] = _matvec_prob_normal_transpose
+
+
+def _check_out_type(out_dtype):
+  if out_dtype == jnp.float32:
+    return b'_float'
+  elif out_dtype == jnp.float64:
+    return b'_double'
+  else:
+    raise TypeError(f'Only support float or double, while got {out_dtype}')
```

## brainpylib/_src/jitconn_ops/tests/event_matvec_jitconn_performance.py

```diff
@@ -2,15 +2,14 @@
 
 import brainpy as bp
 import brainpy.math as bm
 from jax import jit
 
 import brainpylib
 from brainpylib import jitconn_ops
-from brainpylib.jitconn_ops.event_matvec import event_matvec_prob_conn_homo_weight_v1
 
 
 def compare_sparse_ops(platform='cpu'):
   """
 
   GPU
   ---
@@ -214,28 +213,28 @@
     (int(5e5), int(5e5)),
   ]
 
   for shape in all_shapes:
     for prob in [0.01, 0.05, 0.1, 0.2, 0.4, 0.8]:
       for transpose in [True, False]:
         print(f'shape = {shape}, prob = {prob}, transpose = {transpose}')
-        f1 = jit(lambda e: event_matvec_prob_conn_homo_weight_v1(
-          e, weight, conn_prob=prob, shape=shape, seed=seed, transpose=transpose))
+        # f1 = jit(lambda e: event_matvec_prob_conn_homo_weight_v1(
+        #   e, weight, conn_prob=prob, shape=shape, seed=seed, transpose=transpose))
         f2 = jit(lambda e: jitconn_ops.event_matvec_prob_conn_homo_weight(
           e, weight, conn_prob=prob, shape=shape, seed=seed, transpose=transpose))
 
         rng = bm.random.RandomState()
         events = rng.random(shape[0] if transpose else shape[1]).value < prob
-        f1(events).block_until_ready()
+        # f1(events).block_until_ready()
         f2(events).block_until_ready()
 
-        t0 = time()
-        for _ in range(100):
-          f1(events).block_until_ready()
-        print(f'event_matvec_v1 {time() - t0} s')
+        # t0 = time()
+        # for _ in range(100):
+        #   f1(events).block_until_ready()
+        # print(f'event_matvec_v1 {time() - t0} s')
 
         t0 = time()
         for _ in range(100):
           f2(events).block_until_ready()
         print(f'event_matvec_v2 {time() - t0} s')
         print()
         bm.clear_buffer_memory()
```

## Comparing `brainpylib-0.1.7.dist-info/LICENSE` & `brainpylib-0.1.8.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `brainpylib-0.1.7.dist-info/METADATA` & `brainpylib-0.1.8.dist-info/METADATA`

 * *Files 8% similar despite different names*

```diff
@@ -1,12 +1,11 @@
 Metadata-Version: 2.1
 Name: brainpylib
-Version: 0.1.7
+Version: 0.1.8
 Summary: C++/CUDA Library for BrainPy
-Home-page: https://github.com/brainpy/brainpylib
 Author: BrainPy team
 Author-email: chao.brain@qq.com
 License: GPL-3.0 license
 Keywords: event-driven computation,sparse computation,brainpy
 Classifier: Natural Language :: English
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python
```

### html2text {}

```diff
@@ -1,25 +1,25 @@
-Metadata-Version: 2.1 Name: brainpylib Version: 0.1.7 Summary: C++/CUDA Library
-for BrainPy Home-page: https://github.com/brainpy/brainpylib Author: BrainPy
-team Author-email: chao.brain@qq.com License: GPL-3.0 license Keywords: event-
-driven computation,sparse computation,brainpy Classifier: Natural Language ::
-English Classifier: Operating System :: OS Independent Classifier: Programming
-Language :: Python Classifier: Programming Language :: Python :: 3 Classifier:
-Programming Language :: Python :: 3.7 Classifier: Programming Language ::
-Python :: 3.8 Classifier: Programming Language :: Python :: 3.9 Classifier:
-Programming Language :: Python :: 3.10 Classifier: Programming Language ::
-Python :: 3.11 Classifier: Intended Audience :: Science/Research Classifier:
-License :: OSI Approved :: Apache Software License Classifier: Topic ::
-Scientific/Engineering :: Bio-Informatics Classifier: Topic :: Scientific/
-Engineering :: Mathematics Classifier: Topic :: Scientific/Engineering ::
-Artificial Intelligence Classifier: Topic :: Software Development :: Libraries
-Requires-Python: >=3.7 Description-Content-Type: text/markdown License-File:
-LICENSE Requires-Dist: jax Requires-Dist: numba Requires-Dist: numpy Provides-
-Extra: test Requires-Dist: pytest ; extra == 'test' # brainpylib: C++/CUDA
-Library for [BrainPy](https://github.com/brainpy/BrainPy)
+Metadata-Version: 2.1 Name: brainpylib Version: 0.1.8 Summary: C++/CUDA Library
+for BrainPy Author: BrainPy team Author-email: chao.brain@qq.com License: GPL-
+3.0 license Keywords: event-driven computation,sparse computation,brainpy
+Classifier: Natural Language :: English Classifier: Operating System :: OS
+Independent Classifier: Programming Language :: Python Classifier: Programming
+Language :: Python :: 3 Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8 Classifier: Programming
+Language :: Python :: 3.9 Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11 Classifier: Intended
+Audience :: Science/Research Classifier: License :: OSI Approved :: Apache
+Software License Classifier: Topic :: Scientific/Engineering :: Bio-Informatics
+Classifier: Topic :: Scientific/Engineering :: Mathematics Classifier: Topic ::
+Scientific/Engineering :: Artificial Intelligence Classifier: Topic :: Software
+Development :: Libraries Requires-Python: >=3.7 Description-Content-Type: text/
+markdown License-File: LICENSE Requires-Dist: jax Requires-Dist: numba
+Requires-Dist: numpy Provides-Extra: test Requires-Dist: pytest ; extra ==
+'test' # brainpylib: C++/CUDA Library for [BrainPy](https://github.com/brainpy/
+BrainPy)
   [Supported_Python_Version] [LICENSE] [PyPI_version] [Linux_CI] [Linux_CI]
                                   [Linux_CI]
 ``brainpylib`` aims to provide operators for sparse and event-driven
 computations commonly used in brain dynamics programming. ## Install
 ``brainpylib`` is based on Python (>=3.7) and can be installed on Linux (Ubuntu
 16.04 or later), macOS (10.12 or later), and Windows platforms. Install the
 latest version of ``brainpylib``: ```bash $ pip install brainpylib -U ``` ##
```

## Comparing `brainpylib-0.1.7.dist-info/RECORD` & `brainpylib-0.1.8.dist-info/RECORD`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
-brainpylib/__init__.py,sha256=C1B1TGPYrm8onlxyBtM5jXN3BmdGrPNHittCK_DwbLA,549
+brainpylib/__init__.py,sha256=2ZpyAiTpB1ZRgB0Gn3b3zVq57qIi2GhS1oOVYiuQacQ,549
 brainpylib/compat.py,sha256=pb30F4D8N0IvCF7rBmt1NnzhPXmY_LGqPB4x1AVGzyA,384
-brainpylib/cpu_ops.cp39-win_amd64.pyd,sha256=ea503vlkAistoPljBMf1XyEq05quSEgKP_TMeWhxZCQ,156672
+brainpylib/cpu_ops.cp39-win_amd64.pyd,sha256=As7B0s53910vqJ2h5ZE0AF302Cz9MR58y0CKbk7iszA,156672
 brainpylib/event_ops.py,sha256=4hEFo7hhGWkXE1KMIqDf4EhGTTisjKlot4SJBRAST00,190
 brainpylib/jitconn_ops.py,sha256=6bok2P90chgRapKK2DkradPMKZjKrB65AWyxJed-XYk,756
 brainpylib/op_register.py,sha256=Xlsezoh04df8jjOU-t1lYb-fl4sr38cjrrKirKZ3WVE,193
 brainpylib/sparse_ops.py,sha256=h1aTgUX5TEWRe98eVHJ5eJIEq0_JDOwXcYihFGMluiU,392
 brainpylib/_src/__init__.py,sha256=lVL4VPz3c4kWCgjPtL4uuQLNmgwK92q1ffAsv5El02k,25
 brainpylib/_src/check.py,sha256=AdmlpKYqWLwR0MrcbzxtWEwu6JIfHxc2ENNscnJamsM,1393
 brainpylib/_src/errors.py,sha256=UruQ0oGXNnyVnrQQ8U7OQOrBD4D0THyrEm1GSUNWGBk,411
@@ -28,18 +28,18 @@
 brainpylib/_src/event_ops/event_info_collection.py,sha256=eWcPH54FnG7N9r6s1In6fApF7rNaZcZB_ibOtHbQgok,5279
 brainpylib/_src/event_ops/tests/event_info_VS_jax_operators.py,sha256=KTlrNNbUwjf8s0hMJoBljtARjnHyeS2sEb6b57f4jBE,6301
 brainpylib/_src/event_ops/tests/test_event_csr_matvec.py,sha256=SrQC-sKor3KpZVFM5L1nzdLEVhpdLn_3hj0_QHUk7YA,13768
 brainpylib/_src/event_ops/tests/test_event_csr_matvec_gpu.py,sha256=g83eye-o5gtO0W1wvrbv1Blq-fBkGufe3wFCoic_BJM,396
 brainpylib/_src/event_ops/tests/test_event_info_collection.py,sha256=I_c6fqO1Vcn5mGEQXB2dlTPxfWTwXALLG1Eul3-J34c,1801
 brainpylib/_src/event_ops/tests/test_event_info_collection_gpu.py,sha256=Gq7u2znpadMxqAVm-pKGwxdtkEAl0xNJGEctO_Xx_6A,386
 brainpylib/_src/jitconn_ops/__init__.py,sha256=eorq7uybKPFIVt-h4AaUGkTq6cTPn6-yjpGXcHjOZc0,195
-brainpylib/_src/jitconn_ops/event_matvec.py,sha256=-7CR1zVFD1OUAo8g9DASod4AzCR4pngr1QHKY--lYgM,24180
+brainpylib/_src/jitconn_ops/event_matvec.py,sha256=S0gIbWtCttp8GTT4iKCpnp8noiErGNMvMAtOQbBH464,23990
 brainpylib/_src/jitconn_ops/matmat.py,sha256=Omf7aJp7b3CxWZefWDT5Pn1vOtHQtLRDkUMzxz1Kq5U,16888
-brainpylib/_src/jitconn_ops/matvec.py,sha256=m9ZMe93bIcAdYNz6awEkkuKLttL3EaoHuUAjwbozB_E,28298
-brainpylib/_src/jitconn_ops/tests/event_matvec_jitconn_performance.py,sha256=-_QnsUMTaJskAQ2hCvVvBh34rFcM8CoyZ4MVaRPJel4,7734
+brainpylib/_src/jitconn_ops/matvec.py,sha256=cxDHYAA1afLEUmlWhqEsUan9jymdtpjsgiZSko4QZGo,26086
+brainpylib/_src/jitconn_ops/tests/event_matvec_jitconn_performance.py,sha256=fscOCrVyZTv06vxMoYAz_dHaloJt7xN_phwo7Op6Ato,7661
 brainpylib/_src/jitconn_ops/tests/matmat_jitconn_performance.py,sha256=CXY8pXy_8JZndBP5o2nOhfCj4P-_akr84hi6yFizZoQ,1694
 brainpylib/_src/jitconn_ops/tests/matmat_testcase.py,sha256=5xOPO4XIvSYCncQxpjgF1ZHbU_nju9no27GcznAGdrs,5073
 brainpylib/_src/jitconn_ops/tests/matvec_jitconn_performance.py,sha256=NtFegiXOebTE4ISoquHQNoeGxy3iPD7cXRZ6tcdwRrc,1586
 brainpylib/_src/jitconn_ops/tests/test_event_matvec.py,sha256=lmTqmfqZuKLHH2RGtnFI91CPUHa7P5htJT3oql12IBI,22837
 brainpylib/_src/jitconn_ops/tests/test_event_matvec_gpu.py,sha256=dYjZTic9qW9rWEq61LK05QTWuvGTT0BfG0f7nP9kAvA,404
 brainpylib/_src/jitconn_ops/tests/test_matmat_cpu.py,sha256=gczX42c2SUn0zCrGJpd1c-ou9RFm0yK96V4itCnBlNo,390
 brainpylib/_src/jitconn_ops/tests/test_matmat_gpu.py,sha256=AHnb8YwvRlD6CJVWoMc00De_x5HY3txVeGTcmr6qx2c,384
@@ -59,12 +59,12 @@
 brainpylib/_src/sparse_ops/sparse_csr_matvec.py,sha256=130Q-OXtuu5TEcuRe6b5xKgj0g-qwHA1KUQR3VsAX7E,7885
 brainpylib/_src/sparse_ops/utils.py,sha256=D22MTzbYHtDlXPn3eRTgiTHjb5bKOm7l9keOBS-XyV0,5302
 brainpylib/_src/sparse_ops/tests/csr_matvec_VS_cusparse_csr_matvec.py,sha256=Ldtre0T5ZuRBm1OhHlpWkqqfGgyQEYvl9ZK-QMRkHKc,2687
 brainpylib/_src/sparse_ops/tests/test_cusparse_matvec.py,sha256=uTgg2TK8s5UevljSXViIY402e_a5Hsnu47T-24PIbKI,10148
 brainpylib/_src/sparse_ops/tests/test_cusparse_matvec_gpu.py,sha256=BNiPVE35RLzy2lnELiDbB2RcsnQ4oOqRoU6zvbpe38U,405
 brainpylib/_src/sparse_ops/tests/test_sparse_csr_matvec.py,sha256=_dt5oTQyFdejJIYYZ-KCADpBh-3Z5TFwIvMEydQT20c,2610
 brainpylib/_src/sparse_ops/tests/test_sparse_csr_matvec_gpu.py,sha256=5Z0xfv_0Wu488AR90awMHNsmnPmDyMqVTL3HCoVzyI0,378
-brainpylib-0.1.7.dist-info/LICENSE,sha256=y_p8Hru855LwiNwIbkjJLmA0ZMwQQzHvJR5BVplHk1I,35773
-brainpylib-0.1.7.dist-info/METADATA,sha256=HxUEKxZQPkPqvL9xEDQZuE9PrbNkqeA9Q9sehpzIlJk,2898
-brainpylib-0.1.7.dist-info/WHEEL,sha256=eep6QWEFiQfg2wcclssb_WY-D33AnLYLnEKGA9Rn-VU,100
-brainpylib-0.1.7.dist-info/top_level.txt,sha256=YQsJGyIteid0gvfxhHzrTKbdqVTs-qRRhMDKN2vPB2g,30
-brainpylib-0.1.7.dist-info/RECORD,,
+brainpylib-0.1.8.dist-info/LICENSE,sha256=y_p8Hru855LwiNwIbkjJLmA0ZMwQQzHvJR5BVplHk1I,35773
+brainpylib-0.1.8.dist-info/METADATA,sha256=s0wazvjfrahB11-UPKwp_wpnfye-s5nUtVzUWNL0L34,2848
+brainpylib-0.1.8.dist-info/WHEEL,sha256=eep6QWEFiQfg2wcclssb_WY-D33AnLYLnEKGA9Rn-VU,100
+brainpylib-0.1.8.dist-info/top_level.txt,sha256=YQsJGyIteid0gvfxhHzrTKbdqVTs-qRRhMDKN2vPB2g,30
+brainpylib-0.1.8.dist-info/RECORD,,
```

