# Comparing `tmp/google_fhir_views-0.9.1-py3-none-any.whl.zip` & `tmp/google_fhir_views-0.9.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,30 +1,18 @@
-Zip file size: 88052 bytes, number of entries: 28
--rw-r-----  2.0 unx      577 b- defN 23-Apr-04 16:12 build/lib/google/fhir/views/__init__.py
--rw-r-----  2.0 unx     8279 b- defN 23-Apr-04 16:12 build/lib/google/fhir/views/_views_test_base.py
--rw-r-----  2.0 unx    22107 b- defN 23-Apr-04 16:12 build/lib/google/fhir/views/bigquery_runner.py
--rw-r-----  2.0 unx    50985 b- defN 23-Apr-04 16:12 build/lib/google/fhir/views/bigquery_runner_test.py
--rw-r-----  2.0 unx    50106 b- defN 23-Apr-04 16:12 build/lib/google/fhir/views/bigquery_runner_test_v2.py
--rw-r-----  2.0 unx     2536 b- defN 23-Apr-04 16:12 build/lib/google/fhir/views/r4.py
--rw-r-----  2.0 unx      906 b- defN 23-Apr-04 16:12 build/lib/google/fhir/views/r4_test.py
--rw-r-----  2.0 unx    12852 b- defN 23-Apr-04 16:12 build/lib/google/fhir/views/runner_utils.py
--rw-r-----  2.0 unx    10660 b- defN 23-Apr-04 16:12 build/lib/google/fhir/views/runner_utils_test.py
--rw-r-----  2.0 unx     4650 b- defN 23-Apr-04 16:12 build/lib/google/fhir/views/spark_runner.py
--rw-r-----  2.0 unx    10234 b- defN 23-Apr-04 16:12 build/lib/google/fhir/views/spark_runner_test.py
--rw-r-----  2.0 unx    13469 b- defN 23-Apr-04 16:12 build/lib/google/fhir/views/views.py
--rw-r-----  2.0 unx      577 b- defN 23-Apr-04 16:12 google/fhir/views/__init__.py
--rw-r-----  2.0 unx     8279 b- defN 23-Apr-04 16:12 google/fhir/views/_views_test_base.py
--rw-r-----  2.0 unx    22107 b- defN 23-Apr-04 16:12 google/fhir/views/bigquery_runner.py
--rw-r-----  2.0 unx    50985 b- defN 23-Apr-04 16:12 google/fhir/views/bigquery_runner_test.py
--rw-r-----  2.0 unx    50106 b- defN 23-Apr-04 16:12 google/fhir/views/bigquery_runner_test_v2.py
--rw-r-----  2.0 unx     2536 b- defN 23-Apr-04 16:12 google/fhir/views/r4.py
--rw-r-----  2.0 unx      906 b- defN 23-Apr-04 16:12 google/fhir/views/r4_test.py
--rw-r-----  2.0 unx    12852 b- defN 23-Apr-04 16:12 google/fhir/views/runner_utils.py
--rw-r-----  2.0 unx    10660 b- defN 23-Apr-04 16:12 google/fhir/views/runner_utils_test.py
--rw-r-----  2.0 unx     4650 b- defN 23-Apr-04 16:12 google/fhir/views/spark_runner.py
--rw-r-----  2.0 unx    10234 b- defN 23-Apr-04 16:12 google/fhir/views/spark_runner_test.py
--rw-r-----  2.0 unx    13469 b- defN 23-Apr-04 16:12 google/fhir/views/views.py
--rw-r-----  2.0 unx    11716 b- defN 23-Apr-04 17:13 google_fhir_views-0.9.1.dist-info/METADATA
--rw-r-----  2.0 unx       92 b- defN 23-Apr-04 17:13 google_fhir_views-0.9.1.dist-info/WHEEL
--rw-r-----  2.0 unx       13 b- defN 23-Apr-04 17:13 google_fhir_views-0.9.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     2649 b- defN 23-Apr-04 17:13 google_fhir_views-0.9.1.dist-info/RECORD
-28 files, 389192 bytes uncompressed, 83702 bytes compressed:  78.5%
+Zip file size: 49227 bytes, number of entries: 16
+-rw-r--r--  2.0 unx      577 b- defN 23-Apr-17 20:06 google/fhir/views/__init__.py
+-rw-r--r--  2.0 unx     8279 b- defN 23-Apr-17 20:06 google/fhir/views/_views_test_base.py
+-rw-r--r--  2.0 unx    19442 b- defN 23-Apr-17 20:06 google/fhir/views/bigquery_runner.py
+-rw-r--r--  2.0 unx    47444 b- defN 23-Apr-17 20:06 google/fhir/views/bigquery_runner_test.py
+-rw-r--r--  2.0 unx    47288 b- defN 23-Apr-17 20:06 google/fhir/views/bigquery_runner_test_v2.py
+-rw-r--r--  2.0 unx     2536 b- defN 23-Apr-17 20:06 google/fhir/views/r4.py
+-rw-r--r--  2.0 unx      906 b- defN 23-Apr-17 20:06 google/fhir/views/r4_test.py
+-rw-r--r--  2.0 unx    15637 b- defN 23-Apr-17 20:06 google/fhir/views/runner_utils.py
+-rw-r--r--  2.0 unx    14785 b- defN 23-Apr-17 20:06 google/fhir/views/runner_utils_test.py
+-rw-r--r--  2.0 unx     9278 b- defN 23-Apr-17 20:06 google/fhir/views/spark_runner.py
+-rw-r--r--  2.0 unx    14043 b- defN 23-Apr-17 20:06 google/fhir/views/spark_runner_test.py
+-rw-r--r--  2.0 unx    13469 b- defN 23-Apr-17 20:06 google/fhir/views/views.py
+-rw-r--r--  2.0 unx    11666 b- defN 23-Apr-17 20:08 google_fhir_views-0.9.2.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Apr-17 20:08 google_fhir_views-0.9.2.dist-info/WHEEL
+-rw-r--r--  2.0 unx        7 b- defN 23-Apr-17 20:08 google_fhir_views-0.9.2.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1433 b- defN 23-Apr-17 20:08 google_fhir_views-0.9.2.dist-info/RECORD
+16 files, 206882 bytes uncompressed, 46841 bytes compressed:  77.4%
```

## zipnote {}

```diff
@@ -1,43 +1,7 @@
-Filename: build/lib/google/fhir/views/__init__.py
-Comment: 
-
-Filename: build/lib/google/fhir/views/_views_test_base.py
-Comment: 
-
-Filename: build/lib/google/fhir/views/bigquery_runner.py
-Comment: 
-
-Filename: build/lib/google/fhir/views/bigquery_runner_test.py
-Comment: 
-
-Filename: build/lib/google/fhir/views/bigquery_runner_test_v2.py
-Comment: 
-
-Filename: build/lib/google/fhir/views/r4.py
-Comment: 
-
-Filename: build/lib/google/fhir/views/r4_test.py
-Comment: 
-
-Filename: build/lib/google/fhir/views/runner_utils.py
-Comment: 
-
-Filename: build/lib/google/fhir/views/runner_utils_test.py
-Comment: 
-
-Filename: build/lib/google/fhir/views/spark_runner.py
-Comment: 
-
-Filename: build/lib/google/fhir/views/spark_runner_test.py
-Comment: 
-
-Filename: build/lib/google/fhir/views/views.py
-Comment: 
-
 Filename: google/fhir/views/__init__.py
 Comment: 
 
 Filename: google/fhir/views/_views_test_base.py
 Comment: 
 
 Filename: google/fhir/views/bigquery_runner.py
@@ -66,20 +30,20 @@
 
 Filename: google/fhir/views/spark_runner_test.py
 Comment: 
 
 Filename: google/fhir/views/views.py
 Comment: 
 
-Filename: google_fhir_views-0.9.1.dist-info/METADATA
+Filename: google_fhir_views-0.9.2.dist-info/METADATA
 Comment: 
 
-Filename: google_fhir_views-0.9.1.dist-info/WHEEL
+Filename: google_fhir_views-0.9.2.dist-info/WHEEL
 Comment: 
 
-Filename: google_fhir_views-0.9.1.dist-info/top_level.txt
+Filename: google_fhir_views-0.9.2.dist-info/top_level.txt
 Comment: 
 
-Filename: google_fhir_views-0.9.1.dist-info/RECORD
+Filename: google_fhir_views-0.9.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## google/fhir/views/bigquery_runner.py

```diff
@@ -19,15 +19,14 @@
 can be consumed by other tools.
 """
 
 import re
 from typing import Iterable, Optional, Union, cast, Dict
 
 from google.cloud import bigquery
-import numpy
 import pandas
 import sqlalchemy
 import sqlalchemy_bigquery
 
 from google.fhir.r4.proto.core.resources import value_set_pb2
 from google.fhir.core.fhir_path import _bigquery_interpreter
 from google.fhir.core.fhir_path import _fhir_path_data_types
@@ -35,19 +34,14 @@
 from google.fhir.core.fhir_path import fhir_path
 from google.fhir.r4.terminology import terminology_service_client
 from google.fhir.r4.terminology import value_set_tables
 from google.fhir.r4.terminology import value_sets
 from google.fhir.views import runner_utils
 from google.fhir.views import views
 
-_CODEABLE_CONCEPT = 'http://hl7.org/fhir/StructureDefinition/CodeableConcept'
-_CODING = 'http://hl7.org/fhir/StructureDefinition/Coding'
-_CODE = 'http://hl7.org/fhir/StructureDefinition/Code'
-_STRING = 'http://hl7.org/fhirpath/System.String'
-
 
 class BigQueryRunner:
   """FHIR Views runner used to perform queries against BigQuery."""
 
   @classmethod
   def _to_dataset_ref(
       cls,
@@ -173,45 +167,22 @@
 
     if len(view.get_structdef_urls()) > 1 and not internal_v2:
       raise ValueError(
           'Cross Resource views are only allowed in '
           f'v2. {view.get_structdef_urls()}'
       )
 
-    fhir_context = view.get_fhir_path_context()
-    url = list(view.get_structdef_urls())[0]
-    struct_def = fhir_context.get_structure_definition(url)
-    deps = fhir_context.get_dependency_definitions(url)
-    deps.append(struct_def)
-    encoder = fhir_path.FhirPathStandardSqlEncoder(
-        deps,
-        options=fhir_path.SqlGenerationOptions(
-            value_set_codes_table='VALUESET_VIEW'
-        ),
-    )
-    if internal_v2:
-      encoder = _bigquery_interpreter.BigQuerySqlInterpreter(
-          value_set_codes_table='VALUESET_VIEW',
-      )
+    sql_generator = self._build_sql_generator(internal_v2, view)
+    sql_statement = sql_generator.build_sql_statement(include_patient_id_col)
 
-    # URLs to various expressions and tables:
-    dataset = f'{self._fhir_dataset.project}.{self._fhir_dataset.dataset_id}'
-    table_names = self._view_table_names(view)
     view_table_name = (
         f'{self._value_set_codes_table.project}'
         f'.{self._value_set_codes_table.dataset_id}'
         f'.{self._value_set_codes_table.table_id}'
     )
-
-    sql_generator = runner_utils.RunnerSqlGenerator(
-        view, encoder, dataset, table_names
-    )
-
-    sql_statement = sql_generator.build_sql_statement(include_patient_id_col)
-
     # Build the expression containing valueset content, which may be empty.
     valuesets_clause = sql_generator.build_valueset_expression(view_table_name)
 
     if limit is not None and limit < 1:
       raise ValueError('Query limits must be positive integers.')
     limit_clause = '' if limit is None else f' LIMIT {limit}'
 
@@ -229,56 +200,17 @@
     Returns:
       pandas.DataFrame: dataframe of the view contents.
 
     Raises:
       ValueError propagated from the BigQuery client if pandas is not installed.
     """
     df = self.run_query(view, limit).result().to_dataframe()
+    return runner_utils.clean_dataframe(df, view.get_select_expressions())
 
-    # If the view has expressions, we can narrow the non-scalar column list by
-    # checking only for list or struct columns.
-    select_columns = set(view.get_select_expressions().keys())
-    # Ignore the __base__ expression that exists by default.
-    select_columns.discard(views.BASE_BUILDER_KEY)
-
-    if select_columns:
-      non_scalar_cols = [
-          col
-          for (col, expr) in view.get_select_expressions().items()
-          if expr.return_type.returns_collection() or expr.return_type.fields()
-      ]
-    else:
-      # No fields were specified, so we must check any 'object' field
-      # in the dataframe.
-      non_scalar_cols = df.select_dtypes(include=['object']).columns.tolist()
-
-    # Helper function to recursively trim `None` values and empty arrays.
-    def trim_structs(item):
-      if isinstance(item, numpy.ndarray):
-        if not item.any():
-          return None
-        else:
-          return [trim_structs(child) for child in item]
-
-      if isinstance(item, dict):
-        result = {}
-        for key, value in item.items():
-          trimmed_value = trim_structs(value)
-          if trimmed_value is not None:
-            result[key] = trimmed_value
-        return result
-
-      return item
-
-    for col in non_scalar_cols:
-      df[col] = df[col].map(trim_structs)
-
-    return df
-
-  def create_bigquery_view(self, view: views.View, view_name: str) -> None:
+  def create_database_view(self, view: views.View, view_name: str) -> None:
     """Creates a BigQuery view with the given name in the runner's view_dataset.
 
     Args:
       view: the FHIR view that creates
       view_name: the view name passed to the CREATE OR REPLACE VIEW statement.
 
     Raises:
@@ -335,84 +267,47 @@
       A Pandas dataframe containing 'system', 'code', 'display', and 'count'
       columns for codeable concept and coding fields. 'system' and 'display'
       columns are omitted when summarzing raw code fields, since they do not
       have system or display values.
 
       The datframe is ordered by count is in descending order.
     """
+    expr_array_query = self._build_sql_generator(
+        internal_v2=False, view=view
+    ).build_select_for_summarize_code(code_expr)
+
     node_type = code_expr.get_node().return_type()
     if node_type and isinstance(node_type, _fhir_path_data_types.Collection):
       node_type = list(cast(_fhir_path_data_types.Collection, node_type).types)[
           0
       ]
 
-    # TODO(b/239733067): Add constraint filtering to code summarization.
-    if view.get_constraint_expressions():
-      raise NotImplementedError(
-          'Summarization of codes with view constraints not yet implemented.'
-      )
-
-    fhir_context = view.get_fhir_path_context()
-    # Workaround for v1 until it gets deprecated.
-    if len(view.get_structdef_urls()) > 1:
-      raise NotImplementedError(
-          'Summarization of codes with multiple resource views not yet'
-          ' implemented.'
-      )
-
-    url = list(view.get_structdef_urls())[0]
-    struct_def = fhir_context.get_structure_definition(url)
-    elem_def = next(
-        elem
-        for elem in struct_def.snapshot.element
-        if elem.path.value == struct_def.name.value
-    )
-
-    deps = fhir_context.get_dependency_definitions(url)
-    deps.append(struct_def)
-    encoder = fhir_path.FhirPathStandardSqlEncoder(deps)
-
-    select_expression = encoder.encode(
-        structure_definition=struct_def,
-        element_definition=elem_def,
-        fhir_path_expression=code_expr.fhir_path,
-        select_scalars_as_array=True,
-    )
-
-    # Build the select expression from the FHIR resource table.
-    table_names = self._view_table_names(view)
-    dataset = f'{self._fhir_dataset.project}.{self._fhir_dataset.dataset_id}'
-
-    if len(table_names.keys()) == 1:
-      # Query to get the array of code-like fields we will aggregate by.
-      expr_array_query = (
-          f'SELECT {select_expression} as target '
-          f'FROM `{dataset}`.{table_names[url]}'
-      )
-
     # Create a counting aggregation for the appropriate code-like structure.
-    if node_type.url == _CODEABLE_CONCEPT:
+    if node_type.url == runner_utils.CODEABLE_CONCEPT:
       count_query = (
           f'WITH c AS ({expr_array_query}) '
           'SELECT codings.system, codings.code, '
           'codings.display, COUNT(*) count '
           'FROM c, '
           'UNNEST(c.target) concepts, UNNEST(concepts.coding) as codings '
           'GROUP BY 1, 2, 3 ORDER BY count DESC'
       )
-    elif node_type.url == _CODING:
+    elif node_type.url == runner_utils.CODING:
       count_query = (
           f'WITH c AS ({expr_array_query}) '
           'SELECT codings.system, codings.code, '
           'codings.display, COUNT(*) count '
           'FROM c, '
           'UNNEST(c.target) codings '
           'GROUP BY 1, 2, 3 ORDER BY count DESC'
       )
-    elif node_type.url == _CODE or node_type.url == _STRING:
+    elif (
+        node_type.url == runner_utils.CODE
+        or node_type.url == runner_utils.STRING
+    ):
       # Assume simple strings are just code values. Since code is a type of
       # string, the current expression typing analysis may produce a string
       # type here so we accept both string and code.
       count_query = (
           f'WITH c AS ({expr_array_query}) '
           'SELECT code, COUNT(*) count '
           'FROM c, UNNEST(c.target) as code '
@@ -444,14 +339,37 @@
         bigquery.SchemaField('system', 'STRING', mode='REQUIRED'),
         bigquery.SchemaField('code', 'STRING', mode='REQUIRED'),
     ]
     table = bigquery.Table(self._value_set_codes_table, schema=schema)
     table.clustering_fields = ['valueseturi', 'code']
     return self._client.create_table(table, exists_ok=True)
 
+  def _build_sql_generator(self, internal_v2: bool, view: views.View):
+    """Build a RunnerSqlGenerator depending on the runner version."""
+    fhir_context = view.get_fhir_path_context()
+    url = list(view.get_structdef_urls())[0]
+    struct_def = fhir_context.get_structure_definition(url)
+    deps = fhir_context.get_dependency_definitions(url)
+    deps.append(struct_def)
+    encoder = fhir_path.FhirPathStandardSqlEncoder(
+        deps,
+        options=fhir_path.SqlGenerationOptions(
+            value_set_codes_table='VALUESET_VIEW'
+        ),
+    )
+    if internal_v2:
+      encoder = _bigquery_interpreter.BigQuerySqlInterpreter(
+          value_set_codes_table='VALUESET_VIEW',
+      )
+
+    # URLs to various expressions and tables:
+    dataset = f'{self._fhir_dataset.project}.{self._fhir_dataset.dataset_id}'
+    table_names = self._view_table_names(view)
+    return runner_utils.RunnerSqlGenerator(view, encoder, dataset, table_names)
+
   # TODO(b/201107372): Update FHIR-agnostic types to a protocol.
   def materialize_value_sets(
       self,
       value_set_protos: Iterable[value_set_pb2.ValueSet],
       batch_size: int = 500,
   ) -> None:
     """Materialize the given value sets into the value_set_codes_table.
```

## google/fhir/views/bigquery_runner_test.py

```diff
@@ -16,16 +16,14 @@
 
 import datetime
 import textwrap
 from typing import Optional, cast
 from unittest import mock
 
 from google.cloud import bigquery
-import numpy
-import pandas
 
 from absl.testing import absltest
 from absl.testing import parameterized
 from google.fhir.r4.proto.core.resources import value_set_pb2
 from google.fhir.core.fhir_path import context
 from google.fhir.core.utils import fhir_package
 from google.fhir.r4 import r4_package
@@ -252,108 +250,14 @@
         FROM UNNEST(ARRAY(SELECT comparison_
         FROM (SELECT (birthDate < '1960-01-01') AS comparison_)
         WHERE comparison_ IS NOT NULL)) AS logic_)"""
         ),
         born_before_1960,
     )
 
-  def testQueryToDataFrame_forPatient_succeeds(self):
-    pat = self._views.view_of('Patient')
-    simple_view = pat.select(
-        {'name': pat.name.given, 'birthDate': pat.birthDate}
-    )
-
-    mock_job = mock.create_autospec(bigquery.QueryJob, instance=True)
-    expected_mock_df = mock_job.result.return_value.to_dataframe.return_value
-    self.mock_bigquery_client.query.return_value = mock_job
-
-    returned_df = self.runner.to_dataframe(simple_view)
-    # Ensure expected SQL was passed to BigQuery and the dataframe was returned
-    # up the stack.
-    expected_sql = self.runner.to_sql(simple_view, include_patient_id_col=False)
-    self.mock_bigquery_client.query.assert_called_once_with(expected_sql)
-    self.assertEqual(expected_mock_df, returned_df)
-
-  def testQueryToDataFrame_TrimsStructWithSelect_succeeds(self):
-    pat = self._views.view_of('Patient')
-    simple_view = pat.select({
-        'name': pat.name.given,
-        'address': pat.address,
-        'maritalStatus': pat.maritalStatus,
-    })
-
-    mock_job = mock.create_autospec(bigquery.QueryJob, instance=True)
-    fake_df = pandas.DataFrame.from_dict({
-        'name': ['Bob'],
-        'address': [numpy.empty(shape=0)],
-        'maritalStatus': [{
-            'coding': numpy.array([{
-                'system': 'urn:examplesystem',
-                'code': 'S',
-                'display': None,
-            }]),
-            'text': None,
-        }],
-    })
-    mock_job.result.return_value.to_dataframe.return_value = fake_df
-    self.mock_bigquery_client.query.return_value = mock_job
-
-    returned_df = self.runner.to_dataframe(simple_view)
-    self.mock_bigquery_client.query.assert_called_once()
-
-    # Assert simple fields are unchanged.
-    self.assertEqual(['Bob'], returned_df['name'].values)
-
-    # Empty arrays should be converted to no values (so users can easily use
-    # dropna() and other pandas features).
-    self.assertEqual([None], returned_df['address'].values)
-
-    # 'None' values should be trimmed from nested structures.
-    self.assertEqual(
-        [{'coding': [{'system': 'urn:examplesystem', 'code': 'S'}]}],
-        returned_df['maritalStatus'].values,
-    )
-
-  def testQueryToDataFrame_TrimsStructNoSelect_succeeds(self):
-    # Use base query directly to ensure trimming logic works
-    # without explicit select columns.
-    pat = self._views.view_of('Patient')
-
-    mock_job = mock.create_autospec(bigquery.QueryJob, instance=True)
-    fake_df = pandas.DataFrame.from_dict({
-        'name': ['Bob'],
-        'address': [numpy.empty(shape=0)],
-        'maritalStatus': [{
-            'coding': numpy.array([{
-                'system': 'urn:examplesystem',
-                'code': 'S',
-                'display': None,
-            }]),
-            'text': None,
-        }],
-    })
-    mock_job.result.return_value.to_dataframe.return_value = fake_df
-    self.mock_bigquery_client.query.return_value = mock_job
-
-    returned_df = self.runner.to_dataframe(pat)
-    self.mock_bigquery_client.query.assert_called_once()
-
-    # Assert simple fields are unchanged.
-    self.assertEqual(['Bob'], returned_df['name'].values)
-
-    # Empty arrays should be converted to no values (so users can easily use
-    # dropna() and other pandas features).
-    self.assertEqual([None], returned_df['address'].values)
-
-    # 'None' values should be trimmed from nested structures.
-    self.assertEqual(
-        [{'coding': [{'system': 'urn:examplesystem', 'code': 'S'}]}],
-        returned_df['maritalStatus'].values,
-    )
-
   def testWhereMemberOfToSql_withValuesFromContext_succeeds(self):
     pat = self._views.view_of('Patient')
     unmarried_value_set = (
         r4.value_set('urn:test:valueset')
         .with_codes('http://hl7.org/fhir/v3/MaritalStatus', ['U', 'S'])
         .build()
     )
@@ -672,15 +576,15 @@
             'http://a-value.set/id'
         )
     )
 
     mock_job = mock.create_autospec(bigquery.QueryJob, instance=True)
     self.mock_bigquery_client.query.return_value = mock_job
 
-    self.runner.create_bigquery_view(statin_meds, 'statin_meds_view')
+    self.runner.create_database_view(statin_meds, 'statin_meds_view')
 
     # Ensure expected SQL was passed to BigQuery and job was returned.
     expected_sql = textwrap.dedent(
         'CREATE OR REPLACE VIEW '
         '`test_project.test_dataset.statin_meds_view` AS\n'
         f'{self.runner.to_sql(statin_meds, include_patient_id_col=False)}'
     )
@@ -715,15 +619,15 @@
     simple_view = pat.select(
         {'name': pat.name.given, 'birthDate': pat.birthDate}
     )
 
     mock_job = mock.create_autospec(bigquery.QueryJob, instance=True)
     self.mock_bigquery_client.query.return_value = mock_job
 
-    self.runner.create_bigquery_view(simple_view, 'simple_patient_view')
+    self.runner.create_database_view(simple_view, 'simple_patient_view')
 
     # Ensure expected SQL was passed to BigQuery and job was returned.
     expected_sql = (
         'CREATE OR REPLACE VIEW '
         '`test_project.test_dataset.simple_patient_view` AS\n'
         f'{self.runner.to_sql(simple_view, include_patient_id_col=False)}'
     )
```

## google/fhir/views/bigquery_runner_test_v2.py

```diff
@@ -15,16 +15,14 @@
 """Tests for bigquery_runner."""
 
 import datetime
 import textwrap
 from typing import Any, Optional, cast
 from unittest import mock
 from google.cloud import bigquery
-import numpy
-import pandas
 
 from absl.testing import absltest
 from absl.testing import parameterized
 from google.fhir.core.fhir_path import context
 from google.fhir.core.utils import fhir_package
 from google.fhir.r4 import r4_package
 from google.fhir.r4.terminology import terminology_service_client
@@ -302,91 +300,14 @@
     returned_df = self.runner.to_dataframe(simple_view)
     # Ensure expected SQL was passed to BigQuery and the dataframe was returned
     # up the stack.
     expected_sql = self.runner.to_sql(simple_view, include_patient_id_col=False)
     self.mock_bigquery_client.query.assert_called_once_with(expected_sql)
     self.assertEqual(expected_mock_df, returned_df)
 
-  def testQueryToDataFrame_TrimsStructWithSelect_succeeds(self):
-    """Test structure trimming of explicitly defined columns."""
-    pat = self._views.view_of('Patient')
-    simple_view = pat.select({
-        'name': pat.name.given,
-        'address': pat.address,
-        'maritalStatus': pat.maritalStatus,
-    })
-
-    mock_job = mock.create_autospec(bigquery.QueryJob, instance=True)
-    fake_df = pandas.DataFrame.from_dict({
-        'name': ['Bob'],
-        'address': [numpy.empty(shape=0)],
-        'maritalStatus': [{
-            'coding': numpy.array([{
-                'system': 'urn:examplesystem',
-                'code': 'S',
-                'display': None,
-            }]),
-            'text': None,
-        }],
-    })
-    mock_job.result.return_value.to_dataframe.return_value = fake_df
-    self.mock_bigquery_client.query.return_value = mock_job
-
-    returned_df = self.runner.to_dataframe(simple_view)
-    self.mock_bigquery_client.query.assert_called_once()
-
-    # Assert simple fields are unchanged.
-    self.assertEqual(['Bob'], returned_df['name'].values)
-
-    # Empty arrays should be converted to no values (so users can easily use
-    # dropna() and other pandas features).
-    self.assertEqual([None], returned_df['address'].values)
-
-    # 'None' values should be trimmed from nested structures.
-    self.assertEqual(
-        [{'coding': [{'system': 'urn:examplesystem', 'code': 'S'}]}],
-        returned_df['maritalStatus'].values,
-    )
-
-  def testQueryToDataFrame_TrimsStructNoSelect_succeeds(self):
-    """Test the base query directly to ensure struct trimming logic works."""
-    pat = self._views.view_of('Patient')
-
-    mock_job = mock.create_autospec(bigquery.QueryJob, instance=True)
-    fake_df = pandas.DataFrame.from_dict({
-        'name': ['Bob'],
-        'address': [numpy.empty(shape=0)],
-        'maritalStatus': [{
-            'coding': numpy.array([{
-                'system': 'urn:examplesystem',
-                'code': 'S',
-                'display': None,
-            }]),
-            'text': None,
-        }],
-    })
-    mock_job.result.return_value.to_dataframe.return_value = fake_df
-    self.mock_bigquery_client.query.return_value = mock_job
-
-    returned_df = self.runner.to_dataframe(pat)
-    self.mock_bigquery_client.query.assert_called_once()
-
-    # Assert simple fields are unchanged.
-    self.assertEqual(['Bob'], returned_df['name'].values)
-
-    # Empty arrays should be converted to no values (so users can easily use
-    # dropna() and other pandas features).
-    self.assertEqual([None], returned_df['address'].values)
-
-    # 'None' values should be trimmed from nested structures.
-    self.assertEqual(
-        [{'coding': [{'system': 'urn:examplesystem', 'code': 'S'}]}],
-        returned_df['maritalStatus'].values,
-    )
-
   def testTimestampComparison_succeeds(self):
     """Test timestamp comparison."""
 
     start_date = datetime.date(2012, 1, 1)
     end_date = datetime.date(2013, 1, 1)
 
     eob = self._views.view_of('ExplanationOfBenefit')
@@ -737,15 +658,15 @@
             'http://a-value.set/id'
         )
     )
 
     mock_job = mock.create_autospec(bigquery.QueryJob, instance=True)
     self.mock_bigquery_client.query.return_value = mock_job
 
-    self.runner.create_bigquery_view(statin_meds, 'statin_meds_view')
+    self.runner.create_database_view(statin_meds, 'statin_meds_view')
 
     # Ensure expected SQL was passed to BigQuery and job was returned.
     expected_sql = textwrap.dedent(
         'CREATE OR REPLACE VIEW '
         '`test_project.test_dataset.statin_meds_view` AS\n'
         f'{self.runner.to_sql(statin_meds, internal_v2=True, include_patient_id_col=False)}'
     )
@@ -782,15 +703,15 @@
     simple_view = pat.select(
         {'name': pat.name.given, 'birthDate': pat.birthDate}
     )
 
     mock_job = mock.create_autospec(bigquery.QueryJob, instance=True)
     self.mock_bigquery_client.query.return_value = mock_job
 
-    self.runner.create_bigquery_view(simple_view, 'simple_patient_view')
+    self.runner.create_database_view(simple_view, 'simple_patient_view')
 
     # Ensure expected SQL was passed to BigQuery and job was returned.
     expected_sql = (
         'CREATE OR REPLACE VIEW '
         '`test_project.test_dataset.simple_patient_view` AS\n'
         f'{self.runner.to_sql(simple_view, include_patient_id_col=False)}'
     )
```

## google/fhir/views/runner_utils.py

```diff
@@ -13,21 +13,30 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Utility class to generate SQL expressions for different encoders."""
 
 import itertools
 from typing import Collection, Mapping, MutableSequence, Optional, Sequence, Union
 
+import immutabledict
+import numpy
+import pandas as pd
+
 from google.fhir.core.fhir_path import _bigquery_interpreter
 from google.fhir.core.fhir_path import _evaluation
 from google.fhir.core.fhir_path import _spark_interpreter
 from google.fhir.core.fhir_path import expressions
 from google.fhir.core.fhir_path import fhir_path
 from google.fhir.views import views
 
+CODEABLE_CONCEPT = 'http://hl7.org/fhir/StructureDefinition/CodeableConcept'
+CODING = 'http://hl7.org/fhir/StructureDefinition/Coding'
+CODE = 'http://hl7.org/fhir/StructureDefinition/Code'
+STRING = 'http://hl7.org/fhirpath/System.String'
+
 
 class RunnerSqlGenerator:
   """Generates SQL for the different encoders."""
 
   def __init__(
       self,
       view: views.View,
@@ -289,14 +298,44 @@
       )
     if value_set_rows:
       rows_expression = '\nUNION ALL '.join(value_set_rows)
       return f'WITH VALUESET_VIEW AS ({rows_expression})\n'
 
     return ''
 
+  def build_select_for_summarize_code(
+      self, code_expr: expressions.Builder
+  ) -> str:
+    """Builds select statement for use in summarize_codes functions for runners."""
+    # TODO(b/239733067): Add constraint filtering to code summarization.
+    if self._view.get_constraint_expressions():
+      raise NotImplementedError(
+          'Summarization of codes with view constraints not yet implemented.'
+      )
+
+    # Workaround for v1 until it gets deprecated.
+    if (
+        len(self._view.get_structdef_urls()) > 1
+        or len(self._table_names.keys()) != 1
+    ):
+      raise NotImplementedError(
+          'Summarization of codes with multiple resource views not yet'
+          ' implemented.'
+      )
+
+    select_expression = self._encode(
+        builder=code_expr, select_scalars_as_array=True
+    )
+
+    url = list(self._view.get_structdef_urls())[0]
+    return (
+        f'SELECT {select_expression} as target '
+        f'FROM `{self._dataset}`.{self._table_names[url]}'
+    )
+
   def _encode(
       self,
       builder: expressions.Builder,
       select_scalars_as_array: bool,
       use_resource_alias: bool = False,
   ) -> str:
     """Encodes the expression to SQL."""
@@ -353,7 +392,61 @@
 
   # Recursively get valuesets from operands, which will terminate at
   # primitive leafs or message-level nodes.
   for operand_node in node.operands():
     nodes.extend(_memberof_nodes_from_node(operand_node))
 
   return nodes
+
+
+def clean_dataframe(
+    df: pd.DataFrame,
+    select_expressions_map: immutabledict.immutabledict[
+        str, expressions.Builder
+    ],
+) -> pd.DataFrame:
+  """Cleans dataframe retrieved from backend.
+
+  Args:
+    df: Dataframe to clean
+    select_expressions_map: If the view has expressions, we can narrow the
+      non-scalar column list by checking only for list or struct columns.
+
+  Returns:
+    Cleaned dataframe
+  """
+  select_columns = set(select_expressions_map.keys())
+  select_columns.discard(views.BASE_BUILDER_KEY)
+
+  if select_columns:
+    non_scalar_cols = [
+        col
+        for (col, expr) in select_expressions_map.items()
+        if expr.return_type.returns_collection() or expr.return_type.fields()
+    ]
+  else:
+    # No fields were specified, so we must check any 'object' field
+    # in the dataframe.
+    non_scalar_cols = df.select_dtypes(include=['object']).columns.tolist()
+
+  # Helper function to recursively trim `None` values and empty arrays.
+  def trim_structs(item):
+    if isinstance(item, numpy.ndarray):
+      if not item.any():
+        return None
+      else:
+        return [trim_structs(child) for child in item]
+
+    if isinstance(item, dict):
+      result = {}
+      for key, value in item.items():
+        trimmed_value = trim_structs(value)
+        if trimmed_value is not None:
+          result[key] = trimmed_value
+      return result
+
+    return item
+
+  for col in non_scalar_cols:
+    df[col] = df[col].map(trim_structs)
+
+  return df
```

## google/fhir/views/runner_utils_test.py

```diff
@@ -12,14 +12,17 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for runner_utils."""
 
 import textwrap
 
+import numpy
+import pandas as pd
+
 from absl.testing import absltest
 from google.fhir.core.fhir_path import _bigquery_interpreter
 from google.fhir.core.fhir_path import _spark_interpreter
 from google.fhir.core.fhir_path import context
 from google.fhir.core.fhir_path import fhir_path
 from google.fhir.core.utils import fhir_package
 from google.fhir.r4 import r4_package
@@ -242,10 +245,128 @@
         },
     ).build_valueset_expression('VALUESET_VIEW')
     expected_output = textwrap.dedent("""\
         WITH VALUESET_VIEW AS (SELECT valueseturi, valuesetversion, system, code FROM VALUESET_VIEW)
         """)
     self.assertMultiLineEqual(expected_output, sql_statement)
 
+  def testBuildSelectForSummarizeCode_succeeds(self):
+    """Tests summarizing codes."""
+    observation = self._views.view_of('Observation')
+    encoder = _bigquery_interpreter.BigQuerySqlInterpreter(
+        value_set_codes_table='VALUESET_VIEW'
+    )
+    returned_sql = runner_utils.RunnerSqlGenerator(
+        view=observation,
+        encoder=encoder,
+        dataset='test_project.test_dataset',
+        table_names={
+            'http://hl7.org/fhir/StructureDefinition/Observation': 'Observation'
+        },
+    ).build_select_for_summarize_code(observation.category)
+
+    # Ensure expected SQL was passed to BigQuery and the dataframe was returned
+    # up the stack.
+    expected_sql = (
+        'SELECT ARRAY(SELECT category_element_\n'
+        'FROM (SELECT category_element_\nFROM UNNEST(category) AS '
+        'category_element_ WITH OFFSET AS element_offset)\n'
+        'WHERE category_element_ IS NOT NULL) as target FROM '
+        '`test_project.test_dataset`.Observation'
+    )
+    self.assertEqual(expected_sql, returned_sql)
+
+  def testCleanDataFrame_forPatient_succeeds(self):
+    """Test to_dataframe()."""
+    patient = self._views.view_of('Patient')
+    simple_view = patient.select(
+        {'gender': patient.gender, 'birthDate': patient.birthDate}
+    )
+    expected_df = pd.DataFrame({
+        'gender': ['male', 'male', 'female', 'female', 'male'],
+        'birthDate': [
+            '2002-01-01',
+            '2009-08-02',
+            '2016-10-17',
+            '2017-05-23',
+            '2003-01-17',
+        ],
+    })
+
+    returned_df = runner_utils.clean_dataframe(
+        expected_df, simple_view.get_select_expressions()
+    )
+    self.assertTrue(expected_df.equals(returned_df))
+
+  def testCleanDataFrame_TrimsStructWithSelect_succeeds(self):
+    """Test structure trimming of explicitly defined columns."""
+    patient = self._views.view_of('Patient')
+    simple_view = patient.select({
+        'name': patient.name.given,
+        'address': patient.address,
+        'maritalStatus': patient.maritalStatus,
+    })
+    fake_df = pd.DataFrame.from_dict({
+        'name': ['Bob'],
+        'address': [numpy.empty(shape=0)],
+        'maritalStatus': [{
+            'coding': numpy.array([{
+                'system': 'urn:examplesystem',
+                'code': 'S',
+                'display': None,
+            }]),
+            'text': None,
+        }],
+    })
+
+    returned_df = runner_utils.clean_dataframe(
+        fake_df, simple_view.get_select_expressions()
+    )
+
+    # Assert simple fields are unchanged.
+    self.assertEqual(['Bob'], returned_df['name'].values)
+
+    # Empty arrays should be converted to no values (so users can easily use
+    # dropna() and other pandas features).
+    self.assertEqual([None], returned_df['address'].values)
+
+    # 'None' values should be trimmed from nested structures.
+    self.assertEqual(
+        [{'coding': [{'system': 'urn:examplesystem', 'code': 'S'}]}],
+        returned_df['maritalStatus'].values,
+    )
+
+  def testCleanDataFrame_TrimsStructNoSelect_succeeds(self):
+    """Test the base query directly to ensure struct trimming logic works."""
+    fake_df = pd.DataFrame.from_dict({
+        'name': ['Bob'],
+        'address': [numpy.empty(shape=0)],
+        'maritalStatus': [{
+            'coding': numpy.array([{
+                'system': 'urn:examplesystem',
+                'code': 'S',
+                'display': None,
+            }]),
+            'text': None,
+        }],
+    })
+
+    returned_df = runner_utils.clean_dataframe(
+        fake_df, self._views.view_of('Patient').get_select_expressions()
+    )
+
+    # Assert simple fields are unchanged.
+    self.assertEqual(['Bob'], returned_df['name'].values)
+
+    # Empty arrays should be converted to no values (so users can easily use
+    # dropna() and other pandas features).
+    self.assertEqual([None], returned_df['address'].values)
+
+    # 'None' values should be trimmed from nested structures.
+    self.assertEqual(
+        [{'coding': [{'system': 'urn:examplesystem', 'code': 'S'}]}],
+        returned_df['maritalStatus'].values,
+    )
+
 
 if __name__ == '__main__':
   absltest.main()
```

## google/fhir/views/spark_runner.py

```diff
@@ -16,19 +16,22 @@
 
 This module allows users to run FHIR Views against Spark. Users may retrieve
 results through the Spark library used here, or create Spark views that
 can be consumed by other tools.
 """
 
 import re
-from typing import Dict, Optional
+from typing import Dict, Optional, cast
 
+import pandas
 from sqlalchemy import engine
 
+from google.fhir.core.fhir_path import _fhir_path_data_types
 from google.fhir.core.fhir_path import _spark_interpreter
+from google.fhir.core.fhir_path import expressions
 from google.fhir.views import runner_utils
 from google.fhir.views import views
 
 
 class SparkRunner:
   """FHIR Views runner used to perform queries against Spark."""
 
@@ -121,7 +124,127 @@
         name = (
             re.sub(pattern=r'([A-Z]+)', repl=r'_\1', string=name)
             .lower()
             .lstrip('_')
         )
       names[structdef_url] = name
     return names
+
+  def to_dataframe(
+      self, view: views.View, limit: Optional[int] = None
+  ) -> pandas.DataFrame:
+    """Returns a Pandas dataframe of the results.
+
+    Args:
+      view: the view that defines the query to run.
+      limit: optional limit of the number of items to return.
+
+    Returns:
+      pandas.DataFrame: dataframe of the view contents.
+
+    Raises:
+      ValueError propagated from the Spark client if pandas is not installed.
+    """
+    df = pandas.read_sql_query(
+        sql=self.to_sql(view, limit=limit, include_patient_id_col=False),
+        con=self._engine,
+    )
+    return runner_utils.clean_dataframe(df, view.get_select_expressions())
+
+  def summarize_codes(
+      self, view: views.View, code_expr: expressions.Builder
+  ) -> pandas.DataFrame:
+    """Returns a summary count of distinct code values for the given expression.
+
+    This method is primarily intended for exploratory data analysis, so users
+    new to a dataset can quickly see the most common code values in the system.
+
+    Here is an example usage:
+
+    >>> obs = views.view_of('Observation')
+    >>> obs_codes_count_df = runner.summarize_codes(obs, obs.code)
+    >>> obs_category_count_df = runner.summarize_codes(obs, obs.category)
+
+    It also works for nested fields, like:
+
+    >>> pat = views.view_of('Patient')
+    >>> rel_count_df = runner.summarize_codes(pat, pat.contact.relationship)
+
+    Args:
+      view: the view containing code values to summarize.
+      code_expr: a FHIRPath expression referencing a codeable concept, coding,
+        or code field to count.
+
+    Returns:
+      A Pandas dataframe containing 'system', 'code', 'display', and 'count'
+      columns for codeable concept and coding fields. 'system' and 'display'
+      columns are omitted when summarzing raw code fields, since they do not
+      have system or display values.
+
+      The datframe is ordered by count is in descending order.
+    """
+    expr_array_query = runner_utils.RunnerSqlGenerator(
+        view=view,
+        encoder=_spark_interpreter.SparkSqlInterpreter(),
+        dataset=f'{self._fhir_dataset}',
+        table_names=self._view_table_names(view),
+    ).build_select_for_summarize_code(code_expr)
+
+    node_type = code_expr.get_node().return_type()
+    if node_type and isinstance(node_type, _fhir_path_data_types.Collection):
+      node_type = list(cast(_fhir_path_data_types.Collection, node_type).types)[
+          0
+      ]
+
+    # Create a counting aggregation for the appropriate code-like structure.
+    if node_type.url == runner_utils.CODEABLE_CONCEPT:
+      count_query = (
+          f'WITH c AS ({expr_array_query}) '
+          'SELECT codings.system, codings.code, '
+          'codings.display, COUNT(*) count '
+          'FROM c '
+          'LATERAL VIEW EXPLODE(c.target) AS concepts '
+          'LATERAL VIEW EXPLODE(concepts.coding) AS codings '
+          'GROUP BY 1, 2, 3 ORDER BY count DESC'
+      )
+    elif node_type.url == runner_utils.CODING:
+      count_query = (
+          f'WITH c AS ({expr_array_query}) '
+          'SELECT codings.system, codings.code, '
+          'codings.display, COUNT(*) count '
+          'FROM c '
+          'LATERAL VIEW EXPLODE(c.target) AS codings '
+          'GROUP BY 1, 2, 3 ORDER BY count DESC'
+      )
+    elif (
+        node_type.url == runner_utils.CODE
+        or node_type.url == runner_utils.STRING
+    ):
+      # Assume simple strings are just code values. Since code is a type of
+      # string, the current expression typing analysis may produce a string
+      # type here so we accept both string and code.
+      count_query = (
+          f'WITH c AS ({expr_array_query}) '
+          'SELECT code, COUNT(*) count '
+          'FROM c LATERAL VIEW EXPLODE(c.target) as code '
+          'GROUP BY 1 ORDER BY count DESC'
+      )
+    else:
+      raise ValueError(
+          'Field must be a FHIR CodeableConcept, Coding, or Code; '
+          f'got {node_type.url}.'
+      )
+
+    return pandas.read_sql_query(sql=count_query, con=self._engine)
+
+  def create_database_view(self, view: views.View, view_name: str) -> None:
+    """Creates a Spark view with the given name in the runner's view_dataset.
+
+    Args:
+      view: the FHIR view that creates
+      view_name: the view name passed to the CREATE OR REPLACE VIEW statement.
+    """
+    view_sql = (
+        f'CREATE OR REPLACE VIEW {self._view_dataset}.{view_name} AS\n'
+        f'{self.to_sql(view, include_patient_id_col=False)}'
+    )
+    self._engine.execute(view_sql).fetchall()
```

## google/fhir/views/spark_runner_test.py

```diff
@@ -14,14 +14,16 @@
 # limitations under the License.
 """Tests for spark_runner."""
 
 import datetime
 
 from typing import Optional
 from unittest import mock
+
+import pandas
 from sqlalchemy import engine
 
 from absl.testing import absltest
 from absl.testing import parameterized
 from google.fhir.core.fhir_path import context
 from google.fhir.r4 import r4_package
 from google.fhir.views import r4
@@ -57,14 +59,18 @@
 
 
 class SparkRunnerTest(parameterized.TestCase):
   """Tests the Spark Runner."""
 
   def setUp(self):
     super().setUp()
+    self.mock_read_sql_query = self.enter_context(
+        mock.patch.object(pandas, 'read_sql_query', autospec=True)
+    )
+
     self.addCleanup(mock.patch.stopall)
     self.mock_spark_engine = mock.create_autospec(engine.Engine, instance=True)
     self.runner = spark_runner.SparkRunner(
         query_engine=self.mock_spark_engine,
         fhir_dataset='default',
         value_set_codes_table='vs_table',
     )
@@ -260,10 +266,95 @@
             ' (SELECT telecom AS telecom_element_))) WHERE start IS NOT NULL)'
             ' AS telecom,(SELECT id) AS __patientId__ FROM `default`.Patient'
         ),
         view=telecom,
         runner=self.runner,
     )
 
+  def testQueryToDataFrame_forPatient_succeeds(self):
+    """Test to_dataframe()."""
+    patient = self._views.view_of('Patient')
+    simple_view = patient.select(
+        {'gender': patient.gender, 'birthDate': patient.birthDate}
+    )
+    expected_df = pandas.DataFrame({
+        'gender': ['male', 'male', 'female', 'female', 'male'],
+        'birthDate': [
+            '2002-01-01',
+            '2009-08-02',
+            '2016-10-17',
+            '2017-05-23',
+            '2003-01-17',
+        ],
+    })
+    self.mock_read_sql_query.return_value = expected_df
+
+    returned_df = self.runner.to_dataframe(simple_view)
+    self.assertTrue(expected_df.equals(returned_df))
+
+  def testSummarizeCodes_forObservationCodeable_succeeds(self):
+    obs = self._views.view_of('Observation')
+    self.runner.summarize_codes(obs, obs.category)
+    expected_sql = (
+        'WITH c AS (SELECT (SELECT COLLECT_LIST(category_element_)\nFROM'
+        ' (SELECT category_element_\nFROM (SELECT EXPLODE(category_element_) AS'
+        ' category_element_ FROM (SELECT category AS'
+        ' category_element_)))\nWHERE category_element_ IS NOT NULL) as target'
+        ' FROM `default`.Observation) SELECT codings.system, codings.code,'
+        ' codings.display, COUNT(*) count FROM c LATERAL VIEW EXPLODE(c.target)'
+        ' AS concepts LATERAL VIEW EXPLODE(concepts.coding) AS codings GROUP BY'
+        ' 1, 2, 3 ORDER BY count DESC'
+    )
+    self.mock_read_sql_query.assert_called_once_with(
+        expected_sql, self.mock_spark_engine
+    )
+
+  def testSummarizeCodes_forObservationStatusCode_succeeds(self):
+    obs = self._views.view_of('Observation')
+    self.runner.summarize_codes(obs, obs.status)
+    expected_sql = (
+        'WITH c AS (SELECT (SELECT COLLECT_LIST(status)\nFROM (SELECT'
+        ' status)\nWHERE status IS NOT NULL) as target FROM'
+        ' `default`.Observation) SELECT code, COUNT(*) count FROM c LATERAL'
+        ' VIEW EXPLODE(c.target) as code GROUP BY 1 ORDER BY count DESC'
+    )
+    self.mock_read_sql_query.assert_called_once_with(
+        expected_sql, self.mock_spark_engine
+    )
+
+  def testSummarizeCodes_forObservationCoding_succeeds(self):
+    obs = self._views.view_of('Observation')
+    self.runner.summarize_codes(obs, obs.code.coding)
+    expected_sql = (
+        'WITH c AS (SELECT (SELECT COLLECT_LIST(coding_element_)\nFROM (SELECT'
+        ' coding_element_\nFROM (SELECT code) LATERAL VIEW'
+        ' POSEXPLODE(code.coding) AS index_coding_element_,'
+        ' coding_element_)\nWHERE coding_element_ IS NOT NULL) as target FROM'
+        ' `default`.Observation) SELECT codings.system, codings.code,'
+        ' codings.display, COUNT(*) count FROM c LATERAL VIEW EXPLODE(c.target)'
+        ' AS codings GROUP BY 1, 2, 3 ORDER BY count DESC'
+    )
+    self.mock_read_sql_query.assert_called_once_with(
+        expected_sql, self.mock_spark_engine
+    )
+
+  def testSummarizeCodes_forObservationNonCodeField_raisesError(self):
+    obs = self._views.view_of('Observation')
+    with self.assertRaises(ValueError):
+      self.runner.summarize_codes(obs, obs.referenceRange)
+
+  def testCreateView_forPatient_succeeds(self):
+    """Tests creating a view for a Patient."""
+    pat = self._views.view_of('Patient')
+    simple_view = pat.select(
+        {'name': pat.name.given, 'birthDate': pat.birthDate}
+    )
+    self.runner.create_database_view(simple_view, 'simple_patient_view')
+    expected_sql = (
+        'CREATE OR REPLACE VIEW '
+        'default.simple_patient_view AS\n'
+        f'{self.runner.to_sql(simple_view, include_patient_id_col=False)}'
+    )
+    self.mock_spark_engine.execute.assert_called_once_with(expected_sql)
 
 if __name__ == '__main__':
   absltest.main()
```

## Comparing `google_fhir_views-0.9.1.dist-info/METADATA` & `google_fhir_views-0.9.2.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,41 +1,40 @@
 Metadata-Version: 2.1
 Name: google-fhir-views
-Version: 0.9.1
+Version: 0.9.2
 Summary: Tools to create views of FHIR data for analysis.
 Home-page: https://github.com/google/fhir-py
 Download-URL: https://github.com/google-py/fhir/releases
 Author: Google LLC
 Author-email: google-fhir-pypi@google.com
 License: Apache 2.0
 Keywords: google,fhir,python,healthcare
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: MacOS :: MacOS X
 Classifier: Operating System :: POSIX :: Linux
-Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
-Requires-Python: >=3.7, <3.11
+Requires-Python: >=3.8, <3.11
 Description-Content-Type: text/markdown
 Requires-Dist: absl-py (~=1.1)
-Requires-Dist: google-fhir-core (~=0.9.1)
+Requires-Dist: google-fhir-core (~=0.9.2)
 Requires-Dist: immutabledict (~=2.2)
 Requires-Dist: pandas (~=1.1)
 Requires-Dist: protobuf (~=3.19)
 Requires-Dist: python-dateutil (~=2.8)
 Requires-Dist: requests-mock (~=1.9)
 Requires-Dist: requests (~=2.27)
 Requires-Dist: backports.zoneinfo (~=0.2.1) ; python_version < "3.9"
 Provides-Extra: bigquery
 Requires-Dist: google-cloud-bigquery (~=3.1) ; extra == 'bigquery'
-Requires-Dist: google-fhir-core[bigquery] (~=0.9.1) ; extra == 'bigquery'
+Requires-Dist: google-fhir-core[bigquery] (~=0.9.2) ; extra == 'bigquery'
 Requires-Dist: sqlalchemy-bigquery (~=1.4) ; extra == 'bigquery'
 Requires-Dist: sqlalchemy (~=1.4) ; extra == 'bigquery'
 Provides-Extra: r4
-Requires-Dist: google-fhir-r4 (~=0.9.1) ; extra == 'r4'
+Requires-Dist: google-fhir-r4 (~=0.9.2) ; extra == 'r4'
 
 # FHIR Views
 
 ## Introduction
 
 FHIR Views is a way to define simple, tabular views over complex FHIR data and
 turn them into queries that use
@@ -234,32 +233,32 @@
 ```py
 injury_conds =  cond.select({
     'id': cond.id,
     'patientId': cond.subject.idFor('Patient'),
     'codes': cond.code}
     ).where(cond.code.memberOf(injury_value_set_url))
 
-runner.create_bigquery_view(injury_conds, 'injury_conditions')
+runner.create_database_view(injury_conds, 'injury_conditions')
 ```
 
 ## Saving FHIR Views as BigQuery Views
 
 While `runner.to_dataframe` is convenient to retrieve data for local analysis,
 it's often useful to create such flattened views in BigQuery itself. They can be
 easily queried with much simpler SQL, or used by a variety of business
 intelligence or other data analysis tools.
 
-For this reason, the BigQueryRunner offers a `create_bigquery_view` method that
+For this reason, the BigQueryRunner offers a `create_database_view` method that
 will convert the view definition into a
 [BigQuery View](https://cloud.google.com/bigquery/docs/views), which can then
 just be consumed as if it was a first-class table that is updated when the
 underlying data is updated. Here's an example:
 
 ```py
-runner.create_bigquery_view(ldl_obs, 'ldl_observations')
+runner.create_database_view(ldl_obs, 'ldl_observations')
 ```
 
 By default the view is created in the fhir_dataset used by the runner, but this
 isn't always desirable (for example, a user may want to do their analysis in
 their own, isolated dataset). Therefore it's common to specify a `view_dataset`
 when creating the runner as the target for any views created. Here's an example:
```

