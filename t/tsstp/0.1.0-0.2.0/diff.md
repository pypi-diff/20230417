# Comparing `tmp/TSSTP-0.1.0-py3-none-any.whl.zip` & `tmp/tsstp-0.2.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,40 +1,42 @@
-Zip file size: 28910 bytes, number of entries: 38
--rw-rw-rw-  2.0 fat        0 b- defN 23-Feb-22 02:26 test/__init__.py
--rw-rw-rw-  2.0 fat     2820 b- defN 23-Feb-22 02:26 test/test_data_parse.py
--rw-rw-rw-  2.0 fat      636 b- defN 23-Feb-22 02:26 test/test_filter.py
--rw-rw-rw-  2.0 fat     1300 b- defN 23-Feb-22 02:26 test/test_if_match.py
--rw-rw-rw-  2.0 fat     1469 b- defN 23-Feb-22 02:26 test/test_loop_match.py
--rw-rw-rw-  2.0 fat     1126 b- defN 23-Feb-22 02:26 test/test_match.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Feb-22 02:26 test/test_output.py
--rw-rw-rw-  2.0 fat      839 b- defN 23-Feb-22 02:26 test/test_poscar_parse.py
--rw-rw-rw-  2.0 fat      396 b- defN 23-Feb-22 02:26 test/test_search.py
--rw-rw-rw-  2.0 fat     1491 b- defN 23-Feb-22 02:26 test/test_template_match.py
--rw-rw-rw-  2.0 fat     2547 b- defN 23-Feb-22 02:26 test/test_template_parse.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Feb-22 02:26 test/test_template_str_match.py
--rw-rw-rw-  2.0 fat      703 b- defN 23-Feb-22 02:26 test/test_xdacar_parse.py
--rw-rw-rw-  2.0 fat      825 b- defN 23-Feb-22 02:26 test/testsymbolnode.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Feb-22 02:26 tsstp/__init__.py
--rw-rw-rw-  2.0 fat    14747 b- defN 23-Feb-22 02:26 tsstp/datatemplate.py
--rw-rw-rw-  2.0 fat     3500 b- defN 23-Feb-22 02:26 tsstp/extract_symbol.py
--rw-rw-rw-  2.0 fat     1049 b- defN 23-Feb-22 02:26 tsstp/filter.py
--rw-rw-rw-  2.0 fat     3603 b- defN 23-Feb-22 02:26 tsstp/internal_ds.py
--rw-rw-rw-  2.0 fat     2717 b- defN 23-Feb-22 02:26 tsstp/semantic_model.py
--rw-rw-rw-  2.0 fat    10086 b- defN 23-Feb-22 02:26 tsstp/template_parser.py
--rw-rw-rw-  2.0 fat     4966 b- defN 23-Feb-22 02:26 tsstp/utils.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Feb-22 02:26 tsstp/input/__init__.py
--rw-rw-rw-  2.0 fat     7342 b- defN 23-Feb-22 02:26 tsstp/input/file_input.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Feb-22 02:26 tsstp/matcher/__init__.py
--rw-rw-rw-  2.0 fat     2022 b- defN 23-Feb-22 02:26 tsstp/matcher/if_matcher.py
--rw-rw-rw-  2.0 fat     3756 b- defN 23-Feb-22 02:26 tsstp/matcher/loop_matcher.py
--rw-rw-rw-  2.0 fat     1904 b- defN 23-Feb-22 02:26 tsstp/matcher/multi_template_matcher.py
--rw-rw-rw-  2.0 fat     1113 b- defN 23-Feb-22 02:26 tsstp/matcher/string_matcher.py
--rw-rw-rw-  2.0 fat     2877 b- defN 23-Feb-22 02:26 tsstp/matcher/template_matcher.py
--rw-rw-rw-  2.0 fat     1390 b- defN 23-Feb-22 02:26 tsstp/matcher/template_str_matcher.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Feb-22 02:26 tsstp/output/__init__.py
--rw-rw-rw-  2.0 fat      710 b- defN 23-Feb-22 02:26 tsstp/output/result.py
--rw-rw-rw-  2.0 fat      313 b- defN 23-Mar-13 03:04 TSSTP-0.1.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Mar-13 03:04 TSSTP-0.1.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       41 b- defN 23-Mar-13 03:04 TSSTP-0.1.0.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat       11 b- defN 23-Mar-13 03:04 TSSTP-0.1.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     3025 b- defN 23-Mar-13 03:04 TSSTP-0.1.0.dist-info/RECORD
-38 files, 79416 bytes uncompressed, 24110 bytes compressed:  69.6%
+Zip file size: 32672 bytes, number of entries: 40
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-17 01:10 test/__init__.py
+-rw-rw-rw-  2.0 fat     2917 b- defN 23-Apr-17 01:10 test/benchmark_test.py
+-rw-rw-rw-  2.0 fat     2945 b- defN 23-Apr-17 01:10 test/test_data_parse.py
+-rw-rw-rw-  2.0 fat      636 b- defN 23-Apr-17 01:10 test/test_filter.py
+-rw-rw-rw-  2.0 fat     1302 b- defN 23-Apr-17 01:10 test/test_if_match.py
+-rw-rw-rw-  2.0 fat     1469 b- defN 23-Apr-17 01:10 test/test_loop_match.py
+-rw-rw-rw-  2.0 fat     1597 b- defN 23-Apr-17 01:10 test/test_match.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-17 01:10 test/test_output.py
+-rw-rw-rw-  2.0 fat     1380 b- defN 23-Apr-17 01:10 test/test_poscar_parse.py
+-rw-rw-rw-  2.0 fat      538 b- defN 23-Apr-17 01:10 test/test_search.py
+-rw-rw-rw-  2.0 fat     1491 b- defN 23-Apr-17 01:10 test/test_template_match.py
+-rw-rw-rw-  2.0 fat     3418 b- defN 23-Apr-17 01:10 test/test_template_parse.py
+-rw-rw-rw-  2.0 fat     1203 b- defN 23-Apr-17 01:10 test/test_xdacar_parse.py
+-rw-rw-rw-  2.0 fat      825 b- defN 23-Apr-17 01:10 test/testsymbolnode.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-17 01:10 tsstp/__init__.py
+-rw-rw-rw-  2.0 fat    13605 b- defN 23-Apr-17 01:10 tsstp/datatemplate.py
+-rw-rw-rw-  2.0 fat     3285 b- defN 23-Apr-17 01:10 tsstp/extract_symbol.py
+-rw-rw-rw-  2.0 fat     1049 b- defN 23-Apr-17 01:10 tsstp/filter.py
+-rw-rw-rw-  2.0 fat     3603 b- defN 23-Apr-17 01:10 tsstp/internal_ds.py
+-rw-rw-rw-  2.0 fat     2717 b- defN 23-Apr-17 01:10 tsstp/semantic_model.py
+-rw-rw-rw-  2.0 fat     1950 b- defN 23-Apr-17 01:10 tsstp/template_grammar.lark
+-rw-rw-rw-  2.0 fat    10086 b- defN 23-Apr-17 01:10 tsstp/template_parser.py
+-rw-rw-rw-  2.0 fat     4799 b- defN 23-Apr-17 01:10 tsstp/utils.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-17 01:10 tsstp/input/__init__.py
+-rw-rw-rw-  2.0 fat     8472 b- defN 23-Apr-17 01:10 tsstp/input/file_input.py
+-rw-rw-rw-  2.0 fat      634 b- defN 23-Apr-17 01:10 tsstp/input/token_grammar.lark
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-17 01:10 tsstp/matcher/__init__.py
+-rw-rw-rw-  2.0 fat     1861 b- defN 23-Apr-17 01:10 tsstp/matcher/if_matcher.py
+-rw-rw-rw-  2.0 fat     3199 b- defN 23-Apr-17 01:10 tsstp/matcher/loop_matcher.py
+-rw-rw-rw-  2.0 fat     1903 b- defN 23-Apr-17 01:10 tsstp/matcher/multi_template_matcher.py
+-rw-rw-rw-  2.0 fat     1113 b- defN 23-Apr-17 01:10 tsstp/matcher/string_matcher.py
+-rw-rw-rw-  2.0 fat     2727 b- defN 23-Apr-17 01:10 tsstp/matcher/template_matcher.py
+-rw-rw-rw-  2.0 fat     1390 b- defN 23-Apr-17 01:10 tsstp/matcher/template_str_matcher.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-17 01:10 tsstp/output/__init__.py
+-rw-rw-rw-  2.0 fat      710 b- defN 23-Apr-17 01:10 tsstp/output/result.py
+-rw-rw-rw-  2.0 fat     2769 b- defN 23-Apr-17 01:22 tsstp-0.2.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Apr-17 01:22 tsstp-0.2.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       59 b- defN 23-Apr-17 01:22 tsstp-0.2.0.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat       11 b- defN 23-Apr-17 01:22 tsstp-0.2.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     3192 b- defN 23-Apr-17 01:22 tsstp-0.2.0.dist-info/RECORD
+40 files, 88947 bytes uncompressed, 27624 bytes compressed:  68.9%
```

## zipnote {}

```diff
@@ -1,10 +1,13 @@
 Filename: test/__init__.py
 Comment: 
 
+Filename: test/benchmark_test.py
+Comment: 
+
 Filename: test/test_data_parse.py
 Comment: 
 
 Filename: test/test_filter.py
 Comment: 
 
 Filename: test/test_if_match.py
@@ -27,17 +30,14 @@
 
 Filename: test/test_template_match.py
 Comment: 
 
 Filename: test/test_template_parse.py
 Comment: 
 
-Filename: test/test_template_str_match.py
-Comment: 
-
 Filename: test/test_xdacar_parse.py
 Comment: 
 
 Filename: test/testsymbolnode.py
 Comment: 
 
 Filename: tsstp/__init__.py
@@ -54,26 +54,32 @@
 
 Filename: tsstp/internal_ds.py
 Comment: 
 
 Filename: tsstp/semantic_model.py
 Comment: 
 
+Filename: tsstp/template_grammar.lark
+Comment: 
+
 Filename: tsstp/template_parser.py
 Comment: 
 
 Filename: tsstp/utils.py
 Comment: 
 
 Filename: tsstp/input/__init__.py
 Comment: 
 
 Filename: tsstp/input/file_input.py
 Comment: 
 
+Filename: tsstp/input/token_grammar.lark
+Comment: 
+
 Filename: tsstp/matcher/__init__.py
 Comment: 
 
 Filename: tsstp/matcher/if_matcher.py
 Comment: 
 
 Filename: tsstp/matcher/loop_matcher.py
@@ -93,23 +99,23 @@
 
 Filename: tsstp/output/__init__.py
 Comment: 
 
 Filename: tsstp/output/result.py
 Comment: 
 
-Filename: TSSTP-0.1.0.dist-info/METADATA
+Filename: tsstp-0.2.0.dist-info/METADATA
 Comment: 
 
-Filename: TSSTP-0.1.0.dist-info/WHEEL
+Filename: tsstp-0.2.0.dist-info/WHEEL
 Comment: 
 
-Filename: TSSTP-0.1.0.dist-info/entry_points.txt
+Filename: tsstp-0.2.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: TSSTP-0.1.0.dist-info/top_level.txt
+Filename: tsstp-0.2.0.dist-info/top_level.txt
 Comment: 
 
-Filename: TSSTP-0.1.0.dist-info/RECORD
+Filename: tsstp-0.2.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## test/test_data_parse.py

```diff
@@ -1,18 +1,20 @@
 import os
 from tsstp.input.file_input import Input
 
+# 获取当前脚本所在目录的绝对路径
+current_dir = os.path.dirname(os.path.abspath(__file__))
 def test_data_parse():
-    basepath = 'D:\\workspace\\MDI\\datatemplate\\test\\testdata\\'
+    basepath = os.path.join(current_dir, 'testdata\\')
     file_path_list = [basepath + x for x in os.listdir(basepath)]
 
     poscar_file = basepath + 'POSCAR'
     test_data = basepath + 'text_file.txt'
     outcar = basepath + 'OUTCAR'
-
+    xdatcar = basepath + 'XDATCAR'
 
     poscar_data = '''
     POSCAR\(4)                              
        1.00000000000000     
          8.3879995346000005    0.0000000000000000    0.0000000000000000
          0.0000000000000000    8.3879995346000005    0.0000000000000000
          0.0000000000000000    0.0000000000000000   23.0000000000000000
@@ -54,13 +56,13 @@
     """
 
     multi_temp_data3 = """
     free energy    TOTEN  =      -565.56798583 eV
     """
 
 
-    input_data = Input(string=multi_temp_data3)
-    input_obj = Input(path=outcar)
-    token_list = input_data.parse()
-    print('token_list:',token_list)
+    input_data = Input(string=poscar_data)
+    input_obj = Input(path=xdatcar)
+    token_list = input_obj.parse()
+    print('token_list:',token_list[:6])
     print('len of token_list:',len(token_list))
 test_data_parse()
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## test/test_if_match.py

```diff
@@ -71,8 +71,9 @@
             {{ row }} ~ 3
         {% endloop %}
     {% endif %}
     """
     parser = DataTemplate(if_template12, if_data2)
     parser.parse()
     result = parser.result()
-    print(f'--result:{result}')
+    print(f'--result:{result}')
+
```

## test/test_match.py

```diff
@@ -1,11 +1,15 @@
-from pprint import pprint
-
+import os
+import cProfile
+import pstats
 from tsstp.datatemplate import DataTemplate
 
+# 获取当前脚本所在目录的绝对路径
+current_dir = os.path.dirname(os.path.abspath(__file__))
+
 def test_match():
     match_template1 = """
     loop
     {{ key }} = {{ value }}
     {{ key1 }} = {{ value1 }}
     {{ key2 }} = {{ value2 }}
     end
@@ -24,16 +28,27 @@
     eigenvalues  EBANDS =  {{ EBANDS }}
     atomic energy  EATOM  = {{ EATOM }}
     Solvation  Ediel_sol  = {{ Ediel_sol }}
     {{ separator2 }} ~ n
     free energy  TOTEN  =  {{ TOTEN }} eV
     energy without entropy =  {{ entropy }} energy(sigma->0) =  {{ energy }} 
     """
-    basepath = 'D:\\workspace\\MDI\\datatemplate\\test\\testdata\\'
+    basepath = os.path.join(current_dir, 'testdata\\')
     text_file = basepath + 'text_file.txt'
     outcar = basepath + 'OUTCAR'
-    parser = DataTemplate(final_energy, outcar)
+    outcar_t = basepath + 'outcar_template.txt'
 
+    parser = DataTemplate(outcar_t, outcar)
     result = parser.match()
-    print(f'--result:{result}')
+    # print(f'--result:{result}')
+
+# run the function with profiling enabled
+cProfile.run('test_match()', 'profile_stats')
+
+# create a pstats object from the stats file
+p = pstats.Stats('profile_stats')
+
+# sort the stats by cumulative time spent in each function
+p.sort_stats('cumulative')
 
-test_match()
+# print out the top 10 functions by cumulative time spent
+p.print_stats(10)
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## test/test_poscar_parse.py

```diff
@@ -1,9 +1,14 @@
 from tsstp.datatemplate import DataTemplate
+import cProfile
+import pstats
+import os
 
+# 获取当前脚本所在目录的绝对路径
+current_dir = os.path.dirname(os.path.abspath(__file__))
 def test_poscar_parse():
     poscar_template = """
     {{ head }}
     {{ Scaling }}
     {% loop 3 %}
         {{ lattice_vector }} ~ 3
     {% endloop %}
@@ -20,13 +25,29 @@
           {{ Coordinates }} ~ 6
         {% endloop %}
     {% endif %}
     {% loop n %}
         {{ other }}
     {% endloop %}
     """
-    basepath = 'D:\\workspace\\MDI\\datatemplate\\semi_structure_parser\\testdata\\'
+    basepath = os.path.join(current_dir, 'testdata\\')
     poscar_file = basepath + 'POSCAR'
+    poscar_t = basepath + 'poscar_template.txt'
+
     parser = DataTemplate(poscar_template, poscar_file)
     parser.parse()
-    result = parser.get_result()
-    print(f'--result:{result}')
+    result = parser.result()
+    print(f'--result:{result}')
+
+test_poscar_parse()
+# # run the function with profiling enabled
+# cProfile.run('test_poscar_parse()', 'profile_stats')
+#
+# # create a pstats object from the stats file
+# p = pstats.Stats('profile_stats')
+#
+# # sort the stats by cumulative time spent in each function
+# p.sort_stats('cumulative')
+#
+# # print out the top 10 functions by cumulative time spent
+# p.print_stats(10)
+
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## test/test_search.py

```diff
@@ -1,15 +1,21 @@
 from tsstp.datatemplate import DataTemplate
+import os
+
+# 获取当前脚本所在目录的绝对路径
+current_dir = os.path.dirname(os.path.abspath(__file__))
 
 def test_search():
     search_template1 = """
     {{ key }} = {{ value }}
     """
 
-    basepath = 'D:\\workspace\\MDI\\datatemplate\\test\\testdata\\'
+    basepath = os.path.join(current_dir, 'testdata\\')
     text_file = basepath + 'text_file.txt'
     incar = basepath + 'INCAR'
-    parser = DataTemplate(search_template1, incar)
+    incar_t = basepath + 'incar_template.txt'
+
+    parser = DataTemplate(incar_t, incar)
     result = parser.search()
     print(f'--result:{result}')
 
-test_search()
+test_search()
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## test/test_template_parse.py

```diff
@@ -78,16 +78,54 @@
     {{ line9}} ~ n
     {{ line10}} ~ n
     {{ line11}} ~ n
     {{ line12}} ~ n
     {{ line13}} ~ n
     energy without entropy = {{ sada }} ~ n
     """
+    xdatacar_template = """
+    {{ head }}
+    {{ Scaling }}
+    {% loop 3 %}
+        {{ lattice_vector }} ~ 3
+    {% endloop %}
+    {{ elements }} ~ 3
+    {{ element_num }} ~ 3
+    {% loop n %}
+        Direct configuration = {{ loop_index }}
+        {% loop sum(element_num) %}
+            {{ Coordinates }} ~ 3
+        {% endloop %}
+    {% endloop %}
+    """
+    poscar_template = """
+    {{ head }}
+    {{ Scaling }}
+    {% loop 3 %}
+        {{ lattice_vector }} ~ 3
+    {% endloop %}
+    {{ elements }} ~ 3
+    {{ element_num }} ~ 3
+    {{ model }}
+    {% if direct %}
+        {% loop sum(element_num) %}
+          {{ Coordinates }} ~ 3
+        {% endloop %}
+    {% else %}
+        {{ Direct }}
+        {% loop sum(element_num) %}
+          {{ Coordinates }} ~ 6
+        {% endloop %}
+    {% endif %}
+    {% loop n %}
+        {{ other }}
+    {% endloop %}
+    """
 
-    template_obj = TemplateParser(final_energy)
+    template_obj = TemplateParser(poscar_template)
     template_AST = template_obj.get_AST()
     template_dict = template_obj.get_dict_AST()
     semantic_model = template_obj.get_semantic_model()
     start_model = semantic_model.models[0]
     end_model = semantic_model.models[-1]
     print(f"start:{start_model},end:{end_model}")
     print(f"type end model{type(end_model)}")
```

## test/test_xdacar_parse.py

```diff
@@ -1,27 +1,46 @@
 from tsstp.datatemplate import DataTemplate
+import cProfile
+import pstats
+import os
+
+# 获取当前脚本所在目录的绝对路径
+current_dir = os.path.dirname(os.path.abspath(__file__))
 
 def test_xdatcar_parse():
     xdatacar_template = """
     {{ head }}
     {{ Scaling }}
     {% loop 3 %}
         {{ lattice_vector }} ~ 3
     {% endloop %}
     {{ elements }} ~ 3
     {{ element_num }} ~ 3
     {% loop n %}
-        Direct configuration = {{ loop_index }}
+        Direct configuration= {{ loop_index }}
         {% loop sum(element_num) %}
             {{ Coordinates }} ~ 3
         {% endloop %}
     {% endloop %}
     """
 
-    basepath = 'D:\\workspace\\MDI\\datatemplate\\test\\testdata\\'
+    basepath = os.path.join(current_dir, 'testdata\\')
     xdatacar_file = basepath + 'XDATCAR'
+    xdatacar_t = basepath + 'xdatcar_template.txt'
+
+    parser = DataTemplate(xdatacar_t, xdatacar_file)
 
-    parser = DataTemplate(xdatacar_template, xdatacar_file)
     parser.parse()
     result = parser.result()
-    print(f'--result:{result}')
-test_xdatcar_parse()
+    # print(f'--result:{result}')
+
+# run the function with profiling enabled
+cProfile.run('test_xdatcar_parse()', 'profile_stats')
+
+# create a pstats object from the stats file
+p = pstats.Stats('profile_stats')
+
+# sort the stats by cumulative time spent in each function
+p.sort_stats('cumulative')
+
+# print out the top 10 functions by cumulative time spent
+p.print_stats(10)
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## tsstp/datatemplate.py

```diff
@@ -1,20 +1,18 @@
 import json
 import os
 import traceback
 import importlib
 
-import xmltodict
-
 from tsstp.template_parser import TemplateParser
 from tsstp.semantic_model import TemplateModel, StringModel, StringTemplateModel, \
     MultiTemplateModel, LoopModel, IfModel
 from tsstp.internal_ds import SymbolTable
 from tsstp.output.result import Output
-from tsstp.input.file_input import Input, search_lines, search_muti_lines
+from tsstp.input.file_input import Input, search_lines, search_muti_lines, search_file_multilines
 from tsstp.matcher import  loop_matcher as lm, if_matcher as im
 from tsstp.utils import calculate_lines, get_loop_number
 from tsstp.extract_symbol import base_unit_extract
 
 class DataTemplate:
     '''
     The parser system main class to load data, templates, then output final results.
@@ -33,14 +31,17 @@
     # save the extracted data
     symbol_table = SymbolTable()
 
     def __init__(self, template_text, data):
         self.template = template_text
         self.data = data
 
+        # Initialize a single instance of Input()
+        self.input_obj = Input()
+
     def result(self,format=''):
         '''
         Method to get parsing results,results can be formatted.
         :param format:Specify the format to be returned, currently it is  xml or json.
         :return: formatted output result.
 
         Example::
@@ -55,53 +56,47 @@
         '''
         dt = importlib.import_module('tsstp.datatemplate')
 
         template_result = dt.DataTemplate.symbol_table.template_table
         result = Output(template_result).to_json()
         if format.lower() == 'xml':
             result = Output(template_result).to_xml()
-        dt.DataTemplate.symbol_table.template_table.clear()
+        dt.DataTemplate.symbol_table.clear()
         return result
 
     def parse(self, data_token='', semantic_model=''):
         '''
         main method to parse input data with semantic model which transform from template text.
         '''
         global token_list
         if not data_token and not semantic_model:
             data_token = self.data_parse()
             semantic_model = self.template_parse()
         models = semantic_model.models
-        # print(f"tokens:{data_token},length:{len(data_token)},model:{models}")
 
         dt = importlib.import_module('tsstp.datatemplate')
 
         row_number = 0
         for index, model in enumerate(models):
-            # print("main parser model:", model)
             if isinstance(model, (TemplateModel, StringModel, StringTemplateModel, MultiTemplateModel)):
                 token_list = data_token[row_number]
                 # symbol_list = self._base_tem_parse(model, token_list)
                 try:
                     symbol_list = base_unit_extract(model, token_list)
                     dt.DataTemplate.symbol_table.add_list(symbol_list)
                 except Exception as e:
-                    print(f"{e} at line:{' '.join([t.value for t in data_token[0]])}")
+                    traceback.print_exc()
                 row_number += 1
 
             elif isinstance(model, IfModel):
-                # stmts = self._get_if_stmt(model)
                 from tsstp.matcher.if_matcher import get_if_stmt
                 stmts = get_if_stmt(model)
-                if stmts:
-                    print(f'ifmodel stmt:{stmts}')
-                    # lines_num = self.calculate_lines(stmts)
-                else:
+
+                if not stmts:
                     continue
-                # print(f"looplength:{lines_num},loopstmts:{stmts}")
 
                 # where ifmodel contain s loop model, which get length of token list
                 has_loop = [model for model in stmts if isinstance(model, LoopModel)]
                 if has_loop:
                     for loop in has_loop:
                         if loop.loop_num == 'n' or loop.loop_num == 'N':
                             token_list = data_token[row_number:]
@@ -110,41 +105,38 @@
                             token_list = data_token[row_number:row_number + lines_num]
                             row_number += lines_num
                 else:
                     lines_num = calculate_lines(stmts)
                     token_list = data_token[row_number:row_number + lines_num]
                     row_number += lines_num
 
-                # print("if token list:", token_list)
-                # symbol_list = self._if_match(model, token_list)
                 try:
                     symbol_list = im.match(model, token_list)
                     dt.DataTemplate.symbol_table.add_list(symbol_list)
                 except Exception as e:
-                    print(f"{e} at line:{' '.join([t.value for t in data_token[0]])}")
+                    traceback.print_exc()
 
             elif isinstance(model, LoopModel):
                 tem_list = [model]
                 symbol_list = []
                 loop_num = model.loop_num
                 # loop_num = self._get_loop_number(loop_num)
                 loop_num = get_loop_number(loop_num)
                 if loop_num == 'n':
                     # the n loop statement should be the last statement
                     if models[-1] != model:
                         raise ValueError(
                             "Incorrect template grammar: n loop and there should be no subsequent statements!")
                     # if loop number is n，get statement number and change token lists per loop.
                     tokens = data_token[row_number:]
-                    # print("n tokens:", tokens)
                     try:
                         symbol_list = lm.match(model, tokens)
                         dt.DataTemplate.symbol_table.add_list(symbol_list)
                     except Exception as e:
-                        print(f"{e} at line:{' '.join([t.value for t in data_token[0]])}")
+                        traceback.print_exc()
                     break
 
                 elif isinstance(loop_num, int):
                     lines_num = calculate_lines(tem_list)
 
                     token_list = data_token[row_number:row_number + lines_num]
                     symbol_list = lm.match(model, token_list)
@@ -173,37 +165,34 @@
         '''
 
         semantic_model = self.template_parse()
         models = semantic_model.models
 
         start_model = models[0]
         end_model = models[-1]
-        print(f"match_models:{models}")
-        print(f"start:{start_model},end:{end_model}")
         # Determine if the template meets the requirements of match method
         if isinstance(start_model, (StringModel, StringTemplateModel, MultiTemplateModel)) and \
                 isinstance(end_model, (StringModel, StringTemplateModel, MultiTemplateModel)):
             start_string = start_model.get_no_space_string()
             end_string = end_model.get_no_space_string()
-            print(f"start_string:{start_string},end_string:{end_string}")
         else:
             raise ValueError("invalid template text: the start and end of template line need contains string!")
 
         # get data ready to parse
-        source_data = self.read_input_data()
+        # source_data = self.read_input_data()
         results = []
-
-        parsing_data = search_muti_lines(source_data, start_string, end_string)
-        print(f"lengh parsing_data:{len(parsing_data)}")
+        if os.path.isfile(self.data):
+            parsing_data = search_file_multilines(self.data, start_string, end_string)
+        else:
+            parsing_data = search_muti_lines(self.data, start_string, end_string)
         for each_data in parsing_data:
             # data blank,then continue next loop
             if not each_data:
                 continue
             token_list = self.data_parse(data=each_data)
-            # print(f"len of line:{len(token_list)}")
             self.parse(data_token=token_list, semantic_model=semantic_model)
             template_result = DataTemplate.symbol_table.template_table
             single_result = Output(template_result).to_dict()
             results.append(single_result)
             # clear symbol_table for next parse
             DataTemplate.symbol_table.clear()
 
@@ -224,15 +213,14 @@
         '''
         semantic_model = self.template_parse()
         model_list = semantic_model.models[0]
         if isinstance(model_list, MultiTemplateModel):
             models = model_list.models
         else:
             models = model_list
-        print(f"models:{models}")
 
         # Positioning strings to get data from input
         p_string = ''
 
         # list contain template info for get input data
         parsing_info = []
         for ele in models:
@@ -251,56 +239,51 @@
                     continue
                 else:
                     has_str = True
             else:
                 continue
         if not has_str:
             raise ValueError(f'template must contain string identifier!')
-        print(f"parsing_info:{parsing_info}")
 
         source_data = self.read_input_data()
         if p_string:
             input_data = search_lines(source_data, parsing_info)
         else:
             raise ValueError("")
-        print(f"input_data:{input_data}")
 
         def get_data_with_tem(parsing_info: list, input_data: str):
             result = []
             index = 0
             # input_data = [x for x in input_data.split(' ') if x != '']
             input_data += '\n'
             input = Input(string=input_data)
             input_token = input.parse()
-            print(f"data:{input_data},token:{input_token[0]}")
 
             for ele in parsing_info:
                 if isinstance(ele, str):
                     if ele == 'n' or ele == 'N':
                         result.extend(input_data[index:])
                         break
                     else:
-                        print(f"index:{input_data[index]};{index}")
+                        # print(f"index:{input_data[index]};{index}")
                         result.append(input_data[index])
                         index += 1
                 else:
                     result.extend(input_data[index:index + ele])
                     index += ele
 
-            # print(f"result:{result}")
             result_str = ' '.join(result)
 
             return result_str + '\n'
 
         # splitting each row of data according to the template information
         # parsing_data = []
         # for line in input_data:
         #     parsing_data.append(get_data_with_tem(parsing_info, line))
 
-        # print(f"parsing_data:{parsing_data}")
         results = []
 
         for each_data in input_data:
             each_data += '\n'
             token_list = self.data_parse(data=each_data)
             self.parse(data_token=token_list, semantic_model=semantic_model)
             template_result = DataTemplate.symbol_table.template_table
@@ -311,15 +294,15 @@
 
         return results
 
     def read_template_text(self):
         # get temaplte text from string or OS path
 
         if os.path.isfile(self.template):
-            with open('data.txt', 'r', encoding='utf-8') as f:
+            with open(self.template, 'r', encoding='utf-8') as f:
                 template_text = f.read()
         else:
             template_text = self.template
         return template_text
 
     def template_parse(self):
         '''
@@ -333,15 +316,14 @@
         return semantic_model
 
     def read_input_data(self):
         '''
         read data from input string or filepath.
         :return: content in file or str.
         '''
-        # print(f'read data:{self.data} isfile:{os.path.isfile(self.data)}')
         if os.path.isfile(self.data):
             file_obj = Input(path=self.data)
             content = file_obj.get_content()
         else:
             input_obj = Input(string=self.data)
             content = input_obj.get_content()
 
@@ -349,20 +331,18 @@
 
     def data_parse(self, data=''):
         '''
         parse input data and convert it to list of tokens.
         :return: list of tokens.
         '''
         if data:
-            file_obj = Input()
-            token_list = file_obj.parse(data)
+            token_list = self.input_obj.parse(data)
         else:
             content = self.read_input_data()
-            input_obj = Input()
-            token_list = input_obj.parse(content)
+            token_list = self.input_obj.parse(content)
 
         return token_list
 
     def _indicator_match(self, indicator: str or None, token: list):
         # Check if a token list match an indicator from template
 
         # todo string类型和symbol类型的token应该与string类型的标识符匹配
```

## tsstp/extract_symbol.py

```diff
@@ -42,20 +42,17 @@
     :param templates: list
     :param token: list of tokens.
     :return: list of SymbolNodes.
     """
     from tsstp.datatemplate import DataTemplate
 
     symbol_list = []
-    # print("muti_tems:", templates)
-    # print("muti_tokens:", token)
     row_number = 0
 
     for template in templates:
-        # print("muti_template:", template)
         if isinstance(template, (TemplateModel, StringModel, StringTemplateModel)):
             token_list = token[row_number]
             symbol = base_unit_extract(template, token_list)
             row_number += 1
             symbol_list.extend(symbol)
             DataTemplate.symbol_table.add_list(symbol)
 
@@ -67,25 +64,23 @@
             if loop_num == 'n':
                 # the n loop statement should be the last statement
                 if templates[-1] != template:
                     raise ValueError(
                         "Incorrect template grammar: n loop and there should be no subsequent statements!")
                 # if loop number is n，get statement number and change token lists per loop.
                 tokens = token[row_number:]
-                # print("n tokens:", tokens)
                 symbol = loop_matcher.match(template, tokens)
                 DataTemplate.symbol_table.add_list(symbol)
                 symbol_list.extend(symbol)
 
                 break
 
             elif isinstance(loop_num, int):
                 lines_length = calculate_lines(tem_list)
                 token_list = token[row_number:row_number + lines_length]
-                # print("number tokens:", token_list)
                 symbol = loop_matcher.match(template, token_list)
                 DataTemplate.symbol_table.add_list(symbol)
                 symbol_list.extend(symbol)
 
             else:
                 raise ValueError('Wrong loop number!')
```

## tsstp/utils.py

```diff
@@ -20,23 +20,21 @@
             stmt = template.stmt
             else_stmt = template.stmt
 
             from tsstp import datatemplate as dt
             con_exist = dt.DataTemplate.symbol_table.is_exist(condition)
 
             if condition and else_stmt:
-                # 条件和else语句都存在
                 if con_exist:
                     # condition existed, execute stmt
                     row_number += calculate_lines(stmt)
                 else:
                     # condition does not existed, execute else_stmt
                     row_number += calculate_lines(else_stmt)
             elif condition and not else_stmt:
-                # 条件存在,但是else语句不存在
                 if con_exist:
                     row_number += calculate_lines(stmt)
                 else:
                     row_number += 0
             else:
                 raise ValueError('The conditional statement value is wrong!')
         else:
@@ -66,15 +64,14 @@
 
 
 def get_loop_number(loop_num: tuple or str) -> int or str:
     # get loop number used in Templatemodel or loopmodel
     if isinstance(loop_num, str):
         # element number quote previous variable name
         from tsstp import datatemplate as dt
-        # print(f'symbol table{dt.DataTemplate.symbol_table}')
 
         if loop_num.isdigit():
             loop_num = int(loop_num)
         elif loop_num == 'n':
             loop_num = 'n'
 
         elif dt.DataTemplate.symbol_table.is_exist(loop_num):
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## tsstp/input/file_input.py

```diff
@@ -21,46 +21,54 @@
         token_list = data.parse()
     """
 
     def __init__(self, path='', string=''):
         self.filepath = path
         # normal String to raw String
         self.data = r'{}'.format(string)
+        self.parser = Lark.open('token_grammar.lark', rel_to=__file__, parser='lalr')
 
     def get_content(self):
         content = ''
         if os.path.exists(self.filepath):
             try:
                 with open(self.filepath, 'r',encoding='utf-8') as file_obj:
-                    content = file_obj.read()
+                    content = file_obj.read().rstrip()
             except Exception as e:
                 traceback.print_exc()
         elif self.data:
             content = self.data
         else:
             raise ValueError("Input error!")
 
         return content
 
     def parse(self, input_data=''):
         token_list = []
-        parser = Lark.open('token_grammar.lark', rel_to=__file__)
+        # parser = Lark.open('token_grammar.lark', rel_to=__file__)
 
         if input_data:
-
-            data_token = parser.parse(input_data)
+            data_token = self.parser.parse(input_data.rstrip())
         else:
-            data_token = parser.parse(self.get_content())
+            data_token = self.parser.parse(self.get_content())
 
         for ele in data_token.children:
             if isinstance(ele, Tree):
                 token_list.append(ele.children)
 
         return token_list
 
+    def parse_line(self, line):
+        token_list = []
+        data_token = self.parser.parse(line.rstrip())
+        for ele in data_token.children:
+            if isinstance(ele, Tree):
+                token_list.append(ele.children)
+        return token_list
+
 
 def search_muti_lines(source: str, start_string: str, end_string: str):
     """Search for the given string in file and return lines between start and end"""
     list_of_results = []
     result = []
 
     append_flag = False
@@ -74,15 +82,38 @@
             # if append_flag is true, keep adding data
             result.append(line.strip())
         if append_flag and (end_string in line_string or end_string == line_string):
             # if line start with end_string,
             append_flag = False
             list_of_results.append('\n'.join(result)+'\n')
             result = []
-    # print(f"list_of_results:{list_of_results}")
+    return list_of_results
+
+
+def search_file_multilines(file_path: str, start_string: str, end_string: str):
+    """Search for the given string in file and return lines between start and end"""
+    list_of_results = []
+    result = []
+
+    append_flag = False
+    with open(file_path) as file:
+        for line in file:
+            # Starting line data
+            line_string = line.replace(' ','')
+            if start_string in line_string:
+                # if line start with start_string, add the data to result
+                append_flag = True
+            if append_flag:
+                # if append_flag is true, keep adding data
+                result.append(line.strip())
+            if append_flag and (end_string in line_string or end_string == line_string):
+                # if line start with end_string,
+                append_flag = False
+                list_of_results.append('\n'.join(result)+'\n')
+                result = []
     return list_of_results
 
 
 def search_lines(source: str, parsing_info:list):
     """Search for the given string in file and return lines containing that string,
     along with line numbers"""
     list_of_results = []
@@ -110,37 +141,36 @@
                         # raise ValueError(f"template number:n can not be used this way!")
                     else:
                         if ele in value_list:
                             first_str_index = value_list.index(ele)
                             break
                 else:
                     element_number += ele
-            print(f"index:{first_str_index},ele_number:{element_number}")
             if first_str_index != element_number:
                 flag = False
         except Exception as e:
             flag = False
             pass
-        print(f"flag:{flag}")
         return flag
 
+    input_obj = Input()
     for line in source.splitlines():
         line += '\n'
-        line_obj = Input(string=line)
-        line_token = line_obj.parse()
+        # line_obj = Input(string=line)
+        # line_token = line_obj.parse()
+        line_token = input_obj.parse_line(line)
         if is_target(line_token[0],parsing_info):
             # If yes, then add the line number & line as a tuple in the list
-            print(f"line:{line}")
             list_of_results.append(line)
 
     return list_of_results
 
 
 if __name__ == '__main__':
-    basepath = 'D:\\workspace\\MDI\\datatemplate\\semi_structure_parser\\testdata\\'
+    basepath = 'D:\\workspace\\MDI\\datatemplate\\test\\testdata\\'
     file_path_list = [basepath + x for x in os.listdir(basepath)]
 
     poscar_file = basepath + 'POSCAR'
     test_data = basepath + 'text_file.txt'
 
     # file_obj = Input(test_data)
     # content = file_obj.get_content()
@@ -150,15 +180,15 @@
     # print("line content :", line_content)
 
     poscar_data = '''
     POSCAR\(4)                              
        1.00000000000000     
          8.3879995346000005    0.0000000000000000    0.0000000000000000
          0.0000000000000000    8.3879995346000005    0.0000000000000000
-         0.0000000000000000    0.0000000000000000   23.0000000000000000
+           0.00000000E+00  0.00000000E+00  0.00000000E+00
        O    Fe   Ni
         50    33     1
     Selective dynamics
     Direct
       0.5915683032071809  0.4076348462601602  0.3236733991222093   T   T   T
       0.6870646622156227  0.3019365196287453  0.3103828207546814   T   T   T
       0.3630810210536513  0.3675710238882842  0.4462171285301444   T   T   T
@@ -170,28 +200,25 @@
       0.1460929563952582  0.1511753838855331  0.4495652323750344   T   T   T
       0.8481353013092298  0.8527124318740430  0.4499216601262892   T   T   T
       0.3622735022466660  0.6366474075220162  0.5337409646875285   T   T   T
       0.6304779841965802  0.3684030218642275  0.5368171582936877   T   T   T
       0.1128001793327870  0.8852245477662335  0.5437372635670031   T   T   T
       0.8854582153913581  0.1118876070999771  0.5389489857602635   T   T   T
       0.1089344200672412  0.3674780231491631  0.5405416957824908   T   T   T
-      0.8828364160941208  0.6354067641796362  0.5395969860907350   T   T   T
-    '''
+      0.8828364160941208  0.6354067641796362  0.5395969860907350   T   T   T'''
     test_data = """
      POSCAR\(4)                              
     1.00000000000000     
       8.3879995346000005    0.0000000000000000    0.0000000000000000
       0.0000000000000000    8.3879995346000005    0.0000000000000000
       0.0000000000000000    0.0000000000000000   23.0000000000000000
     O    Fe   Ni
      50    33     1
     Direct configuration=     1
      """
     # {'string': ['CeO', '2', '-', '111', '-', 'ovac', '-', '11', 'Au']}
-    # print(r'{}'.format(poscar_data))
     ne_number = """
-    -0.00050255  0.99951883  0.43019119
-    """
-    input_obj = Input(path=poscar_file)
+    -0.00050255  0.99951883  0.43019119"""
+    input_obj = Input(string=ne_number)
     token_list = input_obj.parse()
-    print('token_list:',token_list[:10])
+    print('token_list:',token_list)
     print('len of token_list:',len(token_list))
```

## tsstp/matcher/if_matcher.py

```diff
@@ -11,39 +11,35 @@
     match and extract data from token list with IfModel model.
     :param if_model:  IfModel which represented by dataclass.
     :param token:  list of tokens.
     :return: list of nested SymbolsNodes.
     """
 
     stmts = get_if_stmt(if_model)
-    # print("if exec stmts:", stmts)
     symbol = muti_unit_extract(stmts, token)
 
     return symbol
 
 
 def get_if_stmt(model: IfModel) -> list:
     # Get the statements to be executed in if model which contain condition,stmt and else_stmt
     condition_str = model.condition
     stmt = model.stmt
     else_stmt = model.else_stmt
-    # print("if model:", model)
     from tsstp import datatemplate as dt
 
     def is_conditon(condition: str) -> bool:
         # whether the condition statement is true or false
         global con_flag
         con_list = condition.split(' ')
-        # print("con list:", con_list)
         if len(con_list) == 1:
             con_flag = dt.DataTemplate.symbol_table.value_exist(condition)
         else:
             # get variable name and value in condition string
             variable_name = re.search(r'[a-zA-Z]\w{1,}', condition).group()
-            # print("variable_name:", variable_name)
             variable_node = dt.DataTemplate.symbol_table.lookup_template(variable_name)
             variable_value = variable_node.value
             # execute condition string and decide whether the condition is true or false
             exec(f'{variable_name}={variable_value}')
             con_flag = eval(condition)
 
         return con_flag
```

## tsstp/matcher/loop_matcher.py

```diff
@@ -11,19 +11,15 @@
     :param loop_model:  IfModel which represented by dataclass.
     :param token:  list of tokens.
     :return: list of nested SymbolsNodes.
     """
     loop_num = loop_model.loop_num
     stmt_list = loop_model.stmt
     # Number of token list lines per match
-    # todo
     lines_number = calculate_lines(stmt_list)
-    # print("loop model input models:", loop_model)
-    # print(f"tokenlen:{len(token)},loop model tokens:{token}")
-    # print("loop model lines_number:", lines_number)
 
     symbol_list = []
     row_number = 0
     # todo
     loop_num = get_loop_number(loop_num)
 
     def nest_match(stmts: list, token: list) -> list[SymbolNode]:
@@ -55,34 +51,28 @@
                 symbol_name = temp_symbol[0].key
                 for tok in token_list:
                     nest_symbol = base_unit_extract(nest_stmt, tok)
                     value_list.extend([sym.value for sym in nest_symbol if sym.type == 'template'])
 
                 reshape_value = [value for row in value_list for value in row]
                 symbol = SymbolNode('template', symbol_name, reshape_value)
-                # print("nest loop match symbol:", symbol)
                 row_number += lines_length
                 symbols.append(symbol)
             else:
                 raise ValueError()
         return symbols
 
     if loop_num == 'n':
         loop_count = 0
         while token[lines_number * loop_count:]:
-            # print("n token:", token[lines_number * loop_count:lines_number * (loop_count + 1)])
             symbols = nest_match(stmt_list, token[lines_number * loop_count:lines_number * (loop_count + 1)])
             loop_count += 1
             symbol_list.extend(symbols)
 
     elif isinstance(loop_num, int):
         for i in range(loop_num):
-            # print("int token:", token[lines_number * i:lines_number * (i + 1)])
             symbols = nest_match(stmt_list, token[lines_number * i:lines_number * (i + 1)])
-            # print("symbols:", symbols)
             symbol_list.extend(symbols)
     else:
         symbols = None
     symbol_list = merge_symbol(symbol_list)
-    # print("loop symbolist", symbol_list)
-    # print("len(loop symbolist)", len(symbol_list))
     return symbol_list
```

## tsstp/matcher/multi_template_matcher.py

```diff
@@ -7,15 +7,14 @@
     """
     basic multi templates statement such as {{ x }} str {{ y }} ~ 3  {{ z }} ~ n
     match and extract data from token list with MultiTemplateModel model.
     :param template_string: MultiTemplateModel model which represented by dataclass.
     :param token:  list of tokens.
     :return: list of nested SymbolsNodes.
     """
-
     index = 0
     symbol_list = []
     for mod in muti_temp_model.models:
         if isinstance(mod, TemplateModel):
             # template match
             ele_num = get_loop_number(mod.elements_number)
             if ele_num == 'n':
```

## tsstp/matcher/template_matcher.py

```diff
@@ -10,17 +10,15 @@
     :param template: template model which represented by dataclass.
     :param token:  list of tokens.
     :return: list of nested SymbolsNodes.
     """
     symbol_list = []
 
     # elements number store in string
-    # print(f'matcher loop:{template.elements_number}')
     element_number = get_loop_number(template.elements_number)
-    # print(f'matcher loop number:{element_number}')
 
     # template variable name
     variable_name = template.variable
     # symbol node in Symbol table
     symbol = SymbolNode(type='template', key=variable_name)
 
     #todo 　Whether an indicator is used
@@ -69,9 +67,8 @@
             raise ValueError('Variable name does not exist')
     else:
         raise ValueError('Number of elements error')
 
     symbol_list.append(symbol)
     if filter:
         filter(symbol_list)
-    # print("symbol_list:", symbol_list)
     return symbol_list
```

## Comparing `TSSTP-0.1.0.dist-info/RECORD` & `tsstp-0.2.0.dist-info/RECORD`

 * *Files 16% similar despite different names*

```diff
@@ -1,38 +1,40 @@
 test/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-test/test_data_parse.py,sha256=Qr64hgbz9vWnpE-NyOa2CqHH1UEe74r2kbZjPtXoy3M,2820
+test/benchmark_test.py,sha256=VbXYvb9ptYc9zwrAAYDz8i_S2KNj_Q7u3toXZupFGjA,2917
+test/test_data_parse.py,sha256=A8HYRbAe01slJZaBXqb-ISKnrZ_eaHmXYK_qLWVGo0Q,2945
 test/test_filter.py,sha256=XsBsOBhlfePNb_Aha5DW1mwXCFOikHO_kKL74Ha18yA,636
-test/test_if_match.py,sha256=di1gthh1xDmGBaYhbIyypyK9P_ryxsXmEVjOzezCLKk,1300
+test/test_if_match.py,sha256=Ze8ZkrcrsXcbaIEXw3bFQQtZpFvN4gNx29Tgd8ElprM,1302
 test/test_loop_match.py,sha256=X-aDMmoJgy7GHiSCUl_B2pNY9Ei9PKbX3tsHClJAiBU,1469
-test/test_match.py,sha256=vyruBGG0qRRvta-pWMadHxU1w97HgYu44SR8T6X5R2g,1126
+test/test_match.py,sha256=EH8jF6m4XEcycnQu0fT3dgjst_AYMHJBzp1Awz1kQ1g,1597
 test/test_output.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-test/test_poscar_parse.py,sha256=zNa78W6h1oUefDW8-MYMAbOSH6lbKJNHJUNXYmGr55o,839
-test/test_search.py,sha256=JOZ3EbrygTrL99wxQio9BuFV0H7GIX9j33z8uQrcWJs,396
+test/test_poscar_parse.py,sha256=Hax7jPYZxShddxljw0f7U5XiItx07EJnPWM7ERRGO60,1380
+test/test_search.py,sha256=rhsr4zgD8kZMl_FR_cgBwmScdYpGPsbGBpZmqzEXHrs,538
 test/test_template_match.py,sha256=-tW1kJmTddOEcCxLFMYbAATdKvirQKuDbTPO6l1wvZw,1491
-test/test_template_parse.py,sha256=E7gzmtYnE1s6XzMy0ptoqeAqbjaaFwBVpa6uhAzWBCQ,2547
-test/test_template_str_match.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-test/test_xdacar_parse.py,sha256=u5MsaiVqSZDc6jdnZ_GjZSy87YrvYEzEJYsHRkCCuSk,703
+test/test_template_parse.py,sha256=uQ39T6TMrcKo1ogw09x6aMMBnDenta2uO_9yypKcs_Y,3418
+test/test_xdacar_parse.py,sha256=Fc3TxTLiYWHOg0rqjkCqnI2yfD2XQLJ6fqyVsJx2RYA,1203
 test/testsymbolnode.py,sha256=QVUwnDDGteoFBTUtnJInDDL8rmofd-_MfeXrNXMvO9Y,825
 tsstp/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-tsstp/datatemplate.py,sha256=_FXGMw7hhK43rsuJjUdy54Gz1IfbMY5vGKr7IISz1Pw,14747
-tsstp/extract_symbol.py,sha256=ykn2pAL-IYKD50QYDfFbE1v5mx5ykUdG24L8w2v5-2w,3500
+tsstp/datatemplate.py,sha256=ejcPSJ6BLLUELgtb5oK21MV5Py-uAEcFhQeXDFCrIZM,13605
+tsstp/extract_symbol.py,sha256=VnKpazgs2EjMr2ASNNSTFq61_wQ_VAW4vBOSmFb1Ka0,3285
 tsstp/filter.py,sha256=NulAhEofoy5GcNtAkB3FwZq_5NSIQsz15_vtmIz8-C8,1049
 tsstp/internal_ds.py,sha256=lELJcurFJd1ftQUku4zMixiUcQ54hXjgCUAMHz7YOmM,3603
 tsstp/semantic_model.py,sha256=FHodvtkoKhfjHqNJLnmZhQONtYnkvkJqHYjGmIoMWMU,2717
+tsstp/template_grammar.lark,sha256=EvsvAsnCfh_O_g6kxHNp9CcLbJq2A8Uy1_-xsAjFKhg,1950
 tsstp/template_parser.py,sha256=WboSrTVDdAGuu-jr8GEACX9biXz0DlR1P7NPiWtbJUY,10086
-tsstp/utils.py,sha256=LPLpAeKyWad7hO3rfW0AEDgB9NWaFFrGlUl18EY1wvU,4966
+tsstp/utils.py,sha256=Vxbbk70B01Glf5Yw8DbKKHwpqEdj-1LbpS1-7xePC5A,4799
 tsstp/input/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-tsstp/input/file_input.py,sha256=bzrlaC8LpXdLria-ZgFWPvuN1en96F5xJsO4eglexxI,7342
+tsstp/input/file_input.py,sha256=Y2Rdcru7m5dDIqTvHlZw72oV9BOVg2V9Z6hn1XQU4-Q,8472
+tsstp/input/token_grammar.lark,sha256=UIXizNo3nSj5wSX2TR4nqwo4sQEcxfkuSJs2mY6_c_g,634
 tsstp/matcher/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-tsstp/matcher/if_matcher.py,sha256=dAK193lHXxVVICW8Afh4EjXdqfXqTfNbrpwLY2xb0S8,2022
-tsstp/matcher/loop_matcher.py,sha256=tjkyCRuFuQA7bYqcadk0yPPIZuwr5kdfdTC4E0e6Epw,3756
-tsstp/matcher/multi_template_matcher.py,sha256=vP_vlxV5vlFbBSKH70ZqFGctxNtOorIfnsH0X_5IvSg,1904
+tsstp/matcher/if_matcher.py,sha256=daqhcnYiWFhbHNEWPwj99rQERrMH3m_2Z9GobkPducg,1861
+tsstp/matcher/loop_matcher.py,sha256=KOHG1yffOmonUBOmtZDfjZ7GJhVvD9Iwpg6yKC8cp9Q,3199
+tsstp/matcher/multi_template_matcher.py,sha256=0qVDL8Q2BkGqj_dmQ-m9P3hy0iYnQ58A2slTzm3xXbs,1903
 tsstp/matcher/string_matcher.py,sha256=-aGo6wSpd_kd5-kYABpDTSADs5IZrCmZB8S6H7w78Wg,1113
-tsstp/matcher/template_matcher.py,sha256=VHkpKjKBRSOgev5TrEoVUWPdNjq5nHUEgo9V3yYzoAg,2877
+tsstp/matcher/template_matcher.py,sha256=phX5xGpkAMd81fTFeXl3Wb1kvX32CcEDm86tNakfAoI,2727
 tsstp/matcher/template_str_matcher.py,sha256=F_XLOFk17ZFbI3HjA24BG1VruY-8R7sEh4xcPodLIwg,1390
 tsstp/output/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 tsstp/output/result.py,sha256=6zDREpOVjwDevYhAhuQ-nHC14ZYZn_3vTPZ4PfRAIwA,710
-TSSTP-0.1.0.dist-info/METADATA,sha256=yl2GsPXZr9bDzGymwHl4Z6Y-pzlzRIR0C3DL-cgFqOw,313
-TSSTP-0.1.0.dist-info/WHEEL,sha256=ewwEueio1C2XeHTvT17n8dZUJgOvyCWCt0WVNLClP9o,92
-TSSTP-0.1.0.dist-info/entry_points.txt,sha256=MN4sAOWRV4CoNH4UmsUdAw51B9oO_9uqgsmoU16Kd_I,41
-TSSTP-0.1.0.dist-info/top_level.txt,sha256=Id43ClViWppJGF4l3mJQJrIiqivbSdreyOQ-72Xs3iM,11
-TSSTP-0.1.0.dist-info/RECORD,,
+tsstp-0.2.0.dist-info/METADATA,sha256=-nWsckEHoGcxohxhYbCSrk_wS63YpoSRTT7GIhaMous,2769
+tsstp-0.2.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+tsstp-0.2.0.dist-info/entry_points.txt,sha256=JqQt7SP_EmMUbKyY-XsojP9j6xRf4M5B5ekk3XP9LrA,59
+tsstp-0.2.0.dist-info/top_level.txt,sha256=Id43ClViWppJGF4l3mJQJrIiqivbSdreyOQ-72Xs3iM,11
+tsstp-0.2.0.dist-info/RECORD,,
```

