# Comparing `tmp/textgen-0.2.1.tar.gz` & `tmp/textgen-0.2.2.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "textgen-0.2.1.tar", last modified: Fri Apr 14 03:39:06 2023, max compression
+gzip compressed data, was "textgen-0.2.2.tar", last modified: Mon Apr 17 14:32:15 2023, max compression
```

## Comparing `textgen-0.2.1.tar` & `textgen-0.2.2.tar`

### file list

```diff
@@ -1,68 +1,68 @@
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-14 03:39:06.101297 textgen-0.2.1/
--rw-r--r--   0 xuming     (501) staff       (20)    11357 2021-08-05 02:59:06.000000 textgen-0.2.1/LICENSE
--rw-r--r--   0 xuming     (501) staff       (20)    31932 2023-04-14 03:39:06.101662 textgen-0.2.1/PKG-INFO
--rw-r--r--   0 xuming     (501) staff       (20)    26931 2023-04-14 03:27:18.000000 textgen-0.2.1/README.md
--rw-r--r--   0 xuming     (501) staff       (20)      309 2023-04-14 03:39:06.102206 textgen-0.2.1/setup.cfg
--rw-r--r--   0 xuming     (501) staff       (20)     1419 2023-04-14 03:39:03.000000 textgen-0.2.1/setup.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-14 03:39:06.051495 textgen-0.2.1/textgen/
--rw-r--r--   0 xuming     (501) staff       (20)     1386 2023-04-14 03:39:03.000000 textgen-0.2.1/textgen/__init__.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-14 03:39:06.061156 textgen-0.2.1/textgen/augment/
--rw-r--r--   0 xuming     (501) staff       (20)      133 2022-06-30 07:59:18.000000 textgen-0.2.1/textgen/augment/__init__.py
--rw-r--r--   0 xuming     (501) staff       (20)     1493 2022-05-09 11:34:10.000000 textgen-0.2.1/textgen/augment/sentence_level_augment.py
--rw-r--r--   0 xuming     (501) staff       (20)     3582 2023-04-12 09:49:33.000000 textgen-0.2.1/textgen/augment/text_augment.py
--rw-r--r--   0 xuming     (501) staff       (20)     2256 2022-07-04 08:00:34.000000 textgen-0.2.1/textgen/augment/tokenizer.py
--rw-r--r--   0 xuming     (501) staff       (20)     1971 2021-08-05 02:59:06.000000 textgen-0.2.1/textgen/augment/translate_api.py
--rw-r--r--   0 xuming     (501) staff       (20)    13027 2022-09-26 07:39:41.000000 textgen-0.2.1/textgen/augment/word_level_augment.py
--rw-r--r--   0 xuming     (501) staff       (20)     1240 2022-05-09 11:34:10.000000 textgen-0.2.1/textgen/augment/word_vocab.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-14 03:39:06.065210 textgen-0.2.1/textgen/chatglm/
--rw-r--r--   0 xuming     (501) staff       (20)       80 2023-03-26 02:30:43.000000 textgen-0.2.1/textgen/chatglm/__init__.py
--rw-r--r--   0 xuming     (501) staff       (20)    31490 2023-04-13 13:14:14.000000 textgen-0.2.1/textgen/chatglm/chatglm_model.py
--rw-r--r--   0 xuming     (501) staff       (20)     6270 2023-04-13 05:04:59.000000 textgen-0.2.1/textgen/chatglm/chatglm_utils.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-14 03:39:06.068221 textgen-0.2.1/textgen/config/
--rw-r--r--   0 xuming     (501) staff       (20)      140 2021-08-05 02:59:06.000000 textgen-0.2.1/textgen/config/__init__.py
--rw-r--r--   0 xuming     (501) staff       (20)     1845 2022-06-30 03:18:51.000000 textgen-0.2.1/textgen/config/global_args.py
--rw-r--r--   0 xuming     (501) staff       (20)    14437 2023-04-13 13:05:38.000000 textgen-0.2.1/textgen/config/model_args.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-14 03:39:06.069715 textgen-0.2.1/textgen/custom_models/
--rwxr-xr-x   0 xuming     (501) staff       (20)        0 2021-08-05 02:59:06.000000 textgen-0.2.1/textgen/custom_models/__init__.py
--rwxr-xr-x   0 xuming     (501) staff       (20)    32645 2022-08-24 03:27:39.000000 textgen-0.2.1/textgen/custom_models/models.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-14 03:39:06.073006 textgen-0.2.1/textgen/data/
--rwxr-xr-x   0 xuming     (501) staff       (20)    48098 2019-08-25 14:57:21.000000 textgen-0.2.1/textgen/data/HowNetPOSWord.txt
--rw-r--r--   0 xuming     (501) staff       (20)    17438 2021-10-25 11:27:11.000000 textgen-0.2.1/textgen/data/stopwords.txt
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-14 03:39:06.076325 textgen-0.2.1/textgen/language_generation/
--rwxr-xr-x   0 xuming     (501) staff       (20)      292 2021-08-05 02:59:06.000000 textgen-0.2.1/textgen/language_generation/__init__.py
--rw-r--r--   0 xuming     (501) staff       (20)    10056 2022-07-05 07:48:33.000000 textgen-0.2.1/textgen/language_generation/language_generation_model.py
--rw-r--r--   0 xuming     (501) staff       (20)     2829 2022-06-14 12:08:00.000000 textgen-0.2.1/textgen/language_generation/language_generation_utils.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-14 03:39:06.082343 textgen-0.2.1/textgen/language_modeling/
--rwxr-xr-x   0 xuming     (501) staff       (20)      407 2022-09-10 06:38:25.000000 textgen-0.2.1/textgen/language_modeling/__init__.py
--rwxr-xr-x   0 xuming     (501) staff       (20)    56833 2022-09-09 11:41:34.000000 textgen-0.2.1/textgen/language_modeling/language_modeling_model.py
--rw-r--r--   0 xuming     (501) staff       (20)     8938 2022-06-14 12:08:00.000000 textgen-0.2.1/textgen/language_modeling/language_modeling_utils.py
--rw-r--r--   0 xuming     (501) staff       (20)    65595 2022-11-27 05:36:55.000000 textgen-0.2.1/textgen/language_modeling/songnet_model.py
--rw-r--r--   0 xuming     (501) staff       (20)    17103 2022-12-05 07:28:16.000000 textgen-0.2.1/textgen/language_modeling/songnet_utils.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-14 03:39:06.086202 textgen-0.2.1/textgen/llama/
--rw-r--r--   0 xuming     (501) staff       (20)       80 2023-04-10 03:44:03.000000 textgen-0.2.1/textgen/llama/__init__.py
--rw-r--r--   0 xuming     (501) staff       (20)    23037 2023-04-13 13:50:42.000000 textgen-0.2.1/textgen/llama/llama_model.py
--rw-r--r--   0 xuming     (501) staff       (20)     5753 2023-04-13 14:13:30.000000 textgen-0.2.1/textgen/llama/llama_utils.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-14 03:39:06.093854 textgen-0.2.1/textgen/seq2seq/
--rwxr-xr-x   0 xuming     (501) staff       (20)      313 2022-08-07 03:00:11.000000 textgen-0.2.1/textgen/seq2seq/__init__.py
--rw-r--r--   0 xuming     (501) staff       (20)    73142 2023-04-12 10:22:29.000000 textgen-0.2.1/textgen/seq2seq/bart_seq2seq_model.py
--rw-r--r--   0 xuming     (501) staff       (20)    18590 2023-04-12 10:19:10.000000 textgen-0.2.1/textgen/seq2seq/bart_seq2seq_utils.py
--rw-r--r--   0 xuming     (501) staff       (20)    23456 2022-08-25 03:06:56.000000 textgen-0.2.1/textgen/seq2seq/conv_seq2seq_model.py
--rw-r--r--   0 xuming     (501) staff       (20)     4345 2022-05-10 10:48:02.000000 textgen-0.2.1/textgen/seq2seq/data_reader.py
--rw-r--r--   0 xuming     (501) staff       (20)    19319 2022-08-25 03:06:56.000000 textgen-0.2.1/textgen/seq2seq/seq2seq_model.py
--rw-r--r--   0 xuming     (501) staff       (20)    10564 2023-04-12 10:22:29.000000 textgen-0.2.1/textgen/seq2seq/seq2seq_trainer.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-14 03:39:06.098379 textgen-0.2.1/textgen/t5/
--rwxr-xr-x   0 xuming     (501) staff       (20)      272 2022-08-23 06:18:39.000000 textgen-0.2.1/textgen/t5/__init__.py
--rw-r--r--   0 xuming     (501) staff       (20)    50493 2022-11-16 12:03:51.000000 textgen-0.2.1/textgen/t5/copyt5_model.py
--rw-r--r--   0 xuming     (501) staff       (20)     6917 2022-11-16 13:02:34.000000 textgen-0.2.1/textgen/t5/copyt5_utils.py
--rw-r--r--   0 xuming     (501) staff       (20)    50541 2023-02-16 08:10:32.000000 textgen-0.2.1/textgen/t5/t5_model.py
--rw-r--r--   0 xuming     (501) staff       (20)     7146 2022-11-17 02:54:45.000000 textgen-0.2.1/textgen/t5/t5_utils.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-14 03:39:06.100461 textgen-0.2.1/textgen/unsup_generation/
--rw-r--r--   0 xuming     (501) staff       (20)      246 2022-09-06 02:24:58.000000 textgen-0.2.1/textgen/unsup_generation/__init__.py
--rw-r--r--   0 xuming     (501) staff       (20)     3456 2022-09-26 07:01:36.000000 textgen-0.2.1/textgen/unsup_generation/tgls_model.py
--rw-r--r--   0 xuming     (501) staff       (20)    27678 2022-09-06 02:29:56.000000 textgen-0.2.1/textgen/unsup_generation/tgls_util.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-14 03:39:06.053838 textgen-0.2.1/textgen.egg-info/
--rw-r--r--   0 xuming     (501) staff       (20)    31932 2023-04-14 03:39:05.000000 textgen-0.2.1/textgen.egg-info/PKG-INFO
--rw-r--r--   0 xuming     (501) staff       (20)     1692 2023-04-14 03:39:05.000000 textgen-0.2.1/textgen.egg-info/SOURCES.txt
--rw-r--r--   0 xuming     (501) staff       (20)        1 2023-04-14 03:39:05.000000 textgen-0.2.1/textgen.egg-info/dependency_links.txt
--rw-r--r--   0 xuming     (501) staff       (20)      158 2023-04-14 03:39:05.000000 textgen-0.2.1/textgen.egg-info/requires.txt
--rw-r--r--   0 xuming     (501) staff       (20)        8 2023-04-14 03:39:05.000000 textgen-0.2.1/textgen.egg-info/top_level.txt
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-17 14:32:15.105381 textgen-0.2.2/
+-rw-r--r--   0 xuming     (501) staff       (20)    11357 2021-08-05 02:59:06.000000 textgen-0.2.2/LICENSE
+-rw-r--r--   0 xuming     (501) staff       (20)    33563 2023-04-17 14:32:15.105904 textgen-0.2.2/PKG-INFO
+-rw-r--r--   0 xuming     (501) staff       (20)    28546 2023-04-17 14:03:20.000000 textgen-0.2.2/README.md
+-rw-r--r--   0 xuming     (501) staff       (20)      309 2023-04-17 14:32:15.106668 textgen-0.2.2/setup.cfg
+-rw-r--r--   0 xuming     (501) staff       (20)     1427 2023-04-17 14:02:51.000000 textgen-0.2.2/setup.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-17 14:32:15.047149 textgen-0.2.2/textgen/
+-rw-r--r--   0 xuming     (501) staff       (20)     1386 2023-04-17 13:39:14.000000 textgen-0.2.2/textgen/__init__.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-17 14:32:15.055964 textgen-0.2.2/textgen/augment/
+-rw-r--r--   0 xuming     (501) staff       (20)      133 2022-06-30 07:59:18.000000 textgen-0.2.2/textgen/augment/__init__.py
+-rw-r--r--   0 xuming     (501) staff       (20)     1493 2022-05-09 11:34:10.000000 textgen-0.2.2/textgen/augment/sentence_level_augment.py
+-rw-r--r--   0 xuming     (501) staff       (20)     3582 2023-04-12 09:49:33.000000 textgen-0.2.2/textgen/augment/text_augment.py
+-rw-r--r--   0 xuming     (501) staff       (20)     2256 2022-07-04 08:00:34.000000 textgen-0.2.2/textgen/augment/tokenizer.py
+-rw-r--r--   0 xuming     (501) staff       (20)     1971 2021-08-05 02:59:06.000000 textgen-0.2.2/textgen/augment/translate_api.py
+-rw-r--r--   0 xuming     (501) staff       (20)    13027 2022-09-26 07:39:41.000000 textgen-0.2.2/textgen/augment/word_level_augment.py
+-rw-r--r--   0 xuming     (501) staff       (20)     1240 2022-05-09 11:34:10.000000 textgen-0.2.2/textgen/augment/word_vocab.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-17 14:32:15.058842 textgen-0.2.2/textgen/chatglm/
+-rw-r--r--   0 xuming     (501) staff       (20)       80 2023-03-26 02:30:43.000000 textgen-0.2.2/textgen/chatglm/__init__.py
+-rw-r--r--   0 xuming     (501) staff       (20)    31478 2023-04-17 14:30:17.000000 textgen-0.2.2/textgen/chatglm/chatglm_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)     6270 2023-04-13 05:04:59.000000 textgen-0.2.2/textgen/chatglm/chatglm_utils.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-17 14:32:15.062480 textgen-0.2.2/textgen/config/
+-rw-r--r--   0 xuming     (501) staff       (20)      140 2021-08-05 02:59:06.000000 textgen-0.2.2/textgen/config/__init__.py
+-rw-r--r--   0 xuming     (501) staff       (20)     1845 2022-06-30 03:18:51.000000 textgen-0.2.2/textgen/config/global_args.py
+-rw-r--r--   0 xuming     (501) staff       (20)    14497 2023-04-14 05:57:19.000000 textgen-0.2.2/textgen/config/model_args.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-17 14:32:15.064526 textgen-0.2.2/textgen/custom_models/
+-rwxr-xr-x   0 xuming     (501) staff       (20)        0 2021-08-05 02:59:06.000000 textgen-0.2.2/textgen/custom_models/__init__.py
+-rwxr-xr-x   0 xuming     (501) staff       (20)    32645 2022-08-24 03:27:39.000000 textgen-0.2.2/textgen/custom_models/models.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-17 14:32:15.068106 textgen-0.2.2/textgen/data/
+-rwxr-xr-x   0 xuming     (501) staff       (20)    48098 2019-08-25 14:57:21.000000 textgen-0.2.2/textgen/data/HowNetPOSWord.txt
+-rw-r--r--   0 xuming     (501) staff       (20)    17438 2021-10-25 11:27:11.000000 textgen-0.2.2/textgen/data/stopwords.txt
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-17 14:32:15.071986 textgen-0.2.2/textgen/language_generation/
+-rwxr-xr-x   0 xuming     (501) staff       (20)      292 2021-08-05 02:59:06.000000 textgen-0.2.2/textgen/language_generation/__init__.py
+-rw-r--r--   0 xuming     (501) staff       (20)    10056 2022-07-05 07:48:33.000000 textgen-0.2.2/textgen/language_generation/language_generation_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)     2829 2022-06-14 12:08:00.000000 textgen-0.2.2/textgen/language_generation/language_generation_utils.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-17 14:32:15.079396 textgen-0.2.2/textgen/language_modeling/
+-rwxr-xr-x   0 xuming     (501) staff       (20)      407 2022-09-10 06:38:25.000000 textgen-0.2.2/textgen/language_modeling/__init__.py
+-rwxr-xr-x   0 xuming     (501) staff       (20)    56833 2022-09-09 11:41:34.000000 textgen-0.2.2/textgen/language_modeling/language_modeling_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)     8938 2022-06-14 12:08:00.000000 textgen-0.2.2/textgen/language_modeling/language_modeling_utils.py
+-rw-r--r--   0 xuming     (501) staff       (20)    65595 2022-11-27 05:36:55.000000 textgen-0.2.2/textgen/language_modeling/songnet_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)    17103 2022-12-05 07:28:16.000000 textgen-0.2.2/textgen/language_modeling/songnet_utils.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-17 14:32:15.084416 textgen-0.2.2/textgen/llama/
+-rw-r--r--   0 xuming     (501) staff       (20)       80 2023-04-10 03:44:03.000000 textgen-0.2.2/textgen/llama/__init__.py
+-rw-r--r--   0 xuming     (501) staff       (20)    23156 2023-04-17 04:08:29.000000 textgen-0.2.2/textgen/llama/llama_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)     5753 2023-04-13 14:13:30.000000 textgen-0.2.2/textgen/llama/llama_utils.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-17 14:32:15.093358 textgen-0.2.2/textgen/seq2seq/
+-rwxr-xr-x   0 xuming     (501) staff       (20)      313 2022-08-07 03:00:11.000000 textgen-0.2.2/textgen/seq2seq/__init__.py
+-rw-r--r--   0 xuming     (501) staff       (20)    73131 2023-04-14 09:07:24.000000 textgen-0.2.2/textgen/seq2seq/bart_seq2seq_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)    18590 2023-04-12 10:19:10.000000 textgen-0.2.2/textgen/seq2seq/bart_seq2seq_utils.py
+-rw-r--r--   0 xuming     (501) staff       (20)    23456 2022-08-25 03:06:56.000000 textgen-0.2.2/textgen/seq2seq/conv_seq2seq_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)     4345 2022-05-10 10:48:02.000000 textgen-0.2.2/textgen/seq2seq/data_reader.py
+-rw-r--r--   0 xuming     (501) staff       (20)    19319 2022-08-25 03:06:56.000000 textgen-0.2.2/textgen/seq2seq/seq2seq_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)    10564 2023-04-12 10:22:29.000000 textgen-0.2.2/textgen/seq2seq/seq2seq_trainer.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-17 14:32:15.101326 textgen-0.2.2/textgen/t5/
+-rwxr-xr-x   0 xuming     (501) staff       (20)      272 2022-08-23 06:18:39.000000 textgen-0.2.2/textgen/t5/__init__.py
+-rw-r--r--   0 xuming     (501) staff       (20)    50493 2022-11-16 12:03:51.000000 textgen-0.2.2/textgen/t5/copyt5_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)     6917 2022-11-16 13:02:34.000000 textgen-0.2.2/textgen/t5/copyt5_utils.py
+-rw-r--r--   0 xuming     (501) staff       (20)    50530 2023-04-14 09:07:24.000000 textgen-0.2.2/textgen/t5/t5_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)     7146 2022-11-17 02:54:45.000000 textgen-0.2.2/textgen/t5/t5_utils.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-17 14:32:15.103988 textgen-0.2.2/textgen/unsup_generation/
+-rw-r--r--   0 xuming     (501) staff       (20)      246 2022-09-06 02:24:58.000000 textgen-0.2.2/textgen/unsup_generation/__init__.py
+-rw-r--r--   0 xuming     (501) staff       (20)     3456 2022-09-26 07:01:36.000000 textgen-0.2.2/textgen/unsup_generation/tgls_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)    27678 2022-09-06 02:29:56.000000 textgen-0.2.2/textgen/unsup_generation/tgls_util.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-17 14:32:15.050098 textgen-0.2.2/textgen.egg-info/
+-rw-r--r--   0 xuming     (501) staff       (20)    33563 2023-04-17 14:32:14.000000 textgen-0.2.2/textgen.egg-info/PKG-INFO
+-rw-r--r--   0 xuming     (501) staff       (20)     1692 2023-04-17 14:32:14.000000 textgen-0.2.2/textgen.egg-info/SOURCES.txt
+-rw-r--r--   0 xuming     (501) staff       (20)        1 2023-04-17 14:32:14.000000 textgen-0.2.2/textgen.egg-info/dependency_links.txt
+-rw-r--r--   0 xuming     (501) staff       (20)      166 2023-04-17 14:32:14.000000 textgen-0.2.2/textgen.egg-info/requires.txt
+-rw-r--r--   0 xuming     (501) staff       (20)        8 2023-04-17 14:32:14.000000 textgen-0.2.2/textgen.egg-info/top_level.txt
```

### Comparing `textgen-0.2.1/LICENSE` & `textgen-0.2.2/LICENSE`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/PKG-INFO` & `textgen-0.2.2/PKG-INFO`

 * *Files 11% similar despite different names*

```diff
@@ -1,116 +1,102 @@
 Metadata-Version: 2.1
 Name: textgen
-Version: 0.2.1
+Version: 0.2.2
 Summary: Text Generation Model
 Home-page: https://github.com/shibing624/textgen
 Author: XuMing
 Author-email: xuming624@qq.com
 License: Apache 2.0
 Description: [![PyPI version](https://badge.fury.io/py/textgen.svg)](https://badge.fury.io/py/textgen)
         [![Downloads](https://pepy.tech/badge/textgen)](https://pepy.tech/project/textgen)
         [![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)
-        [![GitHub contributors](https://img.shields.io/github/contributors/shibing624/textgen.svg)](https://github.com/shibing624/textgen/graphs/contributors)
         [![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
         [![python_version](https://img.shields.io/badge/Python-3.8%2B-green.svg)](requirements.txt)
         [![GitHub issues](https://img.shields.io/github/issues/shibing624/textgen.svg)](https://github.com/shibing624/textgen/issues)
         [![Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact)
         
         # TextGen
         
-        ğŸŒˆ Implementation of Text Generation models.
+        ğŸŒˆImplementation of Text Generation models.
         
         **textgen**å®ç°äº†å¤šç§æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼šLLAMAã€ChatGLMã€UDAã€GPT2ã€Seq2Seqã€BARTã€T5ã€SongNetç­‰æ¨¡å‹ï¼Œå¼€ç®±å³ç”¨ã€‚
         
         **Guide**
         
         - [Feature](#Feature)
         - [Install](#install)
         - [Usage](#usage)
         - [Contact](#Contact)
-        - [Reference](#reference)
+        - [License](#License)
         
-        # Feature
+        ## ğŸ˜ŠFeature
         
-        ## æ–‡æœ¬ç”Ÿæˆ
+        ### æ–‡æœ¬ç”Ÿæˆ
         
         1. seq2seq: Seq2Seqã€ConvSeq2Seqã€BART
         2. language_modeling: GPT2ã€SongNetã€ChatGLMã€LLAMA
         3. t5: T5ã€CopyT5
         
-        ## æ–‡æœ¬æ‰©å¢
+        ### æ–‡æœ¬æ‰©å¢
         
-        ### è¯ç²’åº¦æ‰©å¢
+        #### è¯ç²’åº¦æ‰©å¢
         
         1. UDAï¼Œéæ ¸å¿ƒè¯æ›¿æ¢
         2. EDAï¼Œç®€å•æ•°æ®å¢å¼ºæŠ€æœ¯ï¼šç›¸ä¼¼è¯ã€åŒä¹‰è¯æ›¿æ¢ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢
         
-        ### å¥ç²’åº¦æ‰©å¢
+        #### å¥ç²’åº¦æ‰©å¢
         
         1. å›è¯‘ï¼ˆBT, Back Translateï¼‰ï¼šä¸­æ–‡-è‹±æ–‡-ä¸­æ–‡
         2. GPT2æ¨¡å‹ç»­å†™ï¼šçŸ­æ–‡æœ¬->é•¿æ–‡æœ¬
         3. BARTæ‘˜è¦æ¨¡å‹ï¼šé•¿æ–‡æœ¬->çŸ­æ–‡æœ¬
         4. TGLSï¼šæ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
         
-        ## åŠŸèƒ½åˆ—è¡¨
+        ### åŠŸèƒ½åˆ—è¡¨
         
         - [ChatGLM](textgen/chatglm)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†ChatGLM-6Bæ¨¡å‹LoRAå¾®è°ƒè®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºå¥å­çº é”™ã€å¯¹è¯ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
         - [LLAMA](textgen/llama)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†LLAMAæ¨¡å‹LoRAå¾®è°ƒè®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºå¤šè½®å¯¹è¯ç”Ÿæˆä»»åŠ¡
-        - [UDA(éæ ¸å¿ƒè¯æ›¿æ¢)/EDA](textgen/augment/word_level_augment.py)ï¼šæœ¬é¡¹ç›®å‚è€ƒGoogleçš„UDA(éæ ¸å¿ƒè¯æ›¿æ¢)
-          ç®—æ³•å’ŒEDAç®—æ³•ï¼ŒåŸºäºTF-IDFå°†å¥å­ä¸­éƒ¨åˆ†ä¸é‡è¦è¯æ›¿æ¢ä¸ºåŒä¹‰è¯ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢ç­‰æ–¹æ³•ï¼Œäº§ç”Ÿæ–°çš„æ–‡æœ¬ï¼Œå®ç°äº†æ–‡æœ¬æ‰©å¢
+        - [UDA(éæ ¸å¿ƒè¯æ›¿æ¢)/EDA](textgen/augment/word_level_augment.py)ï¼šæœ¬é¡¹ç›®å‚è€ƒGoogleçš„UDA(éæ ¸å¿ƒè¯æ›¿æ¢)ç®—æ³•å’ŒEDAç®—æ³•ï¼ŒåŸºäºTF-IDFå°†å¥å­ä¸­éƒ¨åˆ†ä¸é‡è¦è¯æ›¿æ¢ä¸ºåŒä¹‰è¯ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢ç­‰æ–¹æ³•ï¼Œäº§ç”Ÿæ–°çš„æ–‡æœ¬ï¼Œå®ç°äº†æ–‡æœ¬æ‰©å¢
         - [BT(å›è¯‘)](textgen/augment/sentence_level_augment.py)ï¼šæœ¬é¡¹ç›®åŸºäºç™¾åº¦ç¿»è¯‘APIå®ç°äº†å›è¯‘åŠŸèƒ½ï¼Œå…ˆæŠŠä¸­æ–‡å¥å­ç¿»è¯‘ä¸ºè‹±æ–‡ï¼Œå†æŠŠè‹±æ–‡ç¿»è¯‘ä¸ºæ–°çš„ä¸­æ–‡
         - [Seq2Seq](textgen/seq2seq)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†Seq2Seqã€ConvSeq2Seqã€BARTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€æ‘˜è¦ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
         - [T5](textgen/t5)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†T5å’ŒCopyT5æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€å¯¹è”ç”Ÿæˆã€æ–‡æ¡ˆæ’°å†™ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
         - [GPT2](textgen/language_modeling)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†GTP2æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡ç« ç”Ÿæˆã€å¯¹è”ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
         - [SongNet](textgen/language_modeling/songnet_model.py)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†SongNetæ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºè§„èŒƒæ ¼å¼çš„è¯—è¯ã€æ­Œè¯ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
-        - [TGLS](textgen/unsup_generation)
-          ï¼šæœ¬é¡¹ç›®å®ç°äº†[TGLS](https://www.jiqizhixin.com/articles/2020-08-11-5)æ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œæ˜¯ä¸€ç§â€œå…ˆæœç´¢åå­¦ä¹ â€çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡åå¤è¿­ä»£å­¦ä¹ å€™é€‰é›†ï¼Œæœ€ç»ˆæ¨¡å‹èƒ½ç”Ÿæˆç±»ä¼¼å€™é€‰é›†çš„é«˜è´¨é‡ç›¸ä¼¼æ–‡æœ¬
+        - [TGLS](textgen/unsup_generation)ï¼šæœ¬é¡¹ç›®å®ç°äº†[TGLS](https://www.jiqizhixin.com/articles/2020-08-11-5)æ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œæ˜¯ä¸€ç§â€œå…ˆæœç´¢åå­¦ä¹ â€çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡åå¤è¿­ä»£å­¦ä¹ å€™é€‰é›†ï¼Œæœ€ç»ˆæ¨¡å‹èƒ½ç”Ÿæˆç±»ä¼¼å€™é€‰é›†çš„é«˜è´¨é‡ç›¸ä¼¼æ–‡æœ¬
         
-        ## Release Models
+        ### Release Models
         
         releaseåŸºäº`textgen`è®­ç»ƒçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å·²ç»releaseåˆ°HuggingFace modelsï¼ŒæŒ‡å®šæ¨¡å‹åç§°`textgen`ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚
         
-        |Model|Arch|Introduce|Training|Inference| |:-- |:--- |:--- |:--- |:--- |
-        |[shibing624/prompt-t5-base-chinese](https://huggingface.co/shibing624/prompt-t5-base-chinese)|T5|ä¸­æ–‡NLPå¤šä»»åŠ¡Promptæ¨¡å‹|[prompt-t5-base-chinese.md](https://github.com/shibing624/textgen/blob/main/docs/prompt-t5-base-chinese.md)|[predict
-        script](https://github.com/shibing624/textgen/blob/main/examples/t5_prompt_demo.py)|
-        |[shibing624/t5-chinese-couplet](https://huggingface.co/shibing624/t5-chinese-couplet)|T5|fine-tunedä¸­æ–‡å¯¹è”åçš„æ¨¡å‹|[å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)|[predict
-        script](https://github.com/shibing624/textgen/blob/main/examples/t5_couplet_demo.py)|
-        |[shibing624/songnet-base-chinese](https://huggingface.co/shibing624/songnet-base-chinese)|SongNet|SongNeté¢„è®­ç»ƒæ¨¡å‹|-|-|
-        |[shibing624/songnet-base-chinese-songci](https://huggingface.co/shibing624/songnet-base-chinese-songci)|SongNet|fine-tunedå®‹è¯åçš„æ¨¡å‹|[training
-        script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict
-        script](https://github.com/shibing624/textgen/blob/main/examples/songnet_songci_demo.py)|
-        |[shibing624/songnet-base-chinese-couplet](https://huggingface.co/shibing624/songnet-base-chinese-couplet)|SongNet|fine-tunedå¯¹è”åçš„æ¨¡å‹|[training
-        script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict
-        script](https://github.com/shibing624/textgen/blob/main/examples/songnet_couplet_demo.py)|
-        |[shibing624/chatglm-6b-csc-zh-lora](https://huggingface.co/shibing624/chatglm-6b-csc-zh-lora)|ChatGLM-6B|åœ¨27ä¸‡ä¸­æ–‡æ‹¼å†™çº é”™æ•°æ®[shibing624/CSC](https://huggingface.co/datasets/shibing624/CSC)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆChatGLM-6Bï¼Œçº é”™æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡|[training
-        script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_csc_demo.py)|[predict
-        script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/csc_demo.py)|
-        |[shibing624/chatglm-6b-belle-zh-lora](https://huggingface.co/shibing624/chatglm-6b-belle-zh-lora)|ChatGLM-6B|åœ¨100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆChatGLM-6Bï¼Œé—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡|[training
-        script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_hfdataset_demo.py)|[predict
-        script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_hfdataset_demo.py)|
-        |[shibing624/llama-13b-belle-zh-lora](https://huggingface.co/shibing624/llama-13b-belle-zh-lora)|ChatGLM-6B|åœ¨100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆLlama-13Bï¼Œé—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡|[training
-        script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_hfdataset_demo.py)|[predict
-        script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_hfdataset_demo.py)|
         
-        # Demo
+        |Model| Arch       |Introduce| Training                                                                                                                                     | Inference                                                                                                             | 
+        |:-- |:-----------|:--- |:---------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------|
+        |[shibing624/prompt-t5-base-chinese](https://huggingface.co/shibing624/prompt-t5-base-chinese)| T5         |ä¸­æ–‡NLPå¤šä»»åŠ¡Promptæ¨¡å‹| [prompt-t5-base-chinese.md](https://github.com/shibing624/textgen/blob/main/docs/prompt-t5-base-chinese.md)                                  | [predict script](https://github.com/shibing624/textgen/blob/main/examples/t5/t5_prompt_demo.py)                       |
+        |[shibing624/t5-chinese-couplet](https://huggingface.co/shibing624/t5-chinese-couplet)| T5         |fine-tunedä¸­æ–‡å¯¹è”åçš„æ¨¡å‹| [å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md) | [predict script](https://github.com/shibing624/textgen/blob/main/examples/t5/t5_couplet_demo.py)                      |
+        |[shibing624/songnet-base-chinese](https://huggingface.co/shibing624/songnet-base-chinese)| SongNet    |SongNeté¢„è®­ç»ƒæ¨¡å‹| -                                                                                                                                            | -                                                                                                                     |
+        |[shibing624/songnet-base-chinese-songci](https://huggingface.co/shibing624/songnet-base-chinese-songci)| SongNet    |fine-tunedå®‹è¯åçš„æ¨¡å‹| [training script](https://github.com/shibing624/textgen/blob/main/examples/songnet/training_zh_songnet_demo.py)                              | [predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet/songnet_songci_demo.py)             |
+        |[shibing624/songnet-base-chinese-couplet](https://huggingface.co/shibing624/songnet-base-chinese-couplet)| SongNet    |fine-tunedå¯¹è”åçš„æ¨¡å‹| [training script](https://github.com/shibing624/textgen/blob/main/examples/songnet/training_zh_songnet_demo.py)                                 | [predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet/songnet_couplet_demo.py)            |
+        |[shibing624/chatglm-6b-csc-zh-lora](https://huggingface.co/shibing624/chatglm-6b-csc-zh-lora)| ChatGLM-6B |åœ¨27ä¸‡ä¸­æ–‡æ‹¼å†™çº é”™æ•°æ®[shibing624/CSC](https://huggingface.co/datasets/shibing624/CSC)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆChatGLM-6Bï¼Œçº é”™æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡| [training script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_csc_demo.py)                             | [predict script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/csc_demo.py)                        |
+        |[shibing624/chatglm-6b-belle-zh-lora](https://huggingface.co/shibing624/chatglm-6b-belle-zh-lora)| ChatGLM-6B |åœ¨100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆChatGLM-6Bï¼Œé—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡| [training script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_hfdataset_demo.py)                       | [predict script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_hfdataset_demo.py) |
+        |[shibing624/llama-13b-belle-zh-lora](https://huggingface.co/shibing624/llama-13b-belle-zh-lora)| LLAMA-13B  |åœ¨100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆLlama-13Bï¼Œé—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡| [training script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_hfdataset_demo.py)                           | [predict script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_hfdataset_demo.py)     |
+        
+        ## ğŸš€Demo
         
         HuggingFace Demo: https://huggingface.co/spaces/shibing624/chinese-couplet-generate
         
         ![](docs/hf.png)
         
         run example: [examples/gradio_demo.py](examples/gradio_demo.py) to see the demo:
         
         ```shell
         python examples/gradio_demo.py
         ```
         
-        model trained
-        by [examples/T5/T5_Finetune_Chinese_Couplet.ipynb](https://github.com/shibing624/textgen/blob/main/examples/T5/T5_Finetune_Chinese_Couplet.ipynb)
+        model trained by [examples/t5/T5_Finetune_Chinese_Couplet.ipynb](https://github.com/shibing624/textgen/blob/main/examples/t5/T5_Finetune_Chinese_Couplet.ipynb)
         
-        # Install
+        ## ğŸ’¾Install
         
         ```shell
         pip install git+https://github.com/huggingface/transformers
         pip install git+https://github.com/huggingface/peft
         pip install -U textgen
         ```
         
@@ -119,42 +105,57 @@
         ```shell
         pip install torch # conda install pytorch
         git clone https://github.com/shibing624/textgen.git
         cd textgen
         python setup.py install
         ```
         
-        # Usage
+        ## ğŸ˜Usage
         
-        ## ChatGLM-6B LoRA æ¨¡å‹
+        ### ChatGLM-6B LoRA æ¨¡å‹
         
-        ### ä½¿ç”¨ChatGLM-6B LoRAå¾®è°ƒåçš„æ¨¡å‹
+        å®‰è£…æœ€æ–°å¼€å‘ç‰ˆçš„peftåº“ï¼Œæ”¯æŒLoRAæ¨¡å‹
+        
+        ```shell
+        pip install git+https://github.com/huggingface/peft
+        ```
+        
+        #### ä½¿ç”¨ChatGLM-6B LoRAå¾®è°ƒåçš„æ¨¡å‹
         
         example: [examples/chatglm/predict_demo.py](https://github.com/shibing624/textgen/blob/main/examples/chatglm/predict_demo.py)
         
         ```python
         import sys
         
         sys.path.append('../..')
         from textgen import ChatGlmModel
         
         model = ChatGlmModel("chatglm", "THUDM/chatglm-6b", lora_name="shibing624/chatglm-6b-csc-zh-lora")
         r = model.predict(["å¯¹ä¸‹é¢ä¸­æ–‡æ‹¼å†™çº é”™ï¼š\nå°‘å…ˆé˜Ÿå‘˜å› è¯¥ä¸ºè€äººè®©åã€‚\nç­”ï¼š"])
         print(r)  # ['å°‘å…ˆé˜Ÿå‘˜åº”è¯¥ä¸ºè€äººè®©åº§ã€‚\né”™è¯¯å­—ï¼šå› ï¼Œå']
         ```
         
-        ### è®­ç»ƒChatGLM-6B LoRAæ¨¡å‹
+        PSï¼šç”±äºä½¿ç”¨äº†å¼€å‘ä¸­çš„peftåº“ï¼Œå¯èƒ½ç”±äºç‰ˆæœ¬æ›´æ–°ï¼Œå¯¼è‡´LoRAæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå»ºè®®ä½¿ç”¨ä¸‹é¢çš„è®­ç»ƒæ–¹æ³•ï¼Œè‡ªå·±è®­ç»ƒLoRAæ¨¡å‹ã€‚
+        
+        #### è®­ç»ƒChatGLM-6B LoRAæ¨¡å‹
         
         æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†ï¼Œæ•°æ®é›†æ ¼å¼å‚è€ƒ[examples/data/zh_csc_test.tsv](https://github.com/shibing624/textgen/blob/main/examples/data/zh_csc_test.tsv)ã€‚
         
         example: [examples/chatglm/training_chatglm_demo.py](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_demo.py)
         
-        ## LLAMA LoRA æ¨¡å‹
+        ### LLAMA LoRA æ¨¡å‹
         
-        ### ä½¿ç”¨LLAMA LoRAå¾®è°ƒåçš„æ¨¡å‹
+        å®‰è£…æœ€æ–°å¼€å‘ç‰ˆçš„transformerså’Œpeftåº“ï¼Œæ”¯æŒLLAMAã€LoRAæ¨¡å‹
+        
+        ```shell
+        pip install transformers>=4.28.1
+        pip install git+https://github.com/huggingface/peft
+        ```
+        
+        #### ä½¿ç”¨LLAMA LoRAå¾®è°ƒåçš„æ¨¡å‹
         
         example: [examples/llama/predict_demo.py](https://github.com/shibing624/textgen/blob/main/examples/llama/predict_demo.py)
         
         ```python
         import sys
         
         sys.path.append('../..')
@@ -167,19 +168,19 @@
         
         model = LlamaModel("llama", "decapoda-research/llama-7b-hf", lora_name="ziqingyang/chinese-alpaca-lora-7b")
         predict_sentence = generate_prompt("é—®ï¼šç”¨ä¸€å¥è¯æè¿°åœ°çƒä¸ºä»€ä¹ˆæ˜¯ç‹¬ä¸€æ— äºŒçš„ã€‚\nç­”ï¼š")
         r = model.predict([predict_sentence])
         print(r)  # ['åœ°çƒæ˜¯å”¯ä¸€ä¸€é¢—æ‹¥æœ‰ç”Ÿå‘½çš„è¡Œæ˜Ÿã€‚']
         ```
         
-        ### è®­ç»ƒLLAMA LoRAæ¨¡å‹
+        #### è®­ç»ƒLLAMA LoRAæ¨¡å‹
         
         example: [examples/llama/training_llama_demo.py](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_demo.py)
         
-        ## ConvSeq2Seq æ¨¡å‹
+        ### ConvSeq2Seq æ¨¡å‹
         
         è®­ç»ƒå¹¶é¢„æµ‹ConvSeq2Seqæ¨¡å‹ï¼š
         
         example: [examples/seq2sesq/training_convseq2seq_model_demo.py](https://github.com/shibing624/textgen/blob/main/examples/seq2seq/training_convseq2seq_model_demo.py)
         
         ```python
         import argparse
@@ -224,30 +225,30 @@
         output:
         
         ```bash
         inputs: ["ä»€ä¹ˆæ˜¯ai", "ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº", "ä½ çŸ¥é“çƒ­åŠ›å­¦å—"]
         outputs: ['äººå·¥æ™ºèƒ½æ˜¯å·¥ç¨‹å’Œç§‘å­¦çš„åˆ†æ”¯,è‡´åŠ›äºæ„å»ºæ€ç»´çš„æœºå™¨ã€‚', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•è¿è„‘ä¸Šå·¥ä½œï¼', 'æˆ‘ä¸èƒ½é”™çƒ­æ˜¯ä¸€ä¸ªç–¯ç‹‚çš„äººå·¥æ™ºèƒ½"200å¹´ã€‚']
         ```
         
-        ## BART æ¨¡å‹
+        ### BART æ¨¡å‹
         
         è®­ç»ƒå¹¶é¢„æµ‹BARTæ¨¡å‹ï¼š
         
         example: [examples/seq2sesq/training_bartseq2seq_zh_demo.py](https://github.com/shibing624/textgen/blob/main/examples/seq2seq/training_bartseq2seq_zh_demo.py)
         
         output:
         
         ```shell
         inputs: ['ä»€ä¹ˆæ˜¯ai', 'ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº', 'ä½ çŸ¥é“çƒ­åŠ›å­¦å—']
         outputs: ['äººå·¥æ™ºèƒ½æ˜¯å·¥ç¨‹å’Œç§‘å­¦çš„åˆ†æ”¯,è‡´åŠ›äºæ„', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•ç”µè„‘ä¸Š', 'ä»€ä¹ˆæ˜¯çƒ­åŠ›å­¦å—ï¼Ÿ']
         ```
         
-        ## T5 æ¨¡å‹
+        ### T5 æ¨¡å‹
         
-        example: [examples/T5/training_zh_t5_model_demo.py](https://github.com/shibing624/textgen/blob/main/examples/T5/training_zh_t5_model_demo.py)
+        example: [examples/t5/training_zh_t5_model_demo.py](https://github.com/shibing624/textgen/blob/main/examples/t5/training_zh_t5_model_demo.py)
         
         ```python
         import argparse
         from loguru import logger
         import pandas as pd
         import sys
         
@@ -338,48 +339,48 @@
         output:
         
         ```shell
         inputs: ['ä»€ä¹ˆæ˜¯ai', 'ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº', 'ä½ çŸ¥é“çƒ­åŠ›å­¦å—']
         outputs: ['äººå·¥æ™ºèƒ½æœ‰ä¸¤ä¸ªå¹¿ä¹‰çš„å®šä¹‰,ä»»ä½•æ‹Ÿäººçš„æœºæ¢°,å¦‚åœ¨å¡é›·å°”capeks', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨Python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•ç”µè„‘ä¸Šå·¥ä½œ!', 'ä»€ä¹ˆæ˜¯çƒ­åŠ›å­¦']
         ```
         
-        ## GPT2 æ¨¡å‹
+        ### GPT2 æ¨¡å‹
         
-        ### ä¸­æ–‡GPT2 - æ–‡ç« ç”Ÿæˆ
+        #### ä¸­æ–‡GPT2 - æ–‡ç« ç”Ÿæˆ
         
         ä½¿ç”¨ä¸­æ–‡æ•°æ®é›†ï¼ˆæ®µè½æ ¼å¼ï¼Œ`\n`é—´éš”ï¼‰ï¼Œè®­ç»ƒGPT2æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºè¯—æ­Œç”Ÿæˆã€æ–‡ç« ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
         
-        example: [examples/language_generation/training_zh_gpt2_demo.py](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_gpt2_demo.py)
+        example: [examples/gpt2/training_zh_gpt2_demo.py](https://github.com/shibing624/textgen/blob/main/examples/gpt2/training_zh_gpt2_demo.py)
         
-        ### ä¸­æ–‡GPT2 - å¯¹è”ç”Ÿæˆ
+        #### ä¸­æ–‡GPT2 - å¯¹è”ç”Ÿæˆ
         
         ä½¿ç”¨ä¸­æ–‡å¯¹è”æ•°æ®é›†ï¼ˆtsvæ ¼å¼ï¼Œ`\t`é—´éš”ï¼‰ï¼Œè‡ªå®šä¹‰æ•°æ®é›†è¯»å–Datasetï¼Œè®­ç»ƒGPT2æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºå¯¹è”ç”Ÿæˆã€å¯¹è¯ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
         
-        example: [examples/language_generation/training_couplet_gpt2_demo.py](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_couplet_gpt2_demo.py)
+        example: [examples/gpt2/training_couplet_gpt2_demo.py](https://github.com/shibing624/textgen/blob/main/examples/gpt2/training_couplet_gpt2_demo.py)
         
-        #### GPT2 vs T5ï¼š
+        GPT2 vs T5ï¼š
         
         1. éƒ½æ˜¯ä»Transformeræ”¹è¿›æ¥çš„ï¼ŒT5åŒæ—¶æœ‰ç¼–ç å™¨å’Œè§£ç å™¨ï¼ŒGPT2åªæœ‰è§£ç å™¨
         2. T5çš„æ¨¡å‹ä¼˜åŠ¿æ˜¯å¤„ç†ç»™å®šè¾“å…¥ï¼Œäº§å‡ºå¯¹åº”è¾“å‡ºçš„ä»»åŠ¡ï¼Œå¦‚ç¿»è¯‘ã€å¯¹è¯ã€é—®ç­”ç­‰
         3. GPT2çš„æ¨¡å‹ä¼˜åŠ¿æ˜¯è‡ªç”±åˆ›ä½œï¼Œå¦‚å†™ä¸€ç¯‡çŸ­æ–‡
         4. T5çš„å¯¹è”ç”Ÿæˆæ•ˆæœå¥½äºGPT2ã€GPT2çš„è¯—è¯ç”Ÿæˆæ•ˆæœå¥½äºT5
         
         - [å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)
         - [å¤è¯—ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%8F%A4%E8%AF%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)
         
-        ## SongNet æ¨¡å‹
+        ### SongNet æ¨¡å‹
         
         æ ¼å¼æ§åˆ¶çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œpaperè§[SongNet: Rigid Formats Controlled Text Generation](https://arxiv.org/abs/2004.08022)ï¼Œ
         é€‚ç”¨äºå¼ºéŸµå¾‹æ ¼å¼è¦æ±‚çš„è¯—æ­Œã€å¯¹è”ã€æ­Œè¯ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
         
-        example: [examples/language_generation/training_zh_songnet_demo.py](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)
+        example: [examples/songnet/training_zh_songnet_demo.py](https://github.com/shibing624/textgen/blob/main/examples/songnet/training_zh_songnet_demo.py)
         
-        ## Keyword Text Augmentation(EDA/UDA)
+        ### Keyword Text Augmentation(EDA/UDA)
         
-        example: [examples/text_augmentation_demo.py](examples/text_augmentation_demo.py)
+        example: [examples/text_augmentation/text_augmentation_demo.py](examples/text_augmentation/text_augmentation_demo.py)
         
         ```python
         import sys
         
         sys.path.append('..')
         from textgen.augment import TextAugment
         
@@ -416,19 +417,19 @@
         random-0.2: ('ä¸»è¦é™ªé™ªæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ä¸»è¦è®¡ç®—æœºè§†è§‰ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå—é™äºå†…å®¹', [('ç ”ç©¶', 'é™ªé™ª', 2, 4), ('ã€', 'ä¸»è¦', 13, 15), ('ç›¸å…³', 'å—é™äº', 27, 30)])
         insert-0.2: ('ä¸»è¦ç ”ç©¶æœºå™¨æœºå™¨å­¦ä¹ å­¦ä¹ ã€æ·±åº¦æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿç›¸å…³å†…å®¹', [('æœºå™¨', 'æœºå™¨æœºå™¨', 4, 8), ('å­¦ä¹ ', 'å­¦ä¹ å­¦ä¹ ', 8, 12), ('æ·±åº¦', 'æ·±åº¦æ·±åº¦', 13, 17)])
         delete-0.2: ('ä¸»è¦ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€å¯¹è¯ç³»ç»Ÿç›¸å…³å†…å®¹', [('æ™ºèƒ½', '', 20, 20)])
         tfidf-0.2: ('ä¸€æ˜¯ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºå¬è§‰ã€æ™ºèƒ½äº¤è°ˆç³»ç»Ÿå¯†åˆ‡ç›¸å…³å†…å®¹', [('ä¸»è¦', 'ä¸€æ˜¯', 0, 2), ('è§†è§‰', 'å¬è§‰', 17, 19), ('å¯¹è¯', 'äº¤è°ˆ', 22, 24), ('ç›¸å…³', 'å¯†åˆ‡ç›¸å…³', 26, 30)])
         mix-0.2: ('ä¸»è¦ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ã€è®¡ç®—æœºå¬è§‰ã€æ™ºèƒ½å¯¹è¯è½¯ä»¶ç³»ç»Ÿç›¸å…³å†…å®¹', [('å­¦ä¹ ', 'å­¦', 11, 12), ('è§†è§‰', 'å¬è§‰', 16, 18), ('ç³»ç»Ÿ', 'è½¯ä»¶ç³»ç»Ÿ', 23, 27)])
         ```
         
-        ## TGLS æ¨¡å‹ï¼ˆæ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼‰
+        ### TGLS æ¨¡å‹ï¼ˆæ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼‰
         
         æ— ç›‘ç£çš„ä¸­æ–‡ç”µå•†è¯„è®ºç”Ÿæˆï¼šä»**ç”µå•†è¯„è®º**ä¸­æå–ç”¨æˆ·è¡¨è¾¾è§‚ç‚¹çš„çŸ­å¥å¹¶è¿›è¡Œç»„åˆæ¥ç”Ÿæˆä»¿çœŸè¯„è®ºã€‚
         
-        example: [examples/unsup_generation_demo.py](examples/unsup_generation_demo.py)
+        example: [examples/unsup_generation/unsup_generation_demo.py](examples/unsup_generation/unsup_generation_demo.py)
         
         ```python
         import os
         import sys
         
         sys.path.append('..')
         from textgen.unsup_generation import TglsModel, load_list
@@ -483,71 +484,72 @@
         å¸Œæœ›å¥½ç”¨ï¼Œé¢è†œç”¨è¿‡äº†å¾ˆå¥½ç”¨ï¼Œçš®è‚¤æ°´å«©å…‰æ»‘ç™½çš™ï¼Œè¡¥æ°´ä¸é”™ï¼Œä»·æ ¼ä¹Ÿåˆé€‚ã€‚
         å°±æ˜¯ç²¾åæ¶²å¤ªå°‘äº†ï¼Œä¿æ¹¿æ•ˆæœä¸é”™ã€‚
         é¢è†œçš„è¡¥æ°´æ•ˆæœéå¸¸å¥½ï¼Œä¿æ¹¿æ•ˆæœç¡®å®å¾ˆèµï¼Œè¿™ä¸ªé¢è†œç›¸å¯¹äºèƒ¶åŸè›‹ç™½å’Œç¾ç™½çš„é‚£ä¸¤æ¬¾çš„é¢è†œçº¸è¦åšä¸€äº›ï¼Œçœ‹ç€ä»·æ ¼åˆé€‚ã€‚
         ```
         
         å‰10å¥æ˜¯çœŸå®ç”¨æˆ·è¯„è®ºï¼Œå10å¥æ˜¯ç”Ÿæˆçš„ã€‚
         
-        # Dataset
+        ## ğŸ“šDataset 
         
         1. 50ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†ï¼š[BelleGroup/train_0.5M_CN](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)
         2. 100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†ï¼š[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)
-        3.
-        5ä¸‡æ¡è‹±æ–‡ChatGPTæŒ‡ä»¤Alpacaæ•°æ®é›†ï¼š[50k English Stanford Alpaca dataset](https://github.com/tatsu-lab/stanford_alpaca#data-release)
+        3. 5ä¸‡æ¡è‹±æ–‡ChatGPTæŒ‡ä»¤Alpacaæ•°æ®é›†ï¼š[50k English Stanford Alpaca dataset](https://github.com/tatsu-lab/stanford_alpaca#data-release)
         4. 2ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Alpacaæ•°æ®é›†ï¼š[shibing624/alpaca-zh](https://huggingface.co/datasets/shibing624/alpaca-zh)
-        5. 69ä¸‡æ¡ä¸­æ–‡æŒ‡ä»¤Guanacoæ•°æ®é›†(Belle50ä¸‡æ¡+Guanaco19ä¸‡æ¡)
-           ï¼š[Chinese-Vicuna/guanaco_belle_merge_v1.0](https://huggingface.co/datasets/Chinese-Vicuna/guanaco_belle_merge_v1.0)
+        5. 69ä¸‡æ¡ä¸­æ–‡æŒ‡ä»¤Guanacoæ•°æ®é›†(Belle50ä¸‡æ¡+Guanaco19ä¸‡æ¡)ï¼š[Chinese-Vicuna/guanaco_belle_merge_v1.0](https://huggingface.co/datasets/Chinese-Vicuna/guanaco_belle_merge_v1.0)
         
-        # Contact
+        ## â˜ï¸Contact
         
         - Issue(å»ºè®®)
           ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/textgen.svg)](https://github.com/shibing624/textgen/issues)
         - é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com
         - å¾®ä¿¡æˆ‘ï¼š åŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸å-NLP* è¿›NLPäº¤æµç¾¤ã€‚
         
         <img src="docs/wechat.jpeg" width="200" />
         
-        # Citation
+        ## ğŸ˜‡Citation
         
         å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†textgenï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š
         
         ```latex
         @misc{textgen,
           title={textgen: Text Generation Tool},
           author={Xu Ming},
           year={2021},
           howpublished={\url{https://github.com/shibing624/textgen}},
         }
         ```
         
-        # License
+        ## ğŸ¤—License
         
         æˆæƒåè®®ä¸º [The Apache License 2.0](/LICENSE)ï¼Œå¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ textgençš„é“¾æ¥å’Œæˆæƒåè®®ã€‚
         
-        # Contribute
+        ## ğŸ˜Contribute
         
         é¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š
         
         - åœ¨`tests`æ·»åŠ ç›¸åº”çš„å•å…ƒæµ‹è¯•
         - ä½¿ç”¨`python -m pytest`æ¥è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿æ‰€æœ‰å•æµ‹éƒ½æ˜¯é€šè¿‡çš„
         
         ä¹‹åå³å¯æäº¤PRã€‚
         
-        ## Reference
+        ## ğŸ’•Acknowledgements 
         
         - [PaddlePaddle/ERNIE](https://github.com/PaddlePaddle/ERNIE)
         - [minimaxir/textgenrnn](https://github.com/minimaxir/textgenrnn)
         - [minimaxir/gpt-2-simple](https://github.com/minimaxir/gpt-2-simple)
         - [asyml/texar](https://github.com/asyml/texar)
         - [yangjianxin1/GPT2-chitchat](https://github.com/yangjianxin1/GPT2-chitchat)
         - [williamSYSU/TextGAN-PyTorch](https://github.com/williamSYSU/TextGAN-PyTorch)
         - [RUCAIBox/TextBox](https://github.com/RUCAIBox/TextBox)
-        - [Tiiiger/bert_score]()
+        - [Tiiiger/bert_score](https://github.com/Tiiiger/bert_score)
+        - [ThilinaRajapakse/simpletransformers](https://github.com/ThilinaRajapakse/simpletransformers)
         - [1YCxZ/Fake-review-generation](https://github.com/1YCxZ/Fake-review-generation)
         - [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora/blob/main/finetune.py)
+        
+        Thanks for their great work!
 Keywords: textgen,text-generation,Text Generation Tool,ernie-gen,chinese text generation
 Platform: UNKNOWN
 Classifier: Intended Audience :: Developers
 Classifier: Operating System :: OS Independent
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Programming Language :: Python :: 3
 Classifier: Topic :: Text Processing :: Linguistic
```

### Comparing `textgen-0.2.1/README.md` & `textgen-0.2.2/README.md`

 * *Files 8% similar despite different names*

```diff
@@ -1,108 +1,94 @@
 [![PyPI version](https://badge.fury.io/py/textgen.svg)](https://badge.fury.io/py/textgen)
 [![Downloads](https://pepy.tech/badge/textgen)](https://pepy.tech/project/textgen)
 [![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)
-[![GitHub contributors](https://img.shields.io/github/contributors/shibing624/textgen.svg)](https://github.com/shibing624/textgen/graphs/contributors)
 [![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
 [![python_version](https://img.shields.io/badge/Python-3.8%2B-green.svg)](requirements.txt)
 [![GitHub issues](https://img.shields.io/github/issues/shibing624/textgen.svg)](https://github.com/shibing624/textgen/issues)
 [![Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact)
 
 # TextGen
 
-ğŸŒˆ Implementation of Text Generation models.
+ğŸŒˆImplementation of Text Generation models.
 
 **textgen**å®ç°äº†å¤šç§æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼šLLAMAã€ChatGLMã€UDAã€GPT2ã€Seq2Seqã€BARTã€T5ã€SongNetç­‰æ¨¡å‹ï¼Œå¼€ç®±å³ç”¨ã€‚
 
 **Guide**
 
 - [Feature](#Feature)
 - [Install](#install)
 - [Usage](#usage)
 - [Contact](#Contact)
-- [Reference](#reference)
+- [License](#License)
 
-# Feature
+## ğŸ˜ŠFeature
 
-## æ–‡æœ¬ç”Ÿæˆ
+### æ–‡æœ¬ç”Ÿæˆ
 
 1. seq2seq: Seq2Seqã€ConvSeq2Seqã€BART
 2. language_modeling: GPT2ã€SongNetã€ChatGLMã€LLAMA
 3. t5: T5ã€CopyT5
 
-## æ–‡æœ¬æ‰©å¢
+### æ–‡æœ¬æ‰©å¢
 
-### è¯ç²’åº¦æ‰©å¢
+#### è¯ç²’åº¦æ‰©å¢
 
 1. UDAï¼Œéæ ¸å¿ƒè¯æ›¿æ¢
 2. EDAï¼Œç®€å•æ•°æ®å¢å¼ºæŠ€æœ¯ï¼šç›¸ä¼¼è¯ã€åŒä¹‰è¯æ›¿æ¢ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢
 
-### å¥ç²’åº¦æ‰©å¢
+#### å¥ç²’åº¦æ‰©å¢
 
 1. å›è¯‘ï¼ˆBT, Back Translateï¼‰ï¼šä¸­æ–‡-è‹±æ–‡-ä¸­æ–‡
 2. GPT2æ¨¡å‹ç»­å†™ï¼šçŸ­æ–‡æœ¬->é•¿æ–‡æœ¬
 3. BARTæ‘˜è¦æ¨¡å‹ï¼šé•¿æ–‡æœ¬->çŸ­æ–‡æœ¬
 4. TGLSï¼šæ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
 
-## åŠŸèƒ½åˆ—è¡¨
+### åŠŸèƒ½åˆ—è¡¨
 
 - [ChatGLM](textgen/chatglm)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†ChatGLM-6Bæ¨¡å‹LoRAå¾®è°ƒè®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºå¥å­çº é”™ã€å¯¹è¯ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
 - [LLAMA](textgen/llama)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†LLAMAæ¨¡å‹LoRAå¾®è°ƒè®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºå¤šè½®å¯¹è¯ç”Ÿæˆä»»åŠ¡
-- [UDA(éæ ¸å¿ƒè¯æ›¿æ¢)/EDA](textgen/augment/word_level_augment.py)ï¼šæœ¬é¡¹ç›®å‚è€ƒGoogleçš„UDA(éæ ¸å¿ƒè¯æ›¿æ¢)
-  ç®—æ³•å’ŒEDAç®—æ³•ï¼ŒåŸºäºTF-IDFå°†å¥å­ä¸­éƒ¨åˆ†ä¸é‡è¦è¯æ›¿æ¢ä¸ºåŒä¹‰è¯ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢ç­‰æ–¹æ³•ï¼Œäº§ç”Ÿæ–°çš„æ–‡æœ¬ï¼Œå®ç°äº†æ–‡æœ¬æ‰©å¢
+- [UDA(éæ ¸å¿ƒè¯æ›¿æ¢)/EDA](textgen/augment/word_level_augment.py)ï¼šæœ¬é¡¹ç›®å‚è€ƒGoogleçš„UDA(éæ ¸å¿ƒè¯æ›¿æ¢)ç®—æ³•å’ŒEDAç®—æ³•ï¼ŒåŸºäºTF-IDFå°†å¥å­ä¸­éƒ¨åˆ†ä¸é‡è¦è¯æ›¿æ¢ä¸ºåŒä¹‰è¯ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢ç­‰æ–¹æ³•ï¼Œäº§ç”Ÿæ–°çš„æ–‡æœ¬ï¼Œå®ç°äº†æ–‡æœ¬æ‰©å¢
 - [BT(å›è¯‘)](textgen/augment/sentence_level_augment.py)ï¼šæœ¬é¡¹ç›®åŸºäºç™¾åº¦ç¿»è¯‘APIå®ç°äº†å›è¯‘åŠŸèƒ½ï¼Œå…ˆæŠŠä¸­æ–‡å¥å­ç¿»è¯‘ä¸ºè‹±æ–‡ï¼Œå†æŠŠè‹±æ–‡ç¿»è¯‘ä¸ºæ–°çš„ä¸­æ–‡
 - [Seq2Seq](textgen/seq2seq)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†Seq2Seqã€ConvSeq2Seqã€BARTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€æ‘˜è¦ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
 - [T5](textgen/t5)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†T5å’ŒCopyT5æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€å¯¹è”ç”Ÿæˆã€æ–‡æ¡ˆæ’°å†™ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
 - [GPT2](textgen/language_modeling)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†GTP2æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡ç« ç”Ÿæˆã€å¯¹è”ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
 - [SongNet](textgen/language_modeling/songnet_model.py)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†SongNetæ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºè§„èŒƒæ ¼å¼çš„è¯—è¯ã€æ­Œè¯ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
-- [TGLS](textgen/unsup_generation)
-  ï¼šæœ¬é¡¹ç›®å®ç°äº†[TGLS](https://www.jiqizhixin.com/articles/2020-08-11-5)æ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œæ˜¯ä¸€ç§â€œå…ˆæœç´¢åå­¦ä¹ â€çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡åå¤è¿­ä»£å­¦ä¹ å€™é€‰é›†ï¼Œæœ€ç»ˆæ¨¡å‹èƒ½ç”Ÿæˆç±»ä¼¼å€™é€‰é›†çš„é«˜è´¨é‡ç›¸ä¼¼æ–‡æœ¬
+- [TGLS](textgen/unsup_generation)ï¼šæœ¬é¡¹ç›®å®ç°äº†[TGLS](https://www.jiqizhixin.com/articles/2020-08-11-5)æ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œæ˜¯ä¸€ç§â€œå…ˆæœç´¢åå­¦ä¹ â€çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡åå¤è¿­ä»£å­¦ä¹ å€™é€‰é›†ï¼Œæœ€ç»ˆæ¨¡å‹èƒ½ç”Ÿæˆç±»ä¼¼å€™é€‰é›†çš„é«˜è´¨é‡ç›¸ä¼¼æ–‡æœ¬
 
-## Release Models
+### Release Models
 
 releaseåŸºäº`textgen`è®­ç»ƒçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å·²ç»releaseåˆ°HuggingFace modelsï¼ŒæŒ‡å®šæ¨¡å‹åç§°`textgen`ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚
 
-|Model|Arch|Introduce|Training|Inference| |:-- |:--- |:--- |:--- |:--- |
-|[shibing624/prompt-t5-base-chinese](https://huggingface.co/shibing624/prompt-t5-base-chinese)|T5|ä¸­æ–‡NLPå¤šä»»åŠ¡Promptæ¨¡å‹|[prompt-t5-base-chinese.md](https://github.com/shibing624/textgen/blob/main/docs/prompt-t5-base-chinese.md)|[predict
-script](https://github.com/shibing624/textgen/blob/main/examples/t5_prompt_demo.py)|
-|[shibing624/t5-chinese-couplet](https://huggingface.co/shibing624/t5-chinese-couplet)|T5|fine-tunedä¸­æ–‡å¯¹è”åçš„æ¨¡å‹|[å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)|[predict
-script](https://github.com/shibing624/textgen/blob/main/examples/t5_couplet_demo.py)|
-|[shibing624/songnet-base-chinese](https://huggingface.co/shibing624/songnet-base-chinese)|SongNet|SongNeté¢„è®­ç»ƒæ¨¡å‹|-|-|
-|[shibing624/songnet-base-chinese-songci](https://huggingface.co/shibing624/songnet-base-chinese-songci)|SongNet|fine-tunedå®‹è¯åçš„æ¨¡å‹|[training
-script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict
-script](https://github.com/shibing624/textgen/blob/main/examples/songnet_songci_demo.py)|
-|[shibing624/songnet-base-chinese-couplet](https://huggingface.co/shibing624/songnet-base-chinese-couplet)|SongNet|fine-tunedå¯¹è”åçš„æ¨¡å‹|[training
-script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict
-script](https://github.com/shibing624/textgen/blob/main/examples/songnet_couplet_demo.py)|
-|[shibing624/chatglm-6b-csc-zh-lora](https://huggingface.co/shibing624/chatglm-6b-csc-zh-lora)|ChatGLM-6B|åœ¨27ä¸‡ä¸­æ–‡æ‹¼å†™çº é”™æ•°æ®[shibing624/CSC](https://huggingface.co/datasets/shibing624/CSC)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆChatGLM-6Bï¼Œçº é”™æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡|[training
-script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_csc_demo.py)|[predict
-script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/csc_demo.py)|
-|[shibing624/chatglm-6b-belle-zh-lora](https://huggingface.co/shibing624/chatglm-6b-belle-zh-lora)|ChatGLM-6B|åœ¨100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆChatGLM-6Bï¼Œé—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡|[training
-script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_hfdataset_demo.py)|[predict
-script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_hfdataset_demo.py)|
-|[shibing624/llama-13b-belle-zh-lora](https://huggingface.co/shibing624/llama-13b-belle-zh-lora)|ChatGLM-6B|åœ¨100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆLlama-13Bï¼Œé—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡|[training
-script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_hfdataset_demo.py)|[predict
-script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_hfdataset_demo.py)|
 
-# Demo
+|Model| Arch       |Introduce| Training                                                                                                                                     | Inference                                                                                                             | 
+|:-- |:-----------|:--- |:---------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------|
+|[shibing624/prompt-t5-base-chinese](https://huggingface.co/shibing624/prompt-t5-base-chinese)| T5         |ä¸­æ–‡NLPå¤šä»»åŠ¡Promptæ¨¡å‹| [prompt-t5-base-chinese.md](https://github.com/shibing624/textgen/blob/main/docs/prompt-t5-base-chinese.md)                                  | [predict script](https://github.com/shibing624/textgen/blob/main/examples/t5/t5_prompt_demo.py)                       |
+|[shibing624/t5-chinese-couplet](https://huggingface.co/shibing624/t5-chinese-couplet)| T5         |fine-tunedä¸­æ–‡å¯¹è”åçš„æ¨¡å‹| [å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md) | [predict script](https://github.com/shibing624/textgen/blob/main/examples/t5/t5_couplet_demo.py)                      |
+|[shibing624/songnet-base-chinese](https://huggingface.co/shibing624/songnet-base-chinese)| SongNet    |SongNeté¢„è®­ç»ƒæ¨¡å‹| -                                                                                                                                            | -                                                                                                                     |
+|[shibing624/songnet-base-chinese-songci](https://huggingface.co/shibing624/songnet-base-chinese-songci)| SongNet    |fine-tunedå®‹è¯åçš„æ¨¡å‹| [training script](https://github.com/shibing624/textgen/blob/main/examples/songnet/training_zh_songnet_demo.py)                              | [predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet/songnet_songci_demo.py)             |
+|[shibing624/songnet-base-chinese-couplet](https://huggingface.co/shibing624/songnet-base-chinese-couplet)| SongNet    |fine-tunedå¯¹è”åçš„æ¨¡å‹| [training script](https://github.com/shibing624/textgen/blob/main/examples/songnet/training_zh_songnet_demo.py)                                 | [predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet/songnet_couplet_demo.py)            |
+|[shibing624/chatglm-6b-csc-zh-lora](https://huggingface.co/shibing624/chatglm-6b-csc-zh-lora)| ChatGLM-6B |åœ¨27ä¸‡ä¸­æ–‡æ‹¼å†™çº é”™æ•°æ®[shibing624/CSC](https://huggingface.co/datasets/shibing624/CSC)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆChatGLM-6Bï¼Œçº é”™æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡| [training script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_csc_demo.py)                             | [predict script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/csc_demo.py)                        |
+|[shibing624/chatglm-6b-belle-zh-lora](https://huggingface.co/shibing624/chatglm-6b-belle-zh-lora)| ChatGLM-6B |åœ¨100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆChatGLM-6Bï¼Œé—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡| [training script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_hfdataset_demo.py)                       | [predict script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_hfdataset_demo.py) |
+|[shibing624/llama-13b-belle-zh-lora](https://huggingface.co/shibing624/llama-13b-belle-zh-lora)| LLAMA-13B  |åœ¨100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆLlama-13Bï¼Œé—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡| [training script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_hfdataset_demo.py)                           | [predict script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_hfdataset_demo.py)     |
+
+## ğŸš€Demo
 
 HuggingFace Demo: https://huggingface.co/spaces/shibing624/chinese-couplet-generate
 
 ![](docs/hf.png)
 
 run example: [examples/gradio_demo.py](examples/gradio_demo.py) to see the demo:
 
 ```shell
 python examples/gradio_demo.py
 ```
 
-model trained
-by [examples/T5/T5_Finetune_Chinese_Couplet.ipynb](https://github.com/shibing624/textgen/blob/main/examples/T5/T5_Finetune_Chinese_Couplet.ipynb)
+model trained by [examples/t5/T5_Finetune_Chinese_Couplet.ipynb](https://github.com/shibing624/textgen/blob/main/examples/t5/T5_Finetune_Chinese_Couplet.ipynb)
 
-# Install
+## ğŸ’¾Install
 
 ```shell
 pip install git+https://github.com/huggingface/transformers
 pip install git+https://github.com/huggingface/peft
 pip install -U textgen
 ```
 
@@ -111,42 +97,57 @@
 ```shell
 pip install torch # conda install pytorch
 git clone https://github.com/shibing624/textgen.git
 cd textgen
 python setup.py install
 ```
 
-# Usage
+## ğŸ˜Usage
 
-## ChatGLM-6B LoRA æ¨¡å‹
+### ChatGLM-6B LoRA æ¨¡å‹
 
-### ä½¿ç”¨ChatGLM-6B LoRAå¾®è°ƒåçš„æ¨¡å‹
+å®‰è£…æœ€æ–°å¼€å‘ç‰ˆçš„peftåº“ï¼Œæ”¯æŒLoRAæ¨¡å‹
+
+```shell
+pip install git+https://github.com/huggingface/peft
+```
+
+#### ä½¿ç”¨ChatGLM-6B LoRAå¾®è°ƒåçš„æ¨¡å‹
 
 example: [examples/chatglm/predict_demo.py](https://github.com/shibing624/textgen/blob/main/examples/chatglm/predict_demo.py)
 
 ```python
 import sys
 
 sys.path.append('../..')
 from textgen import ChatGlmModel
 
 model = ChatGlmModel("chatglm", "THUDM/chatglm-6b", lora_name="shibing624/chatglm-6b-csc-zh-lora")
 r = model.predict(["å¯¹ä¸‹é¢ä¸­æ–‡æ‹¼å†™çº é”™ï¼š\nå°‘å…ˆé˜Ÿå‘˜å› è¯¥ä¸ºè€äººè®©åã€‚\nç­”ï¼š"])
 print(r)  # ['å°‘å…ˆé˜Ÿå‘˜åº”è¯¥ä¸ºè€äººè®©åº§ã€‚\né”™è¯¯å­—ï¼šå› ï¼Œå']
 ```
 
-### è®­ç»ƒChatGLM-6B LoRAæ¨¡å‹
+PSï¼šç”±äºä½¿ç”¨äº†å¼€å‘ä¸­çš„peftåº“ï¼Œå¯èƒ½ç”±äºç‰ˆæœ¬æ›´æ–°ï¼Œå¯¼è‡´LoRAæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå»ºè®®ä½¿ç”¨ä¸‹é¢çš„è®­ç»ƒæ–¹æ³•ï¼Œè‡ªå·±è®­ç»ƒLoRAæ¨¡å‹ã€‚
+
+#### è®­ç»ƒChatGLM-6B LoRAæ¨¡å‹
 
 æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†ï¼Œæ•°æ®é›†æ ¼å¼å‚è€ƒ[examples/data/zh_csc_test.tsv](https://github.com/shibing624/textgen/blob/main/examples/data/zh_csc_test.tsv)ã€‚
 
 example: [examples/chatglm/training_chatglm_demo.py](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_demo.py)
 
-## LLAMA LoRA æ¨¡å‹
+### LLAMA LoRA æ¨¡å‹
 
-### ä½¿ç”¨LLAMA LoRAå¾®è°ƒåçš„æ¨¡å‹
+å®‰è£…æœ€æ–°å¼€å‘ç‰ˆçš„transformerså’Œpeftåº“ï¼Œæ”¯æŒLLAMAã€LoRAæ¨¡å‹
+
+```shell
+pip install transformers>=4.28.1
+pip install git+https://github.com/huggingface/peft
+```
+
+#### ä½¿ç”¨LLAMA LoRAå¾®è°ƒåçš„æ¨¡å‹
 
 example: [examples/llama/predict_demo.py](https://github.com/shibing624/textgen/blob/main/examples/llama/predict_demo.py)
 
 ```python
 import sys
 
 sys.path.append('../..')
@@ -159,19 +160,19 @@
 
 model = LlamaModel("llama", "decapoda-research/llama-7b-hf", lora_name="ziqingyang/chinese-alpaca-lora-7b")
 predict_sentence = generate_prompt("é—®ï¼šç”¨ä¸€å¥è¯æè¿°åœ°çƒä¸ºä»€ä¹ˆæ˜¯ç‹¬ä¸€æ— äºŒçš„ã€‚\nç­”ï¼š")
 r = model.predict([predict_sentence])
 print(r)  # ['åœ°çƒæ˜¯å”¯ä¸€ä¸€é¢—æ‹¥æœ‰ç”Ÿå‘½çš„è¡Œæ˜Ÿã€‚']
 ```
 
-### è®­ç»ƒLLAMA LoRAæ¨¡å‹
+#### è®­ç»ƒLLAMA LoRAæ¨¡å‹
 
 example: [examples/llama/training_llama_demo.py](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_demo.py)
 
-## ConvSeq2Seq æ¨¡å‹
+### ConvSeq2Seq æ¨¡å‹
 
 è®­ç»ƒå¹¶é¢„æµ‹ConvSeq2Seqæ¨¡å‹ï¼š
 
 example: [examples/seq2sesq/training_convseq2seq_model_demo.py](https://github.com/shibing624/textgen/blob/main/examples/seq2seq/training_convseq2seq_model_demo.py)
 
 ```python
 import argparse
@@ -216,30 +217,30 @@
 output:
 
 ```bash
 inputs: ["ä»€ä¹ˆæ˜¯ai", "ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº", "ä½ çŸ¥é“çƒ­åŠ›å­¦å—"]
 outputs: ['äººå·¥æ™ºèƒ½æ˜¯å·¥ç¨‹å’Œç§‘å­¦çš„åˆ†æ”¯,è‡´åŠ›äºæ„å»ºæ€ç»´çš„æœºå™¨ã€‚', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•è¿è„‘ä¸Šå·¥ä½œï¼', 'æˆ‘ä¸èƒ½é”™çƒ­æ˜¯ä¸€ä¸ªç–¯ç‹‚çš„äººå·¥æ™ºèƒ½"200å¹´ã€‚']
 ```
 
-## BART æ¨¡å‹
+### BART æ¨¡å‹
 
 è®­ç»ƒå¹¶é¢„æµ‹BARTæ¨¡å‹ï¼š
 
 example: [examples/seq2sesq/training_bartseq2seq_zh_demo.py](https://github.com/shibing624/textgen/blob/main/examples/seq2seq/training_bartseq2seq_zh_demo.py)
 
 output:
 
 ```shell
 inputs: ['ä»€ä¹ˆæ˜¯ai', 'ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº', 'ä½ çŸ¥é“çƒ­åŠ›å­¦å—']
 outputs: ['äººå·¥æ™ºèƒ½æ˜¯å·¥ç¨‹å’Œç§‘å­¦çš„åˆ†æ”¯,è‡´åŠ›äºæ„', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•ç”µè„‘ä¸Š', 'ä»€ä¹ˆæ˜¯çƒ­åŠ›å­¦å—ï¼Ÿ']
 ```
 
-## T5 æ¨¡å‹
+### T5 æ¨¡å‹
 
-example: [examples/T5/training_zh_t5_model_demo.py](https://github.com/shibing624/textgen/blob/main/examples/T5/training_zh_t5_model_demo.py)
+example: [examples/t5/training_zh_t5_model_demo.py](https://github.com/shibing624/textgen/blob/main/examples/t5/training_zh_t5_model_demo.py)
 
 ```python
 import argparse
 from loguru import logger
 import pandas as pd
 import sys
 
@@ -330,48 +331,48 @@
 output:
 
 ```shell
 inputs: ['ä»€ä¹ˆæ˜¯ai', 'ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº', 'ä½ çŸ¥é“çƒ­åŠ›å­¦å—']
 outputs: ['äººå·¥æ™ºèƒ½æœ‰ä¸¤ä¸ªå¹¿ä¹‰çš„å®šä¹‰,ä»»ä½•æ‹Ÿäººçš„æœºæ¢°,å¦‚åœ¨å¡é›·å°”capeks', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨Python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•ç”µè„‘ä¸Šå·¥ä½œ!', 'ä»€ä¹ˆæ˜¯çƒ­åŠ›å­¦']
 ```
 
-## GPT2 æ¨¡å‹
+### GPT2 æ¨¡å‹
 
-### ä¸­æ–‡GPT2 - æ–‡ç« ç”Ÿæˆ
+#### ä¸­æ–‡GPT2 - æ–‡ç« ç”Ÿæˆ
 
 ä½¿ç”¨ä¸­æ–‡æ•°æ®é›†ï¼ˆæ®µè½æ ¼å¼ï¼Œ`\n`é—´éš”ï¼‰ï¼Œè®­ç»ƒGPT2æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºè¯—æ­Œç”Ÿæˆã€æ–‡ç« ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
 
-example: [examples/language_generation/training_zh_gpt2_demo.py](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_gpt2_demo.py)
+example: [examples/gpt2/training_zh_gpt2_demo.py](https://github.com/shibing624/textgen/blob/main/examples/gpt2/training_zh_gpt2_demo.py)
 
-### ä¸­æ–‡GPT2 - å¯¹è”ç”Ÿæˆ
+#### ä¸­æ–‡GPT2 - å¯¹è”ç”Ÿæˆ
 
 ä½¿ç”¨ä¸­æ–‡å¯¹è”æ•°æ®é›†ï¼ˆtsvæ ¼å¼ï¼Œ`\t`é—´éš”ï¼‰ï¼Œè‡ªå®šä¹‰æ•°æ®é›†è¯»å–Datasetï¼Œè®­ç»ƒGPT2æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºå¯¹è”ç”Ÿæˆã€å¯¹è¯ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
 
-example: [examples/language_generation/training_couplet_gpt2_demo.py](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_couplet_gpt2_demo.py)
+example: [examples/gpt2/training_couplet_gpt2_demo.py](https://github.com/shibing624/textgen/blob/main/examples/gpt2/training_couplet_gpt2_demo.py)
 
-#### GPT2 vs T5ï¼š
+GPT2 vs T5ï¼š
 
 1. éƒ½æ˜¯ä»Transformeræ”¹è¿›æ¥çš„ï¼ŒT5åŒæ—¶æœ‰ç¼–ç å™¨å’Œè§£ç å™¨ï¼ŒGPT2åªæœ‰è§£ç å™¨
 2. T5çš„æ¨¡å‹ä¼˜åŠ¿æ˜¯å¤„ç†ç»™å®šè¾“å…¥ï¼Œäº§å‡ºå¯¹åº”è¾“å‡ºçš„ä»»åŠ¡ï¼Œå¦‚ç¿»è¯‘ã€å¯¹è¯ã€é—®ç­”ç­‰
 3. GPT2çš„æ¨¡å‹ä¼˜åŠ¿æ˜¯è‡ªç”±åˆ›ä½œï¼Œå¦‚å†™ä¸€ç¯‡çŸ­æ–‡
 4. T5çš„å¯¹è”ç”Ÿæˆæ•ˆæœå¥½äºGPT2ã€GPT2çš„è¯—è¯ç”Ÿæˆæ•ˆæœå¥½äºT5
 
 - [å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)
 - [å¤è¯—ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%8F%A4%E8%AF%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)
 
-## SongNet æ¨¡å‹
+### SongNet æ¨¡å‹
 
 æ ¼å¼æ§åˆ¶çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œpaperè§[SongNet: Rigid Formats Controlled Text Generation](https://arxiv.org/abs/2004.08022)ï¼Œ
 é€‚ç”¨äºå¼ºéŸµå¾‹æ ¼å¼è¦æ±‚çš„è¯—æ­Œã€å¯¹è”ã€æ­Œè¯ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
 
-example: [examples/language_generation/training_zh_songnet_demo.py](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)
+example: [examples/songnet/training_zh_songnet_demo.py](https://github.com/shibing624/textgen/blob/main/examples/songnet/training_zh_songnet_demo.py)
 
-## Keyword Text Augmentation(EDA/UDA)
+### Keyword Text Augmentation(EDA/UDA)
 
-example: [examples/text_augmentation_demo.py](examples/text_augmentation_demo.py)
+example: [examples/text_augmentation/text_augmentation_demo.py](examples/text_augmentation/text_augmentation_demo.py)
 
 ```python
 import sys
 
 sys.path.append('..')
 from textgen.augment import TextAugment
 
@@ -408,19 +409,19 @@
 random-0.2: ('ä¸»è¦é™ªé™ªæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ä¸»è¦è®¡ç®—æœºè§†è§‰ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå—é™äºå†…å®¹', [('ç ”ç©¶', 'é™ªé™ª', 2, 4), ('ã€', 'ä¸»è¦', 13, 15), ('ç›¸å…³', 'å—é™äº', 27, 30)])
 insert-0.2: ('ä¸»è¦ç ”ç©¶æœºå™¨æœºå™¨å­¦ä¹ å­¦ä¹ ã€æ·±åº¦æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿç›¸å…³å†…å®¹', [('æœºå™¨', 'æœºå™¨æœºå™¨', 4, 8), ('å­¦ä¹ ', 'å­¦ä¹ å­¦ä¹ ', 8, 12), ('æ·±åº¦', 'æ·±åº¦æ·±åº¦', 13, 17)])
 delete-0.2: ('ä¸»è¦ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€å¯¹è¯ç³»ç»Ÿç›¸å…³å†…å®¹', [('æ™ºèƒ½', '', 20, 20)])
 tfidf-0.2: ('ä¸€æ˜¯ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºå¬è§‰ã€æ™ºèƒ½äº¤è°ˆç³»ç»Ÿå¯†åˆ‡ç›¸å…³å†…å®¹', [('ä¸»è¦', 'ä¸€æ˜¯', 0, 2), ('è§†è§‰', 'å¬è§‰', 17, 19), ('å¯¹è¯', 'äº¤è°ˆ', 22, 24), ('ç›¸å…³', 'å¯†åˆ‡ç›¸å…³', 26, 30)])
 mix-0.2: ('ä¸»è¦ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ã€è®¡ç®—æœºå¬è§‰ã€æ™ºèƒ½å¯¹è¯è½¯ä»¶ç³»ç»Ÿç›¸å…³å†…å®¹', [('å­¦ä¹ ', 'å­¦', 11, 12), ('è§†è§‰', 'å¬è§‰', 16, 18), ('ç³»ç»Ÿ', 'è½¯ä»¶ç³»ç»Ÿ', 23, 27)])
 ```
 
-## TGLS æ¨¡å‹ï¼ˆæ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼‰
+### TGLS æ¨¡å‹ï¼ˆæ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼‰
 
 æ— ç›‘ç£çš„ä¸­æ–‡ç”µå•†è¯„è®ºç”Ÿæˆï¼šä»**ç”µå•†è¯„è®º**ä¸­æå–ç”¨æˆ·è¡¨è¾¾è§‚ç‚¹çš„çŸ­å¥å¹¶è¿›è¡Œç»„åˆæ¥ç”Ÿæˆä»¿çœŸè¯„è®ºã€‚
 
-example: [examples/unsup_generation_demo.py](examples/unsup_generation_demo.py)
+example: [examples/unsup_generation/unsup_generation_demo.py](examples/unsup_generation/unsup_generation_demo.py)
 
 ```python
 import os
 import sys
 
 sys.path.append('..')
 from textgen.unsup_generation import TglsModel, load_list
@@ -475,64 +476,65 @@
 å¸Œæœ›å¥½ç”¨ï¼Œé¢è†œç”¨è¿‡äº†å¾ˆå¥½ç”¨ï¼Œçš®è‚¤æ°´å«©å…‰æ»‘ç™½çš™ï¼Œè¡¥æ°´ä¸é”™ï¼Œä»·æ ¼ä¹Ÿåˆé€‚ã€‚
 å°±æ˜¯ç²¾åæ¶²å¤ªå°‘äº†ï¼Œä¿æ¹¿æ•ˆæœä¸é”™ã€‚
 é¢è†œçš„è¡¥æ°´æ•ˆæœéå¸¸å¥½ï¼Œä¿æ¹¿æ•ˆæœç¡®å®å¾ˆèµï¼Œè¿™ä¸ªé¢è†œç›¸å¯¹äºèƒ¶åŸè›‹ç™½å’Œç¾ç™½çš„é‚£ä¸¤æ¬¾çš„é¢è†œçº¸è¦åšä¸€äº›ï¼Œçœ‹ç€ä»·æ ¼åˆé€‚ã€‚
 ```
 
 å‰10å¥æ˜¯çœŸå®ç”¨æˆ·è¯„è®ºï¼Œå10å¥æ˜¯ç”Ÿæˆçš„ã€‚
 
-# Dataset
+## ğŸ“šDataset 
 
 1. 50ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†ï¼š[BelleGroup/train_0.5M_CN](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)
 2. 100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†ï¼š[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)
-3.
-5ä¸‡æ¡è‹±æ–‡ChatGPTæŒ‡ä»¤Alpacaæ•°æ®é›†ï¼š[50k English Stanford Alpaca dataset](https://github.com/tatsu-lab/stanford_alpaca#data-release)
+3. 5ä¸‡æ¡è‹±æ–‡ChatGPTæŒ‡ä»¤Alpacaæ•°æ®é›†ï¼š[50k English Stanford Alpaca dataset](https://github.com/tatsu-lab/stanford_alpaca#data-release)
 4. 2ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Alpacaæ•°æ®é›†ï¼š[shibing624/alpaca-zh](https://huggingface.co/datasets/shibing624/alpaca-zh)
-5. 69ä¸‡æ¡ä¸­æ–‡æŒ‡ä»¤Guanacoæ•°æ®é›†(Belle50ä¸‡æ¡+Guanaco19ä¸‡æ¡)
-   ï¼š[Chinese-Vicuna/guanaco_belle_merge_v1.0](https://huggingface.co/datasets/Chinese-Vicuna/guanaco_belle_merge_v1.0)
+5. 69ä¸‡æ¡ä¸­æ–‡æŒ‡ä»¤Guanacoæ•°æ®é›†(Belle50ä¸‡æ¡+Guanaco19ä¸‡æ¡)ï¼š[Chinese-Vicuna/guanaco_belle_merge_v1.0](https://huggingface.co/datasets/Chinese-Vicuna/guanaco_belle_merge_v1.0)
 
-# Contact
+## â˜ï¸Contact
 
 - Issue(å»ºè®®)
   ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/textgen.svg)](https://github.com/shibing624/textgen/issues)
 - é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com
 - å¾®ä¿¡æˆ‘ï¼š åŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸å-NLP* è¿›NLPäº¤æµç¾¤ã€‚
 
 <img src="docs/wechat.jpeg" width="200" />
 
-# Citation
+## ğŸ˜‡Citation
 
 å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†textgenï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š
 
 ```latex
 @misc{textgen,
   title={textgen: Text Generation Tool},
   author={Xu Ming},
   year={2021},
   howpublished={\url{https://github.com/shibing624/textgen}},
 }
 ```
 
-# License
+## ğŸ¤—License
 
 æˆæƒåè®®ä¸º [The Apache License 2.0](/LICENSE)ï¼Œå¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ textgençš„é“¾æ¥å’Œæˆæƒåè®®ã€‚
 
-# Contribute
+## ğŸ˜Contribute
 
 é¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š
 
 - åœ¨`tests`æ·»åŠ ç›¸åº”çš„å•å…ƒæµ‹è¯•
 - ä½¿ç”¨`python -m pytest`æ¥è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿æ‰€æœ‰å•æµ‹éƒ½æ˜¯é€šè¿‡çš„
 
 ä¹‹åå³å¯æäº¤PRã€‚
 
-## Reference
+## ğŸ’•Acknowledgements 
 
 - [PaddlePaddle/ERNIE](https://github.com/PaddlePaddle/ERNIE)
 - [minimaxir/textgenrnn](https://github.com/minimaxir/textgenrnn)
 - [minimaxir/gpt-2-simple](https://github.com/minimaxir/gpt-2-simple)
 - [asyml/texar](https://github.com/asyml/texar)
 - [yangjianxin1/GPT2-chitchat](https://github.com/yangjianxin1/GPT2-chitchat)
 - [williamSYSU/TextGAN-PyTorch](https://github.com/williamSYSU/TextGAN-PyTorch)
 - [RUCAIBox/TextBox](https://github.com/RUCAIBox/TextBox)
-- [Tiiiger/bert_score]()
+- [Tiiiger/bert_score](https://github.com/Tiiiger/bert_score)
+- [ThilinaRajapakse/simpletransformers](https://github.com/ThilinaRajapakse/simpletransformers)
 - [1YCxZ/Fake-review-generation](https://github.com/1YCxZ/Fake-review-generation)
-- [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora/blob/main/finetune.py)
+- [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora/blob/main/finetune.py)
+
+Thanks for their great work!
```

### Comparing `textgen-0.2.1/setup.py` & `textgen-0.2.2/setup.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 from setuptools import setup, find_packages
 
 with open('README.md', 'r', encoding='utf-8') as f:
     readme = f.read()
 
 setup(
     name='textgen',
-    version='0.2.1',
+    version='0.2.2',
     description='Text Generation Model',
     long_description=readme,
     long_description_content_type='text/markdown',
     author='XuMing',
     author_email='xuming624@qq.com',
     url='https://github.com/shibing624/textgen',
     license="Apache 2.0",
@@ -24,15 +24,15 @@
         'Topic :: Scientific/Engineering :: Artificial Intelligence',
     ],
     python_requires=">=3.6",
     keywords='textgen,text-generation,Text Generation Tool,ernie-gen,chinese text generation',
     install_requires=[
         'loguru',
         'jieba>=0.39',
-        'transformers',
+        'transformers>=4.28.1',
         'datasets',
         'gensim>=4.0.0',
         'text2vec',
         'tensorboard',
         'tensorboardX',
         'tqdm>=4.47.0',
         'pandas',
```

### Comparing `textgen-0.2.1/textgen/__init__.py` & `textgen-0.2.2/textgen/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # -*- coding: utf-8 -*-
 """
 @author:XuMing(xuming624@qq.com)
 @description: 
 """
 
-__version__ = '0.2.1'
+__version__ = '0.2.2'
 
 from textgen.augment.text_augment import TextAugment
 
 from textgen.config.model_args import LanguageGenerationArgs
 from textgen.language_generation.language_generation_model import LanguageGenerationModel
 
 from textgen.config.model_args import LanguageModelingArgs
```

### Comparing `textgen-0.2.1/textgen/augment/sentence_level_augment.py` & `textgen-0.2.2/textgen/augment/sentence_level_augment.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/augment/text_augment.py` & `textgen-0.2.2/textgen/augment/text_augment.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/augment/tokenizer.py` & `textgen-0.2.2/textgen/augment/tokenizer.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/augment/translate_api.py` & `textgen-0.2.2/textgen/augment/translate_api.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/augment/word_level_augment.py` & `textgen-0.2.2/textgen/augment/word_level_augment.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/augment/word_vocab.py` & `textgen-0.2.2/textgen/augment/word_vocab.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/chatglm/chatglm_model.py` & `textgen-0.2.2/textgen/chatglm/chatglm_model.py`

 * *Files 2% similar despite different names*

```diff
@@ -61,15 +61,15 @@
 
         Args:
             model_type: The type of model (chatglm)
             model_name: The exact architecture and trained weights to use. This may be a Hugging Face Transformers compatible pre-trained model, a community model, or the path to a directory containing model files.
             lora_name (optional): Lora name
             args (optional): Default args will be used if this parameter is not provided. If provided, it should be a dict containing the args that should be changed in the default args.
             use_cuda (optional): Use GPU if available. Setting to False will force model to use CPU only.
-            cuda_device (optional): Specific GPU that should be used. Will use the first available GPU by default.
+            cuda_device (int, optional): Specific GPU that should be used. Will use the first available GPU by default.
             **kwargs (optional): For providing proxies, force_download, resume_download, cache_dir and other options specific to the 'from_pretrained' implementation where this will be supplied.
         """  # noqa: ignore flake8"
         model_type = model_type.lower()
         self.args = self._load_model_args(model_name)
 
         if isinstance(args, dict):
             self.args.update_from_dict(args)
@@ -101,22 +101,25 @@
             self.args.int8 = False
 
         self.results = {}
         config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]
         if model_name is None:
             model_name = self.args.model_name_or_path
         config = AutoConfig.from_pretrained(model_name, trust_remote_code=True, **kwargs)
+
         self.model = model_class.from_pretrained(
             model_name,
             config=config,
             trust_remote_code=True,
             load_in_8bit=self.args.int8,
-            torch_dtype=torch.float16 if self.args.fp16 else torch.float32,
-            device_map="auto",
         )
+        if self.args.fp16:
+            self.model.half()
+        self.model.to(self.device)
+
         if self.args.quantization_bit:
             logger.debug(f"Quantized to {self.args.quantization_bit} bit")
             self.model = self.model.quantize(self.args.quantization_bit)
         self.tokenizer_class = tokenizer_class
         if self.args.tokenizer_name:
             self.tokenizer = tokenizer_class.from_pretrained(self.args.tokenizer_name, trust_remote_code=True)
         else:
@@ -256,15 +259,14 @@
             eval_dataset = self.load_and_cache_examples(eval_data, evaluate=True)
             if verbose:
                 logger.debug(f"eval_dataset len: {len(eval_dataset)}, eval_dataset[0]: {eval_dataset[0]}")
 
         # start train
         training_args = TrainingArguments(
             output_dir=self.args.output_dir,
-            auto_find_batch_size=True,
             learning_rate=self.args.learning_rate,
             num_train_epochs=self.args.num_train_epochs,
             logging_dir=f"{self.args.output_dir}/logs",
             logging_steps=self.args.logging_steps,
             max_steps=self.args.max_steps,
             per_device_train_batch_size=self.args.per_device_train_batch_size,
             per_device_eval_batch_size=self.args.per_device_train_batch_size,
@@ -274,14 +276,15 @@
             optim=self.args.optimizer,
             save_strategy=self.args.save_strategy,
             evaluation_strategy=self.args.evaluation_strategy,
             eval_steps=self.args.eval_steps,
             save_total_limit=self.args.save_total_limit,
             fp16=self.args.fp16,
             remove_unused_columns=self.args.remove_unused_columns,
+            report_to=self.args.report_to,
             overwrite_output_dir=self.args.overwrite_output_dir,
             no_cuda=True if self.device == "cpu" else False,
             **kwargs
         )
         # Log on each process the small summary:
         logger.warning(
             f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
@@ -345,16 +348,14 @@
                 lora_path = os.path.join(self.args.output_dir, self.args.lora_name)
                 if lora_path and os.path.exists(lora_path):
                     self.model = PeftModel.from_pretrained(self.model, self.args.output_dir)
                     logger.info(f"Loaded lora model from {lora_path}")
                     self.lora_loaded = True
             if torch.__version__ >= "2" and sys.platform != "win32":
                 self.model = torch.compile(self.model)
-            if self.args.fp16:
-                self.model.half()
 
     def process_response(self, response):
         response = response.strip().replace("[[è®­ç»ƒæ—¶é—´]]", "2023å¹´")
         punkts = [
             [",", "ï¼Œ"],
             ["!", "ï¼"],
             [":", "ï¼š"],
@@ -378,14 +379,16 @@
 
         Returns:
             preds: A python list of the generated sequences.
         """  # noqa: ignore flake8"
 
         if not self.lora_loaded:
             self.load_lora()
+        if self.args.fp16:
+            self.model.half()
         self.model.eval()
 
         all_outputs = []
         # Batching
         for batch in tqdm(
                 [
                     sentences[i: i + self.args.eval_batch_size]
```

### Comparing `textgen-0.2.1/textgen/chatglm/chatglm_utils.py` & `textgen-0.2.2/textgen/chatglm/chatglm_utils.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/config/global_args.py` & `textgen-0.2.2/textgen/config/global_args.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/config/model_args.py` & `textgen-0.2.2/textgen/config/model_args.py`

 * *Files 1% similar despite different names*

```diff
@@ -345,14 +345,15 @@
     quantization_bit: int = None  # if use quantization bit, set 4, else None
     debug: bool = False
     max_seq_length: int = 256  # max length of input sequence
     max_length = 384  # max length of the sequence to be generated
     do_sample: bool = True
     early_stopping: bool = True
     evaluate_generated_text: bool = True
+    report_to = "tensorboard"
     optimizer: str = "adamw_torch"
     save_strategy: str = "steps"
     evaluation_strategy: str = "no"
     eval_steps: int = None
     save_steps: int = 400
     max_eval_samples: int = 20
     length_penalty: float = 2.0
@@ -400,14 +401,15 @@
     max_seq_length: int = 256  # max length of input sequence
     max_length = 384  # max length of the sequence to be generated
     do_sample: bool = True
     early_stopping: bool = True
     evaluate_generated_text: bool = True
     is_chat_task: bool = True
     warmup_steps: int = 50
+    report_to = "tensorboard"
     optimizer: str = "adamw_torch"
     save_strategy: str = "steps"
     eval_steps: int = 200
     save_steps: int = 400
     pad_to_multiple_of: int = 8
     max_eval_samples: int = 20
     length_penalty: float = 2.0
```

### Comparing `textgen-0.2.1/textgen/custom_models/models.py` & `textgen-0.2.2/textgen/custom_models/models.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/data/HowNetPOSWord.txt` & `textgen-0.2.2/textgen/data/HowNetPOSWord.txt`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/data/stopwords.txt` & `textgen-0.2.2/textgen/data/stopwords.txt`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/language_generation/language_generation_model.py` & `textgen-0.2.2/textgen/language_generation/language_generation_model.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/language_generation/language_generation_utils.py` & `textgen-0.2.2/textgen/language_generation/language_generation_utils.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/language_modeling/language_modeling_model.py` & `textgen-0.2.2/textgen/language_modeling/language_modeling_model.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/language_modeling/language_modeling_utils.py` & `textgen-0.2.2/textgen/language_modeling/language_modeling_utils.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/language_modeling/songnet_model.py` & `textgen-0.2.2/textgen/language_modeling/songnet_model.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/language_modeling/songnet_utils.py` & `textgen-0.2.2/textgen/language_modeling/songnet_utils.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/llama/llama_model.py` & `textgen-0.2.2/textgen/llama/llama_model.py`

 * *Files 1% similar despite different names*

```diff
@@ -64,15 +64,15 @@
 
         Args:
             model_type: The type of model (llama)
             model_name: The exact architecture and trained weights to use. This may be a Hugging Face Transformers compatible pre-trained model, a community model, or the path to a directory containing model files.
             lora_name (optional): Lora name
             args (optional): Default args will be used if this parameter is not provided. If provided, it should be a dict containing the args that should be changed in the default args.
             use_cuda (optional): Use GPU if available. Setting to False will force model to use CPU only.
-            cuda_device (optional): Specific GPU that should be used. Will use the first available GPU by default.
+            cuda_device (int, optional): Specific GPU that should be used. Will use the first available GPU by default.
             **kwargs (optional): For providing proxies, force_download, resume_download, cache_dir and other options specific to the 'from_pretrained' implementation where this will be supplied.
         """  # noqa: ignore flake8"
         model_type = model_type.lower()
         self.args = self._load_model_args(model_name)
 
         if isinstance(args, dict):
             self.args.update_from_dict(args)
@@ -86,14 +86,15 @@
             if self.args.n_gpu > 0:
                 torch.cuda.manual_seed_all(self.args.manual_seed)
 
         if use_cuda:
             if torch.cuda.is_available():
                 if cuda_device == -1:
                     self.device = torch.device("cuda")
+                    cuda_device = 0
                 else:
                     self.device = torch.device(f"cuda:{cuda_device}")
             else:
                 raise ValueError(
                     "'use_cuda' set to True when cuda is unavailable."
                     "Make sure CUDA is available or set `use_cuda=False`."
                 )
@@ -105,20 +106,20 @@
             self.args.int8 = False
 
         self.results = {}
         config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]
         if model_name is None:
             model_name = self.args.model_name_or_path
         config = AutoConfig.from_pretrained(model_name, **kwargs)
+
         device_map = "auto"
         world_size = int(os.environ.get("WORLD_SIZE", 1))
         self.ddp = world_size != 1
-        if self.ddp:
-            device_map = {"": int(os.environ.get("LOCAL_RANK") or 0)}
-
+        if self.ddp or torch.cuda.device_count() > 1:
+            device_map = {"": int(os.environ.get("LOCAL_RANK") or cuda_device)}
         self.model = model_class.from_pretrained(
             model_name,
             config=config,
             load_in_8bit=self.args.int8,
             torch_dtype=torch.float16 if self.args.fp16 else torch.float32,
             device_map=device_map,
         )
@@ -272,14 +273,15 @@
             evaluation_strategy='steps' if eval_data is not None else 'no',
             eval_steps=self.args.eval_steps if eval_data is not None else None,
             load_best_model_at_end=True if eval_data is not None else False,
             ddp_find_unused_parameters=False if self.ddp else None,
             save_total_limit=self.args.save_total_limit,
             fp16=self.args.fp16,
             remove_unused_columns=self.args.remove_unused_columns,
+            report_to=self.args.report_to,
             overwrite_output_dir=self.args.overwrite_output_dir,
             no_cuda=True if self.device == "cpu" else False,
             **kwargs
         )
         # Log on each process the small summary:
         logger.warning(
             f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
@@ -380,16 +382,14 @@
             # unwind broken decapoda-research config
             self.tokenizer.padding_side = "left"
             self.model.config.pad_token_id = self.tokenizer.pad_token_id = 0  # unk
             self.model.config.bos_token_id = 1
             self.model.config.eos_token_id = 2
             if torch.__version__ >= "2" and sys.platform != "win32":
                 self.model = torch.compile(self.model)
-            if self.args.fp16:
-                self.model.half()
 
     @torch.no_grad()
     def predict(self, sentences, keep_prompt=False, max_length=None, **kwargs):
         """
         Performs predictions on a list of text.
 
         Args:
@@ -399,14 +399,16 @@
 
         Returns:
             preds: A python list of the generated sequences.
         """  # noqa: ignore flake8"
 
         if not self.lora_loaded:
             self.load_lora()
+        if self.args.fp16:
+            self.model.half()
         self.model.eval()
 
         all_outputs = []
         # Batching
         for batch in tqdm(
                 [
                     sentences[i: i + self.args.eval_batch_size]
```

### Comparing `textgen-0.2.1/textgen/llama/llama_utils.py` & `textgen-0.2.2/textgen/llama/llama_utils.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/seq2seq/bart_seq2seq_model.py` & `textgen-0.2.2/textgen/seq2seq/bart_seq2seq_model.py`

 * *Files 0% similar despite different names*

```diff
@@ -699,15 +699,15 @@
 
         if args.wandb_project:
             wandb.init(
                 project=args.wandb_project,
                 config={**asdict(args)},
                 **args.wandb_kwargs,
             )
-            wandb.run._label(repo="simpletransformers")
+            wandb.run._label(repo="textgen")
             wandb.watch(self.model)
             self.wandb_run_id = wandb.run.id
 
         if args.fp16:
             from torch.cuda import amp
 
             scaler = amp.GradScaler()
```

### Comparing `textgen-0.2.1/textgen/seq2seq/bart_seq2seq_utils.py` & `textgen-0.2.2/textgen/seq2seq/bart_seq2seq_utils.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/seq2seq/conv_seq2seq_model.py` & `textgen-0.2.2/textgen/seq2seq/conv_seq2seq_model.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/seq2seq/data_reader.py` & `textgen-0.2.2/textgen/seq2seq/data_reader.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/seq2seq/seq2seq_model.py` & `textgen-0.2.2/textgen/seq2seq/seq2seq_model.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/seq2seq/seq2seq_trainer.py` & `textgen-0.2.2/textgen/seq2seq/seq2seq_trainer.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/t5/copyt5_model.py` & `textgen-0.2.2/textgen/t5/copyt5_model.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/t5/copyt5_utils.py` & `textgen-0.2.2/textgen/t5/copyt5_utils.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/t5/t5_model.py` & `textgen-0.2.2/textgen/t5/t5_model.py`

 * *Files 1% similar despite different names*

```diff
@@ -476,15 +476,15 @@
 
         if args.wandb_project:
             wandb.init(
                 project=args.wandb_project,
                 config={**asdict(args)},
                 **args.wandb_kwargs,
             )
-            wandb.run._label(repo="simpletransformers")
+            wandb.run._label(repo="textgen")
             wandb.watch(self.model)
             self.wandb_run_id = wandb.run.id
 
         if args.fp16:
             from torch.cuda import amp
 
             scaler = amp.GradScaler()
```

### Comparing `textgen-0.2.1/textgen/t5/t5_utils.py` & `textgen-0.2.2/textgen/t5/t5_utils.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/unsup_generation/tgls_model.py` & `textgen-0.2.2/textgen/unsup_generation/tgls_model.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen/unsup_generation/tgls_util.py` & `textgen-0.2.2/textgen/unsup_generation/tgls_util.py`

 * *Files identical despite different names*

### Comparing `textgen-0.2.1/textgen.egg-info/PKG-INFO` & `textgen-0.2.2/textgen.egg-info/PKG-INFO`

 * *Files 11% similar despite different names*

```diff
@@ -1,116 +1,102 @@
 Metadata-Version: 2.1
 Name: textgen
-Version: 0.2.1
+Version: 0.2.2
 Summary: Text Generation Model
 Home-page: https://github.com/shibing624/textgen
 Author: XuMing
 Author-email: xuming624@qq.com
 License: Apache 2.0
 Description: [![PyPI version](https://badge.fury.io/py/textgen.svg)](https://badge.fury.io/py/textgen)
         [![Downloads](https://pepy.tech/badge/textgen)](https://pepy.tech/project/textgen)
         [![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)
-        [![GitHub contributors](https://img.shields.io/github/contributors/shibing624/textgen.svg)](https://github.com/shibing624/textgen/graphs/contributors)
         [![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
         [![python_version](https://img.shields.io/badge/Python-3.8%2B-green.svg)](requirements.txt)
         [![GitHub issues](https://img.shields.io/github/issues/shibing624/textgen.svg)](https://github.com/shibing624/textgen/issues)
         [![Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact)
         
         # TextGen
         
-        ğŸŒˆ Implementation of Text Generation models.
+        ğŸŒˆImplementation of Text Generation models.
         
         **textgen**å®ç°äº†å¤šç§æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼šLLAMAã€ChatGLMã€UDAã€GPT2ã€Seq2Seqã€BARTã€T5ã€SongNetç­‰æ¨¡å‹ï¼Œå¼€ç®±å³ç”¨ã€‚
         
         **Guide**
         
         - [Feature](#Feature)
         - [Install](#install)
         - [Usage](#usage)
         - [Contact](#Contact)
-        - [Reference](#reference)
+        - [License](#License)
         
-        # Feature
+        ## ğŸ˜ŠFeature
         
-        ## æ–‡æœ¬ç”Ÿæˆ
+        ### æ–‡æœ¬ç”Ÿæˆ
         
         1. seq2seq: Seq2Seqã€ConvSeq2Seqã€BART
         2. language_modeling: GPT2ã€SongNetã€ChatGLMã€LLAMA
         3. t5: T5ã€CopyT5
         
-        ## æ–‡æœ¬æ‰©å¢
+        ### æ–‡æœ¬æ‰©å¢
         
-        ### è¯ç²’åº¦æ‰©å¢
+        #### è¯ç²’åº¦æ‰©å¢
         
         1. UDAï¼Œéæ ¸å¿ƒè¯æ›¿æ¢
         2. EDAï¼Œç®€å•æ•°æ®å¢å¼ºæŠ€æœ¯ï¼šç›¸ä¼¼è¯ã€åŒä¹‰è¯æ›¿æ¢ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢
         
-        ### å¥ç²’åº¦æ‰©å¢
+        #### å¥ç²’åº¦æ‰©å¢
         
         1. å›è¯‘ï¼ˆBT, Back Translateï¼‰ï¼šä¸­æ–‡-è‹±æ–‡-ä¸­æ–‡
         2. GPT2æ¨¡å‹ç»­å†™ï¼šçŸ­æ–‡æœ¬->é•¿æ–‡æœ¬
         3. BARTæ‘˜è¦æ¨¡å‹ï¼šé•¿æ–‡æœ¬->çŸ­æ–‡æœ¬
         4. TGLSï¼šæ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
         
-        ## åŠŸèƒ½åˆ—è¡¨
+        ### åŠŸèƒ½åˆ—è¡¨
         
         - [ChatGLM](textgen/chatglm)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†ChatGLM-6Bæ¨¡å‹LoRAå¾®è°ƒè®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºå¥å­çº é”™ã€å¯¹è¯ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
         - [LLAMA](textgen/llama)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†LLAMAæ¨¡å‹LoRAå¾®è°ƒè®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºå¤šè½®å¯¹è¯ç”Ÿæˆä»»åŠ¡
-        - [UDA(éæ ¸å¿ƒè¯æ›¿æ¢)/EDA](textgen/augment/word_level_augment.py)ï¼šæœ¬é¡¹ç›®å‚è€ƒGoogleçš„UDA(éæ ¸å¿ƒè¯æ›¿æ¢)
-          ç®—æ³•å’ŒEDAç®—æ³•ï¼ŒåŸºäºTF-IDFå°†å¥å­ä¸­éƒ¨åˆ†ä¸é‡è¦è¯æ›¿æ¢ä¸ºåŒä¹‰è¯ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢ç­‰æ–¹æ³•ï¼Œäº§ç”Ÿæ–°çš„æ–‡æœ¬ï¼Œå®ç°äº†æ–‡æœ¬æ‰©å¢
+        - [UDA(éæ ¸å¿ƒè¯æ›¿æ¢)/EDA](textgen/augment/word_level_augment.py)ï¼šæœ¬é¡¹ç›®å‚è€ƒGoogleçš„UDA(éæ ¸å¿ƒè¯æ›¿æ¢)ç®—æ³•å’ŒEDAç®—æ³•ï¼ŒåŸºäºTF-IDFå°†å¥å­ä¸­éƒ¨åˆ†ä¸é‡è¦è¯æ›¿æ¢ä¸ºåŒä¹‰è¯ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢ç­‰æ–¹æ³•ï¼Œäº§ç”Ÿæ–°çš„æ–‡æœ¬ï¼Œå®ç°äº†æ–‡æœ¬æ‰©å¢
         - [BT(å›è¯‘)](textgen/augment/sentence_level_augment.py)ï¼šæœ¬é¡¹ç›®åŸºäºç™¾åº¦ç¿»è¯‘APIå®ç°äº†å›è¯‘åŠŸèƒ½ï¼Œå…ˆæŠŠä¸­æ–‡å¥å­ç¿»è¯‘ä¸ºè‹±æ–‡ï¼Œå†æŠŠè‹±æ–‡ç¿»è¯‘ä¸ºæ–°çš„ä¸­æ–‡
         - [Seq2Seq](textgen/seq2seq)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†Seq2Seqã€ConvSeq2Seqã€BARTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€æ‘˜è¦ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
         - [T5](textgen/t5)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†T5å’ŒCopyT5æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€å¯¹è”ç”Ÿæˆã€æ–‡æ¡ˆæ’°å†™ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
         - [GPT2](textgen/language_modeling)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†GTP2æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡ç« ç”Ÿæˆã€å¯¹è”ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
         - [SongNet](textgen/language_modeling/songnet_model.py)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†SongNetæ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºè§„èŒƒæ ¼å¼çš„è¯—è¯ã€æ­Œè¯ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
-        - [TGLS](textgen/unsup_generation)
-          ï¼šæœ¬é¡¹ç›®å®ç°äº†[TGLS](https://www.jiqizhixin.com/articles/2020-08-11-5)æ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œæ˜¯ä¸€ç§â€œå…ˆæœç´¢åå­¦ä¹ â€çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡åå¤è¿­ä»£å­¦ä¹ å€™é€‰é›†ï¼Œæœ€ç»ˆæ¨¡å‹èƒ½ç”Ÿæˆç±»ä¼¼å€™é€‰é›†çš„é«˜è´¨é‡ç›¸ä¼¼æ–‡æœ¬
+        - [TGLS](textgen/unsup_generation)ï¼šæœ¬é¡¹ç›®å®ç°äº†[TGLS](https://www.jiqizhixin.com/articles/2020-08-11-5)æ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œæ˜¯ä¸€ç§â€œå…ˆæœç´¢åå­¦ä¹ â€çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡åå¤è¿­ä»£å­¦ä¹ å€™é€‰é›†ï¼Œæœ€ç»ˆæ¨¡å‹èƒ½ç”Ÿæˆç±»ä¼¼å€™é€‰é›†çš„é«˜è´¨é‡ç›¸ä¼¼æ–‡æœ¬
         
-        ## Release Models
+        ### Release Models
         
         releaseåŸºäº`textgen`è®­ç»ƒçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å·²ç»releaseåˆ°HuggingFace modelsï¼ŒæŒ‡å®šæ¨¡å‹åç§°`textgen`ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚
         
-        |Model|Arch|Introduce|Training|Inference| |:-- |:--- |:--- |:--- |:--- |
-        |[shibing624/prompt-t5-base-chinese](https://huggingface.co/shibing624/prompt-t5-base-chinese)|T5|ä¸­æ–‡NLPå¤šä»»åŠ¡Promptæ¨¡å‹|[prompt-t5-base-chinese.md](https://github.com/shibing624/textgen/blob/main/docs/prompt-t5-base-chinese.md)|[predict
-        script](https://github.com/shibing624/textgen/blob/main/examples/t5_prompt_demo.py)|
-        |[shibing624/t5-chinese-couplet](https://huggingface.co/shibing624/t5-chinese-couplet)|T5|fine-tunedä¸­æ–‡å¯¹è”åçš„æ¨¡å‹|[å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)|[predict
-        script](https://github.com/shibing624/textgen/blob/main/examples/t5_couplet_demo.py)|
-        |[shibing624/songnet-base-chinese](https://huggingface.co/shibing624/songnet-base-chinese)|SongNet|SongNeté¢„è®­ç»ƒæ¨¡å‹|-|-|
-        |[shibing624/songnet-base-chinese-songci](https://huggingface.co/shibing624/songnet-base-chinese-songci)|SongNet|fine-tunedå®‹è¯åçš„æ¨¡å‹|[training
-        script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict
-        script](https://github.com/shibing624/textgen/blob/main/examples/songnet_songci_demo.py)|
-        |[shibing624/songnet-base-chinese-couplet](https://huggingface.co/shibing624/songnet-base-chinese-couplet)|SongNet|fine-tunedå¯¹è”åçš„æ¨¡å‹|[training
-        script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict
-        script](https://github.com/shibing624/textgen/blob/main/examples/songnet_couplet_demo.py)|
-        |[shibing624/chatglm-6b-csc-zh-lora](https://huggingface.co/shibing624/chatglm-6b-csc-zh-lora)|ChatGLM-6B|åœ¨27ä¸‡ä¸­æ–‡æ‹¼å†™çº é”™æ•°æ®[shibing624/CSC](https://huggingface.co/datasets/shibing624/CSC)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆChatGLM-6Bï¼Œçº é”™æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡|[training
-        script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_csc_demo.py)|[predict
-        script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/csc_demo.py)|
-        |[shibing624/chatglm-6b-belle-zh-lora](https://huggingface.co/shibing624/chatglm-6b-belle-zh-lora)|ChatGLM-6B|åœ¨100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆChatGLM-6Bï¼Œé—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡|[training
-        script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_hfdataset_demo.py)|[predict
-        script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_hfdataset_demo.py)|
-        |[shibing624/llama-13b-belle-zh-lora](https://huggingface.co/shibing624/llama-13b-belle-zh-lora)|ChatGLM-6B|åœ¨100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆLlama-13Bï¼Œé—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡|[training
-        script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_hfdataset_demo.py)|[predict
-        script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_hfdataset_demo.py)|
         
-        # Demo
+        |Model| Arch       |Introduce| Training                                                                                                                                     | Inference                                                                                                             | 
+        |:-- |:-----------|:--- |:---------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------|
+        |[shibing624/prompt-t5-base-chinese](https://huggingface.co/shibing624/prompt-t5-base-chinese)| T5         |ä¸­æ–‡NLPå¤šä»»åŠ¡Promptæ¨¡å‹| [prompt-t5-base-chinese.md](https://github.com/shibing624/textgen/blob/main/docs/prompt-t5-base-chinese.md)                                  | [predict script](https://github.com/shibing624/textgen/blob/main/examples/t5/t5_prompt_demo.py)                       |
+        |[shibing624/t5-chinese-couplet](https://huggingface.co/shibing624/t5-chinese-couplet)| T5         |fine-tunedä¸­æ–‡å¯¹è”åçš„æ¨¡å‹| [å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md) | [predict script](https://github.com/shibing624/textgen/blob/main/examples/t5/t5_couplet_demo.py)                      |
+        |[shibing624/songnet-base-chinese](https://huggingface.co/shibing624/songnet-base-chinese)| SongNet    |SongNeté¢„è®­ç»ƒæ¨¡å‹| -                                                                                                                                            | -                                                                                                                     |
+        |[shibing624/songnet-base-chinese-songci](https://huggingface.co/shibing624/songnet-base-chinese-songci)| SongNet    |fine-tunedå®‹è¯åçš„æ¨¡å‹| [training script](https://github.com/shibing624/textgen/blob/main/examples/songnet/training_zh_songnet_demo.py)                              | [predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet/songnet_songci_demo.py)             |
+        |[shibing624/songnet-base-chinese-couplet](https://huggingface.co/shibing624/songnet-base-chinese-couplet)| SongNet    |fine-tunedå¯¹è”åçš„æ¨¡å‹| [training script](https://github.com/shibing624/textgen/blob/main/examples/songnet/training_zh_songnet_demo.py)                                 | [predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet/songnet_couplet_demo.py)            |
+        |[shibing624/chatglm-6b-csc-zh-lora](https://huggingface.co/shibing624/chatglm-6b-csc-zh-lora)| ChatGLM-6B |åœ¨27ä¸‡ä¸­æ–‡æ‹¼å†™çº é”™æ•°æ®[shibing624/CSC](https://huggingface.co/datasets/shibing624/CSC)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆChatGLM-6Bï¼Œçº é”™æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡| [training script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_csc_demo.py)                             | [predict script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/csc_demo.py)                        |
+        |[shibing624/chatglm-6b-belle-zh-lora](https://huggingface.co/shibing624/chatglm-6b-belle-zh-lora)| ChatGLM-6B |åœ¨100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆChatGLM-6Bï¼Œé—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡| [training script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_hfdataset_demo.py)                       | [predict script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_hfdataset_demo.py) |
+        |[shibing624/llama-13b-belle-zh-lora](https://huggingface.co/shibing624/llama-13b-belle-zh-lora)| LLAMA-13B  |åœ¨100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆLlama-13Bï¼Œé—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡| [training script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_hfdataset_demo.py)                           | [predict script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_hfdataset_demo.py)     |
+        
+        ## ğŸš€Demo
         
         HuggingFace Demo: https://huggingface.co/spaces/shibing624/chinese-couplet-generate
         
         ![](docs/hf.png)
         
         run example: [examples/gradio_demo.py](examples/gradio_demo.py) to see the demo:
         
         ```shell
         python examples/gradio_demo.py
         ```
         
-        model trained
-        by [examples/T5/T5_Finetune_Chinese_Couplet.ipynb](https://github.com/shibing624/textgen/blob/main/examples/T5/T5_Finetune_Chinese_Couplet.ipynb)
+        model trained by [examples/t5/T5_Finetune_Chinese_Couplet.ipynb](https://github.com/shibing624/textgen/blob/main/examples/t5/T5_Finetune_Chinese_Couplet.ipynb)
         
-        # Install
+        ## ğŸ’¾Install
         
         ```shell
         pip install git+https://github.com/huggingface/transformers
         pip install git+https://github.com/huggingface/peft
         pip install -U textgen
         ```
         
@@ -119,42 +105,57 @@
         ```shell
         pip install torch # conda install pytorch
         git clone https://github.com/shibing624/textgen.git
         cd textgen
         python setup.py install
         ```
         
-        # Usage
+        ## ğŸ˜Usage
         
-        ## ChatGLM-6B LoRA æ¨¡å‹
+        ### ChatGLM-6B LoRA æ¨¡å‹
         
-        ### ä½¿ç”¨ChatGLM-6B LoRAå¾®è°ƒåçš„æ¨¡å‹
+        å®‰è£…æœ€æ–°å¼€å‘ç‰ˆçš„peftåº“ï¼Œæ”¯æŒLoRAæ¨¡å‹
+        
+        ```shell
+        pip install git+https://github.com/huggingface/peft
+        ```
+        
+        #### ä½¿ç”¨ChatGLM-6B LoRAå¾®è°ƒåçš„æ¨¡å‹
         
         example: [examples/chatglm/predict_demo.py](https://github.com/shibing624/textgen/blob/main/examples/chatglm/predict_demo.py)
         
         ```python
         import sys
         
         sys.path.append('../..')
         from textgen import ChatGlmModel
         
         model = ChatGlmModel("chatglm", "THUDM/chatglm-6b", lora_name="shibing624/chatglm-6b-csc-zh-lora")
         r = model.predict(["å¯¹ä¸‹é¢ä¸­æ–‡æ‹¼å†™çº é”™ï¼š\nå°‘å…ˆé˜Ÿå‘˜å› è¯¥ä¸ºè€äººè®©åã€‚\nç­”ï¼š"])
         print(r)  # ['å°‘å…ˆé˜Ÿå‘˜åº”è¯¥ä¸ºè€äººè®©åº§ã€‚\né”™è¯¯å­—ï¼šå› ï¼Œå']
         ```
         
-        ### è®­ç»ƒChatGLM-6B LoRAæ¨¡å‹
+        PSï¼šç”±äºä½¿ç”¨äº†å¼€å‘ä¸­çš„peftåº“ï¼Œå¯èƒ½ç”±äºç‰ˆæœ¬æ›´æ–°ï¼Œå¯¼è‡´LoRAæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå»ºè®®ä½¿ç”¨ä¸‹é¢çš„è®­ç»ƒæ–¹æ³•ï¼Œè‡ªå·±è®­ç»ƒLoRAæ¨¡å‹ã€‚
+        
+        #### è®­ç»ƒChatGLM-6B LoRAæ¨¡å‹
         
         æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†ï¼Œæ•°æ®é›†æ ¼å¼å‚è€ƒ[examples/data/zh_csc_test.tsv](https://github.com/shibing624/textgen/blob/main/examples/data/zh_csc_test.tsv)ã€‚
         
         example: [examples/chatglm/training_chatglm_demo.py](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_demo.py)
         
-        ## LLAMA LoRA æ¨¡å‹
+        ### LLAMA LoRA æ¨¡å‹
         
-        ### ä½¿ç”¨LLAMA LoRAå¾®è°ƒåçš„æ¨¡å‹
+        å®‰è£…æœ€æ–°å¼€å‘ç‰ˆçš„transformerså’Œpeftåº“ï¼Œæ”¯æŒLLAMAã€LoRAæ¨¡å‹
+        
+        ```shell
+        pip install transformers>=4.28.1
+        pip install git+https://github.com/huggingface/peft
+        ```
+        
+        #### ä½¿ç”¨LLAMA LoRAå¾®è°ƒåçš„æ¨¡å‹
         
         example: [examples/llama/predict_demo.py](https://github.com/shibing624/textgen/blob/main/examples/llama/predict_demo.py)
         
         ```python
         import sys
         
         sys.path.append('../..')
@@ -167,19 +168,19 @@
         
         model = LlamaModel("llama", "decapoda-research/llama-7b-hf", lora_name="ziqingyang/chinese-alpaca-lora-7b")
         predict_sentence = generate_prompt("é—®ï¼šç”¨ä¸€å¥è¯æè¿°åœ°çƒä¸ºä»€ä¹ˆæ˜¯ç‹¬ä¸€æ— äºŒçš„ã€‚\nç­”ï¼š")
         r = model.predict([predict_sentence])
         print(r)  # ['åœ°çƒæ˜¯å”¯ä¸€ä¸€é¢—æ‹¥æœ‰ç”Ÿå‘½çš„è¡Œæ˜Ÿã€‚']
         ```
         
-        ### è®­ç»ƒLLAMA LoRAæ¨¡å‹
+        #### è®­ç»ƒLLAMA LoRAæ¨¡å‹
         
         example: [examples/llama/training_llama_demo.py](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_demo.py)
         
-        ## ConvSeq2Seq æ¨¡å‹
+        ### ConvSeq2Seq æ¨¡å‹
         
         è®­ç»ƒå¹¶é¢„æµ‹ConvSeq2Seqæ¨¡å‹ï¼š
         
         example: [examples/seq2sesq/training_convseq2seq_model_demo.py](https://github.com/shibing624/textgen/blob/main/examples/seq2seq/training_convseq2seq_model_demo.py)
         
         ```python
         import argparse
@@ -224,30 +225,30 @@
         output:
         
         ```bash
         inputs: ["ä»€ä¹ˆæ˜¯ai", "ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº", "ä½ çŸ¥é“çƒ­åŠ›å­¦å—"]
         outputs: ['äººå·¥æ™ºèƒ½æ˜¯å·¥ç¨‹å’Œç§‘å­¦çš„åˆ†æ”¯,è‡´åŠ›äºæ„å»ºæ€ç»´çš„æœºå™¨ã€‚', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•è¿è„‘ä¸Šå·¥ä½œï¼', 'æˆ‘ä¸èƒ½é”™çƒ­æ˜¯ä¸€ä¸ªç–¯ç‹‚çš„äººå·¥æ™ºèƒ½"200å¹´ã€‚']
         ```
         
-        ## BART æ¨¡å‹
+        ### BART æ¨¡å‹
         
         è®­ç»ƒå¹¶é¢„æµ‹BARTæ¨¡å‹ï¼š
         
         example: [examples/seq2sesq/training_bartseq2seq_zh_demo.py](https://github.com/shibing624/textgen/blob/main/examples/seq2seq/training_bartseq2seq_zh_demo.py)
         
         output:
         
         ```shell
         inputs: ['ä»€ä¹ˆæ˜¯ai', 'ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº', 'ä½ çŸ¥é“çƒ­åŠ›å­¦å—']
         outputs: ['äººå·¥æ™ºèƒ½æ˜¯å·¥ç¨‹å’Œç§‘å­¦çš„åˆ†æ”¯,è‡´åŠ›äºæ„', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•ç”µè„‘ä¸Š', 'ä»€ä¹ˆæ˜¯çƒ­åŠ›å­¦å—ï¼Ÿ']
         ```
         
-        ## T5 æ¨¡å‹
+        ### T5 æ¨¡å‹
         
-        example: [examples/T5/training_zh_t5_model_demo.py](https://github.com/shibing624/textgen/blob/main/examples/T5/training_zh_t5_model_demo.py)
+        example: [examples/t5/training_zh_t5_model_demo.py](https://github.com/shibing624/textgen/blob/main/examples/t5/training_zh_t5_model_demo.py)
         
         ```python
         import argparse
         from loguru import logger
         import pandas as pd
         import sys
         
@@ -338,48 +339,48 @@
         output:
         
         ```shell
         inputs: ['ä»€ä¹ˆæ˜¯ai', 'ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº', 'ä½ çŸ¥é“çƒ­åŠ›å­¦å—']
         outputs: ['äººå·¥æ™ºèƒ½æœ‰ä¸¤ä¸ªå¹¿ä¹‰çš„å®šä¹‰,ä»»ä½•æ‹Ÿäººçš„æœºæ¢°,å¦‚åœ¨å¡é›·å°”capeks', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨Python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•ç”µè„‘ä¸Šå·¥ä½œ!', 'ä»€ä¹ˆæ˜¯çƒ­åŠ›å­¦']
         ```
         
-        ## GPT2 æ¨¡å‹
+        ### GPT2 æ¨¡å‹
         
-        ### ä¸­æ–‡GPT2 - æ–‡ç« ç”Ÿæˆ
+        #### ä¸­æ–‡GPT2 - æ–‡ç« ç”Ÿæˆ
         
         ä½¿ç”¨ä¸­æ–‡æ•°æ®é›†ï¼ˆæ®µè½æ ¼å¼ï¼Œ`\n`é—´éš”ï¼‰ï¼Œè®­ç»ƒGPT2æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºè¯—æ­Œç”Ÿæˆã€æ–‡ç« ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
         
-        example: [examples/language_generation/training_zh_gpt2_demo.py](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_gpt2_demo.py)
+        example: [examples/gpt2/training_zh_gpt2_demo.py](https://github.com/shibing624/textgen/blob/main/examples/gpt2/training_zh_gpt2_demo.py)
         
-        ### ä¸­æ–‡GPT2 - å¯¹è”ç”Ÿæˆ
+        #### ä¸­æ–‡GPT2 - å¯¹è”ç”Ÿæˆ
         
         ä½¿ç”¨ä¸­æ–‡å¯¹è”æ•°æ®é›†ï¼ˆtsvæ ¼å¼ï¼Œ`\t`é—´éš”ï¼‰ï¼Œè‡ªå®šä¹‰æ•°æ®é›†è¯»å–Datasetï¼Œè®­ç»ƒGPT2æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºå¯¹è”ç”Ÿæˆã€å¯¹è¯ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
         
-        example: [examples/language_generation/training_couplet_gpt2_demo.py](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_couplet_gpt2_demo.py)
+        example: [examples/gpt2/training_couplet_gpt2_demo.py](https://github.com/shibing624/textgen/blob/main/examples/gpt2/training_couplet_gpt2_demo.py)
         
-        #### GPT2 vs T5ï¼š
+        GPT2 vs T5ï¼š
         
         1. éƒ½æ˜¯ä»Transformeræ”¹è¿›æ¥çš„ï¼ŒT5åŒæ—¶æœ‰ç¼–ç å™¨å’Œè§£ç å™¨ï¼ŒGPT2åªæœ‰è§£ç å™¨
         2. T5çš„æ¨¡å‹ä¼˜åŠ¿æ˜¯å¤„ç†ç»™å®šè¾“å…¥ï¼Œäº§å‡ºå¯¹åº”è¾“å‡ºçš„ä»»åŠ¡ï¼Œå¦‚ç¿»è¯‘ã€å¯¹è¯ã€é—®ç­”ç­‰
         3. GPT2çš„æ¨¡å‹ä¼˜åŠ¿æ˜¯è‡ªç”±åˆ›ä½œï¼Œå¦‚å†™ä¸€ç¯‡çŸ­æ–‡
         4. T5çš„å¯¹è”ç”Ÿæˆæ•ˆæœå¥½äºGPT2ã€GPT2çš„è¯—è¯ç”Ÿæˆæ•ˆæœå¥½äºT5
         
         - [å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)
         - [å¤è¯—ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%8F%A4%E8%AF%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)
         
-        ## SongNet æ¨¡å‹
+        ### SongNet æ¨¡å‹
         
         æ ¼å¼æ§åˆ¶çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œpaperè§[SongNet: Rigid Formats Controlled Text Generation](https://arxiv.org/abs/2004.08022)ï¼Œ
         é€‚ç”¨äºå¼ºéŸµå¾‹æ ¼å¼è¦æ±‚çš„è¯—æ­Œã€å¯¹è”ã€æ­Œè¯ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
         
-        example: [examples/language_generation/training_zh_songnet_demo.py](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)
+        example: [examples/songnet/training_zh_songnet_demo.py](https://github.com/shibing624/textgen/blob/main/examples/songnet/training_zh_songnet_demo.py)
         
-        ## Keyword Text Augmentation(EDA/UDA)
+        ### Keyword Text Augmentation(EDA/UDA)
         
-        example: [examples/text_augmentation_demo.py](examples/text_augmentation_demo.py)
+        example: [examples/text_augmentation/text_augmentation_demo.py](examples/text_augmentation/text_augmentation_demo.py)
         
         ```python
         import sys
         
         sys.path.append('..')
         from textgen.augment import TextAugment
         
@@ -416,19 +417,19 @@
         random-0.2: ('ä¸»è¦é™ªé™ªæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ä¸»è¦è®¡ç®—æœºè§†è§‰ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå—é™äºå†…å®¹', [('ç ”ç©¶', 'é™ªé™ª', 2, 4), ('ã€', 'ä¸»è¦', 13, 15), ('ç›¸å…³', 'å—é™äº', 27, 30)])
         insert-0.2: ('ä¸»è¦ç ”ç©¶æœºå™¨æœºå™¨å­¦ä¹ å­¦ä¹ ã€æ·±åº¦æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿç›¸å…³å†…å®¹', [('æœºå™¨', 'æœºå™¨æœºå™¨', 4, 8), ('å­¦ä¹ ', 'å­¦ä¹ å­¦ä¹ ', 8, 12), ('æ·±åº¦', 'æ·±åº¦æ·±åº¦', 13, 17)])
         delete-0.2: ('ä¸»è¦ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€å¯¹è¯ç³»ç»Ÿç›¸å…³å†…å®¹', [('æ™ºèƒ½', '', 20, 20)])
         tfidf-0.2: ('ä¸€æ˜¯ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºå¬è§‰ã€æ™ºèƒ½äº¤è°ˆç³»ç»Ÿå¯†åˆ‡ç›¸å…³å†…å®¹', [('ä¸»è¦', 'ä¸€æ˜¯', 0, 2), ('è§†è§‰', 'å¬è§‰', 17, 19), ('å¯¹è¯', 'äº¤è°ˆ', 22, 24), ('ç›¸å…³', 'å¯†åˆ‡ç›¸å…³', 26, 30)])
         mix-0.2: ('ä¸»è¦ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ã€è®¡ç®—æœºå¬è§‰ã€æ™ºèƒ½å¯¹è¯è½¯ä»¶ç³»ç»Ÿç›¸å…³å†…å®¹', [('å­¦ä¹ ', 'å­¦', 11, 12), ('è§†è§‰', 'å¬è§‰', 16, 18), ('ç³»ç»Ÿ', 'è½¯ä»¶ç³»ç»Ÿ', 23, 27)])
         ```
         
-        ## TGLS æ¨¡å‹ï¼ˆæ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼‰
+        ### TGLS æ¨¡å‹ï¼ˆæ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼‰
         
         æ— ç›‘ç£çš„ä¸­æ–‡ç”µå•†è¯„è®ºç”Ÿæˆï¼šä»**ç”µå•†è¯„è®º**ä¸­æå–ç”¨æˆ·è¡¨è¾¾è§‚ç‚¹çš„çŸ­å¥å¹¶è¿›è¡Œç»„åˆæ¥ç”Ÿæˆä»¿çœŸè¯„è®ºã€‚
         
-        example: [examples/unsup_generation_demo.py](examples/unsup_generation_demo.py)
+        example: [examples/unsup_generation/unsup_generation_demo.py](examples/unsup_generation/unsup_generation_demo.py)
         
         ```python
         import os
         import sys
         
         sys.path.append('..')
         from textgen.unsup_generation import TglsModel, load_list
@@ -483,71 +484,72 @@
         å¸Œæœ›å¥½ç”¨ï¼Œé¢è†œç”¨è¿‡äº†å¾ˆå¥½ç”¨ï¼Œçš®è‚¤æ°´å«©å…‰æ»‘ç™½çš™ï¼Œè¡¥æ°´ä¸é”™ï¼Œä»·æ ¼ä¹Ÿåˆé€‚ã€‚
         å°±æ˜¯ç²¾åæ¶²å¤ªå°‘äº†ï¼Œä¿æ¹¿æ•ˆæœä¸é”™ã€‚
         é¢è†œçš„è¡¥æ°´æ•ˆæœéå¸¸å¥½ï¼Œä¿æ¹¿æ•ˆæœç¡®å®å¾ˆèµï¼Œè¿™ä¸ªé¢è†œç›¸å¯¹äºèƒ¶åŸè›‹ç™½å’Œç¾ç™½çš„é‚£ä¸¤æ¬¾çš„é¢è†œçº¸è¦åšä¸€äº›ï¼Œçœ‹ç€ä»·æ ¼åˆé€‚ã€‚
         ```
         
         å‰10å¥æ˜¯çœŸå®ç”¨æˆ·è¯„è®ºï¼Œå10å¥æ˜¯ç”Ÿæˆçš„ã€‚
         
-        # Dataset
+        ## ğŸ“šDataset 
         
         1. 50ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†ï¼š[BelleGroup/train_0.5M_CN](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)
         2. 100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†ï¼š[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)
-        3.
-        5ä¸‡æ¡è‹±æ–‡ChatGPTæŒ‡ä»¤Alpacaæ•°æ®é›†ï¼š[50k English Stanford Alpaca dataset](https://github.com/tatsu-lab/stanford_alpaca#data-release)
+        3. 5ä¸‡æ¡è‹±æ–‡ChatGPTæŒ‡ä»¤Alpacaæ•°æ®é›†ï¼š[50k English Stanford Alpaca dataset](https://github.com/tatsu-lab/stanford_alpaca#data-release)
         4. 2ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Alpacaæ•°æ®é›†ï¼š[shibing624/alpaca-zh](https://huggingface.co/datasets/shibing624/alpaca-zh)
-        5. 69ä¸‡æ¡ä¸­æ–‡æŒ‡ä»¤Guanacoæ•°æ®é›†(Belle50ä¸‡æ¡+Guanaco19ä¸‡æ¡)
-           ï¼š[Chinese-Vicuna/guanaco_belle_merge_v1.0](https://huggingface.co/datasets/Chinese-Vicuna/guanaco_belle_merge_v1.0)
+        5. 69ä¸‡æ¡ä¸­æ–‡æŒ‡ä»¤Guanacoæ•°æ®é›†(Belle50ä¸‡æ¡+Guanaco19ä¸‡æ¡)ï¼š[Chinese-Vicuna/guanaco_belle_merge_v1.0](https://huggingface.co/datasets/Chinese-Vicuna/guanaco_belle_merge_v1.0)
         
-        # Contact
+        ## â˜ï¸Contact
         
         - Issue(å»ºè®®)
           ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/textgen.svg)](https://github.com/shibing624/textgen/issues)
         - é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com
         - å¾®ä¿¡æˆ‘ï¼š åŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸å-NLP* è¿›NLPäº¤æµç¾¤ã€‚
         
         <img src="docs/wechat.jpeg" width="200" />
         
-        # Citation
+        ## ğŸ˜‡Citation
         
         å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†textgenï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š
         
         ```latex
         @misc{textgen,
           title={textgen: Text Generation Tool},
           author={Xu Ming},
           year={2021},
           howpublished={\url{https://github.com/shibing624/textgen}},
         }
         ```
         
-        # License
+        ## ğŸ¤—License
         
         æˆæƒåè®®ä¸º [The Apache License 2.0](/LICENSE)ï¼Œå¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ textgençš„é“¾æ¥å’Œæˆæƒåè®®ã€‚
         
-        # Contribute
+        ## ğŸ˜Contribute
         
         é¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š
         
         - åœ¨`tests`æ·»åŠ ç›¸åº”çš„å•å…ƒæµ‹è¯•
         - ä½¿ç”¨`python -m pytest`æ¥è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿æ‰€æœ‰å•æµ‹éƒ½æ˜¯é€šè¿‡çš„
         
         ä¹‹åå³å¯æäº¤PRã€‚
         
-        ## Reference
+        ## ğŸ’•Acknowledgements 
         
         - [PaddlePaddle/ERNIE](https://github.com/PaddlePaddle/ERNIE)
         - [minimaxir/textgenrnn](https://github.com/minimaxir/textgenrnn)
         - [minimaxir/gpt-2-simple](https://github.com/minimaxir/gpt-2-simple)
         - [asyml/texar](https://github.com/asyml/texar)
         - [yangjianxin1/GPT2-chitchat](https://github.com/yangjianxin1/GPT2-chitchat)
         - [williamSYSU/TextGAN-PyTorch](https://github.com/williamSYSU/TextGAN-PyTorch)
         - [RUCAIBox/TextBox](https://github.com/RUCAIBox/TextBox)
-        - [Tiiiger/bert_score]()
+        - [Tiiiger/bert_score](https://github.com/Tiiiger/bert_score)
+        - [ThilinaRajapakse/simpletransformers](https://github.com/ThilinaRajapakse/simpletransformers)
         - [1YCxZ/Fake-review-generation](https://github.com/1YCxZ/Fake-review-generation)
         - [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora/blob/main/finetune.py)
+        
+        Thanks for their great work!
 Keywords: textgen,text-generation,Text Generation Tool,ernie-gen,chinese text generation
 Platform: UNKNOWN
 Classifier: Intended Audience :: Developers
 Classifier: Operating System :: OS Independent
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Programming Language :: Python :: 3
 Classifier: Topic :: Text Processing :: Linguistic
```

### Comparing `textgen-0.2.1/textgen.egg-info/SOURCES.txt` & `textgen-0.2.2/textgen.egg-info/SOURCES.txt`

 * *Files identical despite different names*

