# Comparing `tmp/hydrolib_core-0.5.0.tar.gz` & `tmp/hydrolib_core-0.5.1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "hydrolib_core-0.5.0.tar", max compression
+gzip compressed data, was "hydrolib_core-0.5.1.tar", max compression
```

## Comparing `hydrolib_core-0.5.0.tar` & `hydrolib_core-0.5.1.tar`

### file list

```diff
@@ -1,87 +1,87 @@
--rw-r--r--   0        0        0     1065 2023-04-17 13:23:29.419781 hydrolib_core-0.5.0/LICENSE
--rw-r--r--   0        0        0     1893 2023-04-17 13:23:29.419781 hydrolib_core-0.5.0/README.md
--rw-r--r--   0        0        0       76 2023-04-17 13:23:29.431781 hydrolib_core-0.5.0/hydrolib/__init__.py
--rw-r--r--   0        0        0       22 2023-04-17 13:24:04.056074 hydrolib_core-0.5.0/hydrolib/core/__init__.py
--rw-r--r--   0        0        0      400 2023-04-17 13:23:29.431781 hydrolib_core-0.5.0/hydrolib/core/base.py
--rw-r--r--   0        0        0    51127 2023-04-17 13:23:29.431781 hydrolib_core-0.5.0/hydrolib/core/basemodel.py
--rw-r--r--   0        0        0      242 2023-04-17 13:23:29.431781 hydrolib_core-0.5.0/hydrolib/core/config.py
--rw-r--r--   0        0        0      422 2023-04-17 13:23:29.431781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/__init__.py
--rw-r--r--   0        0        0      693 2023-04-17 13:23:29.431781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/bc/__init__.py
--rw-r--r--   0        0        0    31932 2023-04-17 13:23:29.431781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/bc/models.py
--rw-r--r--   0        0        0       81 2023-04-17 13:23:29.431781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/common/__init__.py
--rw-r--r--   0        0        0     1184 2023-04-17 13:23:29.431781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/common/models.py
--rw-r--r--   0        0        0      517 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/crosssection/__init__.py
--rw-r--r--   0        0        0    29942 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/crosssection/models.py
--rw-r--r--   0        0        0      160 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/ext/__init__.py
--rw-r--r--   0        0        0    11002 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/ext/models.py
--rw-r--r--   0        0        0      355 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/extold/__init__.py
--rw-r--r--   0        0        0      458 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/extold/common_io.py
--rw-r--r--   0        0        0    24497 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/extold/models.py
--rw-r--r--   0        0        0     4282 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/extold/parser.py
--rw-r--r--   0        0        0     3487 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/extold/serializer.py
--rw-r--r--   0        0        0      202 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/friction/__init__.py
--rw-r--r--   0        0        0     9155 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/friction/models.py
--rw-r--r--   0        0        0      124 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/gui/__init__.py
--rw-r--r--   0        0        0     4742 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/gui/models.py
--rw-r--r--   0        0        0        0 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/ini/__init__.py
--rw-r--r--   0        0        0     4662 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/ini/io_models.py
--rw-r--r--   0        0        0     9812 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/ini/models.py
--rw-r--r--   0        0        0    14682 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/ini/parser.py
--rw-r--r--   0        0        0    13505 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/ini/serializer.py
--rw-r--r--   0        0        0    23760 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/ini/util.py
--rw-r--r--   0        0        0      333 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/inifield/__init__.py
--rw-r--r--   0        0        0     8475 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/inifield/models.py
--rw-r--r--   0        0        0      907 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/mdu/__init__.py
--rw-r--r--   0        0        0    93913 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/mdu/models.py
--rw-r--r--   0        0        0      184 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/net/__init__.py
--rw-r--r--   0        0        0    47307 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/net/models.py
--rw-r--r--   0        0        0    10785 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/net/reader.py
--rw-r--r--   0        0        0     4372 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/net/ugrid_conventions.json
--rw-r--r--   0        0        0    20107 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/net/writer.py
--rw-r--r--   0        0        0      169 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/obs/__init__.py
--rw-r--r--   0        0        0     4150 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/obs/models.py
--rw-r--r--   0        0        0      243 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/obscrosssection/__init__.py
--rw-r--r--   0        0        0     3772 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/obscrosssection/models.py
--rw-r--r--   0        0        0      175 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/onedfield/__init__.py
--rw-r--r--   0        0        0     4852 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/onedfield/models.py
--rw-r--r--   0        0        0      168 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/polyfile/__init__.py
--rw-r--r--   0        0        0     3123 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/polyfile/models.py
--rw-r--r--   0        0        0    20275 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/polyfile/parser.py
--rw-r--r--   0        0        0     3714 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/polyfile/serializer.py
--rw-r--r--   0        0        0      277 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/storagenode/__init__.py
--rw-r--r--   0        0        0     9265 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/storagenode/models.py
--rw-r--r--   0        0        0      671 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/structure/__init__.py
--rw-r--r--   0        0        0    45989 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/structure/models.py
--rw-r--r--   0        0        0       88 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/tim/__init__.py
--rw-r--r--   0        0        0     3035 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/tim/models.py
--rw-r--r--   0        0        0     3824 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/tim/parser.py
--rw-r--r--   0        0        0     4857 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/tim/serializer.py
--rw-r--r--   0        0        0       86 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/xyn/__init__.py
--rw-r--r--   0        0        0     1879 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/xyn/models.py
--rw-r--r--   0        0        0     2525 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/xyn/parser.py
--rw-r--r--   0        0        0     1590 2023-04-17 13:23:29.435781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/xyn/serializer.py
--rw-r--r--   0        0        0       86 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/xyz/__init__.py
--rw-r--r--   0        0        0     1641 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/xyz/models.py
--rw-r--r--   0        0        0     1634 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/xyz/parser.py
--rw-r--r--   0        0        0     1389 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/dflowfm/xyz/serializer.py
--rw-r--r--   0        0        0      557 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/dimr/__init__.py
--rw-r--r--   0        0        0    10233 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/dimr/models.py
--rw-r--r--   0        0        0     1852 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/dimr/parser.py
--rw-r--r--   0        0        0     3136 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/dimr/serializer.py
--rw-r--r--   0        0        0        1 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/geometry/__init__ .py
--rw-r--r--   0        0        0        1 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/modeldata/__init__ .py
--rw-r--r--   0        0        0       85 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/rr/__init__.py
--rw-r--r--   0        0        0      101 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/rr/meteo/__init__.py
--rw-r--r--   0        0        0     3092 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/rr/meteo/models.py
--rw-r--r--   0        0        0     6065 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/rr/meteo/parser.py
--rw-r--r--   0        0        0     8280 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/rr/meteo/serializer.py
--rw-r--r--   0        0        0    22478 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/rr/models.py
--rw-r--r--   0        0        0     1626 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/rr/parser.py
--rw-r--r--   0        0        0    14107 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/rr/serializer.py
--rw-r--r--   0        0        0      122 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/rr/topology/__init__.py
--rw-r--r--   0        0        0     5982 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/rr/topology/models.py
--rw-r--r--   0        0        0     2042 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/rr/topology/parser.py
--rw-r--r--   0        0        0     2750 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/rr/topology/serializer.py
--rw-r--r--   0        0        0     8531 2023-04-17 13:23:29.439781 hydrolib_core-0.5.0/hydrolib/core/utils.py
--rw-r--r--   0        0        0     2131 2023-04-17 13:24:04.096074 hydrolib_core-0.5.0/pyproject.toml
--rw-r--r--   0        0        0     2849 1970-01-01 00:00:00.000000 hydrolib_core-0.5.0/PKG-INFO
+-rw-r--r--   0        0        0       79 2023-03-15 10:07:16.193069 hydrolib_core-0.5.1/hydrolib/__init__.py
+-rw-r--r--   0        0        0       23 2023-04-17 14:49:57.253849 hydrolib_core-0.5.1/hydrolib/core/__init__.py
+-rw-r--r--   0        0        0      416 2023-04-14 13:35:23.823996 hydrolib_core-0.5.1/hydrolib/core/base.py
+-rw-r--r--   0        0        0    52489 2023-04-14 14:04:30.109748 hydrolib_core-0.5.1/hydrolib/core/basemodel.py
+-rw-r--r--   0        0        0      253 2023-03-15 10:07:16.194069 hydrolib_core-0.5.1/hydrolib/core/config.py
+-rw-r--r--   0        0        0      441 2023-04-14 11:29:41.344831 hydrolib_core-0.5.1/hydrolib/core/dflowfm/__init__.py
+-rw-r--r--   0        0        0      730 2023-03-15 10:07:16.194069 hydrolib_core-0.5.1/hydrolib/core/dflowfm/bc/__init__.py
+-rw-r--r--   0        0        0    32784 2023-04-14 13:35:23.824997 hydrolib_core-0.5.1/hydrolib/core/dflowfm/bc/models.py
+-rw-r--r--   0        0        0       84 2023-04-14 11:29:41.344831 hydrolib_core-0.5.1/hydrolib/core/dflowfm/common/__init__.py
+-rw-r--r--   0        0        0     1221 2023-04-14 11:29:41.345772 hydrolib_core-0.5.1/hydrolib/core/dflowfm/common/models.py
+-rw-r--r--   0        0        0      546 2023-03-15 10:07:16.196069 hydrolib_core-0.5.1/hydrolib/core/dflowfm/crosssection/__init__.py
+-rw-r--r--   0        0        0    30678 2023-04-12 09:08:24.401033 hydrolib_core-0.5.1/hydrolib/core/dflowfm/crosssection/models.py
+-rw-r--r--   0        0        0      169 2023-04-12 13:31:27.344166 hydrolib_core-0.5.1/hydrolib/core/dflowfm/ext/__init__.py
+-rw-r--r--   0        0        0    11317 2023-04-12 14:24:51.270177 hydrolib_core-0.5.1/hydrolib/core/dflowfm/ext/models.py
+-rw-r--r--   0        0        0      374 2023-04-14 11:29:41.345772 hydrolib_core-0.5.1/hydrolib/core/dflowfm/extold/__init__.py
+-rw-r--r--   0        0        0      481 2023-04-14 11:29:41.345772 hydrolib_core-0.5.1/hydrolib/core/dflowfm/extold/common_io.py
+-rw-r--r--   0        0        0    25071 2023-04-17 14:40:38.661861 hydrolib_core-0.5.1/hydrolib/core/dflowfm/extold/models.py
+-rw-r--r--   0        0        0     4397 2023-04-14 13:35:23.826002 hydrolib_core-0.5.1/hydrolib/core/dflowfm/extold/parser.py
+-rw-r--r--   0        0        0     3594 2023-04-14 13:35:23.826002 hydrolib_core-0.5.1/hydrolib/core/dflowfm/extold/serializer.py
+-rw-r--r--   0        0        0      211 2023-03-15 10:07:16.197070 hydrolib_core-0.5.1/hydrolib/core/dflowfm/friction/__init__.py
+-rw-r--r--   0        0        0     9410 2023-03-15 10:07:16.197070 hydrolib_core-0.5.1/hydrolib/core/dflowfm/friction/models.py
+-rw-r--r--   0        0        0      131 2023-03-15 10:07:16.198070 hydrolib_core-0.5.1/hydrolib/core/dflowfm/gui/__init__.py
+-rw-r--r--   0        0        0     4874 2023-03-15 10:07:16.198070 hydrolib_core-0.5.1/hydrolib/core/dflowfm/gui/models.py
+-rw-r--r--   0        0        0        0 2023-03-15 10:07:16.198070 hydrolib_core-0.5.1/hydrolib/core/dflowfm/ini/__init__.py
+-rw-r--r--   0        0        0     4826 2023-03-15 10:07:16.198070 hydrolib_core-0.5.1/hydrolib/core/dflowfm/ini/io_models.py
+-rw-r--r--   0        0        0    10126 2023-04-13 10:06:37.641400 hydrolib_core-0.5.1/hydrolib/core/dflowfm/ini/models.py
+-rw-r--r--   0        0        0    15080 2023-04-14 13:35:23.826998 hydrolib_core-0.5.1/hydrolib/core/dflowfm/ini/parser.py
+-rw-r--r--   0        0        0    13840 2023-04-14 13:35:23.827997 hydrolib_core-0.5.1/hydrolib/core/dflowfm/ini/serializer.py
+-rw-r--r--   0        0        0    24384 2023-04-12 18:16:17.251460 hydrolib_core-0.5.1/hydrolib/core/dflowfm/ini/util.py
+-rw-r--r--   0        0        0      352 2023-04-14 11:29:41.347766 hydrolib_core-0.5.1/hydrolib/core/dflowfm/inifield/__init__.py
+-rw-r--r--   0        0        0     8705 2023-04-14 11:29:41.347766 hydrolib_core-0.5.1/hydrolib/core/dflowfm/inifield/models.py
+-rw-r--r--   0        0        0      962 2023-03-15 10:07:16.201069 hydrolib_core-0.5.1/hydrolib/core/dflowfm/mdu/__init__.py
+-rw-r--r--   0        0        0    96029 2023-04-14 13:35:23.827997 hydrolib_core-0.5.1/hydrolib/core/dflowfm/mdu/models.py
+-rw-r--r--   0        0        0      194 2023-03-15 10:07:16.202069 hydrolib_core-0.5.1/hydrolib/core/dflowfm/net/__init__.py
+-rw-r--r--   0        0        0    48587 2023-04-06 07:04:22.343700 hydrolib_core-0.5.1/hydrolib/core/dflowfm/net/models.py
+-rw-r--r--   0        0        0    11085 2023-03-15 10:07:16.202069 hydrolib_core-0.5.1/hydrolib/core/dflowfm/net/reader.py
+-rw-r--r--   0        0        0     4560 2023-04-12 18:16:17.253480 hydrolib_core-0.5.1/hydrolib/core/dflowfm/net/ugrid_conventions.json
+-rw-r--r--   0        0        0    20587 2023-03-15 10:07:16.203069 hydrolib_core-0.5.1/hydrolib/core/dflowfm/net/writer.py
+-rw-r--r--   0        0        0      172 2023-03-15 10:07:16.203069 hydrolib_core-0.5.1/hydrolib/core/dflowfm/obs/__init__.py
+-rw-r--r--   0        0        0     4260 2023-04-03 11:51:20.714510 hydrolib_core-0.5.1/hydrolib/core/dflowfm/obs/models.py
+-rw-r--r--   0        0        0      254 2023-03-15 10:07:16.204070 hydrolib_core-0.5.1/hydrolib/core/dflowfm/obscrosssection/__init__.py
+-rw-r--r--   0        0        0     3868 2023-04-03 11:51:20.715510 hydrolib_core-0.5.1/hydrolib/core/dflowfm/obscrosssection/models.py
+-rw-r--r--   0        0        0      178 2023-03-15 10:07:16.204070 hydrolib_core-0.5.1/hydrolib/core/dflowfm/onedfield/__init__.py
+-rw-r--r--   0        0        0     4995 2023-03-15 10:07:16.204070 hydrolib_core-0.5.1/hydrolib/core/dflowfm/onedfield/models.py
+-rw-r--r--   0        0        0      177 2023-03-15 10:07:16.205078 hydrolib_core-0.5.1/hydrolib/core/dflowfm/polyfile/__init__.py
+-rw-r--r--   0        0        0     3233 2023-04-03 13:21:58.103586 hydrolib_core-0.5.1/hydrolib/core/dflowfm/polyfile/models.py
+-rw-r--r--   0        0        0    20865 2023-04-14 13:35:23.828999 hydrolib_core-0.5.1/hydrolib/core/dflowfm/polyfile/parser.py
+-rw-r--r--   0        0        0     3827 2023-04-14 13:35:23.828999 hydrolib_core-0.5.1/hydrolib/core/dflowfm/polyfile/serializer.py
+-rw-r--r--   0        0        0      294 2023-03-15 10:07:16.206069 hydrolib_core-0.5.1/hydrolib/core/dflowfm/storagenode/__init__.py
+-rw-r--r--   0        0        0     9518 2023-04-03 11:51:20.718510 hydrolib_core-0.5.1/hydrolib/core/dflowfm/storagenode/models.py
+-rw-r--r--   0        0        0      710 2023-03-15 10:07:16.206069 hydrolib_core-0.5.1/hydrolib/core/dflowfm/structure/__init__.py
+-rw-r--r--   0        0        0    47103 2023-04-17 14:40:32.114119 hydrolib_core-0.5.1/hydrolib/core/dflowfm/structure/models.py
+-rw-r--r--   0        0        0       94 2023-04-17 14:40:38.662861 hydrolib_core-0.5.1/hydrolib/core/dflowfm/tim/__init__.py
+-rw-r--r--   0        0        0     3132 2023-04-17 14:40:38.662861 hydrolib_core-0.5.1/hydrolib/core/dflowfm/tim/models.py
+-rw-r--r--   0        0        0     3930 2023-04-17 14:40:38.662861 hydrolib_core-0.5.1/hydrolib/core/dflowfm/tim/parser.py
+-rw-r--r--   0        0        0     4988 2023-04-17 14:40:38.663861 hydrolib_core-0.5.1/hydrolib/core/dflowfm/tim/serializer.py
+-rw-r--r--   0        0        0       92 2023-04-03 13:21:58.105587 hydrolib_core-0.5.1/hydrolib/core/dflowfm/xyn/__init__.py
+-rw-r--r--   0        0        0     1954 2023-04-17 14:40:38.663861 hydrolib_core-0.5.1/hydrolib/core/dflowfm/xyn/models.py
+-rw-r--r--   0        0        0     2602 2023-04-17 14:40:38.663861 hydrolib_core-0.5.1/hydrolib/core/dflowfm/xyn/parser.py
+-rw-r--r--   0        0        0     1638 2023-04-14 13:35:23.831997 hydrolib_core-0.5.1/hydrolib/core/dflowfm/xyn/serializer.py
+-rw-r--r--   0        0        0       92 2023-03-15 10:07:16.207069 hydrolib_core-0.5.1/hydrolib/core/dflowfm/xyz/__init__.py
+-rw-r--r--   0        0        0     1711 2023-04-12 13:31:27.348984 hydrolib_core-0.5.1/hydrolib/core/dflowfm/xyz/models.py
+-rw-r--r--   0        0        0     1693 2023-04-14 13:35:23.831997 hydrolib_core-0.5.1/hydrolib/core/dflowfm/xyz/parser.py
+-rw-r--r--   0        0        0     1432 2023-04-14 13:35:23.831997 hydrolib_core-0.5.1/hydrolib/core/dflowfm/xyz/serializer.py
+-rw-r--r--   0        0        0      592 2023-03-15 10:07:16.208069 hydrolib_core-0.5.1/hydrolib/core/dimr/__init__.py
+-rw-r--r--   0        0        0    10577 2023-04-12 09:08:24.402032 hydrolib_core-0.5.1/hydrolib/core/dimr/models.py
+-rw-r--r--   0        0        0     1913 2023-03-15 10:07:16.209070 hydrolib_core-0.5.1/hydrolib/core/dimr/parser.py
+-rw-r--r--   0        0        0     3232 2023-04-14 11:15:00.138188 hydrolib_core-0.5.1/hydrolib/core/dimr/serializer.py
+-rw-r--r--   0        0        0        2 2023-03-15 10:07:16.209070 hydrolib_core-0.5.1/hydrolib/core/geometry/__init__ .py
+-rw-r--r--   0        0        0        2 2023-03-15 10:07:16.210070 hydrolib_core-0.5.1/hydrolib/core/modeldata/__init__ .py
+-rw-r--r--   0        0        0       88 2023-03-15 10:07:16.210070 hydrolib_core-0.5.1/hydrolib/core/rr/__init__.py
+-rw-r--r--   0        0        0      104 2023-03-15 10:07:16.210070 hydrolib_core-0.5.1/hydrolib/core/rr/meteo/__init__.py
+-rw-r--r--   0        0        0     3194 2023-04-03 13:21:58.108587 hydrolib_core-0.5.1/hydrolib/core/rr/meteo/models.py
+-rw-r--r--   0        0        0     6251 2023-03-15 10:07:16.211070 hydrolib_core-0.5.1/hydrolib/core/rr/meteo/parser.py
+-rw-r--r--   0        0        0     8518 2023-04-03 13:21:58.108587 hydrolib_core-0.5.1/hydrolib/core/rr/meteo/serializer.py
+-rw-r--r--   0        0        0    23015 2023-04-03 13:21:58.109598 hydrolib_core-0.5.1/hydrolib/core/rr/models.py
+-rw-r--r--   0        0        0     1684 2023-04-14 13:35:23.832997 hydrolib_core-0.5.1/hydrolib/core/rr/parser.py
+-rw-r--r--   0        0        0    14333 2023-04-14 13:35:23.832997 hydrolib_core-0.5.1/hydrolib/core/rr/serializer.py
+-rw-r--r--   0        0        0      130 2023-03-15 10:07:16.212070 hydrolib_core-0.5.1/hydrolib/core/rr/topology/__init__.py
+-rw-r--r--   0        0        0     6172 2023-04-03 13:21:58.110596 hydrolib_core-0.5.1/hydrolib/core/rr/topology/models.py
+-rw-r--r--   0        0        0     2116 2023-04-14 13:35:23.833997 hydrolib_core-0.5.1/hydrolib/core/rr/topology/parser.py
+-rw-r--r--   0        0        0     2839 2023-04-14 13:35:23.833997 hydrolib_core-0.5.1/hydrolib/core/rr/topology/serializer.py
+-rw-r--r--   0        0        0     8831 2023-04-03 13:21:58.110596 hydrolib_core-0.5.1/hydrolib/core/utils.py
+-rw-r--r--   0        0        0     1086 2023-03-15 10:07:16.160448 hydrolib_core-0.5.1/LICENSE
+-rw-r--r--   0        0        0     2230 2023-04-17 14:49:57.254848 hydrolib_core-0.5.1/pyproject.toml
+-rw-r--r--   0        0        0     1917 2023-03-15 10:07:16.160448 hydrolib_core-0.5.1/README.md
+-rw-r--r--   0        0        0     2849 1970-01-01 00:00:00.000000 hydrolib_core-0.5.1/PKG-INFO
```

### Comparing `hydrolib_core-0.5.0/README.md` & `hydrolib_core-0.5.1/README.md`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,24 +1,24 @@
-[![Join the chat at https://gitter.im/Deltares/hydrolib](https://badges.gitter.im/Deltares/hydrolib.svg)](https://gitter.im/Deltares/hydrolib?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
-[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
-[![ci](https://github.com/Deltares/HYDROLIB-core/actions/workflows/ci.yml/badge.svg)](https://github.com/Deltares/HYDROLIB-core/actions/workflows/ci.yml)
-[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=Deltares_HYDROLIB-core&metric=alert_status)](https://sonarcloud.io/dashboard?id=Deltares_HYDROLIB-core)
-
-
-# HYDROLIB-core
-HYDROLIB-core is the core library of Python wrappers around the D-HYDRO model files (input and output) and model engines (kernel libraries).
-It can serve as the basis for various pre- and postprocessing tools for a modelling workflow of hydrodynamic simulations.
-
-<div align="center">
-<img src="docs/images/HYDROLIB_logo_paths.svg" width="50%">
-</div>
-
-## More information
-Much more information is available from the dedicated package website.
-
-Some quickstarts:
-* First users: [Installation](https://deltares.github.io/HYDROLIB-core/latest/guides/setup/) and [Tutorials](https://deltares.github.io/HYDROLIB-core/latest/tutorials/tutorials).
-* Developers: [List of supported functionalities](https://deltares.github.io/HYDROLIB-core/latest/topics/dhydro_support/),
-  [API reference](https://deltares.github.io/HYDROLIB-core/latest/reference/api/), and
-  [How to contribute](https://deltares.github.io/HYDROLIB-core/latest/guides/contributing/).
-* Releases: [hydrolib-core on PyPI](https://pypi.org/project/hydrolib-core/), [ChangeLog](https://deltares.github.io/HYDROLIB-core/latest/changelog/).
-* Known issues and requested features: via [GitHub issues](https://github.com/Deltares/HYDROLIB-core/issues).
+[![Join the chat at https://gitter.im/Deltares/hydrolib](https://badges.gitter.im/Deltares/hydrolib.svg)](https://gitter.im/Deltares/hydrolib?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
+[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
+[![ci](https://github.com/Deltares/HYDROLIB-core/actions/workflows/ci.yml/badge.svg)](https://github.com/Deltares/HYDROLIB-core/actions/workflows/ci.yml)
+[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=Deltares_HYDROLIB-core&metric=alert_status)](https://sonarcloud.io/dashboard?id=Deltares_HYDROLIB-core)
+
+
+# HYDROLIB-core
+HYDROLIB-core is the core library of Python wrappers around the D-HYDRO model files (input and output) and model engines (kernel libraries).
+It can serve as the basis for various pre- and postprocessing tools for a modelling workflow of hydrodynamic simulations.
+
+<div align="center">
+<img src="docs/images/HYDROLIB_logo_paths.svg" width="50%">
+</div>
+
+## More information
+Much more information is available from the dedicated package website.
+
+Some quickstarts:
+* First users: [Installation](https://deltares.github.io/HYDROLIB-core/latest/guides/setup/) and [Tutorials](https://deltares.github.io/HYDROLIB-core/latest/tutorials/tutorials).
+* Developers: [List of supported functionalities](https://deltares.github.io/HYDROLIB-core/latest/topics/dhydro_support/),
+  [API reference](https://deltares.github.io/HYDROLIB-core/latest/reference/api/), and
+  [How to contribute](https://deltares.github.io/HYDROLIB-core/latest/guides/contributing/).
+* Releases: [hydrolib-core on PyPI](https://pypi.org/project/hydrolib-core/), [ChangeLog](https://deltares.github.io/HYDROLIB-core/latest/changelog/).
+* Known issues and requested features: via [GitHub issues](https://github.com/Deltares/HYDROLIB-core/issues).
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/basemodel.py` & `hydrolib_core-0.5.1/hydrolib/core/basemodel.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,1362 +1,1362 @@
-"""
-Here we define our Pydantic `BaseModel` with custom settings,
-as well as a `FileModel` that inherits from a `BaseModel` but
-also represents a file on disk.
-
-"""
-import logging
-import shutil
-from abc import ABC, abstractclassmethod, abstractmethod
-from contextlib import contextmanager
-from contextvars import ContextVar
-from enum import IntEnum
-from pathlib import Path
-from typing import (
-    Any,
-    Callable,
-    Dict,
-    Generic,
-    List,
-    Optional,
-    Set,
-    Tuple,
-    Type,
-    TypeVar,
-    Union,
-)
-from weakref import WeakValueDictionary
-
-from pydantic import BaseModel as PydanticBaseModel
-from pydantic import validator
-from pydantic.error_wrappers import ErrorWrapper, ValidationError
-from pydantic.fields import ModelField, PrivateAttr
-
-from hydrolib.core.base import DummmyParser, DummySerializer
-from hydrolib.core.utils import (
-    FilePathStyleConverter,
-    OperatingSystem,
-    PathStyle,
-    get_operating_system,
-    get_path_style_for_current_operating_system,
-    str_is_empty_or_none,
-    to_key,
-)
-
-logger = logging.getLogger(__name__)
-
-# We use ContextVars to keep a reference to the folder
-# we're currently parsing files in. In the future
-# we could move to https://github.com/samuelcolvin/pydantic/issues/1549
-context_file_loading: ContextVar["FileLoadContext"] = ContextVar("file_loading")
-
-
-class BaseModel(PydanticBaseModel):
-    class Config:
-        arbitrary_types_allowed = True
-        validate_assignment = True
-        use_enum_values = True
-        extra = "forbid"  # will throw errors so we can fix our models
-        allow_population_by_field_name = True
-        alias_generator = to_key
-
-    def __init__(self, **data: Any) -> None:
-        """Initialize a BaseModel with the provided data.
-
-        Raises:
-            ValidationError: A validation error when the data is invalid.
-        """
-        try:
-            super().__init__(**data)
-        except ValidationError as e:
-
-            # Give a special message for faulty list input
-            for re in e.raw_errors:
-                if (
-                    hasattr(re, "_loc")
-                    and hasattr(re.exc, "msg_template")
-                    and isinstance(data.get(to_key(re._loc)), list)
-                ):
-                    re.exc.msg_template += (
-                        f". The key {re._loc} might be duplicated in the input file."
-                    )
-
-            # Update error with specific model location name
-            identifier = self._get_identifier(data)
-            if identifier is None:
-                raise e
-            else:
-                # If there is an identifier, include this in the ValidationError messages.
-                raise ValidationError([ErrorWrapper(e, loc=identifier)], self.__class__)
-
-    def is_file_link(self) -> bool:
-        """Generic attribute for models backed by a file."""
-        return False
-
-    def is_intermediate_link(self) -> bool:
-        """Generic attribute for models that have children fields that could contain files."""
-        return self.is_file_link()
-
-    def show_tree(self, indent=0):
-        """Recursive print function for showing a tree of a model."""
-        angle = "âˆŸ" if indent > 0 else ""
-
-        # Only print if we're backed by a file
-        if self.is_file_link():
-            print(" " * indent * 2, angle, self)
-
-        # Otherwise we recurse through the fields of a model
-        for _, value in self:
-            # Handle lists of items
-            if not isinstance(value, list):
-                value = [value]
-            for v in value:
-                if hasattr(v, "is_intermediate_link") and v.is_intermediate_link():
-                    # If the field is only an intermediate, print the name only
-                    if not v.is_file_link():
-                        print(" " * (indent * 2 + 2), angle, v.__class__.__name__)
-                    v.show_tree(indent + 1)
-
-    def _apply_recurse(self, f, *args, **kwargs):
-        # TODO Could we use this function for `show_tree`?
-        for _, value in self:
-            # Handle lists of items
-            if not isinstance(value, list):
-                value = [value]
-            for v in value:
-                if hasattr(v, "is_intermediate_link") and v.is_intermediate_link():
-                    v._apply_recurse(f, *args, **kwargs)
-
-        # Run self as last, so we can make use of the nested updates
-        if self.is_file_link():
-            getattr(self, f)(*args, **kwargs)
-
-    def _get_identifier(self, data: dict) -> Optional[str]:
-        """Get the identifier for this model.
-
-        Args:
-            data (dict): The data from which to retrieve the identifier
-
-        Returns:
-            str: The identifier or None.
-        """
-        return None
-
-
-TAcc = TypeVar("TAcc")
-
-
-class ModelTreeTraverser(Generic[TAcc]):
-    """ModelTreeTraverser is responsible for traversing a ModelTree using the provided
-    functions.
-
-    The ModelTreeTraverser will only traverse BaseModel and derived objects.
-    Type parameter TAcc defines the type of Accumulator to be used.
-    """
-
-    def __init__(
-        self,
-        should_traverse: Optional[Callable[[BaseModel, TAcc], bool]] = None,
-        should_execute: Optional[Callable[[BaseModel, TAcc], bool]] = None,
-        pre_traverse_func: Optional[Callable[[BaseModel, TAcc], TAcc]] = None,
-        post_traverse_func: Optional[Callable[[BaseModel, TAcc], TAcc]] = None,
-    ):
-        """Create a new ModelTreeTraverser with the given functions.
-
-        If a predicate it is not defined, it is assumed to always be true, i.e. we will
-        always traverse to the next node, or always execute the traverse functions.
-
-        If a traverse function is not defined, it will be skipped.
-
-        The traverse functions share an accumulator, i.e. the accumulator argument
-        is passed through all evaluated traverse functions. It is expected that the
-        traverse function return the (potentially) changed accumulator.
-
-        Args:
-            should_traverse (Optional[Callable[[BaseModel, TAcc], bool]], optional):
-                Function to evaluate whether to traverse to the provided BaseModel. Defaults to None.
-            should_execute (Optional[Callable[[BaseModel, TAcc], bool]], optional):
-                Function to evaluate whether to execute the traverse functions for the
-                provided BaseModel. Defaults to None.
-            pre_traverse_func (Callable[[BaseModel, TAcc], TAcc], optional):
-                Traverse function executed before we traverse into the next BaseModel,
-                i.e. top-down traversal. Defaults to None.
-            post_traverse_func (Callable[[BaseModel, TAcc], TAcc], optional):
-                Traverse function executed after we traverse into the next BaseModel,
-                i.e. bottom-up traversal. Defaults to None.
-        """
-        self._should_traverse_func = should_traverse
-        self._should_execute_func = should_execute
-        self._pre_traverse_func = pre_traverse_func
-        self._post_traverse_func = post_traverse_func
-
-    def _should_execute(self, model: BaseModel, acc: TAcc) -> bool:
-        return self._should_execute_func is None or self._should_execute_func(
-            model, acc
-        )
-
-    def _should_execute_pre(self, model: BaseModel, acc: TAcc) -> bool:
-        return self._pre_traverse_func is not None and self._should_execute(model, acc)
-
-    def _should_execute_post(self, model: BaseModel, acc: TAcc) -> bool:
-        return self._post_traverse_func is not None and self._should_execute(model, acc)
-
-    def _should_traverse(self, value: Any, acc: TAcc) -> bool:
-        return isinstance(value, BaseModel) and (
-            self._should_traverse_func is None or self._should_traverse_func(value, acc)
-        )
-
-    def traverse(self, model: BaseModel, acc: TAcc) -> TAcc:
-        """Traverse the model tree of BaseModels including the model as the root, with
-        the provided state of the acc and return the final accumulator.
-
-        The actual executed functions as well as the predicates defining whether these
-        functions should be executed for this model as well as whether child BaseModel
-        objects should be traversed are provided in the constructor of the
-        ModelTreeTraverser.
-
-        The final accumulator is returned.
-
-        Args:
-            model (BaseModel):
-                The root model in which the traversal of the model tree starts.
-            acc (TAcc):
-                The current accumulator.
-
-        Returns:
-            TAcc: The accumulator after the traversal of the model tree.
-        """
-        if self._should_execute_pre(model, acc):
-            acc = self._pre_traverse_func(model, acc)  # type: ignore[arg-type]
-
-        for _, value in model:
-            if not isinstance(value, list):
-                value = [value]
-
-            for v in value:
-                if self._should_traverse(v, acc):
-                    acc = self.traverse(v, acc)
-
-        if self._should_execute_post(model, acc):
-            acc = self._post_traverse_func(model, acc)  # type: ignore[arg-type]
-
-        return acc
-
-
-class ResolveRelativeMode(IntEnum):
-    """ResolveRelativeMode defines the possible resolve modes used within the
-    FilePathResolver.
-
-    It determines how the relative paths to child models within some FileModel
-    should be interpreted. By default it should be relative to the parent model,
-    i.e. the model in which the mode is defined. However there exist some exceptions,
-    where the relative paths should be evaluated relative to some other folder,
-    regardless of the current parent location.
-
-    Options:
-        ToParent:
-            Relative paths should be resolved relative to their direct parent model.
-        ToAnchor:
-            All relative paths should be resolved relative to the specified regardless
-            of subsequent parent model locations.
-    """
-
-    ToParent = 0
-    ToAnchor = 1
-
-
-class FileCasingResolver:
-    """Class for resolving file path in a case-insensitive manner."""
-
-    def resolve(self, path: Path) -> Path:
-        """Resolve the casing of a file path when the file does exist but not with the exact casing.
-
-        Args:
-            path (Path): The path of the file or directory for which the casing needs to be resolved.
-
-        Returns:
-            Path: The file path with the matched casing if a match exists; otherwise, the original file path.
-
-        Raises:
-            NotImplementedError: When this function is called with an operating system other than Windows, Linux or MacOS.
-        """
-
-        operating_system = get_operating_system()
-        if operating_system == OperatingSystem.WINDOWS:
-            return self._resolve_casing_windows(path)
-        if operating_system == OperatingSystem.LINUX:
-            return self._resolve_casing_linux(path)
-        if operating_system == OperatingSystem.MACOS:
-            return self._resolve_casing_macos(path)
-        else:
-            raise NotImplementedError(
-                f"Path case resolving for operating system {operating_system} is not supported yet."
-            )
-
-    def _resolve_casing_windows(self, path: Path):
-        return path.resolve()
-
-    def _resolve_casing_linux(self, path: Path):
-        if path.exists():
-            return path
-
-        if not path.parent.exists() and not str_is_empty_or_none(path.parent.name):
-            path = self._resolve_casing_linux(path.parent) / path.name
-
-        return self._find_match(path)
-
-    def _resolve_casing_macos(self, path: Path):
-        if not str_is_empty_or_none(path.parent.name):
-            path = self._resolve_casing_macos(path.parent) / path.name
-
-        return self._find_match(path)
-
-    def _find_match(self, path: Path):
-        if path.parent.exists():
-            for item in path.parent.iterdir():
-                if item.name.lower() == path.name.lower():
-                    return path.with_name(item.name)
-
-        return path
-
-
-class FilePathResolver:
-    """FilePathResolver is responsible for resolving relative paths.
-
-    The current state to which paths are resolved can be altered by
-    pushing a new parent path to the FilePathResolver, or removing the
-    latest added parent path from the FilePathResolver
-    """
-
-    def __init__(self) -> None:
-        """Create a new empty FilePathResolver."""
-        self._anchors: List[Path] = []
-        self._parents: List[Tuple[Path, ResolveRelativeMode]] = []
-
-    @property
-    def _anchor(self) -> Optional[Path]:
-        return self._anchors[-1] if self._anchors else None
-
-    @property
-    def _direct_parent(self) -> Path:
-        return self._parents[-1][0] if self._parents else Path.cwd()
-
-    def get_current_parent(self) -> Path:
-        """Get the current absolute path with which files are resolved.
-
-        If the current mode is relative to the parent, the latest added
-        parent is added. If the current mode is relative to an anchor
-        path, the latest added anchor path is returned.
-
-        Returns:
-            Path: The absolute path to the current parent.
-        """
-        if self._anchor:
-            return self._anchor
-        return self._direct_parent
-
-    def resolve(self, path: Path) -> Path:
-        """Resolve the provided path to an absolute path given the current state.
-
-        If the provided path is already absolute, it will be returned as is.
-
-        Args:
-            path (Path): The path to resolve
-
-        Returns:
-            Path: An absolute path resolved given the current state.
-        """
-        if path.is_absolute():
-            return path
-
-        parent = self.get_current_parent()
-        return (parent / path).resolve()
-
-    def push_new_parent(
-        self, parent_path: Path, relative_mode: ResolveRelativeMode
-    ) -> None:
-        """Push a new parent_path with the given relative_mode to this FilePathResolver.
-
-        Relative paths added to this FilePathResolver will be resolved with respect
-        to the current state, i.e. similar to FilePathResolver.resolve.
-
-        Args:
-            parent_path (Path): The parent path
-            relative_mode (ResolveRelativeMode): The relative mode used to resolve.
-        """
-        absolute_parent_path = self.resolve(parent_path)
-        if relative_mode == ResolveRelativeMode.ToAnchor:
-            self._anchors.append(absolute_parent_path)
-
-        self._parents.append((absolute_parent_path, relative_mode))
-
-    def pop_last_parent(self) -> None:
-        """Pop the last added parent from this FilePathResolver
-
-        If there are currently no parents defined, nothing will happen.
-        """
-        if not self._parents:
-            return
-
-        _, relative_mode = self._parents.pop()
-
-        if relative_mode == ResolveRelativeMode.ToAnchor:
-            self._anchors.pop()
-
-
-class FileModelCache:
-    """
-    FileModelCache provides a simple structure to register and retrieve FileModel
-    objects.
-    """
-
-    def __init__(self):
-        """Create a new empty FileModelCache."""
-        self._cache_dict: Dict[Path, "FileModel"] = {}
-
-    def retrieve_model(self, path: Path) -> Optional["FileModel"]:
-        """Retrieve the model associated with the (absolute) path if
-        it has been registered before, otherwise return None.
-
-        Returns:
-            [Optional[FileModel]]:
-                The FileModel associated with the Path if it has been registered
-                before, otherwise None.
-        """
-        return self._cache_dict.get(path, None)
-
-    def register_model(self, path: Path, model: "FileModel") -> None:
-        """Register the model with the specified path in this FileModelCache.
-
-        Args:
-            path (Path): The path to associate the model with.
-            model (FileModel): The model to be associated with the path.
-        """
-        self._cache_dict[path] = model
-
-    def is_empty(self) -> bool:
-        """Whether or not this file model cache is empty.
-
-        Returns:
-            bool: Whether or not the cache is empty.
-        """
-        return not any(self._cache_dict)
-
-
-class ModelSaveSettings:
-    """A class that holds the global settings for model saving."""
-
-    _os_path_style = get_path_style_for_current_operating_system()
-
-    def __init__(self, path_style: Optional[PathStyle] = None) -> None:
-        """Initializes a new instance of the ModelSaveSettings class.
-
-        Args:
-            path_style (Optional[PathStyle], optional): Which file path style to use when saving the model. Defaults to the path style that matches the current operating system.
-        """
-
-        if path_style is None:
-            path_style = self._os_path_style
-
-        self._path_style = path_style
-
-    @property
-    def path_style(self) -> PathStyle:
-        """Gets the path style setting.
-
-        Returns:
-            PathStyle: Which path style is used to save the files.
-        """
-        return self._path_style
-
-
-class ModelLoadSettings:
-    """A class that holds the global settings for model loading."""
-
-    def __init__(
-        self, recurse: bool, resolve_casing: bool, path_style: PathStyle
-    ) -> None:
-        """Initializes a new instance of the ModelLoadSettings class.
-
-        Args:
-            recurse (bool): Whether or not to recursively load the whole model.
-            resolve_casing (bool): Whether or not to resolve the file casing.
-            path_style (PathStyle): Which path style is used in the loaded files.
-        """
-        self._recurse = recurse
-        self._resolve_casing = resolve_casing
-        self._path_style = path_style
-
-    @property
-    def recurse(self) -> bool:
-        """Gets the recurse setting.
-
-        Returns:
-            bool: Whether or not to recursively load the whole model.
-        """
-        return self._recurse
-
-    @property
-    def resolve_casing(self) -> bool:
-        """Gets the resolve casing setting.
-
-        Returns:
-            bool: Whether or not to resolve the file casing.
-        """
-        return self._resolve_casing
-
-    @property
-    def path_style(self) -> PathStyle:
-        """Gets the path style setting.
-
-        Returns:
-            PathStyle: Which path style is used in the loaded files.
-        """
-        return self._path_style
-
-
-class FileLoadContext:
-    """FileLoadContext provides the context necessary to resolve paths
-    during the init of a FileModel, as well as ensure the relevant models
-    are only read once.
-    """
-
-    def __init__(self) -> None:
-        """Create a new empty FileLoadContext."""
-        self._path_resolver = FilePathResolver()
-        self._cache = FileModelCache()
-        self._file_casing_resolver = FileCasingResolver()
-        self._file_path_style_converter = FilePathStyleConverter()
-        self._load_settings: Optional[ModelLoadSettings] = None
-
-    def initialize_load_settings(
-        self, recurse: bool, resolve_casing: bool, path_style: PathStyle
-    ):
-        """Initialize the global model load setting. Can only be set once.
-
-        Args:
-            recurse (bool): Whether or not to recursively load the whole model.
-            resolve_casing (bool): Whether or not to resolve the file casing.
-            path_style (PathStyle): Which path style is used in the loaded files.
-        """
-        if self._load_settings is None:
-            self._load_settings = ModelLoadSettings(recurse, resolve_casing, path_style)
-
-    @property
-    def load_settings(self) -> ModelLoadSettings:
-        """Gets the model load settings.
-
-        Raises:
-            ValueError: When the model load settings have not been initialized yet.
-        Returns:
-            ModelLoadSettings: The model load settings.
-
-        """
-        if self._load_settings is None:
-            raise ValueError(
-                f"The model load settings have not been initialized yet. Make sure to call `{self.initialize_load_settings.__name__}` first."
-            )
-
-        return self._load_settings
-
-    def retrieve_model(self, path: Optional[Path]) -> Optional["FileModel"]:
-        """Retrieve the model associated with the path.
-
-        If no model has been associated with the provided path, or the path is None,
-        then None will be returned. Relative paths will be resolved based on the
-        current state of the FileLoadContext.
-
-        Returns:
-            [Optional[FileModel]]:
-                The file model associated with the provided path if it has been
-                registered, else None.
-        """
-        if path is None:
-            return None
-
-        absolute_path = self._path_resolver.resolve(path)
-        return self._cache.retrieve_model(absolute_path)
-
-    def register_model(self, path: Path, model: "FileModel") -> None:
-        """Associate the provided model with the provided path.
-
-        Relative paths will be resolved based on the current state of the
-        FileLoadContext.
-
-        Args:
-            path (Path): The relative path from which the model was loaded.
-            model (FileModel): The loaded model.
-        """
-        absolute_path = self._path_resolver.resolve(path)
-        self._cache.register_model(absolute_path, model)
-
-    def cache_is_empty(self) -> bool:
-        """Whether or not the file model cache is empty.
-
-        Returns:
-            bool: Whether or not the file model cache is empty.
-        """
-        return self._cache.is_empty()
-
-    def get_current_parent(self) -> Path:
-        """Get the current absolute path with which files are resolved.
-
-        If the current mode is relative to the parent, the latest added
-        parent is added. If the current mode is relative to an anchor
-        path, the latest added anchor path is returned.
-
-        Returns:
-            Path: The absolute path to the current parent.
-        """
-        return self._path_resolver.get_current_parent()
-
-    def resolve(self, path: Path) -> Path:
-        """Resolve the provided path.
-
-        If path is already absolute, it will be returned as is. Otherwise
-        it will be resolved based on the current state of this FileLoadContext.
-
-        Args:
-            path (Path): The path to be resolved.
-
-        Returns:
-            Path: An absolute path resolved based on the current state.
-        """
-        return self._path_resolver.resolve(path)
-
-    def push_new_parent(
-        self, parent_path: Path, relative_mode: ResolveRelativeMode
-    ) -> None:
-        """Push a new parent_path with the given relative_mode on this
-        FileLoadContext.
-
-        Args:
-            parent_path (Path): The parent path to be added to this FileLoadContext.
-            relative_mode (ResolveRelativeMode): The relative mode.
-        """
-        self._path_resolver.push_new_parent(parent_path, relative_mode)
-
-    def pop_last_parent(self) -> None:
-        """Pop the last added parent off this FileLoadContext."""
-        self._path_resolver.pop_last_parent()
-
-    def resolve_casing(self, file_path: Path) -> Path:
-        """Resolve the file casing for the provided file path.
-
-        Args:
-            file_path (Path): The file path to resolve the casing for.
-
-        Returns:
-            Path: The resolved file path.
-        """
-        if self.load_settings.resolve_casing:
-            return self._file_casing_resolver.resolve(file_path)
-        return file_path
-
-    def convert_path_style(self, file_path: Path) -> Path:
-        """Resolve the file path by converting it from its own file path style to the path style for the current operating system.
-
-        Args:
-            file_path (Path): The file path to convert to the OS path style.
-
-        Returns:
-            Path: The resolved file path.
-        """
-
-        if file_path.is_absolute():
-            return file_path
-
-        converted_file_path = self._file_path_style_converter.convert_to_os_style(
-            file_path, self.load_settings.path_style
-        )
-        return Path(converted_file_path)
-
-
-@contextmanager
-def file_load_context():
-    """Provide a FileLoadingContext. If none has been created in the context of
-    this call stack yet, a new one will be created, which will be maintained
-    until it goes out of scope.
-
-    Yields:
-        [FileLoadContext]: The file load context.
-    """
-    file_loading_context = context_file_loading.get(None)
-    context_reset_token = None
-
-    if not file_loading_context:
-        file_loading_context = FileLoadContext()
-        context_reset_token = context_file_loading.set(file_loading_context)
-
-    try:
-        yield file_loading_context
-    finally:
-        if context_reset_token is not None:
-            context_file_loading.reset(context_reset_token)
-
-
-def _should_traverse(model: BaseModel, _: FileLoadContext) -> bool:
-    return model.is_intermediate_link()
-
-
-def _should_execute(model: BaseModel, _: FileLoadContext) -> bool:
-    return model.is_file_link()
-
-
-PathOrStr = Union[Path, str]
-
-
-class FileModel(BaseModel, ABC):
-    """Base class to represent models with a file representation.
-
-    It therefore always has a `filepath` and if it is given on
-    initilization, it will parse that file. The filepath can be
-    relative, in which case the paths are expected to be resolved
-    relative to some root model. If a path is absolute, this path
-    will always be used, regardless of a root parent.
-
-    When saving a model, if the current filepath is relative, the
-    last resolved absolute path will be used. If the model has just
-    been read, the
-
-    This class extends the `validate` option of Pydantic,
-    so when when a Path is given to a field with type `FileModel`,
-    it doesn't error, but actually initializes the `FileModel`.
-
-    Attributes:
-        filepath (Optional[Path]):
-            The path of this FileModel. This path can be either absolute or relative.
-            If it is a relative path, it is assumed to be resolved from some root
-            model.
-        save_location (Path):
-            A readonly property corresponding with the (current) save location of this
-            FileModel. If read from a file or after saving recursively or
-            after calling synchronize_filepath, this value will be updated to its new
-            state. If made from memory and filepath is not set, it will correspond with
-            cwd / filename.extension
-    """
-
-    __slots__ = ["__weakref__"]
-    # Use WeakValueDictionary to keep track of file paths with their respective parsed file models.
-    _file_models_cache: WeakValueDictionary = WeakValueDictionary()
-    filepath: Optional[Path] = None
-    # Absolute anchor is used to resolve the save location when the filepath is relative.
-    _absolute_anchor_path: Path = PrivateAttr(default_factory=Path.cwd)
-
-    def __new__(cls, filepath: Optional[PathOrStr] = None, *args, **kwargs):
-        """Create a new model.
-        If the file at the provided file path was already parsed, this instance is returned.
-
-        Args:
-            filepath (Optional[PathOrStr], optional): The file path to the file. Defaults to None.
-
-        Returns:
-            FileModel: A file model.
-        """
-        filepath = FileModel._change_to_path(filepath)
-        with file_load_context() as context:
-            if (file_model := context.retrieve_model(filepath)) is not None:
-                return file_model
-            else:
-                return super().__new__(cls)
-
-    def __init__(
-        self,
-        filepath: Optional[PathOrStr] = None,
-        resolve_casing: bool = False,
-        recurse: bool = True,
-        path_style: Optional[str] = None,
-        *args,
-        **kwargs,
-    ):
-        """Create a new FileModel from the given filepath.
-
-        If no filepath is provided, the model is initialized as an empty
-        model with default values.
-        If the filepath is provided, it is read from disk.
-
-        Args:
-            filepath (Optional[PathOrStr], optional): The file path. Defaults to None.
-            resolve_casing (bool, optional): Whether or not to resolve the file name references so that they match the case with what is on disk. Defaults to False.
-            recurse (bool, optional): Whether or not to recursively load the model. Defaults to True.
-            path_style (Optional[str], optional): Which path style is used in the loaded files. Defaults to the path style that matches the current operating system. Options: 'unix', 'windows'.
-
-        Raises:
-            ValueError: When an unsupported path style is passed.
-        """
-        if not filepath:
-            super().__init__(*args, **kwargs)
-            return
-
-        filepath = FileModel._change_to_path(filepath)
-        path_style = path_style_validator.validate(path_style)
-
-        with file_load_context() as context:
-            context.initialize_load_settings(recurse, resolve_casing, path_style)
-
-            filepath = context.convert_path_style(filepath)
-
-            if not FileModel._should_load_model(context):
-                super().__init__(*args, **kwargs)
-                self.filepath = filepath
-                return
-
-            self._absolute_anchor_path = context.get_current_parent()
-            loading_path = context.resolve(filepath)
-            loading_path = context.resolve_casing(loading_path)
-            if context.load_settings.resolve_casing:
-                filepath = self._get_updated_file_path(filepath, loading_path)
-
-            logger.info(f"Loading data from {filepath}")
-
-            data = self._load(loading_path)
-            context.register_model(filepath, self)
-            data["filepath"] = filepath
-            kwargs.update(data)
-
-            context.register_model(filepath, self)
-
-            # Note: the relative mode needs to be obtained from the data directly
-            # because self._relative_mode has not been resolved yet (this is done as
-            # part of the __init__), however during the __init__ we need to already
-            # have pushed the new parent. As such we cannot move this call later.
-            relative_mode = self._get_relative_mode_from_data(data)
-            context.push_new_parent(filepath.parent, relative_mode)
-
-            super().__init__(*args, **kwargs)
-            self._post_init_load()
-
-            context.pop_last_parent()
-
-    @classmethod
-    def _should_load_model(cls, context: FileLoadContext) -> bool:
-        """Determines whether the file model should be loaded or not.
-        A file model should be loaded when either all models should be loaded recursively,
-        or when no file model has been loaded yet.
-
-        Returns:
-            bool: Whether or not the file model should be loaded or not.
-        """
-        return context.load_settings.recurse or context.cache_is_empty()
-
-    def _post_init_load(self) -> None:
-        """
-        _post_init_load provides a hook into the __init__ of the FileModel which can be
-        used in subclasses for logic that requires the FileModel FileLoadContext.
-
-        It is guaranteed to be called after the pydantic model is, with the FileLoadContext
-        relative to this FileModel being loaded.
-        """
-        pass
-
-    @property
-    def _resolved_filepath(self) -> Optional[Path]:
-        if self.filepath is None:
-            return None
-
-        with file_load_context() as context:
-            return context.resolve(self.filepath)
-
-    @property
-    def save_location(self) -> Optional[Path]:
-        """Get the current save location which will be used when calling `save()`
-
-        This value can be None if the filepath is None and no name can be generated.
-
-        Returns:
-            Path: The location at which this model will be saved.
-        """
-        filepath = self.filepath or self._generate_name()
-
-        if filepath is None:
-            return None
-        elif filepath.is_absolute():
-            return filepath
-        else:
-            return self._absolute_anchor_path / filepath
-
-    def is_file_link(self) -> bool:
-        return True
-
-    def _get_updated_file_path(self, file_path: Path, loading_path: Path) -> Path:
-        """Update the file path with the resolved casing from the loading path.
-        Logs an information message if a file path is updated.
-
-        For example, given:
-            file_path = "To/A/File.txt"
-            loading_path = "D:/path/to/a/file.txt"
-
-        Then the result will be: "to/a/file.txt"
-
-        Args:
-            file_path (Path): The file path.
-            loading_path (Path): The resolved loading path.
-
-        Returns:
-            Path: The updated file path.
-        """
-
-        updated_file_parts = loading_path.parts[-len(file_path.parts) :]
-        updated_file_path = Path(*updated_file_parts)
-
-        if str(updated_file_path) != str(file_path):
-            logger.info(
-                f"Updating file reference from {file_path.name} to {updated_file_path}"
-            )
-
-        return updated_file_path
-
-    @classmethod
-    def validate(cls: Type["FileModel"], value: Any):
-        # Enable initialization with a Path.
-        if isinstance(value, (Path, str)):
-            # Pydantic Model init requires a dict
-            value = {"filepath": Path(value)}
-        return super().validate(value)
-
-    def save(
-        self,
-        filepath: Optional[Path] = None,
-        recurse: bool = False,
-        path_style: Optional[str] = None,
-    ) -> None:
-        """Save the model to disk.
-
-        If recurse is set to True, all of the child FileModels will be saved as well.
-        Relative child models are stored relative to this Model, according to the
-        model file hierarchy specified with the respective filepaths.
-        Absolute paths will be written to their respective locations. Note that this
-        will overwrite any existing files that are stored in this location.
-
-        Note that if recurse is set to True, the save_location properties of the
-        children are updated to their respective new locations.
-
-        If filepath it is specified, the filepath of this FileModel is set to the
-        specified path before the save operation is executed. If none is specified
-        it will use the current filepath.
-
-        If the used filepath is relative, it will be stored at the current
-        save_location. If you only want to save a child model of some root model, it is
-        recommended to first call synchronize_filepaths on the root model, to ensure
-        the child model's save_location is correctly determined.
-
-        Args:
-            filepath (Optional[Path], optional):
-                The file path at which this model is saved. If None is specified
-                it defaults to the filepath currently stored in the filemodel.
-                Defaults to None.
-            recurse (bool, optional):
-                Whether to save all children of this FileModel (when set to True),
-                or only save this model (when set to False). Defaults to False.
-            path_style (Optional[str], optional):
-                With which file path style to save the model. File references will
-                be written with the specified path style. Defaults to the path style
-                used by the current operating system. Options: 'unix', 'windows'.
-
-        Raises:
-            ValueError: When an unsupported path style is passed.
-        """
-        if filepath is not None:
-            self.filepath = filepath
-
-        path_style = path_style_validator.validate(path_style)
-        save_settings = ModelSaveSettings(path_style=path_style)
-
-        # Handle save
-        with file_load_context() as context:
-            context.push_new_parent(self._absolute_anchor_path, self._relative_mode)
-
-            if recurse:
-                self._save_tree(context, save_settings)
-            else:
-                self._save_instance(save_settings)
-
-    def _save_instance(self, save_settings: ModelSaveSettings) -> None:
-        if self.filepath is None:
-            self.filepath = self._generate_name()
-        self._save(save_settings)
-
-    def _save_tree(
-        self, context: FileLoadContext, save_settings: ModelSaveSettings
-    ) -> None:
-        # Ensure all names are generated prior to saving
-        def execute_generate_name(
-            model: BaseModel, acc: FileLoadContext
-        ) -> FileLoadContext:
-            if isinstance(model, FileModel) and model.filepath is None:
-                model.filepath = model._generate_name()
-            return acc
-
-        name_traverser = ModelTreeTraverser[FileLoadContext](
-            should_traverse=_should_traverse,
-            should_execute=_should_execute,
-            post_traverse_func=execute_generate_name,
-        )
-
-        name_traverser.traverse(self, context)
-
-        def save_pre(model: BaseModel, acc: FileLoadContext) -> FileLoadContext:
-            if isinstance(model, FileModel):
-                acc.push_new_parent(model.filepath.parent, model._relative_mode)  # type: ignore[arg-type]
-            return acc
-
-        def save_post(model: BaseModel, acc: FileLoadContext) -> FileLoadContext:
-            if isinstance(model, FileModel):
-                acc.pop_last_parent()
-                model._absolute_anchor_path = acc.get_current_parent()
-                model._save(save_settings)
-            return acc
-
-        save_traverser = ModelTreeTraverser[FileLoadContext](
-            should_traverse=_should_traverse,
-            should_execute=_should_execute,
-            pre_traverse_func=save_pre,
-            post_traverse_func=save_post,
-        )
-        save_traverser.traverse(self, context)
-
-    def synchronize_filepaths(self) -> None:
-        """Synchronize the save_location properties of all child models respective to
-        this FileModel's save_location.
-        """
-
-        def sync_pre(model: BaseModel, acc: FileLoadContext) -> FileLoadContext:
-            if isinstance(model, FileModel):
-                acc.push_new_parent(model.filepath.parent, model._relative_mode)  # type: ignore[arg-type]
-            return acc
-
-        def sync_post(model: BaseModel, acc: FileLoadContext) -> FileLoadContext:
-            if isinstance(model, FileModel):
-                acc.pop_last_parent()
-                model._absolute_anchor_path = acc.get_current_parent()
-            return acc
-
-        traverser = ModelTreeTraverser[FileLoadContext](
-            should_traverse=_should_traverse,
-            should_execute=_should_execute,
-            pre_traverse_func=sync_pre,
-            post_traverse_func=sync_post,
-        )
-
-        with file_load_context() as context:
-            context.push_new_parent(self._absolute_anchor_path, self._relative_mode)
-            traverser.traverse(self, context)
-
-    @property
-    def _relative_mode(self) -> ResolveRelativeMode:
-        """Get the ResolveRelativeMode of this FileModel.
-
-        Returns:
-            ResolveRelativeMode: The ResolveRelativeMode of this FileModel
-        """
-        return ResolveRelativeMode.ToParent
-
-    @classmethod
-    def _get_relative_mode_from_data(cls, data: Dict[str, Any]) -> ResolveRelativeMode:
-        """Gets the ResolveRelativeMode of this FileModel based on the provided data.
-
-        Note that by default, data is not used, and FileModels are always relative to
-        the parent. In exceptional cases, the relative mode can be dependent on the
-        data (i.e. the unvalidated/parsed dictionary fed into the pydantic basemodel).
-        As such the data is provided for such classes where the relative mode is
-        dependent on the state (e.g. the [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]).
-
-        Args:
-            data (Dict[str, Any]):
-                The unvalidated/parsed data which is fed to the pydantic base model,
-                used to determine the ResolveRelativeMode.
-
-        Returns:
-            ResolveRelativeMode: The ResolveRelativeMode of this FileModel
-        """
-        return ResolveRelativeMode.ToParent
-
-    @abstractclassmethod
-    def _generate_name(cls) -> Optional[Path]:
-        """Generate a (default) name for this FileModel.
-
-        Note that if _generate_name in theory can return a None value,
-        if this is possible in the specific implementation, _save should
-        be able to handle filepaths set to None.
-
-        Returns:
-            Optional[Path]:
-                a relative path with the default name of the model.
-        """
-        raise NotImplementedError()
-
-    @abstractmethod
-    def _save(self, save_settings: ModelSaveSettings) -> None:
-        """Save this instance to disk.
-
-        This method needs to be implemented by any class deriving from
-        FileModel, and is used in both the _save_instance and _save_tree
-        methods.
-
-        Args:
-            save_settings (ModelSaveSettings): The model save settings.
-        """
-        raise NotImplementedError()
-
-    @abstractmethod
-    def _load(self, filepath: Path) -> Dict:
-        """Load the data at filepath and returns it as a dictionary.
-
-        If a derived FileModel does not load data from disk, this should
-        return an empty dictionary.
-
-        Args:
-            filepath (Path): Path to the data to load.
-
-        Returns:
-            Dict: The data stored at filepath
-        """
-        raise NotImplementedError()
-
-    def __str__(self) -> str:
-        return str(self.filepath if self.filepath else "")
-
-    @staticmethod
-    def _change_to_path(filepath):
-        if filepath is None:
-            return filepath
-        elif isinstance(filepath, Path):
-            return filepath
-        else:
-            return Path(filepath)
-
-    @validator("filepath")
-    def _conform_filepath_to_pathlib(cls, value):
-        return FileModel._change_to_path(value)
-
-
-class SerializerConfig(BaseModel, ABC):
-    """Class that holds the configuration settings for serialization."""
-
-    float_format: str = ""
-    """str: The string format that will be used for float serialization. If empty, the original number will be serialized. Defaults to an empty string.
-        
-        Examples:
-            Input value = 123.456
-
-            Format    | Output          | Description
-            -------------------------------------------------------------------------------------------------------------------------------------
-            ".0f"     | 123             | Format float with 0 decimal places.
-            "f"       | 123.456000      | Format float with default (=6) decimal places.
-            ".2f"     | 123.46          | Format float with 2 decimal places.
-            "+.1f"    | +123.5          | Format float with 1 decimal place with a + or  sign.
-            "e"       | 1.234560e+02    | Format scientific notation with the letter 'e' with default (=6) decimal places.
-            "E"       | 1.234560E+02    | Format scientific notation with the letter 'E' with default (=6) decimal places.
-            ".3e"     | 1.235e+02       | Format scientific notation with the letter 'e' with 3 decimal places.
-            "<15"     | 123.456         | Left aligned in space with width 15
-            "^15.0f"  |       123       | Center aligned in space with width 15 with 0 decimal places.
-            ">15.1e"  |         1.2e+02 | Right aligned in space with width 15 with scientific notation with 1 decimal place.
-            "*>15.1f" | **********123.5 | Right aligned in space with width 15 with 1 decimal place and fill empty space with *
-            "%"       | 12345.600000%   | Format percentage with default (=6) decimal places.     
-            ".3%"     | 12345.600%      | Format percentage with 3 decimal places.
-
-            More information: https://docs.python.org/3/library/string.html#format-specification-mini-language
-        """
-
-
-class ParsableFileModel(FileModel):
-    """ParsableFileModel defines a FileModel which can be parsed
-    and serialized with a serializer .
-
-    Each ParsableFileModel has a default _filename and _ext,
-    which are used to generate the file name of any instance where
-    the filepath is not (yet) set.
-
-    Children of the ParsableFileModel are expected to implement a
-    serializer function which takes a Path and Dict and writes the
-    ParsableFileModel to disk, and a parser function which takes
-    a Path and outputs a Dict.
-
-    If more complicated solutions are required, a ParsableFileModel
-    child can also opt to overwrite the _serialize and _parse methods,
-    to skip the _get_serializer and _get_parser methods respectively.
-    """
-
-    serializer_config: SerializerConfig = SerializerConfig()
-
-    def _load(self, filepath: Path) -> Dict:
-        # TODO Make this lazy in some cases so it doesn't become slow
-        if filepath.is_file():
-            return self._parse(filepath)
-        else:
-            raise ValueError(f"File: `{filepath}` not found, skipped parsing.")
-
-    def _save(self, save_settings: ModelSaveSettings) -> None:
-        """Save the data of this FileModel.
-
-        _save provides a hook for child models to overwrite the save behaviour as
-        called during the tree traversal.
-
-        Args:
-            save_settings (ModelSaveSettings): The model save settings.
-        """
-        self._serialize(self.dict(), save_settings)
-
-    def _serialize(self, data: dict, save_settings: ModelSaveSettings) -> None:
-        """Serializes the data to file. Should not be called directly, only through `_save`.
-
-        Args:
-            save_settings (ModelSaveSettings): The model save settings.
-        """
-        path = self._resolved_filepath
-        if path is None:
-            # TODO: Do we need to add a warning / exception here
-            return
-
-        path.parent.mkdir(parents=True, exist_ok=True)
-        self._get_serializer()(path, data, self.serializer_config, save_settings)
-
-    def dict(self, *args, **kwargs):
-        kwargs["exclude"] = self._exclude_fields()
-        return super().dict(*args, **kwargs)
-
-    @classmethod
-    def _exclude_fields(cls) -> Set[str]:
-        """A set containing the field names that should not be serialized."""
-        return {"filepath", "serializer_config"}
-
-    @classmethod
-    def _parse(cls, path: Path) -> Dict:
-        return cls._get_parser()(path)
-
-    @classmethod
-    def _generate_name(cls) -> Path:
-        name, ext = cls._filename(), cls._ext()
-        return Path(f"{name}{ext}")
-
-    @abstractclassmethod
-    def _filename(cls) -> str:
-        return "test"
-
-    @abstractclassmethod
-    def _ext(cls) -> str:
-        return ".test"
-
-    @abstractclassmethod
-    def _get_serializer(
-        cls,
-    ) -> Callable[[Path, Dict, SerializerConfig, ModelSaveSettings], None]:
-        return DummySerializer.serialize
-
-    @abstractclassmethod
-    def _get_parser(cls) -> Callable[[Path], Dict]:
-        return DummmyParser.parse
-
-    def _get_identifier(self, data: dict) -> Optional[str]:
-        filepath = data.get("filepath")
-        if filepath:
-            return filepath.name
-        return None
-
-
-class DiskOnlyFileModel(FileModel):
-    """DiskOnlyFileModel provides a stub implementation for file based
-    models which are not explicitly implemented within hydrolib.core.
-
-    It implements the FileModel with a void parser and serializer, and a
-    save method which copies the file associated with the FileModel
-    to a new location if it exists.
-
-    We further explicitly assume that when the filepath is None, no
-    file will be written.
-
-    Actual file model implementations *should not* inherit from the
-    DiskOnlyFileModel and instead inherit directly from FileModel.
-    """
-
-    _source_file_path: Optional[Path] = PrivateAttr(default=None)
-
-    def _post_init_load(self) -> None:
-        # After initialisation we retrieve the _resolved_filepath
-        # this should correspond with the actual absolute path of the
-        # underlying file. Only after saving this path will be updated.
-        super()._post_init_load()
-        self._source_file_path = self._resolved_filepath
-
-    def _load(self, filepath: Path) -> Dict:
-        # We de not load any additional data, as such we return an empty dict.
-        return dict()
-
-    def _save(self, save_settings: ModelSaveSettings) -> None:
-        # The target_file_path contains the new path to write to, while the
-        # _source_file_path contains the original data. If these are not the
-        # same we copy the file and update the underlying source path.
-        target_file_path = self._resolved_filepath
-        if self._can_copy_to(target_file_path):
-            target_file_path.parent.mkdir(parents=True, exist_ok=True)  # type: ignore[arg-type]
-            shutil.copy(self._source_file_path, target_file_path)  # type: ignore[arg-type]
-        self._source_file_path = target_file_path
-
-    def _can_copy_to(self, target_file_path: Optional[Path]) -> bool:
-        return (
-            self._source_file_path is not None
-            and target_file_path is not None
-            and self._source_file_path != target_file_path
-            and self._source_file_path.exists()
-            and self._source_file_path.is_file()
-        )
-
-    @classmethod
-    def _generate_name(cls) -> Optional[Path]:
-        # There is no common name for DiskOnlyFileModel, instead we
-        # do not generate names and skip None filepaths.
-        return None
-
-    def is_intermediate_link(self) -> bool:
-        # If the filepath is not None, there is an underlying file, and as such we need
-        # to traverse it.
-        return self.filepath is not None
-
-
-def validator_set_default_disk_only_file_model_when_none() -> classmethod:
-    """Validator to ensure a default empty DiskOnlyFileModel is created
-    when the corresponding field is initialized with None.
-
-    Returns:
-        classmethod: Validator to adjust None values to empty DiskOnlyFileModel objects
-    """
-
-    def adjust_none(v: Any, field: ModelField) -> Any:
-        if field.type_ is DiskOnlyFileModel and v is None:
-            return {"filepath": None}
-        return v
-
-    return validator("*", allow_reuse=True, pre=True)(adjust_none)
-
-
-class PathStyleValidator:
-    """Class to take care of path style validation."""
-
-    _os_path_style = get_path_style_for_current_operating_system()
-
-    def validate(self, path_style: Optional[str]) -> PathStyle:
-        """Validates the path style as string on whether it is a supported path style.
-        If it is a valid path style the path style enum value will be return as a result.
-
-        Args:
-            path_style (Optional[str]): The path style as string value.
-
-        Returns:
-            PathStyle: The converted PathStyle object.
-
-        Raises:
-            ValueError: When an unsupported path style is passed.
-        """
-        if path_style is None:
-            return self._os_path_style
-
-        supported_path_styles = list(PathStyle)
-        if path_style in supported_path_styles:
-            return PathStyle(path_style)
-
-        supported_path_style_str = ", ".join(([x.value for x in supported_path_styles]))
-        raise ValueError(
-            f"Path style '{path_style}' not supported. Supported path styles: {supported_path_style_str}"
-        )
-
-
-path_style_validator = PathStyleValidator()
+"""
+Here we define our Pydantic `BaseModel` with custom settings,
+as well as a `FileModel` that inherits from a `BaseModel` but
+also represents a file on disk.
+
+"""
+import logging
+import shutil
+from abc import ABC, abstractclassmethod, abstractmethod
+from contextlib import contextmanager
+from contextvars import ContextVar
+from enum import IntEnum
+from pathlib import Path
+from typing import (
+    Any,
+    Callable,
+    Dict,
+    Generic,
+    List,
+    Optional,
+    Set,
+    Tuple,
+    Type,
+    TypeVar,
+    Union,
+)
+from weakref import WeakValueDictionary
+
+from pydantic import BaseModel as PydanticBaseModel
+from pydantic import validator
+from pydantic.error_wrappers import ErrorWrapper, ValidationError
+from pydantic.fields import ModelField, PrivateAttr
+
+from hydrolib.core.base import DummmyParser, DummySerializer
+from hydrolib.core.utils import (
+    FilePathStyleConverter,
+    OperatingSystem,
+    PathStyle,
+    get_operating_system,
+    get_path_style_for_current_operating_system,
+    str_is_empty_or_none,
+    to_key,
+)
+
+logger = logging.getLogger(__name__)
+
+# We use ContextVars to keep a reference to the folder
+# we're currently parsing files in. In the future
+# we could move to https://github.com/samuelcolvin/pydantic/issues/1549
+context_file_loading: ContextVar["FileLoadContext"] = ContextVar("file_loading")
+
+
+class BaseModel(PydanticBaseModel):
+    class Config:
+        arbitrary_types_allowed = True
+        validate_assignment = True
+        use_enum_values = True
+        extra = "forbid"  # will throw errors so we can fix our models
+        allow_population_by_field_name = True
+        alias_generator = to_key
+
+    def __init__(self, **data: Any) -> None:
+        """Initialize a BaseModel with the provided data.
+
+        Raises:
+            ValidationError: A validation error when the data is invalid.
+        """
+        try:
+            super().__init__(**data)
+        except ValidationError as e:
+
+            # Give a special message for faulty list input
+            for re in e.raw_errors:
+                if (
+                    hasattr(re, "_loc")
+                    and hasattr(re.exc, "msg_template")
+                    and isinstance(data.get(to_key(re._loc)), list)
+                ):
+                    re.exc.msg_template += (
+                        f". The key {re._loc} might be duplicated in the input file."
+                    )
+
+            # Update error with specific model location name
+            identifier = self._get_identifier(data)
+            if identifier is None:
+                raise e
+            else:
+                # If there is an identifier, include this in the ValidationError messages.
+                raise ValidationError([ErrorWrapper(e, loc=identifier)], self.__class__)
+
+    def is_file_link(self) -> bool:
+        """Generic attribute for models backed by a file."""
+        return False
+
+    def is_intermediate_link(self) -> bool:
+        """Generic attribute for models that have children fields that could contain files."""
+        return self.is_file_link()
+
+    def show_tree(self, indent=0):
+        """Recursive print function for showing a tree of a model."""
+        angle = "âˆŸ" if indent > 0 else ""
+
+        # Only print if we're backed by a file
+        if self.is_file_link():
+            print(" " * indent * 2, angle, self)
+
+        # Otherwise we recurse through the fields of a model
+        for _, value in self:
+            # Handle lists of items
+            if not isinstance(value, list):
+                value = [value]
+            for v in value:
+                if hasattr(v, "is_intermediate_link") and v.is_intermediate_link():
+                    # If the field is only an intermediate, print the name only
+                    if not v.is_file_link():
+                        print(" " * (indent * 2 + 2), angle, v.__class__.__name__)
+                    v.show_tree(indent + 1)
+
+    def _apply_recurse(self, f, *args, **kwargs):
+        # TODO Could we use this function for `show_tree`?
+        for _, value in self:
+            # Handle lists of items
+            if not isinstance(value, list):
+                value = [value]
+            for v in value:
+                if hasattr(v, "is_intermediate_link") and v.is_intermediate_link():
+                    v._apply_recurse(f, *args, **kwargs)
+
+        # Run self as last, so we can make use of the nested updates
+        if self.is_file_link():
+            getattr(self, f)(*args, **kwargs)
+
+    def _get_identifier(self, data: dict) -> Optional[str]:
+        """Get the identifier for this model.
+
+        Args:
+            data (dict): The data from which to retrieve the identifier
+
+        Returns:
+            str: The identifier or None.
+        """
+        return None
+
+
+TAcc = TypeVar("TAcc")
+
+
+class ModelTreeTraverser(Generic[TAcc]):
+    """ModelTreeTraverser is responsible for traversing a ModelTree using the provided
+    functions.
+
+    The ModelTreeTraverser will only traverse BaseModel and derived objects.
+    Type parameter TAcc defines the type of Accumulator to be used.
+    """
+
+    def __init__(
+        self,
+        should_traverse: Optional[Callable[[BaseModel, TAcc], bool]] = None,
+        should_execute: Optional[Callable[[BaseModel, TAcc], bool]] = None,
+        pre_traverse_func: Optional[Callable[[BaseModel, TAcc], TAcc]] = None,
+        post_traverse_func: Optional[Callable[[BaseModel, TAcc], TAcc]] = None,
+    ):
+        """Create a new ModelTreeTraverser with the given functions.
+
+        If a predicate it is not defined, it is assumed to always be true, i.e. we will
+        always traverse to the next node, or always execute the traverse functions.
+
+        If a traverse function is not defined, it will be skipped.
+
+        The traverse functions share an accumulator, i.e. the accumulator argument
+        is passed through all evaluated traverse functions. It is expected that the
+        traverse function return the (potentially) changed accumulator.
+
+        Args:
+            should_traverse (Optional[Callable[[BaseModel, TAcc], bool]], optional):
+                Function to evaluate whether to traverse to the provided BaseModel. Defaults to None.
+            should_execute (Optional[Callable[[BaseModel, TAcc], bool]], optional):
+                Function to evaluate whether to execute the traverse functions for the
+                provided BaseModel. Defaults to None.
+            pre_traverse_func (Callable[[BaseModel, TAcc], TAcc], optional):
+                Traverse function executed before we traverse into the next BaseModel,
+                i.e. top-down traversal. Defaults to None.
+            post_traverse_func (Callable[[BaseModel, TAcc], TAcc], optional):
+                Traverse function executed after we traverse into the next BaseModel,
+                i.e. bottom-up traversal. Defaults to None.
+        """
+        self._should_traverse_func = should_traverse
+        self._should_execute_func = should_execute
+        self._pre_traverse_func = pre_traverse_func
+        self._post_traverse_func = post_traverse_func
+
+    def _should_execute(self, model: BaseModel, acc: TAcc) -> bool:
+        return self._should_execute_func is None or self._should_execute_func(
+            model, acc
+        )
+
+    def _should_execute_pre(self, model: BaseModel, acc: TAcc) -> bool:
+        return self._pre_traverse_func is not None and self._should_execute(model, acc)
+
+    def _should_execute_post(self, model: BaseModel, acc: TAcc) -> bool:
+        return self._post_traverse_func is not None and self._should_execute(model, acc)
+
+    def _should_traverse(self, value: Any, acc: TAcc) -> bool:
+        return isinstance(value, BaseModel) and (
+            self._should_traverse_func is None or self._should_traverse_func(value, acc)
+        )
+
+    def traverse(self, model: BaseModel, acc: TAcc) -> TAcc:
+        """Traverse the model tree of BaseModels including the model as the root, with
+        the provided state of the acc and return the final accumulator.
+
+        The actual executed functions as well as the predicates defining whether these
+        functions should be executed for this model as well as whether child BaseModel
+        objects should be traversed are provided in the constructor of the
+        ModelTreeTraverser.
+
+        The final accumulator is returned.
+
+        Args:
+            model (BaseModel):
+                The root model in which the traversal of the model tree starts.
+            acc (TAcc):
+                The current accumulator.
+
+        Returns:
+            TAcc: The accumulator after the traversal of the model tree.
+        """
+        if self._should_execute_pre(model, acc):
+            acc = self._pre_traverse_func(model, acc)  # type: ignore[arg-type]
+
+        for _, value in model:
+            if not isinstance(value, list):
+                value = [value]
+
+            for v in value:
+                if self._should_traverse(v, acc):
+                    acc = self.traverse(v, acc)
+
+        if self._should_execute_post(model, acc):
+            acc = self._post_traverse_func(model, acc)  # type: ignore[arg-type]
+
+        return acc
+
+
+class ResolveRelativeMode(IntEnum):
+    """ResolveRelativeMode defines the possible resolve modes used within the
+    FilePathResolver.
+
+    It determines how the relative paths to child models within some FileModel
+    should be interpreted. By default it should be relative to the parent model,
+    i.e. the model in which the mode is defined. However there exist some exceptions,
+    where the relative paths should be evaluated relative to some other folder,
+    regardless of the current parent location.
+
+    Options:
+        ToParent:
+            Relative paths should be resolved relative to their direct parent model.
+        ToAnchor:
+            All relative paths should be resolved relative to the specified regardless
+            of subsequent parent model locations.
+    """
+
+    ToParent = 0
+    ToAnchor = 1
+
+
+class FileCasingResolver:
+    """Class for resolving file path in a case-insensitive manner."""
+
+    def resolve(self, path: Path) -> Path:
+        """Resolve the casing of a file path when the file does exist but not with the exact casing.
+
+        Args:
+            path (Path): The path of the file or directory for which the casing needs to be resolved.
+
+        Returns:
+            Path: The file path with the matched casing if a match exists; otherwise, the original file path.
+
+        Raises:
+            NotImplementedError: When this function is called with an operating system other than Windows, Linux or MacOS.
+        """
+
+        operating_system = get_operating_system()
+        if operating_system == OperatingSystem.WINDOWS:
+            return self._resolve_casing_windows(path)
+        if operating_system == OperatingSystem.LINUX:
+            return self._resolve_casing_linux(path)
+        if operating_system == OperatingSystem.MACOS:
+            return self._resolve_casing_macos(path)
+        else:
+            raise NotImplementedError(
+                f"Path case resolving for operating system {operating_system} is not supported yet."
+            )
+
+    def _resolve_casing_windows(self, path: Path):
+        return path.resolve()
+
+    def _resolve_casing_linux(self, path: Path):
+        if path.exists():
+            return path
+
+        if not path.parent.exists() and not str_is_empty_or_none(path.parent.name):
+            path = self._resolve_casing_linux(path.parent) / path.name
+
+        return self._find_match(path)
+
+    def _resolve_casing_macos(self, path: Path):
+        if not str_is_empty_or_none(path.parent.name):
+            path = self._resolve_casing_macos(path.parent) / path.name
+
+        return self._find_match(path)
+
+    def _find_match(self, path: Path):
+        if path.parent.exists():
+            for item in path.parent.iterdir():
+                if item.name.lower() == path.name.lower():
+                    return path.with_name(item.name)
+
+        return path
+
+
+class FilePathResolver:
+    """FilePathResolver is responsible for resolving relative paths.
+
+    The current state to which paths are resolved can be altered by
+    pushing a new parent path to the FilePathResolver, or removing the
+    latest added parent path from the FilePathResolver
+    """
+
+    def __init__(self) -> None:
+        """Create a new empty FilePathResolver."""
+        self._anchors: List[Path] = []
+        self._parents: List[Tuple[Path, ResolveRelativeMode]] = []
+
+    @property
+    def _anchor(self) -> Optional[Path]:
+        return self._anchors[-1] if self._anchors else None
+
+    @property
+    def _direct_parent(self) -> Path:
+        return self._parents[-1][0] if self._parents else Path.cwd()
+
+    def get_current_parent(self) -> Path:
+        """Get the current absolute path with which files are resolved.
+
+        If the current mode is relative to the parent, the latest added
+        parent is added. If the current mode is relative to an anchor
+        path, the latest added anchor path is returned.
+
+        Returns:
+            Path: The absolute path to the current parent.
+        """
+        if self._anchor:
+            return self._anchor
+        return self._direct_parent
+
+    def resolve(self, path: Path) -> Path:
+        """Resolve the provided path to an absolute path given the current state.
+
+        If the provided path is already absolute, it will be returned as is.
+
+        Args:
+            path (Path): The path to resolve
+
+        Returns:
+            Path: An absolute path resolved given the current state.
+        """
+        if path.is_absolute():
+            return path
+
+        parent = self.get_current_parent()
+        return (parent / path).resolve()
+
+    def push_new_parent(
+        self, parent_path: Path, relative_mode: ResolveRelativeMode
+    ) -> None:
+        """Push a new parent_path with the given relative_mode to this FilePathResolver.
+
+        Relative paths added to this FilePathResolver will be resolved with respect
+        to the current state, i.e. similar to FilePathResolver.resolve.
+
+        Args:
+            parent_path (Path): The parent path
+            relative_mode (ResolveRelativeMode): The relative mode used to resolve.
+        """
+        absolute_parent_path = self.resolve(parent_path)
+        if relative_mode == ResolveRelativeMode.ToAnchor:
+            self._anchors.append(absolute_parent_path)
+
+        self._parents.append((absolute_parent_path, relative_mode))
+
+    def pop_last_parent(self) -> None:
+        """Pop the last added parent from this FilePathResolver
+
+        If there are currently no parents defined, nothing will happen.
+        """
+        if not self._parents:
+            return
+
+        _, relative_mode = self._parents.pop()
+
+        if relative_mode == ResolveRelativeMode.ToAnchor:
+            self._anchors.pop()
+
+
+class FileModelCache:
+    """
+    FileModelCache provides a simple structure to register and retrieve FileModel
+    objects.
+    """
+
+    def __init__(self):
+        """Create a new empty FileModelCache."""
+        self._cache_dict: Dict[Path, "FileModel"] = {}
+
+    def retrieve_model(self, path: Path) -> Optional["FileModel"]:
+        """Retrieve the model associated with the (absolute) path if
+        it has been registered before, otherwise return None.
+
+        Returns:
+            [Optional[FileModel]]:
+                The FileModel associated with the Path if it has been registered
+                before, otherwise None.
+        """
+        return self._cache_dict.get(path, None)
+
+    def register_model(self, path: Path, model: "FileModel") -> None:
+        """Register the model with the specified path in this FileModelCache.
+
+        Args:
+            path (Path): The path to associate the model with.
+            model (FileModel): The model to be associated with the path.
+        """
+        self._cache_dict[path] = model
+
+    def is_empty(self) -> bool:
+        """Whether or not this file model cache is empty.
+
+        Returns:
+            bool: Whether or not the cache is empty.
+        """
+        return not any(self._cache_dict)
+
+
+class ModelSaveSettings:
+    """A class that holds the global settings for model saving."""
+
+    _os_path_style = get_path_style_for_current_operating_system()
+
+    def __init__(self, path_style: Optional[PathStyle] = None) -> None:
+        """Initializes a new instance of the ModelSaveSettings class.
+
+        Args:
+            path_style (Optional[PathStyle], optional): Which file path style to use when saving the model. Defaults to the path style that matches the current operating system.
+        """
+
+        if path_style is None:
+            path_style = self._os_path_style
+
+        self._path_style = path_style
+
+    @property
+    def path_style(self) -> PathStyle:
+        """Gets the path style setting.
+
+        Returns:
+            PathStyle: Which path style is used to save the files.
+        """
+        return self._path_style
+
+
+class ModelLoadSettings:
+    """A class that holds the global settings for model loading."""
+
+    def __init__(
+        self, recurse: bool, resolve_casing: bool, path_style: PathStyle
+    ) -> None:
+        """Initializes a new instance of the ModelLoadSettings class.
+
+        Args:
+            recurse (bool): Whether or not to recursively load the whole model.
+            resolve_casing (bool): Whether or not to resolve the file casing.
+            path_style (PathStyle): Which path style is used in the loaded files.
+        """
+        self._recurse = recurse
+        self._resolve_casing = resolve_casing
+        self._path_style = path_style
+
+    @property
+    def recurse(self) -> bool:
+        """Gets the recurse setting.
+
+        Returns:
+            bool: Whether or not to recursively load the whole model.
+        """
+        return self._recurse
+
+    @property
+    def resolve_casing(self) -> bool:
+        """Gets the resolve casing setting.
+
+        Returns:
+            bool: Whether or not to resolve the file casing.
+        """
+        return self._resolve_casing
+
+    @property
+    def path_style(self) -> PathStyle:
+        """Gets the path style setting.
+
+        Returns:
+            PathStyle: Which path style is used in the loaded files.
+        """
+        return self._path_style
+
+
+class FileLoadContext:
+    """FileLoadContext provides the context necessary to resolve paths
+    during the init of a FileModel, as well as ensure the relevant models
+    are only read once.
+    """
+
+    def __init__(self) -> None:
+        """Create a new empty FileLoadContext."""
+        self._path_resolver = FilePathResolver()
+        self._cache = FileModelCache()
+        self._file_casing_resolver = FileCasingResolver()
+        self._file_path_style_converter = FilePathStyleConverter()
+        self._load_settings: Optional[ModelLoadSettings] = None
+
+    def initialize_load_settings(
+        self, recurse: bool, resolve_casing: bool, path_style: PathStyle
+    ):
+        """Initialize the global model load setting. Can only be set once.
+
+        Args:
+            recurse (bool): Whether or not to recursively load the whole model.
+            resolve_casing (bool): Whether or not to resolve the file casing.
+            path_style (PathStyle): Which path style is used in the loaded files.
+        """
+        if self._load_settings is None:
+            self._load_settings = ModelLoadSettings(recurse, resolve_casing, path_style)
+
+    @property
+    def load_settings(self) -> ModelLoadSettings:
+        """Gets the model load settings.
+
+        Raises:
+            ValueError: When the model load settings have not been initialized yet.
+        Returns:
+            ModelLoadSettings: The model load settings.
+
+        """
+        if self._load_settings is None:
+            raise ValueError(
+                f"The model load settings have not been initialized yet. Make sure to call `{self.initialize_load_settings.__name__}` first."
+            )
+
+        return self._load_settings
+
+    def retrieve_model(self, path: Optional[Path]) -> Optional["FileModel"]:
+        """Retrieve the model associated with the path.
+
+        If no model has been associated with the provided path, or the path is None,
+        then None will be returned. Relative paths will be resolved based on the
+        current state of the FileLoadContext.
+
+        Returns:
+            [Optional[FileModel]]:
+                The file model associated with the provided path if it has been
+                registered, else None.
+        """
+        if path is None:
+            return None
+
+        absolute_path = self._path_resolver.resolve(path)
+        return self._cache.retrieve_model(absolute_path)
+
+    def register_model(self, path: Path, model: "FileModel") -> None:
+        """Associate the provided model with the provided path.
+
+        Relative paths will be resolved based on the current state of the
+        FileLoadContext.
+
+        Args:
+            path (Path): The relative path from which the model was loaded.
+            model (FileModel): The loaded model.
+        """
+        absolute_path = self._path_resolver.resolve(path)
+        self._cache.register_model(absolute_path, model)
+
+    def cache_is_empty(self) -> bool:
+        """Whether or not the file model cache is empty.
+
+        Returns:
+            bool: Whether or not the file model cache is empty.
+        """
+        return self._cache.is_empty()
+
+    def get_current_parent(self) -> Path:
+        """Get the current absolute path with which files are resolved.
+
+        If the current mode is relative to the parent, the latest added
+        parent is added. If the current mode is relative to an anchor
+        path, the latest added anchor path is returned.
+
+        Returns:
+            Path: The absolute path to the current parent.
+        """
+        return self._path_resolver.get_current_parent()
+
+    def resolve(self, path: Path) -> Path:
+        """Resolve the provided path.
+
+        If path is already absolute, it will be returned as is. Otherwise
+        it will be resolved based on the current state of this FileLoadContext.
+
+        Args:
+            path (Path): The path to be resolved.
+
+        Returns:
+            Path: An absolute path resolved based on the current state.
+        """
+        return self._path_resolver.resolve(path)
+
+    def push_new_parent(
+        self, parent_path: Path, relative_mode: ResolveRelativeMode
+    ) -> None:
+        """Push a new parent_path with the given relative_mode on this
+        FileLoadContext.
+
+        Args:
+            parent_path (Path): The parent path to be added to this FileLoadContext.
+            relative_mode (ResolveRelativeMode): The relative mode.
+        """
+        self._path_resolver.push_new_parent(parent_path, relative_mode)
+
+    def pop_last_parent(self) -> None:
+        """Pop the last added parent off this FileLoadContext."""
+        self._path_resolver.pop_last_parent()
+
+    def resolve_casing(self, file_path: Path) -> Path:
+        """Resolve the file casing for the provided file path.
+
+        Args:
+            file_path (Path): The file path to resolve the casing for.
+
+        Returns:
+            Path: The resolved file path.
+        """
+        if self.load_settings.resolve_casing:
+            return self._file_casing_resolver.resolve(file_path)
+        return file_path
+
+    def convert_path_style(self, file_path: Path) -> Path:
+        """Resolve the file path by converting it from its own file path style to the path style for the current operating system.
+
+        Args:
+            file_path (Path): The file path to convert to the OS path style.
+
+        Returns:
+            Path: The resolved file path.
+        """
+
+        if file_path.is_absolute():
+            return file_path
+
+        converted_file_path = self._file_path_style_converter.convert_to_os_style(
+            file_path, self.load_settings.path_style
+        )
+        return Path(converted_file_path)
+
+
+@contextmanager
+def file_load_context():
+    """Provide a FileLoadingContext. If none has been created in the context of
+    this call stack yet, a new one will be created, which will be maintained
+    until it goes out of scope.
+
+    Yields:
+        [FileLoadContext]: The file load context.
+    """
+    file_loading_context = context_file_loading.get(None)
+    context_reset_token = None
+
+    if not file_loading_context:
+        file_loading_context = FileLoadContext()
+        context_reset_token = context_file_loading.set(file_loading_context)
+
+    try:
+        yield file_loading_context
+    finally:
+        if context_reset_token is not None:
+            context_file_loading.reset(context_reset_token)
+
+
+def _should_traverse(model: BaseModel, _: FileLoadContext) -> bool:
+    return model.is_intermediate_link()
+
+
+def _should_execute(model: BaseModel, _: FileLoadContext) -> bool:
+    return model.is_file_link()
+
+
+PathOrStr = Union[Path, str]
+
+
+class FileModel(BaseModel, ABC):
+    """Base class to represent models with a file representation.
+
+    It therefore always has a `filepath` and if it is given on
+    initilization, it will parse that file. The filepath can be
+    relative, in which case the paths are expected to be resolved
+    relative to some root model. If a path is absolute, this path
+    will always be used, regardless of a root parent.
+
+    When saving a model, if the current filepath is relative, the
+    last resolved absolute path will be used. If the model has just
+    been read, the
+
+    This class extends the `validate` option of Pydantic,
+    so when when a Path is given to a field with type `FileModel`,
+    it doesn't error, but actually initializes the `FileModel`.
+
+    Attributes:
+        filepath (Optional[Path]):
+            The path of this FileModel. This path can be either absolute or relative.
+            If it is a relative path, it is assumed to be resolved from some root
+            model.
+        save_location (Path):
+            A readonly property corresponding with the (current) save location of this
+            FileModel. If read from a file or after saving recursively or
+            after calling synchronize_filepath, this value will be updated to its new
+            state. If made from memory and filepath is not set, it will correspond with
+            cwd / filename.extension
+    """
+
+    __slots__ = ["__weakref__"]
+    # Use WeakValueDictionary to keep track of file paths with their respective parsed file models.
+    _file_models_cache: WeakValueDictionary = WeakValueDictionary()
+    filepath: Optional[Path] = None
+    # Absolute anchor is used to resolve the save location when the filepath is relative.
+    _absolute_anchor_path: Path = PrivateAttr(default_factory=Path.cwd)
+
+    def __new__(cls, filepath: Optional[PathOrStr] = None, *args, **kwargs):
+        """Create a new model.
+        If the file at the provided file path was already parsed, this instance is returned.
+
+        Args:
+            filepath (Optional[PathOrStr], optional): The file path to the file. Defaults to None.
+
+        Returns:
+            FileModel: A file model.
+        """
+        filepath = FileModel._change_to_path(filepath)
+        with file_load_context() as context:
+            if (file_model := context.retrieve_model(filepath)) is not None:
+                return file_model
+            else:
+                return super().__new__(cls)
+
+    def __init__(
+        self,
+        filepath: Optional[PathOrStr] = None,
+        resolve_casing: bool = False,
+        recurse: bool = True,
+        path_style: Optional[str] = None,
+        *args,
+        **kwargs,
+    ):
+        """Create a new FileModel from the given filepath.
+
+        If no filepath is provided, the model is initialized as an empty
+        model with default values.
+        If the filepath is provided, it is read from disk.
+
+        Args:
+            filepath (Optional[PathOrStr], optional): The file path. Defaults to None.
+            resolve_casing (bool, optional): Whether or not to resolve the file name references so that they match the case with what is on disk. Defaults to False.
+            recurse (bool, optional): Whether or not to recursively load the model. Defaults to True.
+            path_style (Optional[str], optional): Which path style is used in the loaded files. Defaults to the path style that matches the current operating system. Options: 'unix', 'windows'.
+
+        Raises:
+            ValueError: When an unsupported path style is passed.
+        """
+        if not filepath:
+            super().__init__(*args, **kwargs)
+            return
+
+        filepath = FileModel._change_to_path(filepath)
+        path_style = path_style_validator.validate(path_style)
+
+        with file_load_context() as context:
+            context.initialize_load_settings(recurse, resolve_casing, path_style)
+
+            filepath = context.convert_path_style(filepath)
+
+            if not FileModel._should_load_model(context):
+                super().__init__(*args, **kwargs)
+                self.filepath = filepath
+                return
+
+            self._absolute_anchor_path = context.get_current_parent()
+            loading_path = context.resolve(filepath)
+            loading_path = context.resolve_casing(loading_path)
+            if context.load_settings.resolve_casing:
+                filepath = self._get_updated_file_path(filepath, loading_path)
+
+            logger.info(f"Loading data from {filepath}")
+
+            data = self._load(loading_path)
+            context.register_model(filepath, self)
+            data["filepath"] = filepath
+            kwargs.update(data)
+
+            context.register_model(filepath, self)
+
+            # Note: the relative mode needs to be obtained from the data directly
+            # because self._relative_mode has not been resolved yet (this is done as
+            # part of the __init__), however during the __init__ we need to already
+            # have pushed the new parent. As such we cannot move this call later.
+            relative_mode = self._get_relative_mode_from_data(data)
+            context.push_new_parent(filepath.parent, relative_mode)
+
+            super().__init__(*args, **kwargs)
+            self._post_init_load()
+
+            context.pop_last_parent()
+
+    @classmethod
+    def _should_load_model(cls, context: FileLoadContext) -> bool:
+        """Determines whether the file model should be loaded or not.
+        A file model should be loaded when either all models should be loaded recursively,
+        or when no file model has been loaded yet.
+
+        Returns:
+            bool: Whether or not the file model should be loaded or not.
+        """
+        return context.load_settings.recurse or context.cache_is_empty()
+
+    def _post_init_load(self) -> None:
+        """
+        _post_init_load provides a hook into the __init__ of the FileModel which can be
+        used in subclasses for logic that requires the FileModel FileLoadContext.
+
+        It is guaranteed to be called after the pydantic model is, with the FileLoadContext
+        relative to this FileModel being loaded.
+        """
+        pass
+
+    @property
+    def _resolved_filepath(self) -> Optional[Path]:
+        if self.filepath is None:
+            return None
+
+        with file_load_context() as context:
+            return context.resolve(self.filepath)
+
+    @property
+    def save_location(self) -> Optional[Path]:
+        """Get the current save location which will be used when calling `save()`
+
+        This value can be None if the filepath is None and no name can be generated.
+
+        Returns:
+            Path: The location at which this model will be saved.
+        """
+        filepath = self.filepath or self._generate_name()
+
+        if filepath is None:
+            return None
+        elif filepath.is_absolute():
+            return filepath
+        else:
+            return self._absolute_anchor_path / filepath
+
+    def is_file_link(self) -> bool:
+        return True
+
+    def _get_updated_file_path(self, file_path: Path, loading_path: Path) -> Path:
+        """Update the file path with the resolved casing from the loading path.
+        Logs an information message if a file path is updated.
+
+        For example, given:
+            file_path = "To/A/File.txt"
+            loading_path = "D:/path/to/a/file.txt"
+
+        Then the result will be: "to/a/file.txt"
+
+        Args:
+            file_path (Path): The file path.
+            loading_path (Path): The resolved loading path.
+
+        Returns:
+            Path: The updated file path.
+        """
+
+        updated_file_parts = loading_path.parts[-len(file_path.parts) :]
+        updated_file_path = Path(*updated_file_parts)
+
+        if str(updated_file_path) != str(file_path):
+            logger.info(
+                f"Updating file reference from {file_path.name} to {updated_file_path}"
+            )
+
+        return updated_file_path
+
+    @classmethod
+    def validate(cls: Type["FileModel"], value: Any):
+        # Enable initialization with a Path.
+        if isinstance(value, (Path, str)):
+            # Pydantic Model init requires a dict
+            value = {"filepath": Path(value)}
+        return super().validate(value)
+
+    def save(
+        self,
+        filepath: Optional[Path] = None,
+        recurse: bool = False,
+        path_style: Optional[str] = None,
+    ) -> None:
+        """Save the model to disk.
+
+        If recurse is set to True, all of the child FileModels will be saved as well.
+        Relative child models are stored relative to this Model, according to the
+        model file hierarchy specified with the respective filepaths.
+        Absolute paths will be written to their respective locations. Note that this
+        will overwrite any existing files that are stored in this location.
+
+        Note that if recurse is set to True, the save_location properties of the
+        children are updated to their respective new locations.
+
+        If filepath it is specified, the filepath of this FileModel is set to the
+        specified path before the save operation is executed. If none is specified
+        it will use the current filepath.
+
+        If the used filepath is relative, it will be stored at the current
+        save_location. If you only want to save a child model of some root model, it is
+        recommended to first call synchronize_filepaths on the root model, to ensure
+        the child model's save_location is correctly determined.
+
+        Args:
+            filepath (Optional[Path], optional):
+                The file path at which this model is saved. If None is specified
+                it defaults to the filepath currently stored in the filemodel.
+                Defaults to None.
+            recurse (bool, optional):
+                Whether to save all children of this FileModel (when set to True),
+                or only save this model (when set to False). Defaults to False.
+            path_style (Optional[str], optional):
+                With which file path style to save the model. File references will
+                be written with the specified path style. Defaults to the path style
+                used by the current operating system. Options: 'unix', 'windows'.
+
+        Raises:
+            ValueError: When an unsupported path style is passed.
+        """
+        if filepath is not None:
+            self.filepath = filepath
+
+        path_style = path_style_validator.validate(path_style)
+        save_settings = ModelSaveSettings(path_style=path_style)
+
+        # Handle save
+        with file_load_context() as context:
+            context.push_new_parent(self._absolute_anchor_path, self._relative_mode)
+
+            if recurse:
+                self._save_tree(context, save_settings)
+            else:
+                self._save_instance(save_settings)
+
+    def _save_instance(self, save_settings: ModelSaveSettings) -> None:
+        if self.filepath is None:
+            self.filepath = self._generate_name()
+        self._save(save_settings)
+
+    def _save_tree(
+        self, context: FileLoadContext, save_settings: ModelSaveSettings
+    ) -> None:
+        # Ensure all names are generated prior to saving
+        def execute_generate_name(
+            model: BaseModel, acc: FileLoadContext
+        ) -> FileLoadContext:
+            if isinstance(model, FileModel) and model.filepath is None:
+                model.filepath = model._generate_name()
+            return acc
+
+        name_traverser = ModelTreeTraverser[FileLoadContext](
+            should_traverse=_should_traverse,
+            should_execute=_should_execute,
+            post_traverse_func=execute_generate_name,
+        )
+
+        name_traverser.traverse(self, context)
+
+        def save_pre(model: BaseModel, acc: FileLoadContext) -> FileLoadContext:
+            if isinstance(model, FileModel):
+                acc.push_new_parent(model.filepath.parent, model._relative_mode)  # type: ignore[arg-type]
+            return acc
+
+        def save_post(model: BaseModel, acc: FileLoadContext) -> FileLoadContext:
+            if isinstance(model, FileModel):
+                acc.pop_last_parent()
+                model._absolute_anchor_path = acc.get_current_parent()
+                model._save(save_settings)
+            return acc
+
+        save_traverser = ModelTreeTraverser[FileLoadContext](
+            should_traverse=_should_traverse,
+            should_execute=_should_execute,
+            pre_traverse_func=save_pre,
+            post_traverse_func=save_post,
+        )
+        save_traverser.traverse(self, context)
+
+    def synchronize_filepaths(self) -> None:
+        """Synchronize the save_location properties of all child models respective to
+        this FileModel's save_location.
+        """
+
+        def sync_pre(model: BaseModel, acc: FileLoadContext) -> FileLoadContext:
+            if isinstance(model, FileModel):
+                acc.push_new_parent(model.filepath.parent, model._relative_mode)  # type: ignore[arg-type]
+            return acc
+
+        def sync_post(model: BaseModel, acc: FileLoadContext) -> FileLoadContext:
+            if isinstance(model, FileModel):
+                acc.pop_last_parent()
+                model._absolute_anchor_path = acc.get_current_parent()
+            return acc
+
+        traverser = ModelTreeTraverser[FileLoadContext](
+            should_traverse=_should_traverse,
+            should_execute=_should_execute,
+            pre_traverse_func=sync_pre,
+            post_traverse_func=sync_post,
+        )
+
+        with file_load_context() as context:
+            context.push_new_parent(self._absolute_anchor_path, self._relative_mode)
+            traverser.traverse(self, context)
+
+    @property
+    def _relative_mode(self) -> ResolveRelativeMode:
+        """Get the ResolveRelativeMode of this FileModel.
+
+        Returns:
+            ResolveRelativeMode: The ResolveRelativeMode of this FileModel
+        """
+        return ResolveRelativeMode.ToParent
+
+    @classmethod
+    def _get_relative_mode_from_data(cls, data: Dict[str, Any]) -> ResolveRelativeMode:
+        """Gets the ResolveRelativeMode of this FileModel based on the provided data.
+
+        Note that by default, data is not used, and FileModels are always relative to
+        the parent. In exceptional cases, the relative mode can be dependent on the
+        data (i.e. the unvalidated/parsed dictionary fed into the pydantic basemodel).
+        As such the data is provided for such classes where the relative mode is
+        dependent on the state (e.g. the [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]).
+
+        Args:
+            data (Dict[str, Any]):
+                The unvalidated/parsed data which is fed to the pydantic base model,
+                used to determine the ResolveRelativeMode.
+
+        Returns:
+            ResolveRelativeMode: The ResolveRelativeMode of this FileModel
+        """
+        return ResolveRelativeMode.ToParent
+
+    @abstractclassmethod
+    def _generate_name(cls) -> Optional[Path]:
+        """Generate a (default) name for this FileModel.
+
+        Note that if _generate_name in theory can return a None value,
+        if this is possible in the specific implementation, _save should
+        be able to handle filepaths set to None.
+
+        Returns:
+            Optional[Path]:
+                a relative path with the default name of the model.
+        """
+        raise NotImplementedError()
+
+    @abstractmethod
+    def _save(self, save_settings: ModelSaveSettings) -> None:
+        """Save this instance to disk.
+
+        This method needs to be implemented by any class deriving from
+        FileModel, and is used in both the _save_instance and _save_tree
+        methods.
+
+        Args:
+            save_settings (ModelSaveSettings): The model save settings.
+        """
+        raise NotImplementedError()
+
+    @abstractmethod
+    def _load(self, filepath: Path) -> Dict:
+        """Load the data at filepath and returns it as a dictionary.
+
+        If a derived FileModel does not load data from disk, this should
+        return an empty dictionary.
+
+        Args:
+            filepath (Path): Path to the data to load.
+
+        Returns:
+            Dict: The data stored at filepath
+        """
+        raise NotImplementedError()
+
+    def __str__(self) -> str:
+        return str(self.filepath if self.filepath else "")
+
+    @staticmethod
+    def _change_to_path(filepath):
+        if filepath is None:
+            return filepath
+        elif isinstance(filepath, Path):
+            return filepath
+        else:
+            return Path(filepath)
+
+    @validator("filepath")
+    def _conform_filepath_to_pathlib(cls, value):
+        return FileModel._change_to_path(value)
+
+
+class SerializerConfig(BaseModel, ABC):
+    """Class that holds the configuration settings for serialization."""
+
+    float_format: str = ""
+    """str: The string format that will be used for float serialization. If empty, the original number will be serialized. Defaults to an empty string.
+        
+        Examples:
+            Input value = 123.456
+
+            Format    | Output          | Description
+            -------------------------------------------------------------------------------------------------------------------------------------
+            ".0f"     | 123             | Format float with 0 decimal places.
+            "f"       | 123.456000      | Format float with default (=6) decimal places.
+            ".2f"     | 123.46          | Format float with 2 decimal places.
+            "+.1f"    | +123.5          | Format float with 1 decimal place with a + or  sign.
+            "e"       | 1.234560e+02    | Format scientific notation with the letter 'e' with default (=6) decimal places.
+            "E"       | 1.234560E+02    | Format scientific notation with the letter 'E' with default (=6) decimal places.
+            ".3e"     | 1.235e+02       | Format scientific notation with the letter 'e' with 3 decimal places.
+            "<15"     | 123.456         | Left aligned in space with width 15
+            "^15.0f"  |       123       | Center aligned in space with width 15 with 0 decimal places.
+            ">15.1e"  |         1.2e+02 | Right aligned in space with width 15 with scientific notation with 1 decimal place.
+            "*>15.1f" | **********123.5 | Right aligned in space with width 15 with 1 decimal place and fill empty space with *
+            "%"       | 12345.600000%   | Format percentage with default (=6) decimal places.     
+            ".3%"     | 12345.600%      | Format percentage with 3 decimal places.
+
+            More information: https://docs.python.org/3/library/string.html#format-specification-mini-language
+        """
+
+
+class ParsableFileModel(FileModel):
+    """ParsableFileModel defines a FileModel which can be parsed
+    and serialized with a serializer .
+
+    Each ParsableFileModel has a default _filename and _ext,
+    which are used to generate the file name of any instance where
+    the filepath is not (yet) set.
+
+    Children of the ParsableFileModel are expected to implement a
+    serializer function which takes a Path and Dict and writes the
+    ParsableFileModel to disk, and a parser function which takes
+    a Path and outputs a Dict.
+
+    If more complicated solutions are required, a ParsableFileModel
+    child can also opt to overwrite the _serialize and _parse methods,
+    to skip the _get_serializer and _get_parser methods respectively.
+    """
+
+    serializer_config: SerializerConfig = SerializerConfig()
+
+    def _load(self, filepath: Path) -> Dict:
+        # TODO Make this lazy in some cases so it doesn't become slow
+        if filepath.is_file():
+            return self._parse(filepath)
+        else:
+            raise ValueError(f"File: `{filepath}` not found, skipped parsing.")
+
+    def _save(self, save_settings: ModelSaveSettings) -> None:
+        """Save the data of this FileModel.
+
+        _save provides a hook for child models to overwrite the save behaviour as
+        called during the tree traversal.
+
+        Args:
+            save_settings (ModelSaveSettings): The model save settings.
+        """
+        self._serialize(self.dict(), save_settings)
+
+    def _serialize(self, data: dict, save_settings: ModelSaveSettings) -> None:
+        """Serializes the data to file. Should not be called directly, only through `_save`.
+
+        Args:
+            save_settings (ModelSaveSettings): The model save settings.
+        """
+        path = self._resolved_filepath
+        if path is None:
+            # TODO: Do we need to add a warning / exception here
+            return
+
+        path.parent.mkdir(parents=True, exist_ok=True)
+        self._get_serializer()(path, data, self.serializer_config, save_settings)
+
+    def dict(self, *args, **kwargs):
+        kwargs["exclude"] = self._exclude_fields()
+        return super().dict(*args, **kwargs)
+
+    @classmethod
+    def _exclude_fields(cls) -> Set[str]:
+        """A set containing the field names that should not be serialized."""
+        return {"filepath", "serializer_config"}
+
+    @classmethod
+    def _parse(cls, path: Path) -> Dict:
+        return cls._get_parser()(path)
+
+    @classmethod
+    def _generate_name(cls) -> Path:
+        name, ext = cls._filename(), cls._ext()
+        return Path(f"{name}{ext}")
+
+    @abstractclassmethod
+    def _filename(cls) -> str:
+        return "test"
+
+    @abstractclassmethod
+    def _ext(cls) -> str:
+        return ".test"
+
+    @abstractclassmethod
+    def _get_serializer(
+        cls,
+    ) -> Callable[[Path, Dict, SerializerConfig, ModelSaveSettings], None]:
+        return DummySerializer.serialize
+
+    @abstractclassmethod
+    def _get_parser(cls) -> Callable[[Path], Dict]:
+        return DummmyParser.parse
+
+    def _get_identifier(self, data: dict) -> Optional[str]:
+        filepath = data.get("filepath")
+        if filepath:
+            return filepath.name
+        return None
+
+
+class DiskOnlyFileModel(FileModel):
+    """DiskOnlyFileModel provides a stub implementation for file based
+    models which are not explicitly implemented within hydrolib.core.
+
+    It implements the FileModel with a void parser and serializer, and a
+    save method which copies the file associated with the FileModel
+    to a new location if it exists.
+
+    We further explicitly assume that when the filepath is None, no
+    file will be written.
+
+    Actual file model implementations *should not* inherit from the
+    DiskOnlyFileModel and instead inherit directly from FileModel.
+    """
+
+    _source_file_path: Optional[Path] = PrivateAttr(default=None)
+
+    def _post_init_load(self) -> None:
+        # After initialisation we retrieve the _resolved_filepath
+        # this should correspond with the actual absolute path of the
+        # underlying file. Only after saving this path will be updated.
+        super()._post_init_load()
+        self._source_file_path = self._resolved_filepath
+
+    def _load(self, filepath: Path) -> Dict:
+        # We de not load any additional data, as such we return an empty dict.
+        return dict()
+
+    def _save(self, save_settings: ModelSaveSettings) -> None:
+        # The target_file_path contains the new path to write to, while the
+        # _source_file_path contains the original data. If these are not the
+        # same we copy the file and update the underlying source path.
+        target_file_path = self._resolved_filepath
+        if self._can_copy_to(target_file_path):
+            target_file_path.parent.mkdir(parents=True, exist_ok=True)  # type: ignore[arg-type]
+            shutil.copy(self._source_file_path, target_file_path)  # type: ignore[arg-type]
+        self._source_file_path = target_file_path
+
+    def _can_copy_to(self, target_file_path: Optional[Path]) -> bool:
+        return (
+            self._source_file_path is not None
+            and target_file_path is not None
+            and self._source_file_path != target_file_path
+            and self._source_file_path.exists()
+            and self._source_file_path.is_file()
+        )
+
+    @classmethod
+    def _generate_name(cls) -> Optional[Path]:
+        # There is no common name for DiskOnlyFileModel, instead we
+        # do not generate names and skip None filepaths.
+        return None
+
+    def is_intermediate_link(self) -> bool:
+        # If the filepath is not None, there is an underlying file, and as such we need
+        # to traverse it.
+        return self.filepath is not None
+
+
+def validator_set_default_disk_only_file_model_when_none() -> classmethod:
+    """Validator to ensure a default empty DiskOnlyFileModel is created
+    when the corresponding field is initialized with None.
+
+    Returns:
+        classmethod: Validator to adjust None values to empty DiskOnlyFileModel objects
+    """
+
+    def adjust_none(v: Any, field: ModelField) -> Any:
+        if field.type_ is DiskOnlyFileModel and v is None:
+            return {"filepath": None}
+        return v
+
+    return validator("*", allow_reuse=True, pre=True)(adjust_none)
+
+
+class PathStyleValidator:
+    """Class to take care of path style validation."""
+
+    _os_path_style = get_path_style_for_current_operating_system()
+
+    def validate(self, path_style: Optional[str]) -> PathStyle:
+        """Validates the path style as string on whether it is a supported path style.
+        If it is a valid path style the path style enum value will be return as a result.
+
+        Args:
+            path_style (Optional[str]): The path style as string value.
+
+        Returns:
+            PathStyle: The converted PathStyle object.
+
+        Raises:
+            ValueError: When an unsupported path style is passed.
+        """
+        if path_style is None:
+            return self._os_path_style
+
+        supported_path_styles = list(PathStyle)
+        if path_style in supported_path_styles:
+            return PathStyle(path_style)
+
+        supported_path_style_str = ", ".join(([x.value for x in supported_path_styles]))
+        raise ValueError(
+            f"Path style '{path_style}' not supported. Supported path styles: {supported_path_style_str}"
+        )
+
+
+path_style_validator = PathStyleValidator()
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/bc/models.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/bc/models.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,852 +1,852 @@
-"""Representation of a .bc file in various classes.
-
-    Most relevant classes are:
-
-    *   ForcingModel: toplevel class containing the whole .bc file contents.
-    *   ForcingBase subclasses: containing the actual data columns, for example:
-        TimeSeries, HarmonicComponent, AstronomicComponent, HarmonicCorrection,
-        AstronomicCorrection, Constant, T3D.
-
-"""
-import logging
-import re
-from enum import Enum
-from pathlib import Path
-from typing import Callable, Dict, Iterator, List, Literal, Optional, Set, Union
-
-from pydantic import Extra
-from pydantic.class_validators import root_validator, validator
-from pydantic.fields import Field
-
-from hydrolib.core.basemodel import BaseModel, ModelSaveSettings
-from hydrolib.core.dflowfm.ini.io_models import Property, Section
-from hydrolib.core.dflowfm.ini.models import (
-    DataBlockINIBasedModel,
-    INIGeneral,
-    INIModel,
-)
-from hydrolib.core.dflowfm.ini.parser import Parser, ParserConfig
-from hydrolib.core.dflowfm.ini.serializer import DataBlockINIBasedSerializerConfig
-from hydrolib.core.dflowfm.ini.util import (
-    get_enum_validator,
-    get_from_subclass_defaults,
-    get_split_string_on_delimiter_validator,
-    get_type_based_on_subclass_default_value,
-    make_list_validator,
-    rename_keys_for_backwards_compatibility,
-)
-from hydrolib.core.utils import to_list
-
-logger = logging.getLogger(__name__)
-
-
-class VerticalInterpolation(str, Enum):
-    """Enum class containing the valid values for the vertical position type,
-    which defines what the numeric values for vertical position specification mean.
-    """
-
-    linear = "linear"
-    """str: Linear interpolation between vertical positions."""
-
-    log = "log"
-    """str: Logarithmic interpolation between vertical positions (e.g. vertical velocity profiles)."""
-
-    block = "block"
-    """str: Equal to the value at the directly lower specified vertical position."""
-
-
-class VerticalPositionType(str, Enum):
-    """Enum class containing the valid values for the vertical position type."""
-
-    percentage_bed = "percBed"
-    """str: Percentage with respect to the water depth from the bed upward."""
-
-    z_bed = "ZBed"
-    """str: Absolute distance from the bed upward."""
-
-    z_datum = "ZDatum"
-    """str: z-coordinate with respect to the reference level of the model."""
-
-    z_surf = "ZSurf"
-    """str: Absolute distance from the free surface downward."""
-
-
-class TimeInterpolation(str, Enum):
-    """Enum class containing the valid values for the time interpolation."""
-
-    linear = "linear"
-    """str: Linear interpolation between times."""
-
-    block_from = "blockFrom"
-    """str: Equal to that at the start of the time interval (latest specified time value)."""
-
-    block_to = "blockTo"
-    """str: Equal to that at the end of the time interval (upcoming specified time value)."""
-
-
-class QuantityUnitPair(BaseModel):
-    """A .bc file header lines tuple containing a quantity name, its unit and optionally a vertical position index."""
-
-    quantity: str
-    """str: Name of quantity."""
-
-    unit: str
-    """str: Unit of quantity."""
-
-    vertpositionindex: Optional[int] = Field(alias="vertPositionIndex")
-    """int (optional): This is a (one-based) index into the verticalposition-specification, assigning a vertical position to the quantity (t3D-blocks only)."""
-
-    def _to_properties(self):
-        """Generator function that yields the ini Property objects for a single
-        QuantityUnitPair object."""
-        yield Property(key="quantity", value=self.quantity)
-        yield Property(key="unit", value=self.unit)
-        if self.vertpositionindex is not None:
-            yield Property(key="vertPositionIndex", value=self.vertpositionindex)
-
-
-class VectorQuantityUnitPairs(BaseModel):
-    """A subset of .bc file header lines containing a vector quantity definition,
-    followed by all component quantity names, their unit and optionally their
-    vertical position indexes."""
-
-    class Config:
-        validate_assignment = True
-
-    vectorname: str
-    """str: Name of the vector quantity."""
-
-    elementname: List[str]
-    """List[str]: List of names of the vector component quantities."""
-
-    quantityunitpair: List[QuantityUnitPair]
-    """List[QuantityUnitPair]: List of QuantityUnitPair that define the vector components."""
-
-    @root_validator
-    @classmethod
-    def _validate_quantity_element_names(cls, values: Dict):
-        for idx, name in enumerate(
-            [qup.quantity for qup in values["quantityunitpair"]]
-        ):
-            if name not in values["elementname"]:
-                raise ValueError(
-                    f"quantityunitpair[{idx}], quantity '{name}' must be in vectordefinition's element names: '{VectorQuantityUnitPairs._to_vectordefinition_string(values['vectorname'], values['elementname'])}'."
-                )
-
-        return values
-
-    @staticmethod
-    def _to_vectordefinition_string(vectorname: str, elementname: List[str]):
-        return vectorname + ":" + ",".join(elementname)
-
-    def __str__(self) -> str:
-        return VectorQuantityUnitPairs._to_vectordefinition_string(
-            self.vectorname, self.elementname
-        )
-
-    def _to_properties(self):
-        """Generator function that yields the ini Property objects for a single
-        VectorQuantityUnitPairs object."""
-        yield Property(key="vector", value=str(self))
-
-        for qup in self.quantityunitpair:
-            for prop in qup._to_properties():
-                yield prop
-
-
-ScalarOrVectorQUP = Union[QuantityUnitPair, VectorQuantityUnitPairs]
-
-
-class ForcingBase(DataBlockINIBasedModel):
-    """
-    The base class of a single [Forcing] block in a .bc forcings file.
-
-    Typically subclassed, for the specific types of forcing data, e.g, Harmonic.
-    This model is for example referenced under a
-    [ForcingModel][hydrolib.core.dflowfm.bc.models.ForcingModel]`.forcing[..]`.
-    """
-
-    _header: Literal["Forcing"] = "Forcing"
-    name: str = Field(alias="name")
-    """str: Unique identifier that identifies the location for this forcing data."""
-
-    function: str = Field(alias="function")
-    """str: Function type of the data in the actual datablock."""
-
-    quantityunitpair: List[ScalarOrVectorQUP]
-    """List[ScalarOrVectorQUP]: List of header lines for one or more quantities and their unit. Describes the columns in the actual datablock."""
-
-    def _exclude_fields(self) -> Set:
-        return {"quantityunitpair"}.union(super()._exclude_fields())
-
-    @classmethod
-    def _supports_comments(cls):
-        return True
-
-    @classmethod
-    def _duplicate_keys_as_list(cls):
-        return True
-
-    @root_validator(pre=True)
-    def _validate_quantityunitpair(cls, values):
-        quantityunitpairkey = "quantityunitpair"
-
-        if values.get(quantityunitpairkey) is not None:
-            return values
-
-        quantities = values.get("quantity")
-        if quantities is None:
-            raise ValueError("quantity is not provided")
-        units = values.get("unit")
-        if units is None:
-            raise ValueError("unit is not provided")
-
-        if isinstance(quantities, str) and isinstance(units, str):
-            values[quantityunitpairkey] = [
-                QuantityUnitPair(quantity=quantities, unit=units)
-            ]
-            return values
-
-        if isinstance(quantities, list) and isinstance(units, list):
-            if len(quantities) != len(units):
-                raise ValueError(
-                    "Number of quantities should be equal to number of units"
-                )
-
-            values[quantityunitpairkey] = [
-                QuantityUnitPair(quantity=quantity, unit=unit)
-                for quantity, unit in zip(quantities, units)
-            ]
-            return values
-
-        raise ValueError("Number of quantities should be equal to number of units")
-
-    @validator("function", pre=True)
-    def _set_function(cls, value):
-        return get_from_subclass_defaults(ForcingBase, "function", value)
-
-    @classmethod
-    def validate(cls, v):
-        """Try to initialize subclass based on the `function` field.
-        This field is compared to each `function` field of the derived models of `ForcingBase`
-        or models derived from derived models.
-        The derived model with an equal function type will be initialized.
-
-        Raises:
-            ValueError: When the given type is not a known structure type.
-        """
-
-        # should be replaced by discriminated unions once merged
-        # https://github.com/samuelcolvin/pydantic/pull/2336
-        if isinstance(v, dict):
-            function_string = v.get("function", "").lower()
-            function_type = get_type_based_on_subclass_default_value(
-                cls, "function", function_string
-            )
-
-            if function_type is not None:
-                return function_type(**v)
-
-            else:
-                raise ValueError(
-                    f"Function of {cls.__name__} with name={v.get('name', '')} and function={v.get('function', '')} is not recognized."
-                )
-        return v
-
-    def _get_identifier(self, data: dict) -> Optional[str]:
-        return data.get("name")
-
-    def _to_section(
-        self,
-        config: DataBlockINIBasedSerializerConfig,
-        save_settings: ModelSaveSettings,
-    ) -> Section:
-        section = super()._to_section(config, save_settings)
-
-        for quantity in self.quantityunitpair:
-            for prop in quantity._to_properties():
-                section.content.append(prop)
-
-        return section
-
-    class Config:
-        extra = Extra.ignore
-
-    def __repr__(self) -> str:
-        data = dict(self)
-        data["datablock"] = "<omitted>"
-        representable = BaseModel.construct(**data)
-        return str(representable)
-
-
-class VectorForcingBase(ForcingBase):
-    """
-    The base class of a single [Forcing] block that supports vectors in a .bc forcings file.
-    """
-
-    @root_validator(pre=True)
-    def validate_and_update_quantityunitpairs(cls, values: Dict) -> Dict:
-        """
-        Validates and, if required, updates vector quantity unit pairs.
-
-        Args:
-            values (Dict): Dictionary of values to be used to validate or
-            update vector quantity unit pairs.
-
-        Raises:
-            ValueError: When a quantity unit pair is found in a vector where it does not belong.
-            ValueError: When the number of quantity unit pairs in a vectors is not as expected.
-
-        Returns:
-            Dict: Dictionary of validates values.
-        """
-        quantityunitpairs = values["quantityunitpair"]
-        vector = values.get("vector")
-        number_of_element_repetitions = cls.get_number_of_repetitions(values)
-
-        VectorForcingBase._process_vectordefinition_or_check_quantityunitpairs(
-            vector, quantityunitpairs, number_of_element_repetitions
-        )
-
-        return values
-
-    @staticmethod
-    def _process_vectordefinition_or_check_quantityunitpairs(
-        vectordefs: Optional[List[str]],
-        quantityunitpairs: List[ScalarOrVectorQUP],
-        number_of_element_repetitions: int,
-    ) -> None:
-        """
-        Processes the given vector definition header lines from a .bc file
-        or, if absent, checks whether the existing VectorQuantityUnitPairs
-        objects already have the correct vector length.
-
-        Args:
-            vectordefs (List[str]): List of vector definition values, e.g.,
-                ["vectorname:comp1,comp2,..compN", ...]
-            quantityunitpairs (List[ScalarOrVectorQUP]): list of already parsed
-                and constructed QuantityUnitPair objects, which may be modified
-                in place with some packed VectorQuantityUnitPairs objects.
-            number_of_element_repetitions (int, optional): Number of times each
-                vector element is expected to be present in the subsequent
-                Quantity lines. Typically used for 3D quantities, using the
-                number of vertical layers.
-        """
-
-        if vectordefs is not None and not any(
-            map(lambda qup: isinstance(qup, VectorQuantityUnitPairs), quantityunitpairs)
-        ):
-            # Vector definition line still must be processed and VectorQUPs still created.
-            VectorForcingBase._validate_vectordefinition_and_update_quantityunitpairs(
-                vectordefs, quantityunitpairs, number_of_element_repetitions
-            )
-        else:
-            # VectorQUPs already present; directly validate their vector length.
-            for qup in quantityunitpairs:
-                if isinstance(qup, VectorQuantityUnitPairs):
-                    VectorForcingBase._validate_vectorlength(
-                        qup, number_of_element_repetitions
-                    )
-
-    @staticmethod
-    def _validate_vectordefinition_and_update_quantityunitpairs(
-        vectordefs: Optional[List[str]],
-        quantityunitpairs: List[ScalarOrVectorQUP],
-        number_of_element_repetitions: int,
-    ) -> None:
-        """
-        Validates the given vector definition header lines from a .bc file
-        for a ForcingBase subclass and updates the existing QuantityUnitPair list
-        by packing the vector elements into a VectorQuantityUnitPairs object
-        for each vector definition.
-
-        Args:
-            vectordefs (List[str]): List of vector definition values, e.g.,
-                ["vectorname:comp1,comp2,..compN", ...]
-            quantityunitpairs (List[ScalarOrVectorQUP]): list of already parsed
-                and constructed QuantityUnitPair objects, which will be modified
-                in place with some packed VectorQuantityUnitPairs objects.
-            number_of_element_repetitions (int, optional): Number of times each
-                vector element is expected to be present in the subsequent
-                Quantity lines. Typically used for 3D quantities, using the
-                number of vertical layers.
-        """
-
-        if vectordefs is None:
-            return
-
-        vectordefs = to_list(vectordefs)
-
-        qup_iter = iter(quantityunitpairs)
-
-        # Start a new list, to only keep the scalar QUPs, and add newly
-        # created VectorQUPs.
-        quantityunitpairs_with_vectors = []
-
-        # If one quantity is "time", it must be the first one.
-        if quantityunitpairs[0].quantity == "time":
-            quantityunitpairs_with_vectors.append(quantityunitpairs[0])
-            _ = next(qup_iter)
-
-        # For each vector definition line, greedily find the quantity unit pairs
-        # that form the vector elements, and pack them into a single VectorQuantityUnitPairs oject.
-        for vectordef in vectordefs:
-            VectorForcingBase._find_and_pack_vector_qups(
-                number_of_element_repetitions,
-                qup_iter,
-                quantityunitpairs_with_vectors,
-                vectordef,
-            )
-
-        for remaining_qu_pair in qup_iter:
-            quantityunitpairs_with_vectors.append(remaining_qu_pair)
-
-        quantityunitpairs[:] = quantityunitpairs_with_vectors
-
-    @staticmethod
-    def _find_and_pack_vector_qups(
-        number_of_element_repetitions: int,
-        qup_iter: Iterator[ScalarOrVectorQUP],
-        quantityunitpairs_with_vectors: List[ScalarOrVectorQUP],
-        vectordef: str,
-    ):
-        vectorname, componentdefs = vectordef.split(":")
-        componentnames = re.split(r"[, \t]", componentdefs)
-        n_components = len(componentnames)
-
-        vqu_pair = VectorQuantityUnitPairs(
-            vectorname=vectorname, elementname=componentnames, quantityunitpair=[]
-        )
-
-        n_rep = 0
-        for qu_pair in qup_iter:
-            if qu_pair.quantity in componentnames:
-                # This vector element found, store it.
-                vqu_pair.quantityunitpair.append(qu_pair)
-                n_rep += 1
-                if n_rep == n_components * number_of_element_repetitions:
-                    break
-            else:
-                # This quantity was no vector element being searched for
-                # so keep it as a regular (scalar) QuantityUnitPair.
-                quantityunitpairs_with_vectors.append(qu_pair)
-
-        if VectorForcingBase._validate_vectorlength(
-            vqu_pair, number_of_element_repetitions
-        ):
-            # This VectorQuantityUnitPairs is now complete; add it to result list.
-            quantityunitpairs_with_vectors.append(vqu_pair)
-
-    @staticmethod
-    def _validate_vectorlength(
-        vqu_pair: VectorQuantityUnitPairs,
-        number_of_element_repetitions,
-    ) -> bool:
-        """
-        Checks whether the number of QuantityUnitPairs in a vector quantity
-        matches exactly with number of vector elements in the definition and,
-        optionally, the number of vertical layers.
-
-        Args:
-            vqu_pair (VectorQuantityUnitPairs): the vector quantity object to be checked.
-            number_of_element_repetitions (int, optional): Number of times each
-                vector element is expected to be present in the subsequent
-                Quantity lines. Typically used for 3D quantities, using the
-                number of vertical layers.
-
-        Returns:
-            bool: True if vqu_pair is valid. False return value is hidden because
-                an exception will be raised.
-
-        Raises:
-            ValueError: If number of QuantityUnitPair objects in vqu_pair is not equal
-                to number of element names * number_of_element_repetitions.
-        """
-
-        if not (
-            valid := len(vqu_pair.quantityunitpair)
-            == len(vqu_pair.elementname) * number_of_element_repetitions
-        ):
-            raise ValueError(
-                f"Incorrect number of quantity unit pairs were found; should match the elements in vectordefinition for {vqu_pair.vectorname}"
-                + (
-                    f", and {number_of_element_repetitions} vertical layers"
-                    if number_of_element_repetitions > 1
-                    else ""
-                )
-                + "."
-            )
-
-        return valid
-
-    @validator("function", pre=True)
-    def _set_function(cls, value):
-        return get_from_subclass_defaults(VectorForcingBase, "function", value)
-
-    @classmethod
-    def get_number_of_repetitions(cls, values: Dict) -> int:
-        """Gets the number of expected quantityunitpairs for each vector element. Defaults to 1."""
-        return 1
-
-
-class TimeSeries(VectorForcingBase):
-    """Subclass for a .bc file [Forcing] block with timeseries data."""
-
-    function: Literal["timeseries"] = "timeseries"
-
-    timeinterpolation: TimeInterpolation = Field(alias="timeInterpolation")
-    """TimeInterpolation: The type of time interpolation."""
-
-    offset: float = Field(0.0, alias="offset")
-    """float: All values in the table are increased by the offset (after multiplication by factor). Defaults to 0.0."""
-
-    factor: float = Field(1.0, alias="factor")
-    """float: All values in the table are multiplied with the factor. Defaults to 1.0."""
-
-    _timeinterpolation_validator = get_enum_validator(
-        "timeinterpolation", enum=TimeInterpolation
-    )
-
-    @root_validator(allow_reuse=True, pre=True)
-    def rename_keys(cls, values: Dict) -> Dict:
-        """Renames some old keywords to the currently supported keywords."""
-        return rename_keys_for_backwards_compatibility(
-            values,
-            {
-                "timeinterpolation": ["time_interpolation"],
-            },
-        )
-
-
-class Harmonic(ForcingBase):
-    """Subclass for a .bc file [Forcing] block with harmonic components data."""
-
-    function: Literal["harmonic"] = "harmonic"
-
-    factor: float = Field(1.0, alias="factor")
-    """float: All values in the table are multiplied with the factor. Defaults to 1.0."""
-
-
-class Astronomic(ForcingBase):
-    """Subclass for a .bc file [Forcing] block with astronomic components data."""
-
-    function: Literal["astronomic"] = "astronomic"
-
-    factor: float = Field(1.0, alias="factor")
-    """float: All values in the table are multiplied with the factor. Defaults to 1.0."""
-
-
-class HarmonicCorrection(ForcingBase):
-    """Subclass for a .bc file [Forcing] block with harmonic components correction data."""
-
-    function: Literal["harmonic-correction"] = "harmonic-correction"
-
-
-class AstronomicCorrection(ForcingBase):
-    """Subclass for a .bc file [Forcing] block with astronomic components correction data."""
-
-    function: Literal["astronomic-correction"] = "astronomic-correction"
-
-
-class T3D(VectorForcingBase):
-    """Subclass for a .bc file [Forcing] block with 3D timeseries data."""
-
-    function: Literal["t3d"] = "t3d"
-
-    offset: float = Field(0.0, alias="offset")
-    """float: All values in the table are increased by the offset (after multiplication by factor). Defaults to 0.0."""
-
-    factor: float = Field(1.0, alias="factor")
-    """float: All values in the table are multiplied with the factor. Defaults to 1.0."""
-
-    vertpositions: List[float] = Field(alias="vertPositions")
-    """List[float]: The specification of the vertical positions."""
-
-    vertinterpolation: VerticalInterpolation = Field(
-        VerticalInterpolation.linear, alias="vertInterpolation"
-    )
-    """VerticalInterpolation: The type of vertical interpolation. Defaults to linear."""
-
-    vertpositiontype: VerticalPositionType = Field(alias="vertPositionType")
-    """VerticalPositionType: The vertical position type of the verticalpositions values."""
-
-    timeinterpolation: TimeInterpolation = Field(
-        TimeInterpolation.linear, alias="timeInterpolation"
-    )
-    """TimeInterpolation: The type of time interpolation. Defaults to linear."""
-
-    _keys_to_rename = {
-        "timeinterpolation": ["time_interpolation"],
-        "vertpositions": ["vertical_position_specification"],
-        "vertinterpolation": ["vertical_interpolation"],
-        "vertpositiontype": ["vertical_position_type"],
-        "vertpositionindex": ["vertical_position"],
-    }
-
-    @root_validator(allow_reuse=True, pre=True)
-    def rename_keys(cls, values: Dict) -> Dict:
-        """Renames some old keywords to the currently supported keywords."""
-        return rename_keys_for_backwards_compatibility(values, cls._keys_to_rename)
-
-    _split_to_list = get_split_string_on_delimiter_validator(
-        "vertpositions",
-    )
-
-    _verticalinterpolation_validator = get_enum_validator(
-        "vertinterpolation", enum=VerticalInterpolation
-    )
-    _verticalpositiontype_validator = get_enum_validator(
-        "vertpositiontype",
-        enum=VerticalPositionType,
-        alternative_enum_values={
-            VerticalPositionType.percentage_bed: ["percentage from bed"],
-        },
-    )
-    _timeinterpolation_validator = get_enum_validator(
-        "timeinterpolation", enum=TimeInterpolation
-    )
-
-    @classmethod
-    def get_number_of_repetitions(cls, values: Dict) -> int:
-        verticalpositions = values.get("vertpositions")
-        # Since the renaming root validator may not have been run yet, in this
-        # method we explicitly check old keywords for backwards compatibility:
-        if verticalpositions is None:
-            # try to get the value from any of the older keywords
-            for old_keyword in cls._keys_to_rename["vertpositions"]:
-                verticalpositions = values.get(old_keyword)
-                if verticalpositions is not None:
-                    break
-
-        if verticalpositions is None:
-            raise ValueError("vertPositions is not provided")
-
-        number_of_verticalpositions = (
-            len(verticalpositions)
-            if isinstance(verticalpositions, List)
-            else len(verticalpositions.split())
-        )
-
-        return number_of_verticalpositions
-
-    @root_validator(pre=True)
-    def _validate_quantityunitpairs(cls, values: Dict) -> Dict:
-        quantityunitpairs = values["quantityunitpair"]
-
-        T3D._validate_that_first_unit_is_time_and_has_no_verticalposition(
-            quantityunitpairs
-        )
-
-        number_of_verticalpositions = cls.get_number_of_repetitions(values)
-
-        verticalpositionindexes = values.get("vertpositionindex")
-        if verticalpositionindexes is None:
-            T3D._validate_that_all_quantityunitpairs_have_valid_verticalpositionindex(
-                quantityunitpairs[1:], number_of_verticalpositions
-            )
-        else:
-            T3D._validate_verticalpositionindexes_and_update_quantityunitpairs(
-                verticalpositionindexes,
-                number_of_verticalpositions,
-                quantityunitpairs,
-            )
-
-        return values
-
-    @staticmethod
-    def _validate_that_first_unit_is_time_and_has_no_verticalposition(
-        quantityunitpairs: List[QuantityUnitPair],
-    ) -> None:
-        if quantityunitpairs[0].quantity.lower() != "time":
-            raise ValueError("First quantity should be `time`")
-        if quantityunitpairs[0].vertpositionindex is not None:
-            raise ValueError("`time` quantity cannot have vertical position index")
-
-    @staticmethod
-    def _validate_that_all_quantityunitpairs_have_valid_verticalpositionindex(
-        quantityunitpairs: List[ScalarOrVectorQUP], maximum_verticalpositionindex: int
-    ) -> None:
-        for quantityunitpair in quantityunitpairs:
-            if isinstance(quantityunitpair, VectorQuantityUnitPairs):
-                return T3D._validate_that_all_quantityunitpairs_have_valid_verticalpositionindex(
-                    quantityunitpair.quantityunitpair, maximum_verticalpositionindex
-                )
-
-            verticalpositionindex = quantityunitpair.vertpositionindex
-
-            if not T3D._is_valid_verticalpositionindex(
-                verticalpositionindex, maximum_verticalpositionindex
-            ):
-                raise ValueError(
-                    f"Vertical position index should be between 1 and {maximum_verticalpositionindex}, but {verticalpositionindex} was given"
-                )
-
-    @staticmethod
-    def _validate_verticalpositionindexes_and_update_quantityunitpairs(
-        verticalpositionindexes: List[int],
-        number_of_verticalpositions: int,
-        quantityunitpairs: List[ScalarOrVectorQUP],
-    ) -> None:
-        if verticalpositionindexes is None:
-            raise ValueError("vertPositionIndex is not provided")
-
-        T3D._validate_that_verticalpositionindexes_are_valid(
-            verticalpositionindexes, number_of_verticalpositions
-        )
-
-        T3D._add_verticalpositionindex_to_quantityunitpairs(
-            quantityunitpairs[1:], verticalpositionindexes
-        )
-
-    @staticmethod
-    def _validate_that_verticalpositionindexes_are_valid(
-        verticalpositionindexes: List[int], number_of_vertical_positions: int
-    ) -> None:
-        for verticalpositionindexstring in verticalpositionindexes:
-            verticalpositionindex = (
-                int(verticalpositionindexstring)
-                if verticalpositionindexstring
-                else None
-            )
-            if not T3D._is_valid_verticalpositionindex(
-                verticalpositionindex, number_of_vertical_positions
-            ):
-                raise ValueError(
-                    f"Vertical position index should be between 1 and {number_of_vertical_positions}"
-                )
-
-    @staticmethod
-    def _is_valid_verticalpositionindex(
-        verticalpositionindex: int, number_of_vertical_positions: int
-    ) -> bool:
-        one_based_index_offset = 1
-
-        return (
-            verticalpositionindex is not None
-            and verticalpositionindex >= one_based_index_offset
-            and verticalpositionindex <= number_of_vertical_positions
-        )
-
-    @staticmethod
-    def _add_verticalpositionindex_to_quantityunitpairs(
-        quantityunitpairs: List[ScalarOrVectorQUP], verticalpositionindexes: List[int]
-    ) -> None:
-        i = 0
-
-        for quanityunitpair in quantityunitpairs:
-            if i >= len(verticalpositionindexes):
-                raise ValueError(
-                    "Number of vertical position indexes should be equal to the number of quantities/units - 1"
-                )
-
-            if isinstance(quanityunitpair, VectorQuantityUnitPairs):
-                for qup in quanityunitpair.quantityunitpair:
-                    qup.vertpositionindex = verticalpositionindexes[i]
-                    i = i + 1
-            else:
-                quanityunitpair.vertpositionindex = verticalpositionindexes[i]
-                i = i + 1
-
-        if i != len(verticalpositionindexes):
-            raise ValueError(
-                "Number of vertical position indexes should be equal to the number of quantities/units - 1"
-            )
-
-
-class QHTable(ForcingBase):
-    """Subclass for a .bc file [Forcing] block with Q-h table data."""
-
-    function: Literal["qhtable"] = "qhtable"
-
-
-class Constant(ForcingBase):
-    """Subclass for a .bc file [Forcing] block with constant value data."""
-
-    function: Literal["constant"] = "constant"
-
-    offset: float = Field(0.0, alias="offset")
-    """float: All values in the table are increased by the offset (after multiplication by factor). Defaults to 0.0."""
-
-    factor: float = Field(1.0, alias="factor")
-    """float: All values in the table are multiplied with the factor. Defaults to 1.0."""
-
-
-class ForcingGeneral(INIGeneral):
-    """`[General]` section with .bc file metadata."""
-
-    fileversion: str = Field("1.01", alias="fileVersion")
-    """str: The file version."""
-
-    filetype: Literal["boundConds"] = Field("boundConds", alias="fileType")
-
-
-class ForcingModel(INIModel):
-    """
-    The overall model that contains the contents of one .bc forcings file.
-
-    This model is for example referenced under a
-    [ExtModel][hydrolib.core.dflowfm.ext.models.ExtModel]`.boundary[..].forcingfile[..]`.
-    """
-
-    general: ForcingGeneral = ForcingGeneral()
-    """ForcingGeneral: `[General]` block with file metadata."""
-
-    forcing: List[ForcingBase] = []
-    """List[ForcingBase]: List of `[Forcing]` blocks for all forcing
-    definitions in a single .bc file. Actual data is stored in
-    forcing[..].datablock from [hydrolib.core.dflowfm.ini.models.DataBlockINIBasedModel.datablock]."""
-
-    _split_to_list = make_list_validator("forcing")
-
-    serializer_config: DataBlockINIBasedSerializerConfig = (
-        DataBlockINIBasedSerializerConfig(
-            section_indent=0, property_indent=0, datablock_indent=0
-        )
-    )
-
-    @classmethod
-    def _ext(cls) -> str:
-        return ".bc"
-
-    @classmethod
-    def _filename(cls) -> str:
-        return "boundaryconditions"
-
-    @classmethod
-    def _get_parser(cls) -> Callable:
-        return cls.parse
-
-    @classmethod
-    def parse(cls, filepath: Path):
-        # It's odd to have to disable parsing something as comments
-        # but also need to pass it to the *flattener*.
-        # This method now only supports per model settings, not per section.
-        parser = Parser(ParserConfig(parse_datablocks=True, parse_comments=False))
-
-        with filepath.open(encoding="utf8") as f:
-            for line in f:
-                parser.feed_line(line)
-
-        return parser.finalize().flatten(True, False)
-
-
-class RealTime(str, Enum):
-    """
-    Enum class containing the valid value for the "realtime" reserved
-    keyword for real-time controlled forcing data, e.g., for hydraulic
-    structures.
-
-    This class is used inside the ForcingData Union, to force detection
-    of the realtime keyword, prior to considering it a filename.
-    """
-
-    realtime = "realtime"
-    """str: Realtime data source, externally provided"""
-
-
-ForcingData = Union[float, RealTime, ForcingModel]
-"""Data type that selects from three different types of forcing data:
-*   a scalar float constant
-*   "realtime" keyword, indicating externally controlled.
-*   A ForcingModel coming from a .bc file.
-"""
+"""Representation of a .bc file in various classes.
+
+    Most relevant classes are:
+
+    *   ForcingModel: toplevel class containing the whole .bc file contents.
+    *   ForcingBase subclasses: containing the actual data columns, for example:
+        TimeSeries, HarmonicComponent, AstronomicComponent, HarmonicCorrection,
+        AstronomicCorrection, Constant, T3D.
+
+"""
+import logging
+import re
+from enum import Enum
+from pathlib import Path
+from typing import Callable, Dict, Iterator, List, Literal, Optional, Set, Union
+
+from pydantic import Extra
+from pydantic.class_validators import root_validator, validator
+from pydantic.fields import Field
+
+from hydrolib.core.basemodel import BaseModel, ModelSaveSettings
+from hydrolib.core.dflowfm.ini.io_models import Property, Section
+from hydrolib.core.dflowfm.ini.models import (
+    DataBlockINIBasedModel,
+    INIGeneral,
+    INIModel,
+)
+from hydrolib.core.dflowfm.ini.parser import Parser, ParserConfig
+from hydrolib.core.dflowfm.ini.serializer import DataBlockINIBasedSerializerConfig
+from hydrolib.core.dflowfm.ini.util import (
+    get_enum_validator,
+    get_from_subclass_defaults,
+    get_split_string_on_delimiter_validator,
+    get_type_based_on_subclass_default_value,
+    make_list_validator,
+    rename_keys_for_backwards_compatibility,
+)
+from hydrolib.core.utils import to_list
+
+logger = logging.getLogger(__name__)
+
+
+class VerticalInterpolation(str, Enum):
+    """Enum class containing the valid values for the vertical position type,
+    which defines what the numeric values for vertical position specification mean.
+    """
+
+    linear = "linear"
+    """str: Linear interpolation between vertical positions."""
+
+    log = "log"
+    """str: Logarithmic interpolation between vertical positions (e.g. vertical velocity profiles)."""
+
+    block = "block"
+    """str: Equal to the value at the directly lower specified vertical position."""
+
+
+class VerticalPositionType(str, Enum):
+    """Enum class containing the valid values for the vertical position type."""
+
+    percentage_bed = "percBed"
+    """str: Percentage with respect to the water depth from the bed upward."""
+
+    z_bed = "ZBed"
+    """str: Absolute distance from the bed upward."""
+
+    z_datum = "ZDatum"
+    """str: z-coordinate with respect to the reference level of the model."""
+
+    z_surf = "ZSurf"
+    """str: Absolute distance from the free surface downward."""
+
+
+class TimeInterpolation(str, Enum):
+    """Enum class containing the valid values for the time interpolation."""
+
+    linear = "linear"
+    """str: Linear interpolation between times."""
+
+    block_from = "blockFrom"
+    """str: Equal to that at the start of the time interval (latest specified time value)."""
+
+    block_to = "blockTo"
+    """str: Equal to that at the end of the time interval (upcoming specified time value)."""
+
+
+class QuantityUnitPair(BaseModel):
+    """A .bc file header lines tuple containing a quantity name, its unit and optionally a vertical position index."""
+
+    quantity: str
+    """str: Name of quantity."""
+
+    unit: str
+    """str: Unit of quantity."""
+
+    vertpositionindex: Optional[int] = Field(alias="vertPositionIndex")
+    """int (optional): This is a (one-based) index into the verticalposition-specification, assigning a vertical position to the quantity (t3D-blocks only)."""
+
+    def _to_properties(self):
+        """Generator function that yields the ini Property objects for a single
+        QuantityUnitPair object."""
+        yield Property(key="quantity", value=self.quantity)
+        yield Property(key="unit", value=self.unit)
+        if self.vertpositionindex is not None:
+            yield Property(key="vertPositionIndex", value=self.vertpositionindex)
+
+
+class VectorQuantityUnitPairs(BaseModel):
+    """A subset of .bc file header lines containing a vector quantity definition,
+    followed by all component quantity names, their unit and optionally their
+    vertical position indexes."""
+
+    class Config:
+        validate_assignment = True
+
+    vectorname: str
+    """str: Name of the vector quantity."""
+
+    elementname: List[str]
+    """List[str]: List of names of the vector component quantities."""
+
+    quantityunitpair: List[QuantityUnitPair]
+    """List[QuantityUnitPair]: List of QuantityUnitPair that define the vector components."""
+
+    @root_validator
+    @classmethod
+    def _validate_quantity_element_names(cls, values: Dict):
+        for idx, name in enumerate(
+            [qup.quantity for qup in values["quantityunitpair"]]
+        ):
+            if name not in values["elementname"]:
+                raise ValueError(
+                    f"quantityunitpair[{idx}], quantity '{name}' must be in vectordefinition's element names: '{VectorQuantityUnitPairs._to_vectordefinition_string(values['vectorname'], values['elementname'])}'."
+                )
+
+        return values
+
+    @staticmethod
+    def _to_vectordefinition_string(vectorname: str, elementname: List[str]):
+        return vectorname + ":" + ",".join(elementname)
+
+    def __str__(self) -> str:
+        return VectorQuantityUnitPairs._to_vectordefinition_string(
+            self.vectorname, self.elementname
+        )
+
+    def _to_properties(self):
+        """Generator function that yields the ini Property objects for a single
+        VectorQuantityUnitPairs object."""
+        yield Property(key="vector", value=str(self))
+
+        for qup in self.quantityunitpair:
+            for prop in qup._to_properties():
+                yield prop
+
+
+ScalarOrVectorQUP = Union[QuantityUnitPair, VectorQuantityUnitPairs]
+
+
+class ForcingBase(DataBlockINIBasedModel):
+    """
+    The base class of a single [Forcing] block in a .bc forcings file.
+
+    Typically subclassed, for the specific types of forcing data, e.g, Harmonic.
+    This model is for example referenced under a
+    [ForcingModel][hydrolib.core.dflowfm.bc.models.ForcingModel]`.forcing[..]`.
+    """
+
+    _header: Literal["Forcing"] = "Forcing"
+    name: str = Field(alias="name")
+    """str: Unique identifier that identifies the location for this forcing data."""
+
+    function: str = Field(alias="function")
+    """str: Function type of the data in the actual datablock."""
+
+    quantityunitpair: List[ScalarOrVectorQUP]
+    """List[ScalarOrVectorQUP]: List of header lines for one or more quantities and their unit. Describes the columns in the actual datablock."""
+
+    def _exclude_fields(self) -> Set:
+        return {"quantityunitpair"}.union(super()._exclude_fields())
+
+    @classmethod
+    def _supports_comments(cls):
+        return True
+
+    @classmethod
+    def _duplicate_keys_as_list(cls):
+        return True
+
+    @root_validator(pre=True)
+    def _validate_quantityunitpair(cls, values):
+        quantityunitpairkey = "quantityunitpair"
+
+        if values.get(quantityunitpairkey) is not None:
+            return values
+
+        quantities = values.get("quantity")
+        if quantities is None:
+            raise ValueError("quantity is not provided")
+        units = values.get("unit")
+        if units is None:
+            raise ValueError("unit is not provided")
+
+        if isinstance(quantities, str) and isinstance(units, str):
+            values[quantityunitpairkey] = [
+                QuantityUnitPair(quantity=quantities, unit=units)
+            ]
+            return values
+
+        if isinstance(quantities, list) and isinstance(units, list):
+            if len(quantities) != len(units):
+                raise ValueError(
+                    "Number of quantities should be equal to number of units"
+                )
+
+            values[quantityunitpairkey] = [
+                QuantityUnitPair(quantity=quantity, unit=unit)
+                for quantity, unit in zip(quantities, units)
+            ]
+            return values
+
+        raise ValueError("Number of quantities should be equal to number of units")
+
+    @validator("function", pre=True)
+    def _set_function(cls, value):
+        return get_from_subclass_defaults(ForcingBase, "function", value)
+
+    @classmethod
+    def validate(cls, v):
+        """Try to initialize subclass based on the `function` field.
+        This field is compared to each `function` field of the derived models of `ForcingBase`
+        or models derived from derived models.
+        The derived model with an equal function type will be initialized.
+
+        Raises:
+            ValueError: When the given type is not a known structure type.
+        """
+
+        # should be replaced by discriminated unions once merged
+        # https://github.com/samuelcolvin/pydantic/pull/2336
+        if isinstance(v, dict):
+            function_string = v.get("function", "").lower()
+            function_type = get_type_based_on_subclass_default_value(
+                cls, "function", function_string
+            )
+
+            if function_type is not None:
+                return function_type(**v)
+
+            else:
+                raise ValueError(
+                    f"Function of {cls.__name__} with name={v.get('name', '')} and function={v.get('function', '')} is not recognized."
+                )
+        return v
+
+    def _get_identifier(self, data: dict) -> Optional[str]:
+        return data.get("name")
+
+    def _to_section(
+        self,
+        config: DataBlockINIBasedSerializerConfig,
+        save_settings: ModelSaveSettings,
+    ) -> Section:
+        section = super()._to_section(config, save_settings)
+
+        for quantity in self.quantityunitpair:
+            for prop in quantity._to_properties():
+                section.content.append(prop)
+
+        return section
+
+    class Config:
+        extra = Extra.ignore
+
+    def __repr__(self) -> str:
+        data = dict(self)
+        data["datablock"] = "<omitted>"
+        representable = BaseModel.construct(**data)
+        return str(representable)
+
+
+class VectorForcingBase(ForcingBase):
+    """
+    The base class of a single [Forcing] block that supports vectors in a .bc forcings file.
+    """
+
+    @root_validator(pre=True)
+    def validate_and_update_quantityunitpairs(cls, values: Dict) -> Dict:
+        """
+        Validates and, if required, updates vector quantity unit pairs.
+
+        Args:
+            values (Dict): Dictionary of values to be used to validate or
+            update vector quantity unit pairs.
+
+        Raises:
+            ValueError: When a quantity unit pair is found in a vector where it does not belong.
+            ValueError: When the number of quantity unit pairs in a vectors is not as expected.
+
+        Returns:
+            Dict: Dictionary of validates values.
+        """
+        quantityunitpairs = values["quantityunitpair"]
+        vector = values.get("vector")
+        number_of_element_repetitions = cls.get_number_of_repetitions(values)
+
+        VectorForcingBase._process_vectordefinition_or_check_quantityunitpairs(
+            vector, quantityunitpairs, number_of_element_repetitions
+        )
+
+        return values
+
+    @staticmethod
+    def _process_vectordefinition_or_check_quantityunitpairs(
+        vectordefs: Optional[List[str]],
+        quantityunitpairs: List[ScalarOrVectorQUP],
+        number_of_element_repetitions: int,
+    ) -> None:
+        """
+        Processes the given vector definition header lines from a .bc file
+        or, if absent, checks whether the existing VectorQuantityUnitPairs
+        objects already have the correct vector length.
+
+        Args:
+            vectordefs (List[str]): List of vector definition values, e.g.,
+                ["vectorname:comp1,comp2,..compN", ...]
+            quantityunitpairs (List[ScalarOrVectorQUP]): list of already parsed
+                and constructed QuantityUnitPair objects, which may be modified
+                in place with some packed VectorQuantityUnitPairs objects.
+            number_of_element_repetitions (int, optional): Number of times each
+                vector element is expected to be present in the subsequent
+                Quantity lines. Typically used for 3D quantities, using the
+                number of vertical layers.
+        """
+
+        if vectordefs is not None and not any(
+            map(lambda qup: isinstance(qup, VectorQuantityUnitPairs), quantityunitpairs)
+        ):
+            # Vector definition line still must be processed and VectorQUPs still created.
+            VectorForcingBase._validate_vectordefinition_and_update_quantityunitpairs(
+                vectordefs, quantityunitpairs, number_of_element_repetitions
+            )
+        else:
+            # VectorQUPs already present; directly validate their vector length.
+            for qup in quantityunitpairs:
+                if isinstance(qup, VectorQuantityUnitPairs):
+                    VectorForcingBase._validate_vectorlength(
+                        qup, number_of_element_repetitions
+                    )
+
+    @staticmethod
+    def _validate_vectordefinition_and_update_quantityunitpairs(
+        vectordefs: Optional[List[str]],
+        quantityunitpairs: List[ScalarOrVectorQUP],
+        number_of_element_repetitions: int,
+    ) -> None:
+        """
+        Validates the given vector definition header lines from a .bc file
+        for a ForcingBase subclass and updates the existing QuantityUnitPair list
+        by packing the vector elements into a VectorQuantityUnitPairs object
+        for each vector definition.
+
+        Args:
+            vectordefs (List[str]): List of vector definition values, e.g.,
+                ["vectorname:comp1,comp2,..compN", ...]
+            quantityunitpairs (List[ScalarOrVectorQUP]): list of already parsed
+                and constructed QuantityUnitPair objects, which will be modified
+                in place with some packed VectorQuantityUnitPairs objects.
+            number_of_element_repetitions (int, optional): Number of times each
+                vector element is expected to be present in the subsequent
+                Quantity lines. Typically used for 3D quantities, using the
+                number of vertical layers.
+        """
+
+        if vectordefs is None:
+            return
+
+        vectordefs = to_list(vectordefs)
+
+        qup_iter = iter(quantityunitpairs)
+
+        # Start a new list, to only keep the scalar QUPs, and add newly
+        # created VectorQUPs.
+        quantityunitpairs_with_vectors = []
+
+        # If one quantity is "time", it must be the first one.
+        if quantityunitpairs[0].quantity == "time":
+            quantityunitpairs_with_vectors.append(quantityunitpairs[0])
+            _ = next(qup_iter)
+
+        # For each vector definition line, greedily find the quantity unit pairs
+        # that form the vector elements, and pack them into a single VectorQuantityUnitPairs oject.
+        for vectordef in vectordefs:
+            VectorForcingBase._find_and_pack_vector_qups(
+                number_of_element_repetitions,
+                qup_iter,
+                quantityunitpairs_with_vectors,
+                vectordef,
+            )
+
+        for remaining_qu_pair in qup_iter:
+            quantityunitpairs_with_vectors.append(remaining_qu_pair)
+
+        quantityunitpairs[:] = quantityunitpairs_with_vectors
+
+    @staticmethod
+    def _find_and_pack_vector_qups(
+        number_of_element_repetitions: int,
+        qup_iter: Iterator[ScalarOrVectorQUP],
+        quantityunitpairs_with_vectors: List[ScalarOrVectorQUP],
+        vectordef: str,
+    ):
+        vectorname, componentdefs = vectordef.split(":")
+        componentnames = re.split(r"[, \t]", componentdefs)
+        n_components = len(componentnames)
+
+        vqu_pair = VectorQuantityUnitPairs(
+            vectorname=vectorname, elementname=componentnames, quantityunitpair=[]
+        )
+
+        n_rep = 0
+        for qu_pair in qup_iter:
+            if qu_pair.quantity in componentnames:
+                # This vector element found, store it.
+                vqu_pair.quantityunitpair.append(qu_pair)
+                n_rep += 1
+                if n_rep == n_components * number_of_element_repetitions:
+                    break
+            else:
+                # This quantity was no vector element being searched for
+                # so keep it as a regular (scalar) QuantityUnitPair.
+                quantityunitpairs_with_vectors.append(qu_pair)
+
+        if VectorForcingBase._validate_vectorlength(
+            vqu_pair, number_of_element_repetitions
+        ):
+            # This VectorQuantityUnitPairs is now complete; add it to result list.
+            quantityunitpairs_with_vectors.append(vqu_pair)
+
+    @staticmethod
+    def _validate_vectorlength(
+        vqu_pair: VectorQuantityUnitPairs,
+        number_of_element_repetitions,
+    ) -> bool:
+        """
+        Checks whether the number of QuantityUnitPairs in a vector quantity
+        matches exactly with number of vector elements in the definition and,
+        optionally, the number of vertical layers.
+
+        Args:
+            vqu_pair (VectorQuantityUnitPairs): the vector quantity object to be checked.
+            number_of_element_repetitions (int, optional): Number of times each
+                vector element is expected to be present in the subsequent
+                Quantity lines. Typically used for 3D quantities, using the
+                number of vertical layers.
+
+        Returns:
+            bool: True if vqu_pair is valid. False return value is hidden because
+                an exception will be raised.
+
+        Raises:
+            ValueError: If number of QuantityUnitPair objects in vqu_pair is not equal
+                to number of element names * number_of_element_repetitions.
+        """
+
+        if not (
+            valid := len(vqu_pair.quantityunitpair)
+            == len(vqu_pair.elementname) * number_of_element_repetitions
+        ):
+            raise ValueError(
+                f"Incorrect number of quantity unit pairs were found; should match the elements in vectordefinition for {vqu_pair.vectorname}"
+                + (
+                    f", and {number_of_element_repetitions} vertical layers"
+                    if number_of_element_repetitions > 1
+                    else ""
+                )
+                + "."
+            )
+
+        return valid
+
+    @validator("function", pre=True)
+    def _set_function(cls, value):
+        return get_from_subclass_defaults(VectorForcingBase, "function", value)
+
+    @classmethod
+    def get_number_of_repetitions(cls, values: Dict) -> int:
+        """Gets the number of expected quantityunitpairs for each vector element. Defaults to 1."""
+        return 1
+
+
+class TimeSeries(VectorForcingBase):
+    """Subclass for a .bc file [Forcing] block with timeseries data."""
+
+    function: Literal["timeseries"] = "timeseries"
+
+    timeinterpolation: TimeInterpolation = Field(alias="timeInterpolation")
+    """TimeInterpolation: The type of time interpolation."""
+
+    offset: float = Field(0.0, alias="offset")
+    """float: All values in the table are increased by the offset (after multiplication by factor). Defaults to 0.0."""
+
+    factor: float = Field(1.0, alias="factor")
+    """float: All values in the table are multiplied with the factor. Defaults to 1.0."""
+
+    _timeinterpolation_validator = get_enum_validator(
+        "timeinterpolation", enum=TimeInterpolation
+    )
+
+    @root_validator(allow_reuse=True, pre=True)
+    def rename_keys(cls, values: Dict) -> Dict:
+        """Renames some old keywords to the currently supported keywords."""
+        return rename_keys_for_backwards_compatibility(
+            values,
+            {
+                "timeinterpolation": ["time_interpolation"],
+            },
+        )
+
+
+class Harmonic(ForcingBase):
+    """Subclass for a .bc file [Forcing] block with harmonic components data."""
+
+    function: Literal["harmonic"] = "harmonic"
+
+    factor: float = Field(1.0, alias="factor")
+    """float: All values in the table are multiplied with the factor. Defaults to 1.0."""
+
+
+class Astronomic(ForcingBase):
+    """Subclass for a .bc file [Forcing] block with astronomic components data."""
+
+    function: Literal["astronomic"] = "astronomic"
+
+    factor: float = Field(1.0, alias="factor")
+    """float: All values in the table are multiplied with the factor. Defaults to 1.0."""
+
+
+class HarmonicCorrection(ForcingBase):
+    """Subclass for a .bc file [Forcing] block with harmonic components correction data."""
+
+    function: Literal["harmonic-correction"] = "harmonic-correction"
+
+
+class AstronomicCorrection(ForcingBase):
+    """Subclass for a .bc file [Forcing] block with astronomic components correction data."""
+
+    function: Literal["astronomic-correction"] = "astronomic-correction"
+
+
+class T3D(VectorForcingBase):
+    """Subclass for a .bc file [Forcing] block with 3D timeseries data."""
+
+    function: Literal["t3d"] = "t3d"
+
+    offset: float = Field(0.0, alias="offset")
+    """float: All values in the table are increased by the offset (after multiplication by factor). Defaults to 0.0."""
+
+    factor: float = Field(1.0, alias="factor")
+    """float: All values in the table are multiplied with the factor. Defaults to 1.0."""
+
+    vertpositions: List[float] = Field(alias="vertPositions")
+    """List[float]: The specification of the vertical positions."""
+
+    vertinterpolation: VerticalInterpolation = Field(
+        VerticalInterpolation.linear, alias="vertInterpolation"
+    )
+    """VerticalInterpolation: The type of vertical interpolation. Defaults to linear."""
+
+    vertpositiontype: VerticalPositionType = Field(alias="vertPositionType")
+    """VerticalPositionType: The vertical position type of the verticalpositions values."""
+
+    timeinterpolation: TimeInterpolation = Field(
+        TimeInterpolation.linear, alias="timeInterpolation"
+    )
+    """TimeInterpolation: The type of time interpolation. Defaults to linear."""
+
+    _keys_to_rename = {
+        "timeinterpolation": ["time_interpolation"],
+        "vertpositions": ["vertical_position_specification"],
+        "vertinterpolation": ["vertical_interpolation"],
+        "vertpositiontype": ["vertical_position_type"],
+        "vertpositionindex": ["vertical_position"],
+    }
+
+    @root_validator(allow_reuse=True, pre=True)
+    def rename_keys(cls, values: Dict) -> Dict:
+        """Renames some old keywords to the currently supported keywords."""
+        return rename_keys_for_backwards_compatibility(values, cls._keys_to_rename)
+
+    _split_to_list = get_split_string_on_delimiter_validator(
+        "vertpositions",
+    )
+
+    _verticalinterpolation_validator = get_enum_validator(
+        "vertinterpolation", enum=VerticalInterpolation
+    )
+    _verticalpositiontype_validator = get_enum_validator(
+        "vertpositiontype",
+        enum=VerticalPositionType,
+        alternative_enum_values={
+            VerticalPositionType.percentage_bed: ["percentage from bed"],
+        },
+    )
+    _timeinterpolation_validator = get_enum_validator(
+        "timeinterpolation", enum=TimeInterpolation
+    )
+
+    @classmethod
+    def get_number_of_repetitions(cls, values: Dict) -> int:
+        verticalpositions = values.get("vertpositions")
+        # Since the renaming root validator may not have been run yet, in this
+        # method we explicitly check old keywords for backwards compatibility:
+        if verticalpositions is None:
+            # try to get the value from any of the older keywords
+            for old_keyword in cls._keys_to_rename["vertpositions"]:
+                verticalpositions = values.get(old_keyword)
+                if verticalpositions is not None:
+                    break
+
+        if verticalpositions is None:
+            raise ValueError("vertPositions is not provided")
+
+        number_of_verticalpositions = (
+            len(verticalpositions)
+            if isinstance(verticalpositions, List)
+            else len(verticalpositions.split())
+        )
+
+        return number_of_verticalpositions
+
+    @root_validator(pre=True)
+    def _validate_quantityunitpairs(cls, values: Dict) -> Dict:
+        quantityunitpairs = values["quantityunitpair"]
+
+        T3D._validate_that_first_unit_is_time_and_has_no_verticalposition(
+            quantityunitpairs
+        )
+
+        number_of_verticalpositions = cls.get_number_of_repetitions(values)
+
+        verticalpositionindexes = values.get("vertpositionindex")
+        if verticalpositionindexes is None:
+            T3D._validate_that_all_quantityunitpairs_have_valid_verticalpositionindex(
+                quantityunitpairs[1:], number_of_verticalpositions
+            )
+        else:
+            T3D._validate_verticalpositionindexes_and_update_quantityunitpairs(
+                verticalpositionindexes,
+                number_of_verticalpositions,
+                quantityunitpairs,
+            )
+
+        return values
+
+    @staticmethod
+    def _validate_that_first_unit_is_time_and_has_no_verticalposition(
+        quantityunitpairs: List[QuantityUnitPair],
+    ) -> None:
+        if quantityunitpairs[0].quantity.lower() != "time":
+            raise ValueError("First quantity should be `time`")
+        if quantityunitpairs[0].vertpositionindex is not None:
+            raise ValueError("`time` quantity cannot have vertical position index")
+
+    @staticmethod
+    def _validate_that_all_quantityunitpairs_have_valid_verticalpositionindex(
+        quantityunitpairs: List[ScalarOrVectorQUP], maximum_verticalpositionindex: int
+    ) -> None:
+        for quantityunitpair in quantityunitpairs:
+            if isinstance(quantityunitpair, VectorQuantityUnitPairs):
+                return T3D._validate_that_all_quantityunitpairs_have_valid_verticalpositionindex(
+                    quantityunitpair.quantityunitpair, maximum_verticalpositionindex
+                )
+
+            verticalpositionindex = quantityunitpair.vertpositionindex
+
+            if not T3D._is_valid_verticalpositionindex(
+                verticalpositionindex, maximum_verticalpositionindex
+            ):
+                raise ValueError(
+                    f"Vertical position index should be between 1 and {maximum_verticalpositionindex}, but {verticalpositionindex} was given"
+                )
+
+    @staticmethod
+    def _validate_verticalpositionindexes_and_update_quantityunitpairs(
+        verticalpositionindexes: List[int],
+        number_of_verticalpositions: int,
+        quantityunitpairs: List[ScalarOrVectorQUP],
+    ) -> None:
+        if verticalpositionindexes is None:
+            raise ValueError("vertPositionIndex is not provided")
+
+        T3D._validate_that_verticalpositionindexes_are_valid(
+            verticalpositionindexes, number_of_verticalpositions
+        )
+
+        T3D._add_verticalpositionindex_to_quantityunitpairs(
+            quantityunitpairs[1:], verticalpositionindexes
+        )
+
+    @staticmethod
+    def _validate_that_verticalpositionindexes_are_valid(
+        verticalpositionindexes: List[int], number_of_vertical_positions: int
+    ) -> None:
+        for verticalpositionindexstring in verticalpositionindexes:
+            verticalpositionindex = (
+                int(verticalpositionindexstring)
+                if verticalpositionindexstring
+                else None
+            )
+            if not T3D._is_valid_verticalpositionindex(
+                verticalpositionindex, number_of_vertical_positions
+            ):
+                raise ValueError(
+                    f"Vertical position index should be between 1 and {number_of_vertical_positions}"
+                )
+
+    @staticmethod
+    def _is_valid_verticalpositionindex(
+        verticalpositionindex: int, number_of_vertical_positions: int
+    ) -> bool:
+        one_based_index_offset = 1
+
+        return (
+            verticalpositionindex is not None
+            and verticalpositionindex >= one_based_index_offset
+            and verticalpositionindex <= number_of_vertical_positions
+        )
+
+    @staticmethod
+    def _add_verticalpositionindex_to_quantityunitpairs(
+        quantityunitpairs: List[ScalarOrVectorQUP], verticalpositionindexes: List[int]
+    ) -> None:
+        i = 0
+
+        for quanityunitpair in quantityunitpairs:
+            if i >= len(verticalpositionindexes):
+                raise ValueError(
+                    "Number of vertical position indexes should be equal to the number of quantities/units - 1"
+                )
+
+            if isinstance(quanityunitpair, VectorQuantityUnitPairs):
+                for qup in quanityunitpair.quantityunitpair:
+                    qup.vertpositionindex = verticalpositionindexes[i]
+                    i = i + 1
+            else:
+                quanityunitpair.vertpositionindex = verticalpositionindexes[i]
+                i = i + 1
+
+        if i != len(verticalpositionindexes):
+            raise ValueError(
+                "Number of vertical position indexes should be equal to the number of quantities/units - 1"
+            )
+
+
+class QHTable(ForcingBase):
+    """Subclass for a .bc file [Forcing] block with Q-h table data."""
+
+    function: Literal["qhtable"] = "qhtable"
+
+
+class Constant(ForcingBase):
+    """Subclass for a .bc file [Forcing] block with constant value data."""
+
+    function: Literal["constant"] = "constant"
+
+    offset: float = Field(0.0, alias="offset")
+    """float: All values in the table are increased by the offset (after multiplication by factor). Defaults to 0.0."""
+
+    factor: float = Field(1.0, alias="factor")
+    """float: All values in the table are multiplied with the factor. Defaults to 1.0."""
+
+
+class ForcingGeneral(INIGeneral):
+    """`[General]` section with .bc file metadata."""
+
+    fileversion: str = Field("1.01", alias="fileVersion")
+    """str: The file version."""
+
+    filetype: Literal["boundConds"] = Field("boundConds", alias="fileType")
+
+
+class ForcingModel(INIModel):
+    """
+    The overall model that contains the contents of one .bc forcings file.
+
+    This model is for example referenced under a
+    [ExtModel][hydrolib.core.dflowfm.ext.models.ExtModel]`.boundary[..].forcingfile[..]`.
+    """
+
+    general: ForcingGeneral = ForcingGeneral()
+    """ForcingGeneral: `[General]` block with file metadata."""
+
+    forcing: List[ForcingBase] = []
+    """List[ForcingBase]: List of `[Forcing]` blocks for all forcing
+    definitions in a single .bc file. Actual data is stored in
+    forcing[..].datablock from [hydrolib.core.dflowfm.ini.models.DataBlockINIBasedModel.datablock]."""
+
+    _split_to_list = make_list_validator("forcing")
+
+    serializer_config: DataBlockINIBasedSerializerConfig = (
+        DataBlockINIBasedSerializerConfig(
+            section_indent=0, property_indent=0, datablock_indent=0
+        )
+    )
+
+    @classmethod
+    def _ext(cls) -> str:
+        return ".bc"
+
+    @classmethod
+    def _filename(cls) -> str:
+        return "boundaryconditions"
+
+    @classmethod
+    def _get_parser(cls) -> Callable:
+        return cls.parse
+
+    @classmethod
+    def parse(cls, filepath: Path):
+        # It's odd to have to disable parsing something as comments
+        # but also need to pass it to the *flattener*.
+        # This method now only supports per model settings, not per section.
+        parser = Parser(ParserConfig(parse_datablocks=True, parse_comments=False))
+
+        with filepath.open(encoding="utf8") as f:
+            for line in f:
+                parser.feed_line(line)
+
+        return parser.finalize().flatten(True, False)
+
+
+class RealTime(str, Enum):
+    """
+    Enum class containing the valid value for the "realtime" reserved
+    keyword for real-time controlled forcing data, e.g., for hydraulic
+    structures.
+
+    This class is used inside the ForcingData Union, to force detection
+    of the realtime keyword, prior to considering it a filename.
+    """
+
+    realtime = "realtime"
+    """str: Realtime data source, externally provided"""
+
+
+ForcingData = Union[float, RealTime, ForcingModel]
+"""Data type that selects from three different types of forcing data:
+*   a scalar float constant
+*   "realtime" keyword, indicating externally controlled.
+*   A ForcingModel coming from a .bc file.
+"""
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/common/models.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/common/models.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,37 +1,37 @@
-from enum import Enum
-
-
-class LocationType(str, Enum):
-    """
-    Enum class containing the valid values for the locationType
-    attribute in several classes such as Lateral and ObservationPoint.
-    """
-
-    oned = "1d"
-    """str: Denotes 1D locations (typically 1D pressure points) in a model."""
-
-    twod = "2d"
-    """str: Denotes 2D locations (typically 2D grid cells) in a model."""
-
-    all = "all"
-    """str: Denotes that both 1D and 2D locations may be selected."""
-
-
-class Operand(str, Enum):
-    """
-    Enum class containing the valid values for the operand
-    attribute in several subclasses of AbstractIniField and ExtOldForcing.
-    """
-
-    override = "O"
-    """Existing values are overwritten with the provided values."""
-    append = "A"
-    """Provided values are used where existing values are missing."""
-    add = "+"
-    """Existing values are summed with the provided values."""
-    mult = "*"
-    """Existing values are multiplied with the provided values."""
-    max = "X"
-    """The maximum values of the existing values and provided values are used."""
-    min = "N"
-    """The minimum values of the existing values and provided values are used."""
+from enum import Enum
+
+
+class LocationType(str, Enum):
+    """
+    Enum class containing the valid values for the locationType
+    attribute in several classes such as Lateral and ObservationPoint.
+    """
+
+    oned = "1d"
+    """str: Denotes 1D locations (typically 1D pressure points) in a model."""
+
+    twod = "2d"
+    """str: Denotes 2D locations (typically 2D grid cells) in a model."""
+
+    all = "all"
+    """str: Denotes that both 1D and 2D locations may be selected."""
+
+
+class Operand(str, Enum):
+    """
+    Enum class containing the valid values for the operand
+    attribute in several subclasses of AbstractIniField and ExtOldForcing.
+    """
+
+    override = "O"
+    """Existing values are overwritten with the provided values."""
+    append = "A"
+    """Provided values are used where existing values are missing."""
+    add = "+"
+    """Existing values are summed with the provided values."""
+    mult = "*"
+    """Existing values are multiplied with the provided values."""
+    max = "X"
+    """The maximum values of the existing values and provided values are used."""
+    min = "N"
+    """The minimum values of the existing values and provided values are used."""
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/crosssection/models.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/crosssection/models.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,736 +1,736 @@
-import logging
-from typing import Dict, List, Literal, Optional
-
-from pydantic import Field, root_validator
-from pydantic.class_validators import validator
-
-from hydrolib.core.dflowfm.friction.models import FrictionType
-from hydrolib.core.dflowfm.ini.models import INIBasedModel, INIGeneral, INIModel
-from hydrolib.core.dflowfm.ini.util import (
-    LocationValidationConfiguration,
-    LocationValidationFieldNames,
-    get_enum_validator,
-    get_from_subclass_defaults,
-    get_split_string_on_delimiter_validator,
-    make_list_validator,
-    validate_correct_length,
-    validate_location_specification,
-)
-
-logger = logging.getLogger(__name__)
-
-frictionid_description = 'Name of the roughness variable associated with \
-    this cross section. Either this parameter or \
-    frictionType should be specified. If neither \
-    parameter is specified, the frictionId defaults \
-    to "Main".'
-
-frictiontype_description = "Roughness type associated with this cross section \
-    Either this parameter or frictionId should be specified."
-
-frictionvalue_description = "Roughness value; its meaning depends on the roughness type selected \
-    (only used if frictionType specified)."
-
-
-class CrossDefGeneral(INIGeneral):
-    """The crosssection definition file's `[General]` section with file meta data."""
-
-    fileversion: str = Field("3.00", alias="fileVersion")
-    filetype: Literal["crossDef"] = Field("crossDef", alias="fileType")
-
-
-class CrossLocGeneral(INIGeneral):
-    """The crosssection location file's `[General]` section with file meta data."""
-
-    fileversion: str = Field("3.00", alias="fileVersion")
-    filetype: Literal["crossLoc"] = Field("crossLoc", alias="fileType")
-
-
-class CrossSectionDefinition(INIBasedModel):
-    """
-    A `[Definition]` block for use inside a crosssection definition file,
-    i.e., a [CrossDefModel][hydrolib.core.dflowfm.crosssection.models.CrossDefModel].
-
-    This class is intended as an abstract class: various subclasses should
-    define they actual types of crosssection definitions.
-    """
-
-    # TODO: would we want to load this from something externally and generate these automatically
-    class Comments(INIBasedModel.Comments):
-        id: Optional[str] = "Unique cross-section definition id."
-        thalweg: Optional[str] = Field(
-            "Transverse Y coordinate at which the cross section aligns with the branch "
-            + "(Keyword used by GUI only)."
-        )
-
-    comments: Comments = Comments()
-
-    _header: Literal["Definition"] = "Definition"
-
-    id: str = Field(alias="id")
-    type: str = Field(alias="type")
-    thalweg: Optional[float]
-
-    def _get_identifier(self, data: dict) -> Optional[str]:
-        return data.get("id")
-
-    @classmethod
-    def _duplicate_keys_as_list(cls):
-        return True
-
-    @validator("type", pre=True)
-    def _validate_type(cls, value):
-        return get_from_subclass_defaults(CrossSectionDefinition, "type", value)
-
-    @classmethod
-    def validate(cls, v):
-        """Try to initialize subclass based on the `type` field.
-        This field is compared to each `type` field of the derived models of `CrossSectionDefinition`.
-        The derived model with an equal crosssection definition type will be initialized.
-
-        Raises:
-            ValueError: When the given type is not a known crosssection definition type.
-        """
-
-        # should be replaced by discriminated unions once merged
-        # https://github.com/samuelcolvin/pydantic/pull/2336
-        if isinstance(v, dict):
-            for c in cls.__subclasses__():
-                if (
-                    c.__fields__.get("type").default.lower()
-                    == v.get("type", "").lower()
-                ):
-                    v = c(**v)
-                    break
-            else:
-                raise ValueError(
-                    f"Type of {cls.__name__} with id={v.get('id', '')} and type={v.get('type', '')} is not recognized."
-                )
-        return super().validate(v)
-
-    @staticmethod
-    def _get_friction_root_validator(
-        frictionid_attr: str,
-        frictiontype_attr: str,
-        frictionvalue_attr: str,
-    ):
-        """
-        Make a root_validator that verifies whether the crosssection definition (subclass)
-        has a valid friction specification.
-        Supposed to be embedded in subclasses for their friction fields.
-
-        Args:
-            frictionid_attr: name of the frictionid attribute in the subclass.
-            frictiontype_attr: name of the frictiontype attribute in the subclass.
-            frictionvalue_attr: name of the frictionvalue attribute in the subclass.
-
-        Returns:
-            root_validator: to be embedded in the subclass that needs it.
-        """
-
-        def validate_friction_specification(cls, values):
-            """
-            The actual validator function.
-
-            Args:
-            cls: The subclass for which the root_validator is called.
-            values (dict): Dictionary of values to create a CrossSectionDefinition subclass.
-            """
-            frictionid = values.get(frictionid_attr) or ""
-            frictiontype = values.get(frictiontype_attr) or ""
-            frictionvalue = values.get(frictionvalue_attr) or ""
-
-            if frictionid != "" and (frictiontype != "" or frictionvalue != ""):
-                raise ValueError(
-                    f"Cross section has duplicate friction specification (both {frictionid_attr} and {frictiontype_attr}/{frictionvalue_attr})."
-                )
-
-            return values
-
-        return root_validator(allow_reuse=True)(validate_friction_specification)
-
-
-class CrossDefModel(INIModel):
-    """
-    The overall crosssection definition model that contains the contents of one crossdef file.
-
-    This model is typically referenced under a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.crossdeffile`.
-
-    Attributes:
-        general (CrossdefGeneral): `[General]` block with file metadata.
-        definition (List[CrossSectionDefinition]): List of `[Definition]` blocks for all cross sections.
-    """
-
-    general: CrossDefGeneral = CrossDefGeneral()
-    definition: List[CrossSectionDefinition] = []
-
-    _make_list = make_list_validator("definition")
-
-    @classmethod
-    def _filename(cls) -> str:
-        return "crsdef"
-
-
-class CircleCrsDef(CrossSectionDefinition):
-    """
-    Crosssection definition with `type=circle`, to be included in a crossdef file.
-    Typically inside the definition list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.crossdeffile.definition[..]`
-
-    All lowercased attributes match with the circle input as described in
-    [UM Sec.C.16.1.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.16.1.1).
-    """
-
-    class Comments(CrossSectionDefinition.Comments):
-        type: Optional[str] = Field("Cross section type; must read circle")
-
-        diameter: Optional[str] = Field("Internal diameter of the circle [m].")
-        frictionid: Optional[str] = Field(
-            frictionid_description,
-            alias="frictionId",
-        )
-        frictiontype: Optional[str] = Field(
-            frictiontype_description,
-            alias="frictionType",
-        )
-        frictionvalue: Optional[str] = Field(
-            frictionvalue_description,
-            alias="frictionValue",
-        )
-
-    comments: Comments = Comments()
-
-    type: Literal["circle"] = Field("circle")
-    diameter: float
-    frictionid: Optional[str] = Field(alias="frictionId")
-    frictiontype: Optional[FrictionType] = Field(alias="frictionType")
-    frictionvalue: Optional[float] = Field(alias="frictionValue")
-
-    _friction_validator = CrossSectionDefinition._get_friction_root_validator(
-        "frictionid", "frictiontype", "frictionvalue"
-    )
-    _frictiontype_validator = get_enum_validator("frictiontype", enum=FrictionType)
-
-
-class RectangleCrsDef(CrossSectionDefinition):
-    """
-    Crosssection definition with `type=rectangle`, to be included in a crossdef file.
-    Typically inside the definition list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.crossdeffile.definition[..]`
-
-    All lowercased attributes match with the rectangle input as described in
-    [UM Sec.C.16.1.2](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.16.1.2).
-    """
-
-    class Comments(CrossSectionDefinition.Comments):
-        type: Optional[str] = Field("Cross section type; must read rectangle")
-        width: Optional[str] = Field("Width of the rectangle [m].")
-        height: Optional[str] = Field("Height of the rectangle [m].")
-        closed: Optional[str] = Field("no: Open channel, yes: Closed channel.")
-        frictionid: Optional[str] = Field(
-            frictionid_description,
-            alias="frictionId",
-        )
-        frictiontype: Optional[str] = Field(
-            frictiontype_description,
-            alias="frictionType",
-        )
-        frictionvalue: Optional[str] = Field(
-            frictionvalue_description,
-            alias="frictionValue",
-        )
-
-    comments: Comments = Comments()
-
-    type: Literal["rectangle"] = Field("rectangle")
-    width: float
-    height: float
-    closed: bool = Field(True)
-    frictionid: Optional[str] = Field(alias="frictionId")
-    frictiontype: Optional[FrictionType] = Field(alias="frictionType")
-    frictionvalue: Optional[float] = Field(alias="frictionValue")
-
-    _friction_validator = CrossSectionDefinition._get_friction_root_validator(
-        "frictionid", "frictiontype", "frictionvalue"
-    )
-    _frictiontype_validator = get_enum_validator("frictiontype", enum=FrictionType)
-
-
-class ZWRiverCrsDef(CrossSectionDefinition):
-    """
-    Crosssection definition with `type=zwRiver`, to be included in a crossdef file.
-    Typically inside the definition list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.crossdeffile.definition[..]`
-
-    All lowercased attributes match with the zwRiver input as described in
-    [UM Sec.C.16.1.3](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.16.1.3).
-    """
-
-    class Comments(CrossSectionDefinition.Comments):
-        type: Optional[str] = Field(
-            "Cross section type; must read zwRiver", alias="type"
-        )
-        numlevels: Optional[str] = Field(
-            "Number of levels in the table.", alias="numLevels"
-        )
-        levels: Optional[str] = Field(
-            "Space separated list of monotonic increasing heights/levels [m AD].",
-            alias="levels",
-        )
-        flowwidths: Optional[str] = Field(
-            "Space separated list of flow widths at the selected heights [m)].",
-            alias="flowWidths",
-        )
-        totalwidths: Optional[str] = Field(
-            "Space separated list of total widths at the selected heights [m]. "
-            "Equal to flowWidths if not specified. If specified, the totalWidths"
-            "should be larger than flowWidths.",
-            alias="totalWidths",
-        )
-        leveecrestLevel: Optional[str] = Field(
-            "Crest level of levee [m AD].", alias="leveeCrestlevel"
-        )
-        leveebaselevel: Optional[str] = Field(
-            "Base level of levee [m AD].", alias="leveeBaseLevel"
-        )
-        leveeflowarea: Optional[str] = Field(
-            "Flow area behind levee [m2].", alias="leveeFlowArea"
-        )
-        leveetotalarea: Optional[str] = Field(
-            "Total area behind levee [m2].", alias="leveeTotalArea"
-        )
-        mainwidth: Optional[str] = Field(
-            "Width of main section [m]. Default value: max(flowWidths).",
-            alias="mainWidth",
-        )
-        fp1width: Optional[str] = Field(
-            "Width of floodplain 1 section [m]. Default value: max(flowWidths)-mainWidth",
-            alias="fp1Width",
-        )
-        fp2width: Optional[str] = Field(
-            "Width of floodplain 2 section [m]. Default value: max(flowWidths)-mainWidth-fp1Width",
-            alias="fp2Width",
-        )
-        frictionids: Optional[str] = Field(
-            "Semicolon separated list of roughness variable names associated with the roughness "
-            "sections. Either this parameter or frictionTypes should be specified. If neither "
-            'parameter is specified, the frictionIds default to "Main", "FloodPlain1" '
-            'and "FloodPlain2".',
-            alias="frictionIds",
-        )
-        frictiontypes: Optional[str] = Field(
-            "Semicolon separated list of roughness types associated with the roughness sections. "
-            "Either this parameter or frictionIds should be specified. Can be specified as a "
-            "single value if all roughness sections use the same type.",
-            alias="frictionTypes",
-        )
-        frictionvalues: Optional[str] = Field(
-            "Space separated list of roughness values; their meaning depends on the roughness "
-            "types selected (only used if frictionTypes specified).",
-            alias="frictionValues",
-        )
-
-    comments: Comments = Comments()
-
-    type: Literal["zwRiver"] = Field("zwRiver")
-    numlevels: int = Field(alias="numLevels")
-    levels: List[float]
-    flowwidths: List[float] = Field(alias="flowWidths")
-    totalwidths: Optional[List[float]] = Field(alias="totalWidths")
-    leveecrestLevel: Optional[float] = Field(alias="leveeCrestlevel")
-    leveebaselevel: Optional[float] = Field(alias="leveeBaseLevel")
-    leveeflowarea: Optional[float] = Field(alias="leveeFlowArea")
-    leveetotalrea: Optional[float] = Field(alias="leveeTotalArea")
-    mainwidth: Optional[float] = Field(alias="mainWidth")
-    fp1width: Optional[float] = Field(alias="fp1Width")
-    fp2width: Optional[float] = Field(alias="fp2Width")
-    frictionids: Optional[List[str]] = Field(alias="frictionIds", delimiter=";")
-    frictiontypes: Optional[List[FrictionType]] = Field(
-        alias="frictionTypes", delimiter=";"
-    )
-    frictionvalues: Optional[List[float]] = Field(alias="frictionValues")
-
-    _split_to_list = get_split_string_on_delimiter_validator(
-        "levels",
-        "flowwidths",
-        "totalwidths",
-        "frictionvalues",
-        "frictionids",
-        "frictiontypes",
-    )
-
-    _friction_validator = CrossSectionDefinition._get_friction_root_validator(
-        "frictionids", "frictiontypes", "frictionvalues"
-    )
-    _frictiontype_validator = get_enum_validator("frictiontypes", enum=FrictionType)
-
-    @root_validator(allow_reuse=True)
-    def check_list_lengths(cls, values):
-        """Validates that the length of the levels, flowwidths and totalwidths fields are as expected."""
-        return validate_correct_length(
-            values,
-            "levels",
-            "flowwidths",
-            "totalwidths",
-            length_name="numlevels",
-        )
-
-
-class ZWCrsDef(CrossSectionDefinition):
-    """
-    Crosssection definition with `type=zw`, to be included in a crossdef file.
-    Typically inside the definition list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.crossdeffile.definition[..]`
-
-    All lowercased attributes match with the zw input as described in
-    [UM Sec.C.16.1.4](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.16.1.4).
-    """
-
-    class Comments(CrossSectionDefinition.Comments):
-        type: Optional[str] = Field("Cross section type; must read zw", alias="type")
-        # NOTE: Field "template" deliberately ignored for now.
-        numlevels: Optional[str] = Field(
-            "Number of levels in the table.", alias="numLevels"
-        )
-        levels: Optional[str] = Field(
-            "Space separated list of monotonic increasing heights/levels [m AD].",
-            alias="levels",
-        )
-        flowwidths: Optional[str] = Field(
-            "Space separated list of flow widths at the selected heights [m)].",
-            alias="flowWidths",
-        )
-        totalwidths: Optional[str] = Field(
-            "Space separated list of total widths at the selected heights [m]. "
-            "Equal to flowWidths if not specified. If specified, the totalWidths"
-            "should be larger than flowWidths.",
-            alias="totalWidths",
-        )
-        frictionid: Optional[str] = Field(
-            frictionid_description,
-            alias="frictionId",
-        )
-        frictiontype: Optional[str] = Field(
-            frictiontype_description,
-            alias="frictionType",
-        )
-        frictionvalue: Optional[str] = Field(
-            frictionvalue_description,
-            alias="frictionValue",
-        )
-
-    comments: Comments = Comments()
-
-    type: Literal["zw"] = Field("zw")
-    numlevels: int = Field(alias="numLevels")
-    levels: List[float]
-    flowwidths: List[float] = Field(alias="flowWidths")
-    totalwidths: Optional[List[float]] = Field(alias="totalWidths")
-    frictionid: Optional[str] = Field(alias="frictionId")
-    frictiontype: Optional[FrictionType] = Field(alias="frictionType")
-    frictionvalue: Optional[float] = Field(alias="frictionValue")
-
-    _split_to_list = get_split_string_on_delimiter_validator(
-        "levels",
-        "flowwidths",
-        "totalwidths",
-    )
-
-    @root_validator(allow_reuse=True)
-    def check_list_lengths(cls, values):
-        """Validates that the length of the levels, flowwidths and totalwidths fields are as expected."""
-        return validate_correct_length(
-            values,
-            "levels",
-            "flowwidths",
-            "totalwidths",
-            length_name="numlevels",
-        )
-
-    _friction_validator = CrossSectionDefinition._get_friction_root_validator(
-        "frictionid", "frictiontype", "frictionvalue"
-    )
-    _frictiontype_validator = get_enum_validator("frictiontype", enum=FrictionType)
-
-
-class YZCrsDef(CrossSectionDefinition):
-    """
-    Crosssection definition with `type=yz`, to be included in a crossdef file.
-    Typically inside the definition list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.crossdeffile.definition[..]`
-
-    All lowercased attributes match with the yz input as described in
-    [UM Sec.C.16.1.6](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.16.1.6).
-    """
-
-    class Comments(CrossSectionDefinition.Comments):
-        type: Optional[str] = Field("Cross section type; must read yz", alias="type")
-        conveyance: Optional[str] = Field(
-            "lumped: Lumped, segmented: Vertically segmented. Only the default lumped "
-            + "option is allowed if singleValuedZ = no. In the case of lumped conveyance, "
-            + "only a single uniform roughness for the whole cross section is allowed, "
-            + "i.e., sectionCount must equal 1.",
-        )
-        yzcount: Optional[str] = Field("Number of YZ-coordinates.", alias="yzCount")
-        yCoordinates: Optional[str] = Field(
-            "Space separated list of monotonic increasing y-coordinates [m].",
-            alias="yCoordinates",
-        )
-        zCoordinates: Optional[str] = Field(
-            "Space separated list of single-valued z-coordinates [m AD].",
-            alias="zCoordinates",
-        )
-        sectioncount: Optional[str] = Field(
-            "Number of roughness sections. If the lumped conveyance is selected then "
-            + "sectionCount must equal 1.",
-            alias="sectionCount",
-        )
-        frictionpositions: Optional[str] = Field(
-            "Locations where the roughness sections start and end. Always one location more than "
-            + "sectionCount. The first value should equal 0 and the last value should equal the "
-            + "cross section length. Keyword may be skipped if sectionCount = 1.",
-            alias="frictionPositions",
-        )
-        frictionids: Optional[str] = Field(
-            "Semicolon separated list of roughness variable names associated with the roughness "
-            + "sections. Either this parameter or frictionTypes should be specified. If neither "
-            + 'parameter is specified, the frictionIds default to "Main", "FloodPlain1" '
-            + 'and "FloodPlain2".',
-            alias="frictionIds",
-        )
-        frictiontypes: Optional[str] = Field(
-            "Semicolon separated list of roughness types associated with the roughness sections. "
-            + "Either this parameter or frictionIds should be specified. Can be specified as a "
-            + "single value if all roughness sections use the same type.",
-            alias="frictionTypes",
-        )
-        frictionvalues: Optional[str] = Field(
-            "Space separated list of roughness values; their meaning depends on the roughness "
-            + "types selected (only used if frictionTypes specified).",
-            alias="frictionValues",
-        )
-
-    comments: Comments = Comments()
-
-    type: Literal["yz"] = Field("yz")
-    singlevaluedz: Optional[bool] = Field(alias="singleValuedZ")
-    yzcount: int = Field(alias="yzCount")
-    ycoordinates: List[float] = Field(alias="yCoordinates")
-    zcoordinates: List[float] = Field(alias="zCoordinates")
-    conveyance: Optional[str] = Field("segmented")
-    sectioncount: Optional[int] = Field(1, alias="sectionCount")
-    frictionpositions: Optional[List[float]] = Field(alias="frictionPositions")
-    frictionids: Optional[List[str]] = Field(alias="frictionIds", delimiter=";")
-    frictiontypes: Optional[List[FrictionType]] = Field(
-        alias="frictionTypes", delimiter=";"
-    )
-    frictionvalues: Optional[List[float]] = Field(alias="frictionValues")
-
-    _split_to_list = get_split_string_on_delimiter_validator(
-        "ycoordinates",
-        "zcoordinates",
-        "frictionpositions",
-        "frictionvalues",
-        "frictionids",
-        "frictiontypes",
-    )
-
-    @root_validator(allow_reuse=True)
-    def check_list_lengths_coordinates(cls, values):
-        """Validates that the length of the ycoordinates and zcoordinates fields are as expected."""
-        return validate_correct_length(
-            values,
-            "ycoordinates",
-            "zcoordinates",
-            length_name="yzcount",
-        )
-
-    @root_validator(allow_reuse=True)
-    def check_list_lengths_friction(cls, values):
-        """Validates that the length of the frictionids, frictiontypes and frictionvalues field are as expected."""
-        return validate_correct_length(
-            values,
-            "frictionids",
-            "frictiontypes",
-            "frictionvalues",
-            length_name="sectioncount",
-        )
-
-    @root_validator(allow_reuse=True)
-    def check_list_length_frictionpositions(cls, values):
-        """Validates that the length of the frictionpositions field is as expected."""
-        return validate_correct_length(
-            values,
-            "frictionpositions",
-            length_name="sectioncount",
-            length_incr=1,  # 1 extra for frictionpositions
-        )
-
-    _friction_validator = CrossSectionDefinition._get_friction_root_validator(
-        "frictionids", "frictiontypes", "frictionvalues"
-    )
-    _frictiontype_validator = get_enum_validator("frictiontypes", enum=FrictionType)
-
-
-class XYZCrsDef(YZCrsDef, CrossSectionDefinition):
-    """
-    Crosssection definition with `type=xyz`, to be included in a crossdef file.
-    Typically inside the definition list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.crossdeffile.definition[..]`
-
-    All lowercased attributes match with the xyz input as described in
-    [UM Sec.C.16.1.5](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.16.1.5).
-
-    This class extends the YZCrsDef class with x-coordinates and an optional
-    branchId field. Most other attributes are inherited, but the coordcount
-    is overridden under the Pydantic alias "xyzCount".
-
-    Attributes:
-        yzcount (Optional[int]): dummy attribute that should not be set nor used.
-            Only present to mask the inherited attribute from parent class YZCrsDef.
-        xyzcount (int): Number of XYZ-coordinates. Always use this instead of yzcount.
-    """
-
-    class Comments(YZCrsDef.Comments):
-        type: Optional[str] = Field("Cross section type; must read xyz", alias="type")
-        branchid: Optional[str] = Field(
-            "Branch on which the cross section is located.", alias="branchId"
-        )
-        xyzcount: Optional[str] = Field("Number of XYZ-coordinates.", alias="xyzCount")
-        xCoordinates: Optional[str] = Field(
-            "Space separated list of x-coordinates [m or degrees East].",
-            alias="xCoordinates",
-        )
-        yCoordinates: Optional[str] = Field(
-            "Space separated list of y-coordinates [m or degrees North].",
-            alias="yCoordinates",
-        )
-        zCoordinates: Optional[str] = Field(
-            "Space separated list of z-coordinates [m AD].",
-            alias="zCoordinates",
-        )
-
-    comments: Comments = Comments()
-
-    type: Literal["xyz"] = Field("xyz")
-    branchid: Optional[str] = Field(alias="branchId")
-    yzcount: Optional[int] = Field(
-        alias="yzCount"
-    )  # Trick to not inherit parent's yzcount required field.
-    xyzcount: int = Field(alias="xyzCount")
-    xcoordinates: List[float] = Field(alias="xCoordinates")
-
-    _split_to_list0 = get_split_string_on_delimiter_validator(
-        "xcoordinates",
-    )
-
-    @validator("xyzcount")
-    @classmethod
-    def validate_xyzcount_without_yzcount(cls, field_value: int, values: dict) -> int:
-        """
-        Validates whether this XYZCrsDef does have attribute xyzcount,
-        but not the parent class's yzcount.
-
-        Args:
-            field_value (Optional[Path]): Value given for xyzcount.
-            values (dict): Dictionary of values already validated.
-
-        Raises:
-            ValueError: When yzcount is present.
-
-        Returns:
-            int: The value given for xyzcount.
-        """
-        # Retrieve the algorithm value (if not found use 0).
-        yzcount_value = values.get("yzcount")
-        if field_value is not None and yzcount_value is not None:
-            # yzcount should not be set, when xyzcount is set.
-            raise ValueError(
-                f"xyz cross section definition should not contain field yzCount (rather: xyzCount), current value: {yzcount_value}."
-            )
-        return field_value
-
-    @root_validator(allow_reuse=True)
-    def check_list_lengths_coordinates(cls, values):
-        """Validates that the length of the xcoordinates, ycoordinates and zcoordinates field are as expected."""
-        return validate_correct_length(
-            values,
-            "xcoordinates",
-            "ycoordinates",
-            "zcoordinates",
-            length_name="xyzcount",
-        )
-
-
-class CrossSection(INIBasedModel):
-    """
-    A `[CrossSection]` block for use inside a crosssection location file,
-    i.e., a [CrossLocModel][hydrolib.core.dflowfm.crosssection.models.CrossLocModel].
-
-    Attributes:
-        id (str): Unique cross-section location id.
-        branchid (str, optional): Branch on which the cross section is located.
-        chainage (str, optional): Chainage on the branch (m).
-        x (str, optional): x-coordinate of the location of the cross section.
-        y (str, optional): y-coordinate of the location of the cross section.
-        shift (float, optional): Vertical shift of the cross section definition [m]. Defined positive upwards.
-        definitionid (str): Id of cross section definition.
-    """
-
-    class Comments(INIBasedModel.Comments):
-        id: Optional[str] = "Unique cross-section location id."
-        branchid: Optional[str] = Field(
-            "Branch on which the cross section is located.", alias="branchId"
-        )
-        chainage: Optional[str] = "Chainage on the branch (m)."
-
-        x: Optional[str] = Field(
-            "x-coordinate of the location of the cross section.",
-        )
-        y: Optional[str] = Field(
-            "y-coordinate of the location of the cross section.",
-        )
-        shift: Optional[str] = Field(
-            "Vertical shift of the cross section definition [m]. Defined positive upwards.",
-        )
-        definitionid: Optional[str] = Field(
-            "Id of cross section definition.", alias="definitionId"
-        )
-
-    comments: Comments = Comments()
-
-    _header: Literal["CrossSection"] = "CrossSection"
-    id: str = Field(alias="id")
-
-    branchid: Optional[str] = Field(None, alias="branchId")
-    chainage: Optional[float] = Field(None)
-
-    x: Optional[float] = Field(None)
-    y: Optional[float] = Field(None)
-
-    shift: Optional[float] = Field(0.0)
-    definitionid: str = Field(alias="definitionId")
-
-    @root_validator(allow_reuse=True)
-    def validate_that_location_specification_is_correct(cls, values: Dict) -> Dict:
-        """Validates that the correct location specification is given."""
-        return validate_location_specification(
-            values,
-            config=LocationValidationConfiguration(
-                validate_node=False, validate_num_coordinates=False
-            ),
-            fields=LocationValidationFieldNames(x_coordinates="x", y_coordinates="y"),
-        )
-
-
-class CrossLocModel(INIModel):
-    """
-    The overall crosssection location model that contains the contents of one crossloc file.
-
-    This model is typically referenced under a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.crosslocfile`.
-
-    Attributes:
-        general (CrossLocGeneral): `[General]` block with file metadata.
-        crosssection (List[CrossSection]): List of `[CrossSection]` blocks for all cross section locations.
-    """
-
-    general: CrossLocGeneral = CrossLocGeneral()
-    crosssection: List[CrossSection] = []
-
-    @classmethod
-    def _filename(cls) -> str:
-        return "crsloc"
+import logging
+from typing import Dict, List, Literal, Optional
+
+from pydantic import Field, root_validator
+from pydantic.class_validators import validator
+
+from hydrolib.core.dflowfm.friction.models import FrictionType
+from hydrolib.core.dflowfm.ini.models import INIBasedModel, INIGeneral, INIModel
+from hydrolib.core.dflowfm.ini.util import (
+    LocationValidationConfiguration,
+    LocationValidationFieldNames,
+    get_enum_validator,
+    get_from_subclass_defaults,
+    get_split_string_on_delimiter_validator,
+    make_list_validator,
+    validate_correct_length,
+    validate_location_specification,
+)
+
+logger = logging.getLogger(__name__)
+
+frictionid_description = 'Name of the roughness variable associated with \
+    this cross section. Either this parameter or \
+    frictionType should be specified. If neither \
+    parameter is specified, the frictionId defaults \
+    to "Main".'
+
+frictiontype_description = "Roughness type associated with this cross section \
+    Either this parameter or frictionId should be specified."
+
+frictionvalue_description = "Roughness value; its meaning depends on the roughness type selected \
+    (only used if frictionType specified)."
+
+
+class CrossDefGeneral(INIGeneral):
+    """The crosssection definition file's `[General]` section with file meta data."""
+
+    fileversion: str = Field("3.00", alias="fileVersion")
+    filetype: Literal["crossDef"] = Field("crossDef", alias="fileType")
+
+
+class CrossLocGeneral(INIGeneral):
+    """The crosssection location file's `[General]` section with file meta data."""
+
+    fileversion: str = Field("3.00", alias="fileVersion")
+    filetype: Literal["crossLoc"] = Field("crossLoc", alias="fileType")
+
+
+class CrossSectionDefinition(INIBasedModel):
+    """
+    A `[Definition]` block for use inside a crosssection definition file,
+    i.e., a [CrossDefModel][hydrolib.core.dflowfm.crosssection.models.CrossDefModel].
+
+    This class is intended as an abstract class: various subclasses should
+    define they actual types of crosssection definitions.
+    """
+
+    # TODO: would we want to load this from something externally and generate these automatically
+    class Comments(INIBasedModel.Comments):
+        id: Optional[str] = "Unique cross-section definition id."
+        thalweg: Optional[str] = Field(
+            "Transverse Y coordinate at which the cross section aligns with the branch "
+            + "(Keyword used by GUI only)."
+        )
+
+    comments: Comments = Comments()
+
+    _header: Literal["Definition"] = "Definition"
+
+    id: str = Field(alias="id")
+    type: str = Field(alias="type")
+    thalweg: Optional[float]
+
+    def _get_identifier(self, data: dict) -> Optional[str]:
+        return data.get("id")
+
+    @classmethod
+    def _duplicate_keys_as_list(cls):
+        return True
+
+    @validator("type", pre=True)
+    def _validate_type(cls, value):
+        return get_from_subclass_defaults(CrossSectionDefinition, "type", value)
+
+    @classmethod
+    def validate(cls, v):
+        """Try to initialize subclass based on the `type` field.
+        This field is compared to each `type` field of the derived models of `CrossSectionDefinition`.
+        The derived model with an equal crosssection definition type will be initialized.
+
+        Raises:
+            ValueError: When the given type is not a known crosssection definition type.
+        """
+
+        # should be replaced by discriminated unions once merged
+        # https://github.com/samuelcolvin/pydantic/pull/2336
+        if isinstance(v, dict):
+            for c in cls.__subclasses__():
+                if (
+                    c.__fields__.get("type").default.lower()
+                    == v.get("type", "").lower()
+                ):
+                    v = c(**v)
+                    break
+            else:
+                raise ValueError(
+                    f"Type of {cls.__name__} with id={v.get('id', '')} and type={v.get('type', '')} is not recognized."
+                )
+        return super().validate(v)
+
+    @staticmethod
+    def _get_friction_root_validator(
+        frictionid_attr: str,
+        frictiontype_attr: str,
+        frictionvalue_attr: str,
+    ):
+        """
+        Make a root_validator that verifies whether the crosssection definition (subclass)
+        has a valid friction specification.
+        Supposed to be embedded in subclasses for their friction fields.
+
+        Args:
+            frictionid_attr: name of the frictionid attribute in the subclass.
+            frictiontype_attr: name of the frictiontype attribute in the subclass.
+            frictionvalue_attr: name of the frictionvalue attribute in the subclass.
+
+        Returns:
+            root_validator: to be embedded in the subclass that needs it.
+        """
+
+        def validate_friction_specification(cls, values):
+            """
+            The actual validator function.
+
+            Args:
+            cls: The subclass for which the root_validator is called.
+            values (dict): Dictionary of values to create a CrossSectionDefinition subclass.
+            """
+            frictionid = values.get(frictionid_attr) or ""
+            frictiontype = values.get(frictiontype_attr) or ""
+            frictionvalue = values.get(frictionvalue_attr) or ""
+
+            if frictionid != "" and (frictiontype != "" or frictionvalue != ""):
+                raise ValueError(
+                    f"Cross section has duplicate friction specification (both {frictionid_attr} and {frictiontype_attr}/{frictionvalue_attr})."
+                )
+
+            return values
+
+        return root_validator(allow_reuse=True)(validate_friction_specification)
+
+
+class CrossDefModel(INIModel):
+    """
+    The overall crosssection definition model that contains the contents of one crossdef file.
+
+    This model is typically referenced under a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.crossdeffile`.
+
+    Attributes:
+        general (CrossdefGeneral): `[General]` block with file metadata.
+        definition (List[CrossSectionDefinition]): List of `[Definition]` blocks for all cross sections.
+    """
+
+    general: CrossDefGeneral = CrossDefGeneral()
+    definition: List[CrossSectionDefinition] = []
+
+    _make_list = make_list_validator("definition")
+
+    @classmethod
+    def _filename(cls) -> str:
+        return "crsdef"
+
+
+class CircleCrsDef(CrossSectionDefinition):
+    """
+    Crosssection definition with `type=circle`, to be included in a crossdef file.
+    Typically inside the definition list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.crossdeffile.definition[..]`
+
+    All lowercased attributes match with the circle input as described in
+    [UM Sec.C.16.1.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.16.1.1).
+    """
+
+    class Comments(CrossSectionDefinition.Comments):
+        type: Optional[str] = Field("Cross section type; must read circle")
+
+        diameter: Optional[str] = Field("Internal diameter of the circle [m].")
+        frictionid: Optional[str] = Field(
+            frictionid_description,
+            alias="frictionId",
+        )
+        frictiontype: Optional[str] = Field(
+            frictiontype_description,
+            alias="frictionType",
+        )
+        frictionvalue: Optional[str] = Field(
+            frictionvalue_description,
+            alias="frictionValue",
+        )
+
+    comments: Comments = Comments()
+
+    type: Literal["circle"] = Field("circle")
+    diameter: float
+    frictionid: Optional[str] = Field(alias="frictionId")
+    frictiontype: Optional[FrictionType] = Field(alias="frictionType")
+    frictionvalue: Optional[float] = Field(alias="frictionValue")
+
+    _friction_validator = CrossSectionDefinition._get_friction_root_validator(
+        "frictionid", "frictiontype", "frictionvalue"
+    )
+    _frictiontype_validator = get_enum_validator("frictiontype", enum=FrictionType)
+
+
+class RectangleCrsDef(CrossSectionDefinition):
+    """
+    Crosssection definition with `type=rectangle`, to be included in a crossdef file.
+    Typically inside the definition list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.crossdeffile.definition[..]`
+
+    All lowercased attributes match with the rectangle input as described in
+    [UM Sec.C.16.1.2](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.16.1.2).
+    """
+
+    class Comments(CrossSectionDefinition.Comments):
+        type: Optional[str] = Field("Cross section type; must read rectangle")
+        width: Optional[str] = Field("Width of the rectangle [m].")
+        height: Optional[str] = Field("Height of the rectangle [m].")
+        closed: Optional[str] = Field("no: Open channel, yes: Closed channel.")
+        frictionid: Optional[str] = Field(
+            frictionid_description,
+            alias="frictionId",
+        )
+        frictiontype: Optional[str] = Field(
+            frictiontype_description,
+            alias="frictionType",
+        )
+        frictionvalue: Optional[str] = Field(
+            frictionvalue_description,
+            alias="frictionValue",
+        )
+
+    comments: Comments = Comments()
+
+    type: Literal["rectangle"] = Field("rectangle")
+    width: float
+    height: float
+    closed: bool = Field(True)
+    frictionid: Optional[str] = Field(alias="frictionId")
+    frictiontype: Optional[FrictionType] = Field(alias="frictionType")
+    frictionvalue: Optional[float] = Field(alias="frictionValue")
+
+    _friction_validator = CrossSectionDefinition._get_friction_root_validator(
+        "frictionid", "frictiontype", "frictionvalue"
+    )
+    _frictiontype_validator = get_enum_validator("frictiontype", enum=FrictionType)
+
+
+class ZWRiverCrsDef(CrossSectionDefinition):
+    """
+    Crosssection definition with `type=zwRiver`, to be included in a crossdef file.
+    Typically inside the definition list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.crossdeffile.definition[..]`
+
+    All lowercased attributes match with the zwRiver input as described in
+    [UM Sec.C.16.1.3](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.16.1.3).
+    """
+
+    class Comments(CrossSectionDefinition.Comments):
+        type: Optional[str] = Field(
+            "Cross section type; must read zwRiver", alias="type"
+        )
+        numlevels: Optional[str] = Field(
+            "Number of levels in the table.", alias="numLevels"
+        )
+        levels: Optional[str] = Field(
+            "Space separated list of monotonic increasing heights/levels [m AD].",
+            alias="levels",
+        )
+        flowwidths: Optional[str] = Field(
+            "Space separated list of flow widths at the selected heights [m)].",
+            alias="flowWidths",
+        )
+        totalwidths: Optional[str] = Field(
+            "Space separated list of total widths at the selected heights [m]. "
+            "Equal to flowWidths if not specified. If specified, the totalWidths"
+            "should be larger than flowWidths.",
+            alias="totalWidths",
+        )
+        leveecrestLevel: Optional[str] = Field(
+            "Crest level of levee [m AD].", alias="leveeCrestlevel"
+        )
+        leveebaselevel: Optional[str] = Field(
+            "Base level of levee [m AD].", alias="leveeBaseLevel"
+        )
+        leveeflowarea: Optional[str] = Field(
+            "Flow area behind levee [m2].", alias="leveeFlowArea"
+        )
+        leveetotalarea: Optional[str] = Field(
+            "Total area behind levee [m2].", alias="leveeTotalArea"
+        )
+        mainwidth: Optional[str] = Field(
+            "Width of main section [m]. Default value: max(flowWidths).",
+            alias="mainWidth",
+        )
+        fp1width: Optional[str] = Field(
+            "Width of floodplain 1 section [m]. Default value: max(flowWidths)-mainWidth",
+            alias="fp1Width",
+        )
+        fp2width: Optional[str] = Field(
+            "Width of floodplain 2 section [m]. Default value: max(flowWidths)-mainWidth-fp1Width",
+            alias="fp2Width",
+        )
+        frictionids: Optional[str] = Field(
+            "Semicolon separated list of roughness variable names associated with the roughness "
+            "sections. Either this parameter or frictionTypes should be specified. If neither "
+            'parameter is specified, the frictionIds default to "Main", "FloodPlain1" '
+            'and "FloodPlain2".',
+            alias="frictionIds",
+        )
+        frictiontypes: Optional[str] = Field(
+            "Semicolon separated list of roughness types associated with the roughness sections. "
+            "Either this parameter or frictionIds should be specified. Can be specified as a "
+            "single value if all roughness sections use the same type.",
+            alias="frictionTypes",
+        )
+        frictionvalues: Optional[str] = Field(
+            "Space separated list of roughness values; their meaning depends on the roughness "
+            "types selected (only used if frictionTypes specified).",
+            alias="frictionValues",
+        )
+
+    comments: Comments = Comments()
+
+    type: Literal["zwRiver"] = Field("zwRiver")
+    numlevels: int = Field(alias="numLevels")
+    levels: List[float]
+    flowwidths: List[float] = Field(alias="flowWidths")
+    totalwidths: Optional[List[float]] = Field(alias="totalWidths")
+    leveecrestLevel: Optional[float] = Field(alias="leveeCrestlevel")
+    leveebaselevel: Optional[float] = Field(alias="leveeBaseLevel")
+    leveeflowarea: Optional[float] = Field(alias="leveeFlowArea")
+    leveetotalrea: Optional[float] = Field(alias="leveeTotalArea")
+    mainwidth: Optional[float] = Field(alias="mainWidth")
+    fp1width: Optional[float] = Field(alias="fp1Width")
+    fp2width: Optional[float] = Field(alias="fp2Width")
+    frictionids: Optional[List[str]] = Field(alias="frictionIds", delimiter=";")
+    frictiontypes: Optional[List[FrictionType]] = Field(
+        alias="frictionTypes", delimiter=";"
+    )
+    frictionvalues: Optional[List[float]] = Field(alias="frictionValues")
+
+    _split_to_list = get_split_string_on_delimiter_validator(
+        "levels",
+        "flowwidths",
+        "totalwidths",
+        "frictionvalues",
+        "frictionids",
+        "frictiontypes",
+    )
+
+    _friction_validator = CrossSectionDefinition._get_friction_root_validator(
+        "frictionids", "frictiontypes", "frictionvalues"
+    )
+    _frictiontype_validator = get_enum_validator("frictiontypes", enum=FrictionType)
+
+    @root_validator(allow_reuse=True)
+    def check_list_lengths(cls, values):
+        """Validates that the length of the levels, flowwidths and totalwidths fields are as expected."""
+        return validate_correct_length(
+            values,
+            "levels",
+            "flowwidths",
+            "totalwidths",
+            length_name="numlevels",
+        )
+
+
+class ZWCrsDef(CrossSectionDefinition):
+    """
+    Crosssection definition with `type=zw`, to be included in a crossdef file.
+    Typically inside the definition list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.crossdeffile.definition[..]`
+
+    All lowercased attributes match with the zw input as described in
+    [UM Sec.C.16.1.4](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.16.1.4).
+    """
+
+    class Comments(CrossSectionDefinition.Comments):
+        type: Optional[str] = Field("Cross section type; must read zw", alias="type")
+        # NOTE: Field "template" deliberately ignored for now.
+        numlevels: Optional[str] = Field(
+            "Number of levels in the table.", alias="numLevels"
+        )
+        levels: Optional[str] = Field(
+            "Space separated list of monotonic increasing heights/levels [m AD].",
+            alias="levels",
+        )
+        flowwidths: Optional[str] = Field(
+            "Space separated list of flow widths at the selected heights [m)].",
+            alias="flowWidths",
+        )
+        totalwidths: Optional[str] = Field(
+            "Space separated list of total widths at the selected heights [m]. "
+            "Equal to flowWidths if not specified. If specified, the totalWidths"
+            "should be larger than flowWidths.",
+            alias="totalWidths",
+        )
+        frictionid: Optional[str] = Field(
+            frictionid_description,
+            alias="frictionId",
+        )
+        frictiontype: Optional[str] = Field(
+            frictiontype_description,
+            alias="frictionType",
+        )
+        frictionvalue: Optional[str] = Field(
+            frictionvalue_description,
+            alias="frictionValue",
+        )
+
+    comments: Comments = Comments()
+
+    type: Literal["zw"] = Field("zw")
+    numlevels: int = Field(alias="numLevels")
+    levels: List[float]
+    flowwidths: List[float] = Field(alias="flowWidths")
+    totalwidths: Optional[List[float]] = Field(alias="totalWidths")
+    frictionid: Optional[str] = Field(alias="frictionId")
+    frictiontype: Optional[FrictionType] = Field(alias="frictionType")
+    frictionvalue: Optional[float] = Field(alias="frictionValue")
+
+    _split_to_list = get_split_string_on_delimiter_validator(
+        "levels",
+        "flowwidths",
+        "totalwidths",
+    )
+
+    @root_validator(allow_reuse=True)
+    def check_list_lengths(cls, values):
+        """Validates that the length of the levels, flowwidths and totalwidths fields are as expected."""
+        return validate_correct_length(
+            values,
+            "levels",
+            "flowwidths",
+            "totalwidths",
+            length_name="numlevels",
+        )
+
+    _friction_validator = CrossSectionDefinition._get_friction_root_validator(
+        "frictionid", "frictiontype", "frictionvalue"
+    )
+    _frictiontype_validator = get_enum_validator("frictiontype", enum=FrictionType)
+
+
+class YZCrsDef(CrossSectionDefinition):
+    """
+    Crosssection definition with `type=yz`, to be included in a crossdef file.
+    Typically inside the definition list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.crossdeffile.definition[..]`
+
+    All lowercased attributes match with the yz input as described in
+    [UM Sec.C.16.1.6](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.16.1.6).
+    """
+
+    class Comments(CrossSectionDefinition.Comments):
+        type: Optional[str] = Field("Cross section type; must read yz", alias="type")
+        conveyance: Optional[str] = Field(
+            "lumped: Lumped, segmented: Vertically segmented. Only the default lumped "
+            + "option is allowed if singleValuedZ = no. In the case of lumped conveyance, "
+            + "only a single uniform roughness for the whole cross section is allowed, "
+            + "i.e., sectionCount must equal 1.",
+        )
+        yzcount: Optional[str] = Field("Number of YZ-coordinates.", alias="yzCount")
+        yCoordinates: Optional[str] = Field(
+            "Space separated list of monotonic increasing y-coordinates [m].",
+            alias="yCoordinates",
+        )
+        zCoordinates: Optional[str] = Field(
+            "Space separated list of single-valued z-coordinates [m AD].",
+            alias="zCoordinates",
+        )
+        sectioncount: Optional[str] = Field(
+            "Number of roughness sections. If the lumped conveyance is selected then "
+            + "sectionCount must equal 1.",
+            alias="sectionCount",
+        )
+        frictionpositions: Optional[str] = Field(
+            "Locations where the roughness sections start and end. Always one location more than "
+            + "sectionCount. The first value should equal 0 and the last value should equal the "
+            + "cross section length. Keyword may be skipped if sectionCount = 1.",
+            alias="frictionPositions",
+        )
+        frictionids: Optional[str] = Field(
+            "Semicolon separated list of roughness variable names associated with the roughness "
+            + "sections. Either this parameter or frictionTypes should be specified. If neither "
+            + 'parameter is specified, the frictionIds default to "Main", "FloodPlain1" '
+            + 'and "FloodPlain2".',
+            alias="frictionIds",
+        )
+        frictiontypes: Optional[str] = Field(
+            "Semicolon separated list of roughness types associated with the roughness sections. "
+            + "Either this parameter or frictionIds should be specified. Can be specified as a "
+            + "single value if all roughness sections use the same type.",
+            alias="frictionTypes",
+        )
+        frictionvalues: Optional[str] = Field(
+            "Space separated list of roughness values; their meaning depends on the roughness "
+            + "types selected (only used if frictionTypes specified).",
+            alias="frictionValues",
+        )
+
+    comments: Comments = Comments()
+
+    type: Literal["yz"] = Field("yz")
+    singlevaluedz: Optional[bool] = Field(alias="singleValuedZ")
+    yzcount: int = Field(alias="yzCount")
+    ycoordinates: List[float] = Field(alias="yCoordinates")
+    zcoordinates: List[float] = Field(alias="zCoordinates")
+    conveyance: Optional[str] = Field("segmented")
+    sectioncount: Optional[int] = Field(1, alias="sectionCount")
+    frictionpositions: Optional[List[float]] = Field(alias="frictionPositions")
+    frictionids: Optional[List[str]] = Field(alias="frictionIds", delimiter=";")
+    frictiontypes: Optional[List[FrictionType]] = Field(
+        alias="frictionTypes", delimiter=";"
+    )
+    frictionvalues: Optional[List[float]] = Field(alias="frictionValues")
+
+    _split_to_list = get_split_string_on_delimiter_validator(
+        "ycoordinates",
+        "zcoordinates",
+        "frictionpositions",
+        "frictionvalues",
+        "frictionids",
+        "frictiontypes",
+    )
+
+    @root_validator(allow_reuse=True)
+    def check_list_lengths_coordinates(cls, values):
+        """Validates that the length of the ycoordinates and zcoordinates fields are as expected."""
+        return validate_correct_length(
+            values,
+            "ycoordinates",
+            "zcoordinates",
+            length_name="yzcount",
+        )
+
+    @root_validator(allow_reuse=True)
+    def check_list_lengths_friction(cls, values):
+        """Validates that the length of the frictionids, frictiontypes and frictionvalues field are as expected."""
+        return validate_correct_length(
+            values,
+            "frictionids",
+            "frictiontypes",
+            "frictionvalues",
+            length_name="sectioncount",
+        )
+
+    @root_validator(allow_reuse=True)
+    def check_list_length_frictionpositions(cls, values):
+        """Validates that the length of the frictionpositions field is as expected."""
+        return validate_correct_length(
+            values,
+            "frictionpositions",
+            length_name="sectioncount",
+            length_incr=1,  # 1 extra for frictionpositions
+        )
+
+    _friction_validator = CrossSectionDefinition._get_friction_root_validator(
+        "frictionids", "frictiontypes", "frictionvalues"
+    )
+    _frictiontype_validator = get_enum_validator("frictiontypes", enum=FrictionType)
+
+
+class XYZCrsDef(YZCrsDef, CrossSectionDefinition):
+    """
+    Crosssection definition with `type=xyz`, to be included in a crossdef file.
+    Typically inside the definition list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.crossdeffile.definition[..]`
+
+    All lowercased attributes match with the xyz input as described in
+    [UM Sec.C.16.1.5](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.16.1.5).
+
+    This class extends the YZCrsDef class with x-coordinates and an optional
+    branchId field. Most other attributes are inherited, but the coordcount
+    is overridden under the Pydantic alias "xyzCount".
+
+    Attributes:
+        yzcount (Optional[int]): dummy attribute that should not be set nor used.
+            Only present to mask the inherited attribute from parent class YZCrsDef.
+        xyzcount (int): Number of XYZ-coordinates. Always use this instead of yzcount.
+    """
+
+    class Comments(YZCrsDef.Comments):
+        type: Optional[str] = Field("Cross section type; must read xyz", alias="type")
+        branchid: Optional[str] = Field(
+            "Branch on which the cross section is located.", alias="branchId"
+        )
+        xyzcount: Optional[str] = Field("Number of XYZ-coordinates.", alias="xyzCount")
+        xCoordinates: Optional[str] = Field(
+            "Space separated list of x-coordinates [m or degrees East].",
+            alias="xCoordinates",
+        )
+        yCoordinates: Optional[str] = Field(
+            "Space separated list of y-coordinates [m or degrees North].",
+            alias="yCoordinates",
+        )
+        zCoordinates: Optional[str] = Field(
+            "Space separated list of z-coordinates [m AD].",
+            alias="zCoordinates",
+        )
+
+    comments: Comments = Comments()
+
+    type: Literal["xyz"] = Field("xyz")
+    branchid: Optional[str] = Field(alias="branchId")
+    yzcount: Optional[int] = Field(
+        alias="yzCount"
+    )  # Trick to not inherit parent's yzcount required field.
+    xyzcount: int = Field(alias="xyzCount")
+    xcoordinates: List[float] = Field(alias="xCoordinates")
+
+    _split_to_list0 = get_split_string_on_delimiter_validator(
+        "xcoordinates",
+    )
+
+    @validator("xyzcount")
+    @classmethod
+    def validate_xyzcount_without_yzcount(cls, field_value: int, values: dict) -> int:
+        """
+        Validates whether this XYZCrsDef does have attribute xyzcount,
+        but not the parent class's yzcount.
+
+        Args:
+            field_value (Optional[Path]): Value given for xyzcount.
+            values (dict): Dictionary of values already validated.
+
+        Raises:
+            ValueError: When yzcount is present.
+
+        Returns:
+            int: The value given for xyzcount.
+        """
+        # Retrieve the algorithm value (if not found use 0).
+        yzcount_value = values.get("yzcount")
+        if field_value is not None and yzcount_value is not None:
+            # yzcount should not be set, when xyzcount is set.
+            raise ValueError(
+                f"xyz cross section definition should not contain field yzCount (rather: xyzCount), current value: {yzcount_value}."
+            )
+        return field_value
+
+    @root_validator(allow_reuse=True)
+    def check_list_lengths_coordinates(cls, values):
+        """Validates that the length of the xcoordinates, ycoordinates and zcoordinates field are as expected."""
+        return validate_correct_length(
+            values,
+            "xcoordinates",
+            "ycoordinates",
+            "zcoordinates",
+            length_name="xyzcount",
+        )
+
+
+class CrossSection(INIBasedModel):
+    """
+    A `[CrossSection]` block for use inside a crosssection location file,
+    i.e., a [CrossLocModel][hydrolib.core.dflowfm.crosssection.models.CrossLocModel].
+
+    Attributes:
+        id (str): Unique cross-section location id.
+        branchid (str, optional): Branch on which the cross section is located.
+        chainage (str, optional): Chainage on the branch (m).
+        x (str, optional): x-coordinate of the location of the cross section.
+        y (str, optional): y-coordinate of the location of the cross section.
+        shift (float, optional): Vertical shift of the cross section definition [m]. Defined positive upwards.
+        definitionid (str): Id of cross section definition.
+    """
+
+    class Comments(INIBasedModel.Comments):
+        id: Optional[str] = "Unique cross-section location id."
+        branchid: Optional[str] = Field(
+            "Branch on which the cross section is located.", alias="branchId"
+        )
+        chainage: Optional[str] = "Chainage on the branch (m)."
+
+        x: Optional[str] = Field(
+            "x-coordinate of the location of the cross section.",
+        )
+        y: Optional[str] = Field(
+            "y-coordinate of the location of the cross section.",
+        )
+        shift: Optional[str] = Field(
+            "Vertical shift of the cross section definition [m]. Defined positive upwards.",
+        )
+        definitionid: Optional[str] = Field(
+            "Id of cross section definition.", alias="definitionId"
+        )
+
+    comments: Comments = Comments()
+
+    _header: Literal["CrossSection"] = "CrossSection"
+    id: str = Field(alias="id")
+
+    branchid: Optional[str] = Field(None, alias="branchId")
+    chainage: Optional[float] = Field(None)
+
+    x: Optional[float] = Field(None)
+    y: Optional[float] = Field(None)
+
+    shift: Optional[float] = Field(0.0)
+    definitionid: str = Field(alias="definitionId")
+
+    @root_validator(allow_reuse=True)
+    def validate_that_location_specification_is_correct(cls, values: Dict) -> Dict:
+        """Validates that the correct location specification is given."""
+        return validate_location_specification(
+            values,
+            config=LocationValidationConfiguration(
+                validate_node=False, validate_num_coordinates=False
+            ),
+            fields=LocationValidationFieldNames(x_coordinates="x", y_coordinates="y"),
+        )
+
+
+class CrossLocModel(INIModel):
+    """
+    The overall crosssection location model that contains the contents of one crossloc file.
+
+    This model is typically referenced under a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.crosslocfile`.
+
+    Attributes:
+        general (CrossLocGeneral): `[General]` block with file metadata.
+        crosssection (List[CrossSection]): List of `[CrossSection]` blocks for all cross section locations.
+    """
+
+    general: CrossLocGeneral = CrossLocGeneral()
+    crosssection: List[CrossSection] = []
+
+    @classmethod
+    def _filename(cls) -> str:
+        return "crsloc"
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/ext/models.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/ext/models.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,315 +1,315 @@
-from enum import Enum
-from pathlib import Path
-from typing import Dict, List, Literal, Optional, Union
-
-from pydantic import Field, root_validator, validator
-
-from hydrolib.core.basemodel import (
-    DiskOnlyFileModel,
-    validator_set_default_disk_only_file_model_when_none,
-)
-from hydrolib.core.dflowfm.bc.models import ForcingBase, ForcingData, ForcingModel
-from hydrolib.core.dflowfm.ini.models import (
-    INIBasedModel,
-    INIGeneral,
-    INIModel,
-    INISerializerConfig,
-)
-from hydrolib.core.dflowfm.ini.serializer import INISerializerConfig
-from hydrolib.core.dflowfm.ini.util import (
-    LocationValidationConfiguration,
-    get_enum_validator,
-    get_split_string_on_delimiter_validator,
-    make_list_validator,
-    validate_location_specification,
-)
-from hydrolib.core.dflowfm.polyfile.models import PolyFile
-from hydrolib.core.dflowfm.tim.models import TimModel
-from hydrolib.core.utils import str_is_empty_or_none
-
-
-class Boundary(INIBasedModel):
-    """
-    A `[Boundary]` block for use inside an external forcings file,
-    i.e., a [ExtModel][hydrolib.core.dflowfm.ext.models.ExtModel].
-
-    All lowercased attributes match with the boundary input as described in
-    [UM Sec.C.5.2.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.5.2.1).
-    """
-
-    _disk_only_file_model_should_not_be_none = (
-        validator_set_default_disk_only_file_model_when_none()
-    )
-
-    _header: Literal["Boundary"] = "Boundary"
-    quantity: str = Field(alias="quantity")
-    nodeid: Optional[str] = Field(alias="nodeId")
-    locationfile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="locationFile"
-    )
-    forcingfile: ForcingModel = Field(alias="forcingFile")
-    bndwidth1d: Optional[float] = Field(alias="bndWidth1D")
-    bndbldepth: Optional[float] = Field(alias="bndBlDepth")
-
-    def is_intermediate_link(self) -> bool:
-        return True
-
-    @classmethod
-    def _is_valid_locationfile_data(
-        cls, elem: Union[None, str, Path, DiskOnlyFileModel]
-    ) -> bool:
-        return isinstance(elem, Path) or (
-            isinstance(elem, DiskOnlyFileModel) and elem.filepath is not None
-        )
-
-    @root_validator
-    @classmethod
-    def check_nodeid_or_locationfile_present(cls, values: Dict):
-        """
-        Verifies that either nodeid or locationfile properties have been set.
-
-        Args:
-            values (Dict): Dictionary with values already validated.
-
-        Raises:
-            ValueError: When none of the values are present.
-
-        Returns:
-            Dict: Validated dictionary of values for Boundary.
-        """
-        node_id = values.get("nodeid", None)
-        location_file = values.get("locationfile", None)
-        if str_is_empty_or_none(node_id) and not cls._is_valid_locationfile_data(
-            location_file
-        ):
-            raise ValueError(
-                "Either nodeId or locationFile fields should be specified."
-            )
-        return values
-
-    def _get_identifier(self, data: dict) -> Optional[str]:
-        """
-        Retrieves the identifier for a boundary, which is the nodeid
-
-        Args:
-            data (dict): Dictionary of values for this boundary.
-
-        Returns:
-            str: The nodeid value or None if not found.
-        """
-        return data.get("nodeid")
-
-    @property
-    def forcing(self) -> ForcingBase:
-        """Retrieves the corresponding forcing data for this boundary.
-
-        Returns:
-            ForcingBase: The corresponding forcing data. None when this boundary does not have a forcing file or when the data cannot be found.
-        """
-
-        if self.forcingfile is None:
-            return None
-
-        for forcing in self.forcingfile.forcing:
-
-            if self.nodeid != forcing.name:
-                continue
-
-            for quantity in forcing.quantityunitpair:
-                if quantity.quantity.startswith(self.quantity):
-                    return forcing
-
-        return None
-
-
-class Lateral(INIBasedModel):
-    """
-    A `[Lateral]` block for use inside an external forcings file,
-    i.e., a [ExtModel][hydrolib.core.dflowfm.ext.models.ExtModel].
-
-    All lowercased attributes match with the lateral input as described in
-    [UM Sec.C.5.2.2](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.5.2.2).
-    """
-
-    _header: Literal["Lateral"] = "Lateral"
-    id: str = Field(alias="id")
-    name: str = Field("", alias="name")
-    locationtype: Optional[str] = Field(alias="locationType")
-    nodeid: Optional[str] = Field(alias="nodeId")
-    branchid: Optional[str] = Field(alias="branchId")
-    chainage: Optional[float] = Field(alias="chainage")
-    numcoordinates: Optional[int] = Field(alias="numCoordinates")
-    xcoordinates: Optional[List[float]] = Field(alias="xCoordinates")
-    ycoordinates: Optional[List[float]] = Field(alias="yCoordinates")
-    discharge: ForcingData = Field(alias="discharge")
-
-    def is_intermediate_link(self) -> bool:
-        return True
-
-    _split_to_list = get_split_string_on_delimiter_validator(
-        "xcoordinates", "ycoordinates"
-    )
-
-    @root_validator(allow_reuse=True)
-    def validate_that_location_specification_is_correct(cls, values: Dict) -> Dict:
-        """Validates that the correct location specification is given."""
-        return validate_location_specification(
-            values, config=LocationValidationConfiguration(minimum_num_coordinates=1)
-        )
-
-    def _get_identifier(self, data: dict) -> Optional[str]:
-        return data.get("id") or data.get("name")
-
-    @validator("locationtype")
-    @classmethod
-    def validate_location_type(cls, v: str) -> str:
-        """
-        Method to validate whether the specified location type is correct.
-
-        Args:
-            v (str): Given value for the locationtype field.
-
-        Raises:
-            ValueError: When the value given for locationtype is unknown.
-
-        Returns:
-            str: Validated locationtype string.
-        """
-        possible_values = ["1d", "2d", "all"]
-        if v.lower() not in possible_values:
-            raise ValueError(
-                "Value given ({}) not accepted, should be one of: {}".format(
-                    v, ", ".join(possible_values)
-                )
-            )
-        return v
-
-
-class MeteoForcingFileType(str, Enum):
-    """
-    Enum class containing the valid values for the forcingFileType
-    attribute in Meteo class.
-    """
-
-    bcascii = "bcAscii"
-    """str: Space-uniform time series in <*.bc> file."""
-
-    netcdf = "netcdf"
-    """str: NetCDF, either with gridded data, or multiple station time series."""
-
-    uniform = "uniform"
-    """str: Space-uniform time series in <*.tim> file."""
-
-    allowedvaluestext = "Possible values: bcAscii, netcdf, uniform."
-
-
-class MeteoInterpolationMethod(str, Enum):
-    """
-    Enum class containing the valid values for the interpolationMethod
-    attribute in Meteo class.
-    """
-
-    nearestnb = "nearestNb"
-    """str: Nearest-neighbour interpolation, only with station-data in forcingFileType=netcdf"""
-
-    allowedvaluestext = "Possible values: nearestNb (only with station data in forcingFileType=netcdf ). "
-
-
-class Meteo(INIBasedModel):
-    """
-    A `[Meteo]` block for use inside an external forcings file,
-    i.e., a [ExtModel][hydrolib.core.dflowfm.ext.models.ExtModel].
-
-    All lowercased attributes match with the meteo input as described in
-    [UM Sec.C.5.2.3](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.5.2.3).
-    """
-
-    class Comments(INIBasedModel.Comments):
-        quantity: Optional[str] = Field(
-            "Name of the quantity. See UM Section C.5.3", alias="quantity"
-        )
-        forcingfile: Optional[str] = Field(
-            "Name of file containing the forcing for this meteo quantity.",
-            alias="forcingFile",
-        )
-        forcingfiletype: Optional[str] = Field(
-            "Type of forcingFile.", alias="forcingFileType"
-        )
-        targetmaskfile: Optional[str] = Field(
-            "Name of <*.pol> file to be used as mask. Grid parts inside any polygon will receive the meteo forcing.",
-            alias="targetMaskFile",
-        )
-        targetmaskinvert: Optional[str] = Field(
-            "Flag indicating whether the target mask should be inverted, i.e., outside of all polygons: no or yes.",
-            alias="targetMaskInvert",
-        )
-        interpolationmethod: Optional[str] = Field(
-            "Type of (spatial) interpolation.", alias="interpolationMethod"
-        )
-
-    comments: Comments = Comments()
-
-    _disk_only_file_model_should_not_be_none = (
-        validator_set_default_disk_only_file_model_when_none()
-    )
-
-    _header: Literal["Meteo"] = "Meteo"
-    quantity: str = Field(alias="quantity")
-    forcingfile: Union[TimModel, ForcingModel, DiskOnlyFileModel] = Field(
-        alias="forcingFile"
-    )
-    forcingfiletype: MeteoForcingFileType = Field(alias="forcingFileType")
-    targetmaskfile: Optional[PolyFile] = Field(None, alias="targetMaskFile")
-    targetmaskinvert: Optional[bool] = Field(None, alias="targetMaskInvert")
-    interpolationmethod: Optional[MeteoInterpolationMethod] = Field(
-        alias="interpolationMethod"
-    )
-
-    def is_intermediate_link(self) -> bool:
-        return True
-
-    forcingfiletype_validator = get_enum_validator(
-        "forcingfiletype", enum=MeteoForcingFileType
-    )
-    interpolationmethod_validator = get_enum_validator(
-        "interpolationmethod", enum=MeteoInterpolationMethod
-    )
-
-
-class ExtGeneral(INIGeneral):
-    """The external forcing file's `[General]` section with file meta data."""
-
-    _header: Literal["General"] = "General"
-    fileversion: str = Field("2.01", alias="fileVersion")
-    filetype: Literal["extForce"] = Field("extForce", alias="fileType")
-
-
-class ExtModel(INIModel):
-    """
-    The overall external forcings model that contains the contents of one external forcings file (new format).
-
-    This model is typically referenced under a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.external_forcing.extforcefilenew`.
-
-    Attributes:
-        general (ExtGeneral): `[General]` block with file metadata.
-        boundary (List[Boundary]): List of `[Boundary]` blocks for all boundary conditions.
-        lateral (List[Lateral]): List of `[Lateral]` blocks for all lateral discharges.
-        meteo (List[Meteo]): List of `[Meteo]` blocks for all meteorological forcings.
-    """
-
-    general: ExtGeneral = ExtGeneral()
-    boundary: List[Boundary] = []
-    lateral: List[Lateral] = []
-    meteo: List[Meteo] = []
-    serializer_config: INISerializerConfig = INISerializerConfig(
-        section_indent=0, property_indent=0
-    )
-    _split_to_list = make_list_validator("boundary", "lateral", "meteo")
-
-    @classmethod
-    def _ext(cls) -> str:
-        return ".ext"
-
-    @classmethod
-    def _filename(cls) -> str:
-        return "bnd"
+from enum import Enum
+from pathlib import Path
+from typing import Dict, List, Literal, Optional, Union
+
+from pydantic import Field, root_validator, validator
+
+from hydrolib.core.basemodel import (
+    DiskOnlyFileModel,
+    validator_set_default_disk_only_file_model_when_none,
+)
+from hydrolib.core.dflowfm.bc.models import ForcingBase, ForcingData, ForcingModel
+from hydrolib.core.dflowfm.ini.models import (
+    INIBasedModel,
+    INIGeneral,
+    INIModel,
+    INISerializerConfig,
+)
+from hydrolib.core.dflowfm.ini.serializer import INISerializerConfig
+from hydrolib.core.dflowfm.ini.util import (
+    LocationValidationConfiguration,
+    get_enum_validator,
+    get_split_string_on_delimiter_validator,
+    make_list_validator,
+    validate_location_specification,
+)
+from hydrolib.core.dflowfm.polyfile.models import PolyFile
+from hydrolib.core.dflowfm.tim.models import TimModel
+from hydrolib.core.utils import str_is_empty_or_none
+
+
+class Boundary(INIBasedModel):
+    """
+    A `[Boundary]` block for use inside an external forcings file,
+    i.e., a [ExtModel][hydrolib.core.dflowfm.ext.models.ExtModel].
+
+    All lowercased attributes match with the boundary input as described in
+    [UM Sec.C.5.2.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.5.2.1).
+    """
+
+    _disk_only_file_model_should_not_be_none = (
+        validator_set_default_disk_only_file_model_when_none()
+    )
+
+    _header: Literal["Boundary"] = "Boundary"
+    quantity: str = Field(alias="quantity")
+    nodeid: Optional[str] = Field(alias="nodeId")
+    locationfile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="locationFile"
+    )
+    forcingfile: ForcingModel = Field(alias="forcingFile")
+    bndwidth1d: Optional[float] = Field(alias="bndWidth1D")
+    bndbldepth: Optional[float] = Field(alias="bndBlDepth")
+
+    def is_intermediate_link(self) -> bool:
+        return True
+
+    @classmethod
+    def _is_valid_locationfile_data(
+        cls, elem: Union[None, str, Path, DiskOnlyFileModel]
+    ) -> bool:
+        return isinstance(elem, Path) or (
+            isinstance(elem, DiskOnlyFileModel) and elem.filepath is not None
+        )
+
+    @root_validator
+    @classmethod
+    def check_nodeid_or_locationfile_present(cls, values: Dict):
+        """
+        Verifies that either nodeid or locationfile properties have been set.
+
+        Args:
+            values (Dict): Dictionary with values already validated.
+
+        Raises:
+            ValueError: When none of the values are present.
+
+        Returns:
+            Dict: Validated dictionary of values for Boundary.
+        """
+        node_id = values.get("nodeid", None)
+        location_file = values.get("locationfile", None)
+        if str_is_empty_or_none(node_id) and not cls._is_valid_locationfile_data(
+            location_file
+        ):
+            raise ValueError(
+                "Either nodeId or locationFile fields should be specified."
+            )
+        return values
+
+    def _get_identifier(self, data: dict) -> Optional[str]:
+        """
+        Retrieves the identifier for a boundary, which is the nodeid
+
+        Args:
+            data (dict): Dictionary of values for this boundary.
+
+        Returns:
+            str: The nodeid value or None if not found.
+        """
+        return data.get("nodeid")
+
+    @property
+    def forcing(self) -> ForcingBase:
+        """Retrieves the corresponding forcing data for this boundary.
+
+        Returns:
+            ForcingBase: The corresponding forcing data. None when this boundary does not have a forcing file or when the data cannot be found.
+        """
+
+        if self.forcingfile is None:
+            return None
+
+        for forcing in self.forcingfile.forcing:
+
+            if self.nodeid != forcing.name:
+                continue
+
+            for quantity in forcing.quantityunitpair:
+                if quantity.quantity.startswith(self.quantity):
+                    return forcing
+
+        return None
+
+
+class Lateral(INIBasedModel):
+    """
+    A `[Lateral]` block for use inside an external forcings file,
+    i.e., a [ExtModel][hydrolib.core.dflowfm.ext.models.ExtModel].
+
+    All lowercased attributes match with the lateral input as described in
+    [UM Sec.C.5.2.2](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.5.2.2).
+    """
+
+    _header: Literal["Lateral"] = "Lateral"
+    id: str = Field(alias="id")
+    name: str = Field("", alias="name")
+    locationtype: Optional[str] = Field(alias="locationType")
+    nodeid: Optional[str] = Field(alias="nodeId")
+    branchid: Optional[str] = Field(alias="branchId")
+    chainage: Optional[float] = Field(alias="chainage")
+    numcoordinates: Optional[int] = Field(alias="numCoordinates")
+    xcoordinates: Optional[List[float]] = Field(alias="xCoordinates")
+    ycoordinates: Optional[List[float]] = Field(alias="yCoordinates")
+    discharge: ForcingData = Field(alias="discharge")
+
+    def is_intermediate_link(self) -> bool:
+        return True
+
+    _split_to_list = get_split_string_on_delimiter_validator(
+        "xcoordinates", "ycoordinates"
+    )
+
+    @root_validator(allow_reuse=True)
+    def validate_that_location_specification_is_correct(cls, values: Dict) -> Dict:
+        """Validates that the correct location specification is given."""
+        return validate_location_specification(
+            values, config=LocationValidationConfiguration(minimum_num_coordinates=1)
+        )
+
+    def _get_identifier(self, data: dict) -> Optional[str]:
+        return data.get("id") or data.get("name")
+
+    @validator("locationtype")
+    @classmethod
+    def validate_location_type(cls, v: str) -> str:
+        """
+        Method to validate whether the specified location type is correct.
+
+        Args:
+            v (str): Given value for the locationtype field.
+
+        Raises:
+            ValueError: When the value given for locationtype is unknown.
+
+        Returns:
+            str: Validated locationtype string.
+        """
+        possible_values = ["1d", "2d", "all"]
+        if v.lower() not in possible_values:
+            raise ValueError(
+                "Value given ({}) not accepted, should be one of: {}".format(
+                    v, ", ".join(possible_values)
+                )
+            )
+        return v
+
+
+class MeteoForcingFileType(str, Enum):
+    """
+    Enum class containing the valid values for the forcingFileType
+    attribute in Meteo class.
+    """
+
+    bcascii = "bcAscii"
+    """str: Space-uniform time series in <*.bc> file."""
+
+    netcdf = "netcdf"
+    """str: NetCDF, either with gridded data, or multiple station time series."""
+
+    uniform = "uniform"
+    """str: Space-uniform time series in <*.tim> file."""
+
+    allowedvaluestext = "Possible values: bcAscii, netcdf, uniform."
+
+
+class MeteoInterpolationMethod(str, Enum):
+    """
+    Enum class containing the valid values for the interpolationMethod
+    attribute in Meteo class.
+    """
+
+    nearestnb = "nearestNb"
+    """str: Nearest-neighbour interpolation, only with station-data in forcingFileType=netcdf"""
+
+    allowedvaluestext = "Possible values: nearestNb (only with station data in forcingFileType=netcdf ). "
+
+
+class Meteo(INIBasedModel):
+    """
+    A `[Meteo]` block for use inside an external forcings file,
+    i.e., a [ExtModel][hydrolib.core.dflowfm.ext.models.ExtModel].
+
+    All lowercased attributes match with the meteo input as described in
+    [UM Sec.C.5.2.3](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.5.2.3).
+    """
+
+    class Comments(INIBasedModel.Comments):
+        quantity: Optional[str] = Field(
+            "Name of the quantity. See UM Section C.5.3", alias="quantity"
+        )
+        forcingfile: Optional[str] = Field(
+            "Name of file containing the forcing for this meteo quantity.",
+            alias="forcingFile",
+        )
+        forcingfiletype: Optional[str] = Field(
+            "Type of forcingFile.", alias="forcingFileType"
+        )
+        targetmaskfile: Optional[str] = Field(
+            "Name of <*.pol> file to be used as mask. Grid parts inside any polygon will receive the meteo forcing.",
+            alias="targetMaskFile",
+        )
+        targetmaskinvert: Optional[str] = Field(
+            "Flag indicating whether the target mask should be inverted, i.e., outside of all polygons: no or yes.",
+            alias="targetMaskInvert",
+        )
+        interpolationmethod: Optional[str] = Field(
+            "Type of (spatial) interpolation.", alias="interpolationMethod"
+        )
+
+    comments: Comments = Comments()
+
+    _disk_only_file_model_should_not_be_none = (
+        validator_set_default_disk_only_file_model_when_none()
+    )
+
+    _header: Literal["Meteo"] = "Meteo"
+    quantity: str = Field(alias="quantity")
+    forcingfile: Union[TimModel, ForcingModel, DiskOnlyFileModel] = Field(
+        alias="forcingFile"
+    )
+    forcingfiletype: MeteoForcingFileType = Field(alias="forcingFileType")
+    targetmaskfile: Optional[PolyFile] = Field(None, alias="targetMaskFile")
+    targetmaskinvert: Optional[bool] = Field(None, alias="targetMaskInvert")
+    interpolationmethod: Optional[MeteoInterpolationMethod] = Field(
+        alias="interpolationMethod"
+    )
+
+    def is_intermediate_link(self) -> bool:
+        return True
+
+    forcingfiletype_validator = get_enum_validator(
+        "forcingfiletype", enum=MeteoForcingFileType
+    )
+    interpolationmethod_validator = get_enum_validator(
+        "interpolationmethod", enum=MeteoInterpolationMethod
+    )
+
+
+class ExtGeneral(INIGeneral):
+    """The external forcing file's `[General]` section with file meta data."""
+
+    _header: Literal["General"] = "General"
+    fileversion: str = Field("2.01", alias="fileVersion")
+    filetype: Literal["extForce"] = Field("extForce", alias="fileType")
+
+
+class ExtModel(INIModel):
+    """
+    The overall external forcings model that contains the contents of one external forcings file (new format).
+
+    This model is typically referenced under a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.external_forcing.extforcefilenew`.
+
+    Attributes:
+        general (ExtGeneral): `[General]` block with file metadata.
+        boundary (List[Boundary]): List of `[Boundary]` blocks for all boundary conditions.
+        lateral (List[Lateral]): List of `[Lateral]` blocks for all lateral discharges.
+        meteo (List[Meteo]): List of `[Meteo]` blocks for all meteorological forcings.
+    """
+
+    general: ExtGeneral = ExtGeneral()
+    boundary: List[Boundary] = []
+    lateral: List[Lateral] = []
+    meteo: List[Meteo] = []
+    serializer_config: INISerializerConfig = INISerializerConfig(
+        section_indent=0, property_indent=0
+    )
+    _split_to_list = make_list_validator("boundary", "lateral", "meteo")
+
+    @classmethod
+    def _ext(cls) -> str:
+        return ".ext"
+
+    @classmethod
+    def _filename(cls) -> str:
+        return "bnd"
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/extold/models.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/extold/models.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,574 +1,574 @@
-from enum import Enum, IntEnum
-from pathlib import Path
-from typing import Any, Callable, Dict, List, Optional, Union
-
-from pydantic import Field, root_validator, validator
-
-from hydrolib.core.basemodel import (
-    BaseModel,
-    DiskOnlyFileModel,
-    ModelSaveSettings,
-    ParsableFileModel,
-    SerializerConfig,
-)
-from hydrolib.core.dflowfm.common.models import Operand
-from hydrolib.core.dflowfm.extold.parser import Parser
-from hydrolib.core.dflowfm.extold.serializer import Serializer
-from hydrolib.core.dflowfm.polyfile.models import PolyFile
-from hydrolib.core.dflowfm.tim.models import TimModel
-
-HEADER = """
- QUANTITY    : waterlevelbnd, velocitybnd, dischargebnd, tangentialvelocitybnd, normalvelocitybnd  filetype=9         method=2,3
-             : outflowbnd, neumannbnd, qhbnd, uxuyadvectionvelocitybnd                             filetype=9         method=2,3
-             : salinitybnd                                                                         filetype=9         method=2,3
-             : gateloweredgelevel, damlevel, pump                                                  filetype=9         method=2,3
-             : frictioncoefficient, horizontaleddyviscositycoefficient, advectiontype              filetype=4,7,10    method=4
-             : bedlevel, ibotlevtype                                                               filetype=4,7,10    method=4..9
-             : initialwaterlevel                                                                   filetype=4,7,10,12 method=4..9
-             : initialtemperature                                                                  filetype=4,7,10,12 method=4..9
-             : initialvelocityx, initialvelocityy                                                  filetype=4,7,10,12 method=4..9
-             : initialvelocity                                                                     filetype=12        method=4..9
-             : initialsalinity, initialsalinitytop: use initialsalinity for depth-uniform, or
-             : as bed level value in combination with initialsalinitytop                           filetype=4,7,10    method=4
-             : initialverticaltemperatureprofile                                                   filetype=9,10      method=
-             : initialverticalsalinityprofile                                                      filetype=9,10      method=
-             : windx, windy, windxy, rainfall, atmosphericpressure                                 filetype=1,2,4,6,7,8 method=1,2,3
-             : shiptxy, movingstationtxy                                                           filetype=1         method=1
-             : discharge_salinity_temperature_sorsin                                               filetype=9         method=1
-             : windstresscoefficient                                                               filetype=4,7,10    method=4
-             : nudge_salinity_temperature                                                          filetype=11        method=3
-
- kx = Vectormax = Nr of variables specified on the same time/space frame. Eg. Wind magnitude,direction: kx = 2
- FILETYPE=1  : uniform              kx = 1 value               1 dim array      uni
- FILETYPE=2  : unimagdir            kx = 2 values              1 dim array,     uni mag/dir transf to u,v, in index 1,2
- FILETYPE=3  : svwp                 kx = 3 fields  u,v,p       3 dim array      nointerpolation
- FILETYPE=4  : arcinfo              kx = 1 field               2 dim array      bilin/direct
- FILETYPE=5  : spiderweb            kx = 3 fields              3 dim array      bilin/spw
- FILETYPE=6  : curvi                kx = ?                                      bilin/findnm
- FILETYPE=7  : triangulation        kx = 1 field               1 dim array      triangulation
- FILETYPE=8  : triangulation_magdir kx = 2 fields consisting of Filetype=2      triangulation in (wind) stations
-
- FILETYPE=9  : polyline             kx = 1 For polyline points i= 1 through N specify boundary signals, either as
-                                           timeseries or Fourier components or tidal constituents
-                                           Timeseries are in files *_000i.tim, two columns: time (min)  values
-                                           Fourier components and or tidal constituents are in files *_000i.cmp, three columns
-                                           period (min) or constituent name (e.g. M2), amplitude and phase (deg)
-                                           If no file is specified for a node, its value will be interpolated from surrounding nodes
-                                           If only one signal file is specified, the boundary gets a uniform signal
-                                           For a dischargebnd, only one signal file must be specified
-
- FILETYPE=10 : inside_polygon       kx = 1 field                                uniform value inside polygon for INITIAL fields
- FILETYPE=11 : ncgrid               kx = 1 field                    2 dim array      triangulation (should have proper standard_name in var, e.g., 'precipitation')
- FILETYPE=12 : ncflow (map file)    kx = 1 or 2 field               1 dim array      triangulation
- FILETYPE=14 : ncwave (com file)    kx = 1 field                    1 dim array      triangulation
-
- METHOD  =0  : provider just updates, another provider that pointers to this one does the actual interpolation
-         =1  : intp space and time (getval) keep  2 meteofields in memory
-         =2  : first intp space (update), next intp. time (getval) keep 2 flowfields in memory
-         =3  : save weightfactors, intp space and time (getval),   keep 2 pointer- and weight sets in memory.
-         =4  : only spatial, inside polygon
-         =5  : only spatial, triangulation, (if samples from *.asc file then bilinear)
-         =6  : only spatial, averaging
-         =7  : only spatial, index triangulation
-         =8  : only spatial, smoothing
-         =9  : only spatial, internal diffusion
-         =10 : only initial vertical profiles
-
- OPERAND =O  : Override at all points
-         =+  : Add to previously specified value
-         =*  : Multiply with previously specified value
-         =A  : Apply only if no value specified previously (For Initial fields, similar to Quickin preserving best data specified first)
-         =X  : MAX with prev. spec.
-         =N  : MIN with prev. spec.
-
- EXTRAPOLATION_METHOD (ONLY WHEN METHOD=3)
-         = 0 : No spatial extrapolation.
-         = 1 : Do spatial extrapolation outside of source data bounding box.
-
- MAXSEARCHRADIUS (ONLY WHEN EXTRAPOLATION_METHOD=1)
-         = search radius (in m) for model grid points that lie outside of the source data bounding box.
-
- AVERAGINGTYPE (ONLY WHEN METHOD=6)
-         =1  : SIMPLE AVERAGING
-         =2  : NEAREST NEIGHBOUR
-         =3  : MAX (HIGHEST)
-         =4  : MIN (LOWEST)
-         =5  : INVERSE WEIGHTED DISTANCE-AVERAGE
-         =6  : MINABS
-         =7  : KDTREE (LIKE 1, BUT FAST AVERAGING)
-
- RELATIVESEARCHCELLSIZE : For METHOD=6, the relative search cell size for samples inside cell (default: 1.01)
-
- PERCENTILEMINMAX : (ONLY WHEN AVERAGINGTYPE=3 or 4) Changes the min/max operator to an average of the
-               highest/lowest data points. The value sets the percentage of the total set that is to be included.
-
- NUMMIN  =   : For METHOD=6, minimum required number of source data points in each target cell.
-
- VALUE   =   : Offset value for this provider
-
- FACTOR  =   : Conversion factor for this provider
-
-*************************************************************************************************************
-"""
-
-
-class ExtOldTracerQuantity(str, Enum):
-    """Enum class containing the valid values for the boundary conditions category
-    of the external forcings that are specific to tracers.
-    """
-
-    TracerBnd = "tracerbnd"
-    """User-defined tracer"""
-    InitialTracer = "initialtracer"
-    """Initial tracer"""
-
-
-class ExtOldQuantity(str, Enum):
-    """Enum class containing the valid values for the boundary conditions category
-    of the external forcings.
-    """
-
-    # Boundary conditions
-    WaterLevelBnd = "waterlevelbnd"
-    """Water level"""
-    NeumannBnd = "neumannbnd"
-    """Water level gradient"""
-    RiemannBnd = "riemannbnd"
-    """Riemann invariant"""
-    OutflowBnd = "outflowbnd"
-    """Outflow"""
-    VelocityBnd = "velocitybnd"
-    """Velocity"""
-    DischargeBnd = "dischargebnd"
-    """Discharge"""
-    RiemannVelocityBnd = "riemann_velocitybnd"
-    """Riemann invariant velocity"""
-    SalinityBnd = "salinitybnd"
-    """Salinity"""
-    TemperatureBnd = "temperaturebnd"
-    """Temperature"""
-    SedimentBnd = "sedimentbnd"
-    """Suspended sediment"""
-    UXUYAdvectionVelocityBnd = "uxuyadvectionvelocitybnd"
-    """ux-uy advection velocity"""
-    NormalVelocityBnd = "normalvelocitybnd"
-    """Normal velocity"""
-    TangentialVelocityBnd = "tangentialvelocitybnd"
-    """Tangentional velocity"""
-    QhBnd = "qhbnd"
-    """Discharge-water level dependency"""
-
-    # Meteorological fields
-    WindX = "windx"
-    """Wind x component"""
-    WindY = "windy"
-    """Wind y component"""
-    WindXY = "windxy"
-    """Wind vector"""
-    AirPressureWindXWindY = "airpressure_windx_windy"
-    """Atmospheric pressure and wind components"""
-    AirPressureWindXWindYCharnock = "airpressure_windx_windy_charnock"
-    "Atmospheric pressure and wind components Charnock"
-    AtmosphericPressure = "atmosphericpressure"
-    """Atmospheric pressure"""
-    Rainfall = "rainfall"
-    """Precipitation"""
-    RainfallRate = "rainfall_rate"
-    """Precipitation"""
-    HumidityAirTemperatureCloudiness = "humidity_airtemperature_cloudiness"
-    """Combined heat flux terms"""
-    HumidityAirTemperatureCloudinessSolarRadiation = (
-        "humidity_airtemperature_cloudiness_solarradiation"
-    )
-    """Combined heat flux terms"""
-    DewPointAirTemperatureCloudiness = "dewpoint_airtemperature_cloudiness"
-    """Dew point air temperature cloudiness"""
-    LongWaveRadiation = "longwaveradiation"
-    """Long wave radiation"""
-    SolarRadiation = "solarradiation"
-    """Solar radiation"""
-    DischargeSalinityTemperatureSorSin = "discharge_salinity_temperature_sorsin"
-    """Discharge, salinity temperature source-sinks"""
-    NudgeSalinityTemperature = "nudge_salinity_temperature"
-    """Nudging salinity and temperature"""
-
-    # Structure parameters
-    Pump = "pump"
-    """Pump capacity"""
-    DamLevel = "damlevel"
-    """Dam level"""
-    GateLowerEdgeLevel = "gateloweredgelevel"
-    """Gate lower edge level"""
-    GeneralStructure = "generalstructure"
-    """General structure"""
-
-    # Initial fields
-    InitialWaterLevel = "initialwaterlevel"
-    """Initial water level"""
-    InitialSalinity = "initialsalinity"
-    """Initial salinity"""
-    InitialSalinityTop = "initialsalinitytop"
-    """Initial salinity top layer"""
-    InitialTemperature = "initialtemperature"
-    """Initial temperature"""
-    InitialVerticalTemperatureProfile = "initialverticaltemperatureprofile"
-    """Initial vertical temperature profile"""
-    InitialVerticalSalinityProfile = "initialverticalsalinityprofile"
-    """Initial vertical salinity profile"""
-    BedLevel = "bedlevel"
-    """Bed level"""
-
-    # Spatial physical properties
-    FrictionCoefficient = "frictioncoefficient"
-    """Friction coefficient"""
-    HorizontalEddyViscosityCoefficient = "horizontaleddyviscositycoefficient"
-    """Horizontal eddy viscosity coefficient"""
-    InternalTidesFrictionCoefficient = "internaltidesfrictioncoefficient"
-    """Internal tides friction coefficient"""
-    HorizontalEddyDiffusivityCoefficient = "horizontaleddydiffusivitycoefficient"
-    """Horizontal eddy diffusivity coefficient"""
-    AdvectionType = "advectiontype"
-    """Type of advection scheme"""
-    IBotLevType = "ibotlevtype"
-    """Type of bed-level handling"""
-
-    # Miscellaneous
-    ShiptXY = "shiptxy"
-    """shiptxy"""
-    MovingStationXY = "movingstationxy"
-    """Moving observation point for output (time, x, y)"""
-    WaveSignificantHeight = "wavesignificantheight"
-    """Wave significant height"""
-    WavePeriod = "waveperiod"
-    """Wave period"""
-
-
-class ExtOldFileType(IntEnum):
-    """Enum class containing the valid values for the `filetype` attribute
-    in the [ExtForcing][hydrolib.core.dflowfm.extold.models.ExtForcing] class.
-    """
-
-    TimeSeries = 1
-    """1. Time series"""
-    TimeSeriesMagnitudeAndDirection = 2
-    """2. Time series magnitude and direction"""
-    SpatiallyVaryingWindPressure = 3
-    """3. Spatially varying wind and pressure"""
-    ArcInfo = 4
-    """4. ArcInfo"""
-    SpiderWebData = 5
-    """5. Spiderweb data (cyclones)"""
-    CurvilinearData = 6
-    """6. Space-time data on curvilinear grid"""
-    Samples = 7
-    """7. Samples"""
-    TriangulationMagnitudeAndDirection = 8
-    """8. Triangulation magnitude and direction"""
-    Polyline = 9
-    """9. Polyline (<*.pli>-file)"""
-    NetCDFGridData = 11
-    """11. NetCDF grid data (e.g. meteo fields)"""
-    NetCDFWaveData = 14
-    """14. NetCDF wave data"""
-
-
-class ExtOldMethod(IntEnum):
-    """Enum class containing the valid values for the `method` attribute
-    in the [ExtForcing][hydrolib.core.dflowfm.extold.models.ExtForcing] class.
-    """
-
-    PassThrough = 1
-    """1. Pass through (no interpolation)"""
-    InterpolateTimeAndSpace = 2
-    """2. Interpolate time and space"""
-    InterpolateTimeAndSpaceSaveWeights = 3
-    """3. Interpolate time and space, save weights"""
-    InterpolateSpace = 4
-    """4. Interpolate space"""
-    InterpolateTime = 5
-    """5. Interpolate time"""
-    AveragingSpace = 6
-    """6. Averaging in space"""
-    InterpolateExtrapolateTime = 7
-    """7. Interpolate/Extrapolate time"""
-
-
-class ExtOldExtrapolationMethod(IntEnum):
-    """Enum class containing the valid values for the `extrapolation_method` attribute
-    in the [ExtForcing][hydrolib.core.dflowfm.extold.models.ExtForcing] class.
-    """
-
-    NoSpatialExtrapolation = 0
-    """0. No spatial extrapolation."""
-    SpatialExtrapolationOutsideOfSourceDataBoundingBox = 1
-    """1. Do spatial extrapolation outside of source data bounding box."""
-
-
-class ExtOldForcing(BaseModel):
-    """Class holding the external forcing values."""
-
-    quantity: Union[ExtOldQuantity, str] = Field(alias="QUANTITY")
-    """Union[Quantity, str]: The name of the quantity."""
-
-    filename: Union[PolyFile, TimModel, DiskOnlyFileModel] = Field(
-        None, alias="FILENAME"
-    )
-    """Union[PolyFile, TimModel, DiskOnlyFileModel]: The file associated to this forcing."""
-
-    varname: Optional[str] = Field(None, alias="VARNAME")
-    """Optional[str]: The variable name used in `filename` associated with this forcing; some input files may contain multiple variables."""
-
-    sourcemask: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="SOURCEMASK"
-    )
-    """DiskOnlyFileModel: The file containing a mask."""
-
-    filetype: ExtOldFileType = Field(alias="FILETYPE")
-    """FileType: Indication of the file type.
-    
-    Options:
-    1. Time series
-    2. Time series magnitude and direction
-    3. Spatially varying weather
-    4. ArcInfo
-    5. Spiderweb data (cyclones)
-    6. Curvilinear data
-    7. Samples (C.3)
-    8. Triangulation magnitude and direction
-    9. Polyline (<*.pli>-file, C.2)
-    11. NetCDF grid data (e.g. meteo fields)
-    14. NetCDF wave data
-    """
-
-    method: ExtOldMethod = Field(alias="METHOD")
-    """ExtOldMethod: The method of interpolation.
-    
-    Options:
-    1. Pass through (no interpolation)
-    2. Interpolate time and space
-    3. Interpolate time and space, save weights
-    4. Interpolate space
-    5. Interpolate time
-    6. Averaging space
-    7. Interpolate/Extrapolate time
-    """
-
-    extrapolation_method: Optional[ExtOldExtrapolationMethod] = Field(
-        None, alias="EXTRAPOLATION_METHOD"
-    )
-    """Optional[ExtOldExtrapolationMethod]: The extrapolation method.
-
-    Options:
-    0. No spatial extrapolation.
-    1. Do spatial extrapolation outside of source data bounding box.
-    """
-
-    maxsearchradius: Optional[float] = Field(None, alias="MAXSEARCHRADIUS")
-    """Optional[float]: Search radius (in m) for model grid points that lie outside of the source data bounding box."""
-
-    operand: Operand = Field(alias="OPERAND")
-    """Operand: The operand to use for adding the provided values.
-    
-    Options:    
-    'O' Existing values are overwritten with the provided values.
-    'A' Provided values are used where existing values are missing.
-    '+' Existing values are summed with the provided values.
-    '*' Existing values are multiplied with the provided values.
-    'X' The maximum values of the existing values and provided values are used.
-    'N' The minimum values of the existing values and provided values are used.
-    """
-
-    value: Optional[float] = Field(None, alias="VALUE")
-    """Optional[float]: Custom coefficients for transformation."""
-
-    factor: Optional[float] = Field(None, alias="FACTOR")
-    """Optional[float]: The conversion factor."""
-
-    ifrctyp: Optional[float] = Field(None, alias="IFRCTYP")
-    """Optional[float]: The friction type."""
-
-    averagingtype: Optional[float] = Field(None, alias="AVERAGINGTYPE")
-    """Optional[float]: The averaging type."""
-
-    relativesearchcellsize: Optional[float] = Field(
-        None, alias="RELATIVESEARCHCELLSIZE"
-    )
-    """Optional[float]: The relative search cell size for samples inside a cell."""
-
-    extrapoltol: Optional[float] = Field(None, alias="EXTRAPOLTOL")
-    """Optional[float]: The extrapolation tolerance."""
-
-    percentileminmax: Optional[float] = Field(None, alias="PERCENTILEMINMAX")
-    """Optional[float]: Changes the min/max operator to an average of the highest/lowest data points. The value sets the percentage of the total set that is to be included.."""
-
-    area: Optional[float] = Field(None, alias="AREA")
-    """Optional[float]: The area for sources and sinks."""
-
-    nummin: Optional[int] = Field(None, alias="NUMMIN")
-    """Optional[int]: The minimum required number of source data points in each target cell."""
-
-    @validator("quantity", pre=True)
-    def validate_quantity(cls, value):
-        if isinstance(value, ExtOldQuantity):
-            return value
-
-        def raise_error_tracer_name(quantity: ExtOldTracerQuantity):
-            raise ValueError(
-                f"QUANTITY '{quantity}' should be appended with a tracer name."
-            )
-
-        if isinstance(value, ExtOldTracerQuantity):
-            raise_error_tracer_name(value)
-
-        value_str = str(value)
-        lower_value = value_str.lower()
-
-        for tracer_quantity in ExtOldTracerQuantity:
-            if lower_value.startswith(tracer_quantity):
-                n = len(tracer_quantity)
-                if n == len(value_str):
-                    raise_error_tracer_name(tracer_quantity)
-                return tracer_quantity + value_str[n:]
-
-        if lower_value in list(ExtOldQuantity):
-            return ExtOldQuantity(lower_value)
-
-        supported_value_str = ", ".join(([x.value for x in ExtOldQuantity]))
-        raise ValueError(
-            f"QUANTITY '{value_str}' not supported. Supported values: {supported_value_str}"
-        )
-
-    @validator("operand", pre=True)
-    def validate_operand(cls, value):
-        if isinstance(value, Operand):
-            return value
-
-        if isinstance(value, str):
-
-            for operand in Operand:
-                if value.lower() == operand.value.lower():
-                    return operand
-
-            supported_value_str = ", ".join(([x.value for x in Operand]))
-            raise ValueError(
-                f"OPERAND '{value}' not supported. Supported values: {supported_value_str}"
-            )
-
-        return value
-
-    @root_validator(skip_on_failure=True)
-    def validate_forcing(cls, values):
-        class _Field:
-            def __init__(self, key: str) -> None:
-                self.alias = cls.__fields__[key].alias
-                self.value = values[key]
-
-        def raise_error_only_allowed_when(
-            field: _Field, dependency: _Field, valid_dependency_value: str
-        ):
-            error = f"{field.alias} only allowed when {dependency.alias} is {valid_dependency_value}"
-            raise ValueError(error)
-
-        def only_allowed_when(
-            field: _Field, dependency: _Field, valid_dependency_value: Any
-        ):
-            """This function checks if a particular field is allowed to have a value only when a dependency field has a specific value."""
-
-            if field.value is None or dependency.value == valid_dependency_value:
-                return
-
-            raise_error_only_allowed_when(field, dependency, valid_dependency_value)
-
-        quantity = _Field("quantity")
-        varname = _Field("varname")
-        sourcemask = _Field("sourcemask")
-        filetype = _Field("filetype")
-        method = _Field("method")
-        extrapolation_method = _Field("extrapolation_method")
-        maxsearchradius = _Field("maxsearchradius")
-        value = _Field("value")
-        factor = _Field("factor")
-        ifrctype = _Field("ifrctyp")
-        averagingtype = _Field("averagingtype")
-        relativesearchcellsize = _Field("relativesearchcellsize")
-        extrapoltol = _Field("extrapoltol")
-        percentileminmax = _Field("percentileminmax")
-        area = _Field("area")
-        nummin = _Field("nummin")
-
-        only_allowed_when(varname, filetype, ExtOldFileType.NetCDFGridData)
-
-        if sourcemask.value.filepath is not None and filetype.value not in [
-            ExtOldFileType.ArcInfo,
-            ExtOldFileType.CurvilinearData,
-        ]:
-            raise_error_only_allowed_when(
-                sourcemask, filetype, valid_dependency_value="4 or 6"
-            )
-
-        if (
-            extrapolation_method.value
-            == ExtOldExtrapolationMethod.SpatialExtrapolationOutsideOfSourceDataBoundingBox
-            and method.value != ExtOldMethod.InterpolateTimeAndSpaceSaveWeights
-        ):
-            error = f"{extrapolation_method.alias} only allowed to be 1 when {method.alias} is 3"
-            raise ValueError(error)
-
-        only_allowed_when(
-            maxsearchradius,
-            extrapolation_method,
-            ExtOldExtrapolationMethod.SpatialExtrapolationOutsideOfSourceDataBoundingBox,
-        )
-        only_allowed_when(value, method, ExtOldMethod.InterpolateSpace)
-
-        if factor.value is not None and not quantity.value.startswith(
-            ExtOldTracerQuantity.InitialTracer
-        ):
-            error = f"{factor.alias} only allowed when {quantity.alias} starts with {ExtOldTracerQuantity.InitialTracer}"
-            raise ValueError(error)
-
-        only_allowed_when(ifrctype, quantity, ExtOldQuantity.FrictionCoefficient)
-        only_allowed_when(averagingtype, method, ExtOldMethod.AveragingSpace)
-        only_allowed_when(relativesearchcellsize, method, ExtOldMethod.AveragingSpace)
-        only_allowed_when(extrapoltol, method, ExtOldMethod.InterpolateTime)
-        only_allowed_when(percentileminmax, method, ExtOldMethod.AveragingSpace)
-        only_allowed_when(
-            area, quantity, ExtOldQuantity.DischargeSalinityTemperatureSorSin
-        )
-        only_allowed_when(nummin, method, ExtOldMethod.AveragingSpace)
-
-        return values
-
-
-class ExtOldModel(ParsableFileModel):
-    """
-    The overall external forcings model that contains the contents of one external forcings file (old format).
-
-    This model is typically referenced under a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.external_forcing.extforcefile`.
-    """
-
-    comment: List[str] = HEADER.splitlines()[1:]
-    """List[str]: The comments in the header of the external forcing file."""
-    forcing: List[ExtOldForcing] = []
-    """List[ExtForcing]: The external forcing blocks in the external forcing file."""
-
-    @classmethod
-    def _ext(cls) -> str:
-        return ".ext"
-
-    @classmethod
-    def _filename(cls) -> str:
-        return "externalforcings"
-
-    def dict(self, *args, **kwargs):
-        return dict(comment=self.comment, forcing=[dict(f) for f in self.forcing])
-
-    @classmethod
-    def _get_serializer(
-        cls,
-    ) -> Callable[[Path, Dict, SerializerConfig, ModelSaveSettings], None]:
-        return Serializer.serialize
-
-    @classmethod
-    def _get_parser(cls) -> Callable[[Path], Dict]:
-        return Parser.parse
+from enum import Enum, IntEnum
+from pathlib import Path
+from typing import Any, Callable, Dict, List, Optional, Union
+
+from pydantic import Field, root_validator, validator
+
+from hydrolib.core.basemodel import (
+    BaseModel,
+    DiskOnlyFileModel,
+    ModelSaveSettings,
+    ParsableFileModel,
+    SerializerConfig,
+)
+from hydrolib.core.dflowfm.common.models import Operand
+from hydrolib.core.dflowfm.extold.parser import Parser
+from hydrolib.core.dflowfm.extold.serializer import Serializer
+from hydrolib.core.dflowfm.polyfile.models import PolyFile
+from hydrolib.core.dflowfm.tim.models import TimModel
+
+HEADER = """
+ QUANTITY    : waterlevelbnd, velocitybnd, dischargebnd, tangentialvelocitybnd, normalvelocitybnd  filetype=9         method=2,3
+             : outflowbnd, neumannbnd, qhbnd, uxuyadvectionvelocitybnd                             filetype=9         method=2,3
+             : salinitybnd                                                                         filetype=9         method=2,3
+             : gateloweredgelevel, damlevel, pump                                                  filetype=9         method=2,3
+             : frictioncoefficient, horizontaleddyviscositycoefficient, advectiontype              filetype=4,7,10    method=4
+             : bedlevel, ibotlevtype                                                               filetype=4,7,10    method=4..9
+             : initialwaterlevel                                                                   filetype=4,7,10,12 method=4..9
+             : initialtemperature                                                                  filetype=4,7,10,12 method=4..9
+             : initialvelocityx, initialvelocityy                                                  filetype=4,7,10,12 method=4..9
+             : initialvelocity                                                                     filetype=12        method=4..9
+             : initialsalinity, initialsalinitytop: use initialsalinity for depth-uniform, or
+             : as bed level value in combination with initialsalinitytop                           filetype=4,7,10    method=4
+             : initialverticaltemperatureprofile                                                   filetype=9,10      method=
+             : initialverticalsalinityprofile                                                      filetype=9,10      method=
+             : windx, windy, windxy, rainfall, atmosphericpressure                                 filetype=1,2,4,6,7,8 method=1,2,3
+             : shiptxy, movingstationtxy                                                           filetype=1         method=1
+             : discharge_salinity_temperature_sorsin                                               filetype=9         method=1
+             : windstresscoefficient                                                               filetype=4,7,10    method=4
+             : nudge_salinity_temperature                                                          filetype=11        method=3
+
+ kx = Vectormax = Nr of variables specified on the same time/space frame. Eg. Wind magnitude,direction: kx = 2
+ FILETYPE=1  : uniform              kx = 1 value               1 dim array      uni
+ FILETYPE=2  : unimagdir            kx = 2 values              1 dim array,     uni mag/dir transf to u,v, in index 1,2
+ FILETYPE=3  : svwp                 kx = 3 fields  u,v,p       3 dim array      nointerpolation
+ FILETYPE=4  : arcinfo              kx = 1 field               2 dim array      bilin/direct
+ FILETYPE=5  : spiderweb            kx = 3 fields              3 dim array      bilin/spw
+ FILETYPE=6  : curvi                kx = ?                                      bilin/findnm
+ FILETYPE=7  : triangulation        kx = 1 field               1 dim array      triangulation
+ FILETYPE=8  : triangulation_magdir kx = 2 fields consisting of Filetype=2      triangulation in (wind) stations
+
+ FILETYPE=9  : polyline             kx = 1 For polyline points i= 1 through N specify boundary signals, either as
+                                           timeseries or Fourier components or tidal constituents
+                                           Timeseries are in files *_000i.tim, two columns: time (min)  values
+                                           Fourier components and or tidal constituents are in files *_000i.cmp, three columns
+                                           period (min) or constituent name (e.g. M2), amplitude and phase (deg)
+                                           If no file is specified for a node, its value will be interpolated from surrounding nodes
+                                           If only one signal file is specified, the boundary gets a uniform signal
+                                           For a dischargebnd, only one signal file must be specified
+
+ FILETYPE=10 : inside_polygon       kx = 1 field                                uniform value inside polygon for INITIAL fields
+ FILETYPE=11 : ncgrid               kx = 1 field                    2 dim array      triangulation (should have proper standard_name in var, e.g., 'precipitation')
+ FILETYPE=12 : ncflow (map file)    kx = 1 or 2 field               1 dim array      triangulation
+ FILETYPE=14 : ncwave (com file)    kx = 1 field                    1 dim array      triangulation
+
+ METHOD  =0  : provider just updates, another provider that pointers to this one does the actual interpolation
+         =1  : intp space and time (getval) keep  2 meteofields in memory
+         =2  : first intp space (update), next intp. time (getval) keep 2 flowfields in memory
+         =3  : save weightfactors, intp space and time (getval),   keep 2 pointer- and weight sets in memory.
+         =4  : only spatial, inside polygon
+         =5  : only spatial, triangulation, (if samples from *.asc file then bilinear)
+         =6  : only spatial, averaging
+         =7  : only spatial, index triangulation
+         =8  : only spatial, smoothing
+         =9  : only spatial, internal diffusion
+         =10 : only initial vertical profiles
+
+ OPERAND =O  : Override at all points
+         =+  : Add to previously specified value
+         =*  : Multiply with previously specified value
+         =A  : Apply only if no value specified previously (For Initial fields, similar to Quickin preserving best data specified first)
+         =X  : MAX with prev. spec.
+         =N  : MIN with prev. spec.
+
+ EXTRAPOLATION_METHOD (ONLY WHEN METHOD=3)
+         = 0 : No spatial extrapolation.
+         = 1 : Do spatial extrapolation outside of source data bounding box.
+
+ MAXSEARCHRADIUS (ONLY WHEN EXTRAPOLATION_METHOD=1)
+         = search radius (in m) for model grid points that lie outside of the source data bounding box.
+
+ AVERAGINGTYPE (ONLY WHEN METHOD=6)
+         =1  : SIMPLE AVERAGING
+         =2  : NEAREST NEIGHBOUR
+         =3  : MAX (HIGHEST)
+         =4  : MIN (LOWEST)
+         =5  : INVERSE WEIGHTED DISTANCE-AVERAGE
+         =6  : MINABS
+         =7  : KDTREE (LIKE 1, BUT FAST AVERAGING)
+
+ RELATIVESEARCHCELLSIZE : For METHOD=6, the relative search cell size for samples inside cell (default: 1.01)
+
+ PERCENTILEMINMAX : (ONLY WHEN AVERAGINGTYPE=3 or 4) Changes the min/max operator to an average of the
+               highest/lowest data points. The value sets the percentage of the total set that is to be included.
+
+ NUMMIN  =   : For METHOD=6, minimum required number of source data points in each target cell.
+
+ VALUE   =   : Offset value for this provider
+
+ FACTOR  =   : Conversion factor for this provider
+
+*************************************************************************************************************
+"""
+
+
+class ExtOldTracerQuantity(str, Enum):
+    """Enum class containing the valid values for the boundary conditions category
+    of the external forcings that are specific to tracers.
+    """
+
+    TracerBnd = "tracerbnd"
+    """User-defined tracer"""
+    InitialTracer = "initialtracer"
+    """Initial tracer"""
+
+
+class ExtOldQuantity(str, Enum):
+    """Enum class containing the valid values for the boundary conditions category
+    of the external forcings.
+    """
+
+    # Boundary conditions
+    WaterLevelBnd = "waterlevelbnd"
+    """Water level"""
+    NeumannBnd = "neumannbnd"
+    """Water level gradient"""
+    RiemannBnd = "riemannbnd"
+    """Riemann invariant"""
+    OutflowBnd = "outflowbnd"
+    """Outflow"""
+    VelocityBnd = "velocitybnd"
+    """Velocity"""
+    DischargeBnd = "dischargebnd"
+    """Discharge"""
+    RiemannVelocityBnd = "riemann_velocitybnd"
+    """Riemann invariant velocity"""
+    SalinityBnd = "salinitybnd"
+    """Salinity"""
+    TemperatureBnd = "temperaturebnd"
+    """Temperature"""
+    SedimentBnd = "sedimentbnd"
+    """Suspended sediment"""
+    UXUYAdvectionVelocityBnd = "uxuyadvectionvelocitybnd"
+    """ux-uy advection velocity"""
+    NormalVelocityBnd = "normalvelocitybnd"
+    """Normal velocity"""
+    TangentialVelocityBnd = "tangentialvelocitybnd"
+    """Tangentional velocity"""
+    QhBnd = "qhbnd"
+    """Discharge-water level dependency"""
+
+    # Meteorological fields
+    WindX = "windx"
+    """Wind x component"""
+    WindY = "windy"
+    """Wind y component"""
+    WindXY = "windxy"
+    """Wind vector"""
+    AirPressureWindXWindY = "airpressure_windx_windy"
+    """Atmospheric pressure and wind components"""
+    AirPressureWindXWindYCharnock = "airpressure_windx_windy_charnock"
+    "Atmospheric pressure and wind components Charnock"
+    AtmosphericPressure = "atmosphericpressure"
+    """Atmospheric pressure"""
+    Rainfall = "rainfall"
+    """Precipitation"""
+    RainfallRate = "rainfall_rate"
+    """Precipitation"""
+    HumidityAirTemperatureCloudiness = "humidity_airtemperature_cloudiness"
+    """Combined heat flux terms"""
+    HumidityAirTemperatureCloudinessSolarRadiation = (
+        "humidity_airtemperature_cloudiness_solarradiation"
+    )
+    """Combined heat flux terms"""
+    DewPointAirTemperatureCloudiness = "dewpoint_airtemperature_cloudiness"
+    """Dew point air temperature cloudiness"""
+    LongWaveRadiation = "longwaveradiation"
+    """Long wave radiation"""
+    SolarRadiation = "solarradiation"
+    """Solar radiation"""
+    DischargeSalinityTemperatureSorSin = "discharge_salinity_temperature_sorsin"
+    """Discharge, salinity temperature source-sinks"""
+    NudgeSalinityTemperature = "nudge_salinity_temperature"
+    """Nudging salinity and temperature"""
+
+    # Structure parameters
+    Pump = "pump"
+    """Pump capacity"""
+    DamLevel = "damlevel"
+    """Dam level"""
+    GateLowerEdgeLevel = "gateloweredgelevel"
+    """Gate lower edge level"""
+    GeneralStructure = "generalstructure"
+    """General structure"""
+
+    # Initial fields
+    InitialWaterLevel = "initialwaterlevel"
+    """Initial water level"""
+    InitialSalinity = "initialsalinity"
+    """Initial salinity"""
+    InitialSalinityTop = "initialsalinitytop"
+    """Initial salinity top layer"""
+    InitialTemperature = "initialtemperature"
+    """Initial temperature"""
+    InitialVerticalTemperatureProfile = "initialverticaltemperatureprofile"
+    """Initial vertical temperature profile"""
+    InitialVerticalSalinityProfile = "initialverticalsalinityprofile"
+    """Initial vertical salinity profile"""
+    BedLevel = "bedlevel"
+    """Bed level"""
+
+    # Spatial physical properties
+    FrictionCoefficient = "frictioncoefficient"
+    """Friction coefficient"""
+    HorizontalEddyViscosityCoefficient = "horizontaleddyviscositycoefficient"
+    """Horizontal eddy viscosity coefficient"""
+    InternalTidesFrictionCoefficient = "internaltidesfrictioncoefficient"
+    """Internal tides friction coefficient"""
+    HorizontalEddyDiffusivityCoefficient = "horizontaleddydiffusivitycoefficient"
+    """Horizontal eddy diffusivity coefficient"""
+    AdvectionType = "advectiontype"
+    """Type of advection scheme"""
+    IBotLevType = "ibotlevtype"
+    """Type of bed-level handling"""
+
+    # Miscellaneous
+    ShiptXY = "shiptxy"
+    """shiptxy"""
+    MovingStationXY = "movingstationxy"
+    """Moving observation point for output (time, x, y)"""
+    WaveSignificantHeight = "wavesignificantheight"
+    """Wave significant height"""
+    WavePeriod = "waveperiod"
+    """Wave period"""
+
+
+class ExtOldFileType(IntEnum):
+    """Enum class containing the valid values for the `filetype` attribute
+    in the [ExtForcing][hydrolib.core.dflowfm.extold.models.ExtForcing] class.
+    """
+
+    TimeSeries = 1
+    """1. Time series"""
+    TimeSeriesMagnitudeAndDirection = 2
+    """2. Time series magnitude and direction"""
+    SpatiallyVaryingWindPressure = 3
+    """3. Spatially varying wind and pressure"""
+    ArcInfo = 4
+    """4. ArcInfo"""
+    SpiderWebData = 5
+    """5. Spiderweb data (cyclones)"""
+    CurvilinearData = 6
+    """6. Space-time data on curvilinear grid"""
+    Samples = 7
+    """7. Samples"""
+    TriangulationMagnitudeAndDirection = 8
+    """8. Triangulation magnitude and direction"""
+    Polyline = 9
+    """9. Polyline (<*.pli>-file)"""
+    NetCDFGridData = 11
+    """11. NetCDF grid data (e.g. meteo fields)"""
+    NetCDFWaveData = 14
+    """14. NetCDF wave data"""
+
+
+class ExtOldMethod(IntEnum):
+    """Enum class containing the valid values for the `method` attribute
+    in the [ExtForcing][hydrolib.core.dflowfm.extold.models.ExtForcing] class.
+    """
+
+    PassThrough = 1
+    """1. Pass through (no interpolation)"""
+    InterpolateTimeAndSpace = 2
+    """2. Interpolate time and space"""
+    InterpolateTimeAndSpaceSaveWeights = 3
+    """3. Interpolate time and space, save weights"""
+    InterpolateSpace = 4
+    """4. Interpolate space"""
+    InterpolateTime = 5
+    """5. Interpolate time"""
+    AveragingSpace = 6
+    """6. Averaging in space"""
+    InterpolateExtrapolateTime = 7
+    """7. Interpolate/Extrapolate time"""
+
+
+class ExtOldExtrapolationMethod(IntEnum):
+    """Enum class containing the valid values for the `extrapolation_method` attribute
+    in the [ExtForcing][hydrolib.core.dflowfm.extold.models.ExtForcing] class.
+    """
+
+    NoSpatialExtrapolation = 0
+    """0. No spatial extrapolation."""
+    SpatialExtrapolationOutsideOfSourceDataBoundingBox = 1
+    """1. Do spatial extrapolation outside of source data bounding box."""
+
+
+class ExtOldForcing(BaseModel):
+    """Class holding the external forcing values."""
+
+    quantity: Union[ExtOldQuantity, str] = Field(alias="QUANTITY")
+    """Union[Quantity, str]: The name of the quantity."""
+
+    filename: Union[PolyFile, TimModel, DiskOnlyFileModel] = Field(
+        None, alias="FILENAME"
+    )
+    """Union[PolyFile, TimModel, DiskOnlyFileModel]: The file associated to this forcing."""
+
+    varname: Optional[str] = Field(None, alias="VARNAME")
+    """Optional[str]: The variable name used in `filename` associated with this forcing; some input files may contain multiple variables."""
+
+    sourcemask: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="SOURCEMASK"
+    )
+    """DiskOnlyFileModel: The file containing a mask."""
+
+    filetype: ExtOldFileType = Field(alias="FILETYPE")
+    """FileType: Indication of the file type.
+    
+    Options:
+    1. Time series
+    2. Time series magnitude and direction
+    3. Spatially varying weather
+    4. ArcInfo
+    5. Spiderweb data (cyclones)
+    6. Curvilinear data
+    7. Samples (C.3)
+    8. Triangulation magnitude and direction
+    9. Polyline (<*.pli>-file, C.2)
+    11. NetCDF grid data (e.g. meteo fields)
+    14. NetCDF wave data
+    """
+
+    method: ExtOldMethod = Field(alias="METHOD")
+    """ExtOldMethod: The method of interpolation.
+    
+    Options:
+    1. Pass through (no interpolation)
+    2. Interpolate time and space
+    3. Interpolate time and space, save weights
+    4. Interpolate space
+    5. Interpolate time
+    6. Averaging space
+    7. Interpolate/Extrapolate time
+    """
+
+    extrapolation_method: Optional[ExtOldExtrapolationMethod] = Field(
+        None, alias="EXTRAPOLATION_METHOD"
+    )
+    """Optional[ExtOldExtrapolationMethod]: The extrapolation method.
+
+    Options:
+    0. No spatial extrapolation.
+    1. Do spatial extrapolation outside of source data bounding box.
+    """
+
+    maxsearchradius: Optional[float] = Field(None, alias="MAXSEARCHRADIUS")
+    """Optional[float]: Search radius (in m) for model grid points that lie outside of the source data bounding box."""
+
+    operand: Operand = Field(alias="OPERAND")
+    """Operand: The operand to use for adding the provided values.
+    
+    Options:    
+    'O' Existing values are overwritten with the provided values.
+    'A' Provided values are used where existing values are missing.
+    '+' Existing values are summed with the provided values.
+    '*' Existing values are multiplied with the provided values.
+    'X' The maximum values of the existing values and provided values are used.
+    'N' The minimum values of the existing values and provided values are used.
+    """
+
+    value: Optional[float] = Field(None, alias="VALUE")
+    """Optional[float]: Custom coefficients for transformation."""
+
+    factor: Optional[float] = Field(None, alias="FACTOR")
+    """Optional[float]: The conversion factor."""
+
+    ifrctyp: Optional[float] = Field(None, alias="IFRCTYP")
+    """Optional[float]: The friction type."""
+
+    averagingtype: Optional[float] = Field(None, alias="AVERAGINGTYPE")
+    """Optional[float]: The averaging type."""
+
+    relativesearchcellsize: Optional[float] = Field(
+        None, alias="RELATIVESEARCHCELLSIZE"
+    )
+    """Optional[float]: The relative search cell size for samples inside a cell."""
+
+    extrapoltol: Optional[float] = Field(None, alias="EXTRAPOLTOL")
+    """Optional[float]: The extrapolation tolerance."""
+
+    percentileminmax: Optional[float] = Field(None, alias="PERCENTILEMINMAX")
+    """Optional[float]: Changes the min/max operator to an average of the highest/lowest data points. The value sets the percentage of the total set that is to be included.."""
+
+    area: Optional[float] = Field(None, alias="AREA")
+    """Optional[float]: The area for sources and sinks."""
+
+    nummin: Optional[int] = Field(None, alias="NUMMIN")
+    """Optional[int]: The minimum required number of source data points in each target cell."""
+
+    @validator("quantity", pre=True)
+    def validate_quantity(cls, value):
+        if isinstance(value, ExtOldQuantity):
+            return value
+
+        def raise_error_tracer_name(quantity: ExtOldTracerQuantity):
+            raise ValueError(
+                f"QUANTITY '{quantity}' should be appended with a tracer name."
+            )
+
+        if isinstance(value, ExtOldTracerQuantity):
+            raise_error_tracer_name(value)
+
+        value_str = str(value)
+        lower_value = value_str.lower()
+
+        for tracer_quantity in ExtOldTracerQuantity:
+            if lower_value.startswith(tracer_quantity):
+                n = len(tracer_quantity)
+                if n == len(value_str):
+                    raise_error_tracer_name(tracer_quantity)
+                return tracer_quantity + value_str[n:]
+
+        if lower_value in list(ExtOldQuantity):
+            return ExtOldQuantity(lower_value)
+
+        supported_value_str = ", ".join(([x.value for x in ExtOldQuantity]))
+        raise ValueError(
+            f"QUANTITY '{value_str}' not supported. Supported values: {supported_value_str}"
+        )
+
+    @validator("operand", pre=True)
+    def validate_operand(cls, value):
+        if isinstance(value, Operand):
+            return value
+
+        if isinstance(value, str):
+
+            for operand in Operand:
+                if value.lower() == operand.value.lower():
+                    return operand
+
+            supported_value_str = ", ".join(([x.value for x in Operand]))
+            raise ValueError(
+                f"OPERAND '{value}' not supported. Supported values: {supported_value_str}"
+            )
+
+        return value
+
+    @root_validator(skip_on_failure=True)
+    def validate_forcing(cls, values):
+        class _Field:
+            def __init__(self, key: str) -> None:
+                self.alias = cls.__fields__[key].alias
+                self.value = values[key]
+
+        def raise_error_only_allowed_when(
+            field: _Field, dependency: _Field, valid_dependency_value: str
+        ):
+            error = f"{field.alias} only allowed when {dependency.alias} is {valid_dependency_value}"
+            raise ValueError(error)
+
+        def only_allowed_when(
+            field: _Field, dependency: _Field, valid_dependency_value: Any
+        ):
+            """This function checks if a particular field is allowed to have a value only when a dependency field has a specific value."""
+
+            if field.value is None or dependency.value == valid_dependency_value:
+                return
+
+            raise_error_only_allowed_when(field, dependency, valid_dependency_value)
+
+        quantity = _Field("quantity")
+        varname = _Field("varname")
+        sourcemask = _Field("sourcemask")
+        filetype = _Field("filetype")
+        method = _Field("method")
+        extrapolation_method = _Field("extrapolation_method")
+        maxsearchradius = _Field("maxsearchradius")
+        value = _Field("value")
+        factor = _Field("factor")
+        ifrctype = _Field("ifrctyp")
+        averagingtype = _Field("averagingtype")
+        relativesearchcellsize = _Field("relativesearchcellsize")
+        extrapoltol = _Field("extrapoltol")
+        percentileminmax = _Field("percentileminmax")
+        area = _Field("area")
+        nummin = _Field("nummin")
+
+        only_allowed_when(varname, filetype, ExtOldFileType.NetCDFGridData)
+
+        if sourcemask.value.filepath is not None and filetype.value not in [
+            ExtOldFileType.ArcInfo,
+            ExtOldFileType.CurvilinearData,
+        ]:
+            raise_error_only_allowed_when(
+                sourcemask, filetype, valid_dependency_value="4 or 6"
+            )
+
+        if (
+            extrapolation_method.value
+            == ExtOldExtrapolationMethod.SpatialExtrapolationOutsideOfSourceDataBoundingBox
+            and method.value != ExtOldMethod.InterpolateTimeAndSpaceSaveWeights
+        ):
+            error = f"{extrapolation_method.alias} only allowed to be 1 when {method.alias} is 3"
+            raise ValueError(error)
+
+        only_allowed_when(
+            maxsearchradius,
+            extrapolation_method,
+            ExtOldExtrapolationMethod.SpatialExtrapolationOutsideOfSourceDataBoundingBox,
+        )
+        only_allowed_when(value, method, ExtOldMethod.InterpolateSpace)
+
+        if factor.value is not None and not quantity.value.startswith(
+            ExtOldTracerQuantity.InitialTracer
+        ):
+            error = f"{factor.alias} only allowed when {quantity.alias} starts with {ExtOldTracerQuantity.InitialTracer}"
+            raise ValueError(error)
+
+        only_allowed_when(ifrctype, quantity, ExtOldQuantity.FrictionCoefficient)
+        only_allowed_when(averagingtype, method, ExtOldMethod.AveragingSpace)
+        only_allowed_when(relativesearchcellsize, method, ExtOldMethod.AveragingSpace)
+        only_allowed_when(extrapoltol, method, ExtOldMethod.InterpolateTime)
+        only_allowed_when(percentileminmax, method, ExtOldMethod.AveragingSpace)
+        only_allowed_when(
+            area, quantity, ExtOldQuantity.DischargeSalinityTemperatureSorSin
+        )
+        only_allowed_when(nummin, method, ExtOldMethod.AveragingSpace)
+
+        return values
+
+
+class ExtOldModel(ParsableFileModel):
+    """
+    The overall external forcings model that contains the contents of one external forcings file (old format).
+
+    This model is typically referenced under a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.external_forcing.extforcefile`.
+    """
+
+    comment: List[str] = HEADER.splitlines()[1:]
+    """List[str]: The comments in the header of the external forcing file."""
+    forcing: List[ExtOldForcing] = []
+    """List[ExtForcing]: The external forcing blocks in the external forcing file."""
+
+    @classmethod
+    def _ext(cls) -> str:
+        return ".ext"
+
+    @classmethod
+    def _filename(cls) -> str:
+        return "externalforcings"
+
+    def dict(self, *args, **kwargs):
+        return dict(comment=self.comment, forcing=[dict(f) for f in self.forcing])
+
+    @classmethod
+    def _get_serializer(
+        cls,
+    ) -> Callable[[Path, Dict, SerializerConfig, ModelSaveSettings], None]:
+        return Serializer.serialize
+
+    @classmethod
+    def _get_parser(cls) -> Callable[[Path], Dict]:
+        return Parser.parse
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/extold/parser.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/extold/parser.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,115 +1,115 @@
-from pathlib import Path
-from typing import Dict, List
-
-from hydrolib.core.dflowfm.extold.common_io import ORDERED_FORCING_FIELDS
-
-
-class Parser:
-    """Parser class for parsing the forcing data of the old external forcings file to a dictionary to construct the `ExtOldModel` with."""
-
-    @staticmethod
-    def parse(filepath: Path) -> Dict:
-        """Parses the file at the specified path to the forcing data.
-
-        If a line starts with an asterisk (*) it is considered a comment.
-        Comments are only allowed at the header of the file. Elsewhere, they will be skipped.
-
-        Args:
-            path (Path): The path to parse the data from.
-
-        Returns:
-            Dict[str, List[Any]]: A dictionary with the parsed data, with two keys:
-                - 'comment' (List[str]): A list of the parsed comments.
-                - 'forcing' (List[Dict[str, str]]): A list of the parsed forcing data as dictionaries. Each dictionary represents a forcing block from the file, with the parsed key value pairs.
-
-        Raises:
-            ValueError: Thrown when the order of a forcing block is not correct. Fields should be in the following order:
-            "QUANTITY", "FILENAME", "VARNAME", "SOURCEMASK", "FILETYPE", "METHOD", "OPERAND", "VALUE", "FACTOR", "IFRCTYP",
-            "AVERAGINGTYPE", "RELATIVESEARCHCELLSIZE", "EXTRAPOLTOL", "PERCENTILEMINMAX", "AREA", "NUMMIN"
-        """
-
-        with filepath.open(encoding="utf8") as file:
-            lines = file.readlines()
-
-        comments, start_data_index = Parser._parse_header(lines)
-        forcings = Parser._parse_data(lines, start_data_index)
-
-        return dict(comment=comments, forcing=forcings)
-
-    @staticmethod
-    def _parse_header(lines: List[str]):
-        comments: List[str] = []
-
-        start_data_index = 0
-        for line_index in range(len(lines)):
-
-            line = lines[line_index].strip()
-
-            if len(line) == 0:
-                comments.append(line)
-                continue
-
-            if line.startswith("*"):
-                comments.append(line[1:])
-                continue
-
-            start_data_index = line_index
-            break
-
-        return comments, start_data_index
-
-    @staticmethod
-    def _parse_data(lines: List[str], start_index: int):
-        forcings: List[Dict[str, str]] = []
-        current_forcing: Dict[str, str] = {}
-
-        for line_index in range(start_index, len(lines)):
-
-            line = lines[line_index].strip()
-
-            if line.startswith("*"):
-                continue
-
-            if len(line) == 0:
-                if len(current_forcing) != 0:
-                    Parser._validate_order(current_forcing, line_index)
-                    forcings.append(current_forcing)
-                    current_forcing = {}
-                continue
-
-            key, value = line.split("=", 1)
-            current_forcing[key.strip()] = value.strip()
-
-        if len(current_forcing) != 0:
-            Parser._validate_order(current_forcing, line_index)
-            forcings.append(current_forcing)
-
-        return forcings
-
-    @staticmethod
-    def _validate_order(forcing: Dict[str, str], line_number: int):
-        """Validates the order of the forcing fields given in the forcing block.
-
-        - The fields are compared case insensitive.
-        - For each KNOWN field that was parsed the order is checked.
-        """
-
-        parsed_fields_upper = [f.upper() for f in forcing.keys()]
-        model_fields_upper = [f.upper() for f in ORDERED_FORCING_FIELDS]
-
-        # Get the ordered KNOWN parsed fields, by filtering out unknown fields
-        parsed_fields_ordered = [
-            f for f in model_fields_upper if f in parsed_fields_upper
-        ]
-
-        # Get the unorderd KNOWN parsed fields, by filtering out unknown fields
-        parsed_fields_unordered = [
-            f for f in parsed_fields_upper if f in model_fields_upper
-        ]
-
-        if parsed_fields_unordered != parsed_fields_ordered:
-            line_number_start = line_number + 1 - len(parsed_fields_upper)
-            parsed_fields_ordered_str = ", ".join(parsed_fields_ordered)
-            raise ValueError(
-                f"Line {line_number_start}: Properties should be in the following order: {parsed_fields_ordered_str}"
-            )
+from pathlib import Path
+from typing import Dict, List
+
+from hydrolib.core.dflowfm.extold.common_io import ORDERED_FORCING_FIELDS
+
+
+class Parser:
+    """Parser class for parsing the forcing data of the old external forcings file to a dictionary to construct the `ExtOldModel` with."""
+
+    @staticmethod
+    def parse(filepath: Path) -> Dict:
+        """Parses the file at the specified path to the forcing data.
+
+        If a line starts with an asterisk (*) it is considered a comment.
+        Comments are only allowed at the header of the file. Elsewhere, they will be skipped.
+
+        Args:
+            path (Path): The path to parse the data from.
+
+        Returns:
+            Dict[str, List[Any]]: A dictionary with the parsed data, with two keys:
+                - 'comment' (List[str]): A list of the parsed comments.
+                - 'forcing' (List[Dict[str, str]]): A list of the parsed forcing data as dictionaries. Each dictionary represents a forcing block from the file, with the parsed key value pairs.
+
+        Raises:
+            ValueError: Thrown when the order of a forcing block is not correct. Fields should be in the following order:
+            "QUANTITY", "FILENAME", "VARNAME", "SOURCEMASK", "FILETYPE", "METHOD", "OPERAND", "VALUE", "FACTOR", "IFRCTYP",
+            "AVERAGINGTYPE", "RELATIVESEARCHCELLSIZE", "EXTRAPOLTOL", "PERCENTILEMINMAX", "AREA", "NUMMIN"
+        """
+
+        with filepath.open(encoding="utf8") as file:
+            lines = file.readlines()
+
+        comments, start_data_index = Parser._parse_header(lines)
+        forcings = Parser._parse_data(lines, start_data_index)
+
+        return dict(comment=comments, forcing=forcings)
+
+    @staticmethod
+    def _parse_header(lines: List[str]):
+        comments: List[str] = []
+
+        start_data_index = 0
+        for line_index in range(len(lines)):
+
+            line = lines[line_index].strip()
+
+            if len(line) == 0:
+                comments.append(line)
+                continue
+
+            if line.startswith("*"):
+                comments.append(line[1:])
+                continue
+
+            start_data_index = line_index
+            break
+
+        return comments, start_data_index
+
+    @staticmethod
+    def _parse_data(lines: List[str], start_index: int):
+        forcings: List[Dict[str, str]] = []
+        current_forcing: Dict[str, str] = {}
+
+        for line_index in range(start_index, len(lines)):
+
+            line = lines[line_index].strip()
+
+            if line.startswith("*"):
+                continue
+
+            if len(line) == 0:
+                if len(current_forcing) != 0:
+                    Parser._validate_order(current_forcing, line_index)
+                    forcings.append(current_forcing)
+                    current_forcing = {}
+                continue
+
+            key, value = line.split("=", 1)
+            current_forcing[key.strip()] = value.strip()
+
+        if len(current_forcing) != 0:
+            Parser._validate_order(current_forcing, line_index)
+            forcings.append(current_forcing)
+
+        return forcings
+
+    @staticmethod
+    def _validate_order(forcing: Dict[str, str], line_number: int):
+        """Validates the order of the forcing fields given in the forcing block.
+
+        - The fields are compared case insensitive.
+        - For each KNOWN field that was parsed the order is checked.
+        """
+
+        parsed_fields_upper = [f.upper() for f in forcing.keys()]
+        model_fields_upper = [f.upper() for f in ORDERED_FORCING_FIELDS]
+
+        # Get the ordered KNOWN parsed fields, by filtering out unknown fields
+        parsed_fields_ordered = [
+            f for f in model_fields_upper if f in parsed_fields_upper
+        ]
+
+        # Get the unorderd KNOWN parsed fields, by filtering out unknown fields
+        parsed_fields_unordered = [
+            f for f in parsed_fields_upper if f in model_fields_upper
+        ]
+
+        if parsed_fields_unordered != parsed_fields_ordered:
+            line_number_start = line_number + 1 - len(parsed_fields_upper)
+            parsed_fields_ordered_str = ", ".join(parsed_fields_ordered)
+            raise ValueError(
+                f"Line {line_number_start}: Properties should be in the following order: {parsed_fields_ordered_str}"
+            )
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/extold/serializer.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/polyfile/serializer.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,107 +1,113 @@
-from pathlib import Path
-from typing import Any, Dict, List
-
-from hydrolib.core.basemodel import (
-    DiskOnlyFileModel,
-    FileModel,
-    ModelSaveSettings,
-    SerializerConfig,
-)
-from hydrolib.core.dflowfm.extold.common_io import ORDERED_FORCING_FIELDS
-from hydrolib.core.utils import FilePathStyleConverter
-
-
-class Serializer:
-    """Serializer class for serializing the forcing data of the `ExtOldModel` to file."""
-
-    _file_path_style_converter = FilePathStyleConverter()
-
-    @staticmethod
-    def serialize(
-        path: Path,
-        data: Dict[str, List[Any]],
-        config: SerializerConfig,
-        save_settings: ModelSaveSettings,
-    ) -> None:
-
-        """
-        Serialize the given data and write it to a file at the given path.
-
-        This function may create a new file at the given path, or overwrite an existing file.
-
-        Args:
-            path (Path): The path to write the serialized data to.
-            data (Dict[str, List[Any]]): The data to be serialized. The data should contain two keys:
-                - 'comment' (List[str]): a list of the comments
-                - 'forcing' (List[Dict[str, Any]]): a list of the external forcing data
-            config (SerializerConfig): Configuration settings for the serializer.
-            save_settings (ModelSaveSettings): Settings for how the model should be saved.
-        """
-
-        path.parent.mkdir(parents=True, exist_ok=True)
-
-        serialized_comments: List[str] = []
-        serialized_blocks: List[str] = []
-
-        for comment in data["comment"]:
-            serialized_comment = Serializer._serialize_comment(comment)
-            serialized_comments.append(serialized_comment)
-
-        for forcing in data["forcing"]:
-            serialized_block = Serializer._serialize_forcing(
-                forcing, config, save_settings
-            )
-            serialized_blocks.append(serialized_block)
-
-        file_content: str = (
-            "\n".join(serialized_comments) + "\n" + "\n\n".join(serialized_blocks)
-        )
-
-        with path.open("w", encoding="utf8") as f:
-            f.write(file_content)
-
-    @staticmethod
-    def _serialize_comment(comment: str):
-        return f"*{comment}"
-
-    @staticmethod
-    def _serialize_forcing(
-        forcing: Dict[str, Any],
-        config: SerializerConfig,
-        save_settings: ModelSaveSettings,
-    ) -> str:
-
-        serialized_rows = []
-
-        for key in ORDERED_FORCING_FIELDS:
-            value = forcing.get(key.lower())
-
-            if Serializer._skip_field_serialization(value):
-                continue
-
-            value = Serializer._convert_value(value, config, save_settings)
-
-            serialized_row = f"{key}={value}"
-            serialized_rows.append(serialized_row)
-
-        serialized_block = "\n".join(serialized_rows)
-        return serialized_block
-
-    @classmethod
-    def _convert_value(
-        cls, value: Any, config: SerializerConfig, save_settings: ModelSaveSettings
-    ) -> str:
-        if isinstance(value, float):
-            return f"{value:{config.float_format}}"
-        if isinstance(value, FileModel):
-            return Serializer._file_path_style_converter.convert_from_os_style(
-                value.filepath, save_settings.path_style
-            )
-
-        return str(value)
-
-    @classmethod
-    def _skip_field_serialization(cls, value: Any) -> str:
-        return value is None or (
-            isinstance(value, DiskOnlyFileModel) and value.filepath is None
-        )
+from itertools import chain
+from pathlib import Path
+from typing import Generator, Iterable, Optional, Sequence
+
+from hydrolib.core.basemodel import SerializerConfig
+from hydrolib.core.dflowfm.polyfile.models import (
+    Description,
+    Metadata,
+    Point,
+    PolyObject,
+)
+
+
+class Serializer:
+    """Serializer provides several static serialize methods for the models."""
+
+    @staticmethod
+    def serialize_description(description: Optional[Description]) -> Iterable[str]:
+        """Serialize the Description to a string which can be used within a polyfile.
+
+        Returns:
+            str: The serialised equivalent of this Description
+        """
+        if description is None:
+            return []
+        if description.content == "":
+            return [
+                "*",
+            ]
+        return (f"*{v.rstrip()}" for v in (description.content + "\n").splitlines())
+
+    @staticmethod
+    def serialize_metadata(metadata: Metadata) -> Iterable[str]:
+        """Serialize this Metadata to a string which can be used within a polyfile.
+
+        The number of rows and number of columns are separated by four spaces.
+
+        Returns:
+            str: The serialised equivalent of this Metadata
+        """
+        return [metadata.name, f"{metadata.n_rows}    {metadata.n_columns}"]
+
+    @staticmethod
+    def serialize_point(point: Point, config: SerializerConfig) -> str:
+        """Serialize this Point to a string which can be used within a polyfile.
+
+        the point data is indented with 4 spaces, and the individual values are
+        separated by 4 spaces as well.
+
+        Args:
+            point (Point): The point to serialize.
+            config (SerializerConfig): The serialization configuration.
+
+        Returns:
+            str: The serialised equivalent of this Point
+        """
+        space = 4 * " "
+        float_format = lambda v: f"{v:{config.float_format}}"
+        return space + space.join(
+            float_format(v) for v in Serializer._get_point_values(point)
+        )
+
+    @staticmethod
+    def _get_point_values(point: Point) -> Generator[float, None, None]:
+        yield point.x
+        yield point.y
+        if point.z:
+            yield point.z
+        for value in point.data:
+            yield value
+
+    @staticmethod
+    def serialize_poly_object(
+        obj: PolyObject, config: SerializerConfig
+    ) -> Iterable[str]:
+        """Serialize this PolyObject to a string which can be used within a polyfile.
+
+        Args:
+            obj (PolyObject): The poly object to serializer.
+            config (SerializerConfig): The serialization configuration.
+
+        Returns:
+            str: The serialised equivalent of this PolyObject
+        """
+
+        description = Serializer.serialize_description(obj.description)
+        metadata = Serializer.serialize_metadata(obj.metadata)
+        points = [Serializer.serialize_point(obj, config) for obj in obj.points]
+        return chain(description, metadata, points)
+
+
+def write_polyfile(
+    path: Path, data: Sequence[PolyObject], config: SerializerConfig
+) -> None:
+    """Write the data to a new file at path
+
+    Args:
+        path (Path): The path to write the data to
+        data (Sequence[PolyObject]): The poly objects to write
+        config (SerializerConfig): The serialization configuration.
+    """
+    serialized_poly_objects = [
+        Serializer.serialize_poly_object(poly_object, config) for poly_object in data
+    ]
+    serialized_data = chain.from_iterable(serialized_poly_objects)
+
+    path.parent.mkdir(parents=True, exist_ok=True)
+
+    with path.open("w", encoding="utf8") as f:
+
+        for line in serialized_data:
+            f.write(line)
+            f.write("\n")
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/friction/models.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/friction/models.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,255 +1,255 @@
-import logging
-from enum import Enum
-from pathlib import Path
-from typing import List, Literal, Optional
-
-from pydantic import Field, NonNegativeInt, PositiveInt
-from pydantic.class_validators import validator
-
-from hydrolib.core.basemodel import (
-    DiskOnlyFileModel,
-    validator_set_default_disk_only_file_model_when_none,
-)
-from hydrolib.core.dflowfm.ini.models import INIBasedModel, INIGeneral, INIModel
-from hydrolib.core.dflowfm.ini.util import (
-    get_enum_validator,
-    get_split_string_on_delimiter_validator,
-    make_list_validator,
-)
-
-logger = logging.getLogger(__name__)
-
-
-class FrictionType(str, Enum):
-    """
-    Enum class containing the valid values for the frictionType
-    attribute in several subclasses of Structure/CrossSection/friction.models.
-    """
-
-    chezy = "Chezy"
-    """str: ChÃ©zy C [m 1/2 /s]"""
-
-    manning = "Manning"
-    """str: Manning n [s/m 1/3 ]"""
-
-    walllawnikuradse = "wallLawNikuradse"
-    """str: Nikuradse k_n [m]"""
-
-    whitecolebrook = "WhiteColebrook"
-    """str: Nikuradse k_n [m]"""
-
-    stricklernikuradse = "StricklerNikuradse"
-    """str: Nikuradse k_n [m]"""
-
-    strickler = "Strickler"
-    """str: Strickler k_s [m 1/3 /s]"""
-
-    debosbijkerk = "deBosBijkerk"
-    """str: De Bos-Bijkerk Î³ [1/s]"""
-
-
-class FrictGeneral(INIGeneral):
-    """The friction file's `[General]` section with file meta data."""
-
-    class Comments(INIBasedModel.Comments):
-        fileversion: Optional[str] = Field(
-            "File version. Do not edit this.", alias="fileVersion"
-        )
-        filetype: Optional[str] = Field(
-            "File type. Should be 'roughness'. Do not edit this.",
-            alias="fileType",
-        )
-        frictionvaluesfile: Optional[str] = Field(
-            "Name of <*.bc> file containing the timeseries with friction values. "
-            + "Only needed for functionType = timeSeries.",
-            alias="frictionValuesFile",
-        )
-
-    comments: Comments = Comments()
-    _header: Literal["General"] = "General"
-    fileversion: str = Field("3.01", alias="fileVersion")
-    filetype: Literal["roughness"] = Field("roughness", alias="fileType")
-    frictionvaluesfile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="frictionValuesFile"
-    )
-
-    _disk_only_file_model_should_not_be_none = (
-        validator_set_default_disk_only_file_model_when_none()
-    )
-
-
-class FrictGlobal(INIBasedModel):
-    """A `[Global]` block for use inside a friction file.
-
-    Multiple of such blocks may be present to define multiple frictionId classes.
-    """
-
-    class Comments(INIBasedModel.Comments):
-        frictionid: Optional[str] = Field(
-            "Name of the roughness variable.", alias="frictionId"
-        )
-        frictiontype: Optional[str] = Field(
-            "The global roughness type for this variable, which is used "
-            + "if no branch specific roughness definition is given.",
-            alias="frictionType",
-        )
-        frictionvalue: Optional[str] = Field(
-            "The global default value for this roughness variable.",
-            alias="frictionValue",
-        )
-
-    comments: Comments = Comments()
-    _header: Literal["Global"] = "Global"
-    frictionid: str = Field(alias="frictionId")
-    frictiontype: FrictionType = Field(alias="frictionType")
-    frictionvalue: float = Field(alias="frictionValue")
-
-    _frictiontype_validator = get_enum_validator("frictiontype", enum=FrictionType)
-
-    def _get_identifier(self, data: dict) -> Optional[str]:
-        return data.get("frictionid")
-
-
-class FrictBranch(INIBasedModel):
-    """A `[Branch]` block for use inside a friction file.
-
-    Each block can define the roughness value(s) on a particular branch.
-    """
-
-    class Comments(INIBasedModel.Comments):
-        branchid: Optional[str] = Field("The name of the branch.", alias="branchId")
-        frictiontype: Optional[str] = Field(
-            "The roughness type to be used on this branch.", alias="frictionType"
-        )
-        functiontype: Optional[str] = Field(
-            "Function type for the calculation of the value. "
-            + "Possible values: constant, timeSeries, absDischarge, waterlevel.",
-            alias="functionType",
-        )
-        timeseriesid: Optional[str] = Field(
-            "Refers to a data block in the <*.bc> frictionValuesFile. "
-            + "Only if functionType = timeSeries.",
-            alias="timeSeriesId",
-        )
-        numlevels: Optional[str] = Field(
-            "Number of levels in table. Only if functionType is not constant.",
-            alias="numLevels",
-        )
-        levels: Optional[str] = Field(
-            "Space separated list of discharge [m3/s] or water level [m AD] values. "
-            + "Only if functionType is absDischarge or waterLevel."
-        )
-        numlocations: Optional[str] = Field(
-            "Number of locations on branch. The default 0 implies branch uniform values.",
-            alias="numLocations",
-        )
-        chainage: Optional[str] = Field(
-            "Space separated list of locations on the branch [m]. Locations sorted by "
-            + "increasing chainage. The keyword must be specified if numLocations>0."
-        )
-        frictionvalues: Optional[str] = Field(
-            "numLevels lines containing space separated lists of roughness values: "
-            + "numLocations values per line. If the functionType is constant, then a "
-            + "single line is required. For a uniform roughness per branch "
-            + "(numLocations = 0) a single entry per line is required. The meaning "
-            + "of the values depends on the roughness type selected (see frictionType).",
-            alias="frictionValues",
-        )
-
-    comments: Comments = Comments()
-    _header: Literal["Branch"] = "Branch"
-    branchid: str = Field(alias="branchId")
-    frictiontype: FrictionType = Field(alias="frictionType")
-    functiontype: Optional[str] = Field("constant", alias="functionType")
-    timeseriesid: Optional[str] = Field(alias="timeSeriesId")
-    numlevels: Optional[PositiveInt] = Field(alias="numLevels")
-    levels: Optional[List[float]]
-    numlocations: Optional[NonNegativeInt] = Field(0, alias="numLocations")
-    chainage: Optional[List[float]]
-    frictionvalues: Optional[List[float]] = Field(
-        alias="frictionValues"
-    )  # TODO: turn this into List[List[float]], see issue #143.
-
-    _split_to_list = get_split_string_on_delimiter_validator(
-        "levels",
-        "chainage",
-        "frictionvalues",
-    )
-
-    _frictiontype_validator = get_enum_validator("frictiontype", enum=FrictionType)
-
-    def _get_identifier(self, data: dict) -> Optional[str]:
-        return data.get("branchid")
-
-    @validator("levels", always=True)
-    @classmethod
-    def _validate_levels(cls, v, values):
-        if v is not None and (
-            values["numlevels"] is None or len(v) != values["numlevels"]
-        ):
-            raise ValueError(
-                f"Number of values for levels should be equal to the numLevels value (branchId={values.get('branchid', '')})."
-            )
-
-        return v
-
-    @validator("chainage", always=True)
-    @classmethod
-    def _validate_chainage(cls, v, values):
-        if v is not None and len(v) != values["numlocations"]:
-            raise ValueError(
-                f"Number of values for chainage should be equal to the numLocations value (branchId={values.get('branchid', '')})."
-            )
-
-        return v
-
-    @validator("frictionvalues", always=True)
-    @classmethod
-    def _validate_frictionvalues(cls, v, values):
-        # number of values should be equal to numlocations*numlevels
-        numlevels = (
-            1
-            if (
-                "numlevels" not in values
-                or values["numlevels"] is None
-                or values["numlevels"] == 0
-            )
-            else values["numlevels"]
-        )
-        numvals = max(1, values["numlocations"]) * numlevels
-        if v is not None and len(v) != numvals:
-            raise ValueError(
-                f"Number of values for frictionValues should be equal to the numLocations*numLevels value (branchId={values.get('branchid', '')})."
-            )
-
-        return v
-
-
-class FrictionModel(INIModel):
-    """
-    The overall friction model that contains the contents of one friction file.
-
-    This model is typically referenced under a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.frictfile[..]`.
-
-    Attributes:
-        general (FrictGeneral): `[General]` block with file metadata.
-        global_ (List[FrictGlobal]): Definitions of `[Global]` friction classes.
-        branch (List[FrictBranch]): Definitions of `[Branch]` friction values.
-    """
-
-    general: FrictGeneral = FrictGeneral()
-    global_: List[FrictGlobal] = Field([], alias="global")  # to circumvent built-in kw
-    branch: List[FrictBranch] = []
-
-    _split_to_list = make_list_validator(
-        "global_",
-        "branch",
-    )
-
-    @classmethod
-    def _ext(cls) -> str:
-        return ".ini"
-
-    @classmethod
-    def _filename(cls) -> str:
-        return "roughness"
+import logging
+from enum import Enum
+from pathlib import Path
+from typing import List, Literal, Optional
+
+from pydantic import Field, NonNegativeInt, PositiveInt
+from pydantic.class_validators import validator
+
+from hydrolib.core.basemodel import (
+    DiskOnlyFileModel,
+    validator_set_default_disk_only_file_model_when_none,
+)
+from hydrolib.core.dflowfm.ini.models import INIBasedModel, INIGeneral, INIModel
+from hydrolib.core.dflowfm.ini.util import (
+    get_enum_validator,
+    get_split_string_on_delimiter_validator,
+    make_list_validator,
+)
+
+logger = logging.getLogger(__name__)
+
+
+class FrictionType(str, Enum):
+    """
+    Enum class containing the valid values for the frictionType
+    attribute in several subclasses of Structure/CrossSection/friction.models.
+    """
+
+    chezy = "Chezy"
+    """str: ChÃ©zy C [m 1/2 /s]"""
+
+    manning = "Manning"
+    """str: Manning n [s/m 1/3 ]"""
+
+    walllawnikuradse = "wallLawNikuradse"
+    """str: Nikuradse k_n [m]"""
+
+    whitecolebrook = "WhiteColebrook"
+    """str: Nikuradse k_n [m]"""
+
+    stricklernikuradse = "StricklerNikuradse"
+    """str: Nikuradse k_n [m]"""
+
+    strickler = "Strickler"
+    """str: Strickler k_s [m 1/3 /s]"""
+
+    debosbijkerk = "deBosBijkerk"
+    """str: De Bos-Bijkerk Î³ [1/s]"""
+
+
+class FrictGeneral(INIGeneral):
+    """The friction file's `[General]` section with file meta data."""
+
+    class Comments(INIBasedModel.Comments):
+        fileversion: Optional[str] = Field(
+            "File version. Do not edit this.", alias="fileVersion"
+        )
+        filetype: Optional[str] = Field(
+            "File type. Should be 'roughness'. Do not edit this.",
+            alias="fileType",
+        )
+        frictionvaluesfile: Optional[str] = Field(
+            "Name of <*.bc> file containing the timeseries with friction values. "
+            + "Only needed for functionType = timeSeries.",
+            alias="frictionValuesFile",
+        )
+
+    comments: Comments = Comments()
+    _header: Literal["General"] = "General"
+    fileversion: str = Field("3.01", alias="fileVersion")
+    filetype: Literal["roughness"] = Field("roughness", alias="fileType")
+    frictionvaluesfile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="frictionValuesFile"
+    )
+
+    _disk_only_file_model_should_not_be_none = (
+        validator_set_default_disk_only_file_model_when_none()
+    )
+
+
+class FrictGlobal(INIBasedModel):
+    """A `[Global]` block for use inside a friction file.
+
+    Multiple of such blocks may be present to define multiple frictionId classes.
+    """
+
+    class Comments(INIBasedModel.Comments):
+        frictionid: Optional[str] = Field(
+            "Name of the roughness variable.", alias="frictionId"
+        )
+        frictiontype: Optional[str] = Field(
+            "The global roughness type for this variable, which is used "
+            + "if no branch specific roughness definition is given.",
+            alias="frictionType",
+        )
+        frictionvalue: Optional[str] = Field(
+            "The global default value for this roughness variable.",
+            alias="frictionValue",
+        )
+
+    comments: Comments = Comments()
+    _header: Literal["Global"] = "Global"
+    frictionid: str = Field(alias="frictionId")
+    frictiontype: FrictionType = Field(alias="frictionType")
+    frictionvalue: float = Field(alias="frictionValue")
+
+    _frictiontype_validator = get_enum_validator("frictiontype", enum=FrictionType)
+
+    def _get_identifier(self, data: dict) -> Optional[str]:
+        return data.get("frictionid")
+
+
+class FrictBranch(INIBasedModel):
+    """A `[Branch]` block for use inside a friction file.
+
+    Each block can define the roughness value(s) on a particular branch.
+    """
+
+    class Comments(INIBasedModel.Comments):
+        branchid: Optional[str] = Field("The name of the branch.", alias="branchId")
+        frictiontype: Optional[str] = Field(
+            "The roughness type to be used on this branch.", alias="frictionType"
+        )
+        functiontype: Optional[str] = Field(
+            "Function type for the calculation of the value. "
+            + "Possible values: constant, timeSeries, absDischarge, waterlevel.",
+            alias="functionType",
+        )
+        timeseriesid: Optional[str] = Field(
+            "Refers to a data block in the <*.bc> frictionValuesFile. "
+            + "Only if functionType = timeSeries.",
+            alias="timeSeriesId",
+        )
+        numlevels: Optional[str] = Field(
+            "Number of levels in table. Only if functionType is not constant.",
+            alias="numLevels",
+        )
+        levels: Optional[str] = Field(
+            "Space separated list of discharge [m3/s] or water level [m AD] values. "
+            + "Only if functionType is absDischarge or waterLevel."
+        )
+        numlocations: Optional[str] = Field(
+            "Number of locations on branch. The default 0 implies branch uniform values.",
+            alias="numLocations",
+        )
+        chainage: Optional[str] = Field(
+            "Space separated list of locations on the branch [m]. Locations sorted by "
+            + "increasing chainage. The keyword must be specified if numLocations>0."
+        )
+        frictionvalues: Optional[str] = Field(
+            "numLevels lines containing space separated lists of roughness values: "
+            + "numLocations values per line. If the functionType is constant, then a "
+            + "single line is required. For a uniform roughness per branch "
+            + "(numLocations = 0) a single entry per line is required. The meaning "
+            + "of the values depends on the roughness type selected (see frictionType).",
+            alias="frictionValues",
+        )
+
+    comments: Comments = Comments()
+    _header: Literal["Branch"] = "Branch"
+    branchid: str = Field(alias="branchId")
+    frictiontype: FrictionType = Field(alias="frictionType")
+    functiontype: Optional[str] = Field("constant", alias="functionType")
+    timeseriesid: Optional[str] = Field(alias="timeSeriesId")
+    numlevels: Optional[PositiveInt] = Field(alias="numLevels")
+    levels: Optional[List[float]]
+    numlocations: Optional[NonNegativeInt] = Field(0, alias="numLocations")
+    chainage: Optional[List[float]]
+    frictionvalues: Optional[List[float]] = Field(
+        alias="frictionValues"
+    )  # TODO: turn this into List[List[float]], see issue #143.
+
+    _split_to_list = get_split_string_on_delimiter_validator(
+        "levels",
+        "chainage",
+        "frictionvalues",
+    )
+
+    _frictiontype_validator = get_enum_validator("frictiontype", enum=FrictionType)
+
+    def _get_identifier(self, data: dict) -> Optional[str]:
+        return data.get("branchid")
+
+    @validator("levels", always=True)
+    @classmethod
+    def _validate_levels(cls, v, values):
+        if v is not None and (
+            values["numlevels"] is None or len(v) != values["numlevels"]
+        ):
+            raise ValueError(
+                f"Number of values for levels should be equal to the numLevels value (branchId={values.get('branchid', '')})."
+            )
+
+        return v
+
+    @validator("chainage", always=True)
+    @classmethod
+    def _validate_chainage(cls, v, values):
+        if v is not None and len(v) != values["numlocations"]:
+            raise ValueError(
+                f"Number of values for chainage should be equal to the numLocations value (branchId={values.get('branchid', '')})."
+            )
+
+        return v
+
+    @validator("frictionvalues", always=True)
+    @classmethod
+    def _validate_frictionvalues(cls, v, values):
+        # number of values should be equal to numlocations*numlevels
+        numlevels = (
+            1
+            if (
+                "numlevels" not in values
+                or values["numlevels"] is None
+                or values["numlevels"] == 0
+            )
+            else values["numlevels"]
+        )
+        numvals = max(1, values["numlocations"]) * numlevels
+        if v is not None and len(v) != numvals:
+            raise ValueError(
+                f"Number of values for frictionValues should be equal to the numLocations*numLevels value (branchId={values.get('branchid', '')})."
+            )
+
+        return v
+
+
+class FrictionModel(INIModel):
+    """
+    The overall friction model that contains the contents of one friction file.
+
+    This model is typically referenced under a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.frictfile[..]`.
+
+    Attributes:
+        general (FrictGeneral): `[General]` block with file metadata.
+        global_ (List[FrictGlobal]): Definitions of `[Global]` friction classes.
+        branch (List[FrictBranch]): Definitions of `[Branch]` friction values.
+    """
+
+    general: FrictGeneral = FrictGeneral()
+    global_: List[FrictGlobal] = Field([], alias="global")  # to circumvent built-in kw
+    branch: List[FrictBranch] = []
+
+    _split_to_list = make_list_validator(
+        "global_",
+        "branch",
+    )
+
+    @classmethod
+    def _ext(cls) -> str:
+        return ".ini"
+
+    @classmethod
+    def _filename(cls) -> str:
+        return "roughness"
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/gui/models.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/gui/models.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,132 +1,132 @@
-"""
-namespace for storing the branches as branches.gui file
-"""
-# TODO reconsider the definition and/or filename of the branches.gui (from Prisca)
-
-import logging
-from typing import List, Literal, Optional
-
-from pydantic.class_validators import root_validator, validator
-from pydantic.fields import Field
-
-from hydrolib.core.dflowfm.ini.models import INIBasedModel, INIGeneral, INIModel
-from hydrolib.core.dflowfm.ini.util import make_list_validator
-
-logger = logging.getLogger(__name__)
-
-# FIXME: GUI does not recongnize this section yet
-class BranchGeneral(INIGeneral):
-    """The branches.gui file's `[General]` section with file meta data."""
-
-    class Comments(INIBasedModel.Comments):
-        fileversion: Optional[str] = Field(
-            "File version. Do not edit this.", alias="fileVersion"
-        )
-        filetype: Optional[str] = Field(
-            "File type. Should be 'branches'. Do not edit this.",
-            alias="fileType",
-        )
-
-    comments: Comments = Comments()
-    _header: Literal["General"] = "General"
-    fileversion: str = Field("2.00", alias="fileVersion")
-    filetype: Literal["branches"] = Field("branches", alias="fileType")
-
-
-class Branch(INIBasedModel):
-    """
-    A branch that is included in the branches.gui file.
-    """
-
-    class Comments(INIBasedModel.Comments):
-        name: Optional[str] = "Unique branch id."
-        branchtype: Optional[str] = Field(
-            "Channel = 0, SewerConnection = 1, Pipe = 2.", alias="branchType"
-        )
-        islengthcustom: Optional[str] = Field(
-            "branch length specified by user.", alias="isLengthCustom"
-        )
-        sourcecompartmentname: Optional[str] = Field(
-            "Source compartment name this sewer connection is beginning.",
-            alias="sourceCompartmentName",
-        )
-        targetcompartmentname: Optional[str] = Field(
-            "Source compartment name this sewer connection is beginning.",
-            alias="targetCompartmentName",
-        )
-        material: Optional[str] = Field(
-            "0 = Unknown, 1 = Concrete, 2 = CastIron, 3 = StoneWare, 4 = Hdpe, "
-            "5 = Masonry, 6 = SheetMetal, 7 = Polyester, 8 = Polyvinylchlorid, 9 = Steel"
-        )
-
-    comments: Comments = Comments()
-
-    _header: Literal["Branch"] = "Branch"
-
-    name: str = Field("name", max_length=255, alias="name")
-    branchtype: int = Field(0, alias="branchType")
-    islengthcustom: Optional[bool] = Field(True, alias="isLengthCustom")
-    sourcecompartmentname: Optional[str] = Field(None, alias="sourceCompartmentName")
-    targetcompartmentname: Optional[str] = Field(None, alias="targetCompartmentName")
-    material: Optional[int] = Field(None, alias="material")
-
-    def _get_identifier(self, data: dict) -> Optional[str]:
-        return data.get("name")
-
-    @root_validator
-    @classmethod
-    def _validate_branch(cls, values: dict):
-        if values.get("branchtype") == 2 and (
-            values.get("sourcecompartmentname") is None
-            and values.get("targetcompartmentname") is None
-        ):
-            raise ValueError(
-                "Either sourceCompartmentName or targetCompartmentName should be provided when branchType is 2."
-            )
-
-        return values
-
-    @validator("branchtype")
-    def _validate_branchtype(cls, branchtype: int):
-        allowed_branchtypes = [0, 1, 2]
-        if branchtype not in allowed_branchtypes:
-            str_allowed_branchtypes = [str(i) for i in allowed_branchtypes]
-            error_msg = f"branchType ({branchtype}) is not allowed. Allowed values: {', '.join(str_allowed_branchtypes)}"
-            raise ValueError(error_msg)
-
-        return branchtype
-
-    @validator("material")
-    def _validate_material(cls, material: int):
-        allowed_materials = range(10)
-        if material not in allowed_materials:
-            str_allowed_materials = [str(i) for i in allowed_materials]
-            error_msg = f"material ({material}) is not allowed. Allowed values: {', '.join(str_allowed_materials)}"
-            raise ValueError(error_msg)
-
-        return material
-
-
-class BranchModel(INIModel):
-    """
-    The overall branch model that contains the contents of one branches.gui file.
-
-    This model is not referenced under a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel].
-
-    Attributes:
-        general (BranchGeneral): `[General]` block with file metadata.
-        branch(List[Branch]): List of `[Branch]` blocks for all branches.
-    """
-
-    general: BranchGeneral = BranchGeneral()
-    branch: List[Branch] = []
-
-    _make_list = make_list_validator("branch")
-
-    @classmethod
-    def _ext(cls) -> str:
-        return ".gui"
-
-    @classmethod
-    def _filename(cls) -> str:
-        return "branches"
+"""
+namespace for storing the branches as branches.gui file
+"""
+# TODO reconsider the definition and/or filename of the branches.gui (from Prisca)
+
+import logging
+from typing import List, Literal, Optional
+
+from pydantic.class_validators import root_validator, validator
+from pydantic.fields import Field
+
+from hydrolib.core.dflowfm.ini.models import INIBasedModel, INIGeneral, INIModel
+from hydrolib.core.dflowfm.ini.util import make_list_validator
+
+logger = logging.getLogger(__name__)
+
+# FIXME: GUI does not recongnize this section yet
+class BranchGeneral(INIGeneral):
+    """The branches.gui file's `[General]` section with file meta data."""
+
+    class Comments(INIBasedModel.Comments):
+        fileversion: Optional[str] = Field(
+            "File version. Do not edit this.", alias="fileVersion"
+        )
+        filetype: Optional[str] = Field(
+            "File type. Should be 'branches'. Do not edit this.",
+            alias="fileType",
+        )
+
+    comments: Comments = Comments()
+    _header: Literal["General"] = "General"
+    fileversion: str = Field("2.00", alias="fileVersion")
+    filetype: Literal["branches"] = Field("branches", alias="fileType")
+
+
+class Branch(INIBasedModel):
+    """
+    A branch that is included in the branches.gui file.
+    """
+
+    class Comments(INIBasedModel.Comments):
+        name: Optional[str] = "Unique branch id."
+        branchtype: Optional[str] = Field(
+            "Channel = 0, SewerConnection = 1, Pipe = 2.", alias="branchType"
+        )
+        islengthcustom: Optional[str] = Field(
+            "branch length specified by user.", alias="isLengthCustom"
+        )
+        sourcecompartmentname: Optional[str] = Field(
+            "Source compartment name this sewer connection is beginning.",
+            alias="sourceCompartmentName",
+        )
+        targetcompartmentname: Optional[str] = Field(
+            "Source compartment name this sewer connection is beginning.",
+            alias="targetCompartmentName",
+        )
+        material: Optional[str] = Field(
+            "0 = Unknown, 1 = Concrete, 2 = CastIron, 3 = StoneWare, 4 = Hdpe, "
+            "5 = Masonry, 6 = SheetMetal, 7 = Polyester, 8 = Polyvinylchlorid, 9 = Steel"
+        )
+
+    comments: Comments = Comments()
+
+    _header: Literal["Branch"] = "Branch"
+
+    name: str = Field("name", max_length=255, alias="name")
+    branchtype: int = Field(0, alias="branchType")
+    islengthcustom: Optional[bool] = Field(True, alias="isLengthCustom")
+    sourcecompartmentname: Optional[str] = Field(None, alias="sourceCompartmentName")
+    targetcompartmentname: Optional[str] = Field(None, alias="targetCompartmentName")
+    material: Optional[int] = Field(None, alias="material")
+
+    def _get_identifier(self, data: dict) -> Optional[str]:
+        return data.get("name")
+
+    @root_validator
+    @classmethod
+    def _validate_branch(cls, values: dict):
+        if values.get("branchtype") == 2 and (
+            values.get("sourcecompartmentname") is None
+            and values.get("targetcompartmentname") is None
+        ):
+            raise ValueError(
+                "Either sourceCompartmentName or targetCompartmentName should be provided when branchType is 2."
+            )
+
+        return values
+
+    @validator("branchtype")
+    def _validate_branchtype(cls, branchtype: int):
+        allowed_branchtypes = [0, 1, 2]
+        if branchtype not in allowed_branchtypes:
+            str_allowed_branchtypes = [str(i) for i in allowed_branchtypes]
+            error_msg = f"branchType ({branchtype}) is not allowed. Allowed values: {', '.join(str_allowed_branchtypes)}"
+            raise ValueError(error_msg)
+
+        return branchtype
+
+    @validator("material")
+    def _validate_material(cls, material: int):
+        allowed_materials = range(10)
+        if material not in allowed_materials:
+            str_allowed_materials = [str(i) for i in allowed_materials]
+            error_msg = f"material ({material}) is not allowed. Allowed values: {', '.join(str_allowed_materials)}"
+            raise ValueError(error_msg)
+
+        return material
+
+
+class BranchModel(INIModel):
+    """
+    The overall branch model that contains the contents of one branches.gui file.
+
+    This model is not referenced under a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel].
+
+    Attributes:
+        general (BranchGeneral): `[General]` block with file metadata.
+        branch(List[Branch]): List of `[Branch]` blocks for all branches.
+    """
+
+    general: BranchGeneral = BranchGeneral()
+    branch: List[Branch] = []
+
+    _make_list = make_list_validator("branch")
+
+    @classmethod
+    def _ext(cls) -> str:
+        return ".gui"
+
+    @classmethod
+    def _filename(cls) -> str:
+        return "branches"
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/ini/models.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/ini/models.py`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,314 +1,314 @@
-import logging
-from abc import ABC
-from enum import Enum
-from math import isnan
-from typing import Any, Callable, List, Literal, Optional, Set, Type, Union
-
-from pydantic import Extra, Field, root_validator
-from pydantic.class_validators import validator
-from pydantic.fields import ModelField
-
-from hydrolib.core import __version__ as version
-from hydrolib.core.basemodel import (
-    BaseModel,
-    FileModel,
-    FilePathStyleConverter,
-    ModelSaveSettings,
-    ParsableFileModel,
-)
-
-from ..ini.io_models import CommentBlock, Document, Property, Section
-from .parser import Parser
-from .serializer import (
-    DataBlockINIBasedSerializerConfig,
-    INISerializerConfig,
-    write_ini,
-)
-from .util import make_list_validator
-
-logger = logging.getLogger(__name__)
-
-
-class INIBasedModel(BaseModel, ABC):
-    """INIBasedModel defines the base model for blocks/chapters
-    inside an INIModel (*.ini file).
-
-    INIBasedModel instances can be created from Section instances
-    obtained through parsing ini documents. It further supports
-    adding arbitrary fields to it, which will be written to file.
-    Lastly, no arbitrary types are allowed for the defined fields.
-
-    Attributes:
-        comments (Optional[Comments]):
-            Optional Comments if defined by the user, containing
-            descriptions for all data fields.
-    """
-
-    _header: str
-    _file_path_style_converter = FilePathStyleConverter()
-
-    class Config:
-        extra = Extra.ignore
-        arbitrary_types_allowed = False
-
-    @classmethod
-    def _supports_comments(cls):
-        return True
-
-    @classmethod
-    def _duplicate_keys_as_list(cls):
-        return False
-
-    @classmethod
-    def get_list_delimiter(cls) -> str:
-        """List delimiter string that will be used for serializing
-        list field values for any IniBasedModel, **if** that field has
-        no custom list delimiter.
-
-        This function should be overridden by any subclass for a particular
-        filetype that needs a specific/different list separator.
-        """
-        return " "
-
-    @classmethod
-    def get_list_field_delimiter(cls, field_key: str) -> str:
-        """List delimiter string that will be used for serializing
-        the given field's value.
-        The returned delimiter is either the field's custom list delimiter
-        if that was specified using Field(.., delimiter=".."), or the
-        default list delimiter for the model class that this field belongs
-        to.
-
-        Args:
-            field_key (str): the original field key (not its alias).
-        """
-        delimiter = None
-        if (field := cls.__fields__.get(field_key)) and isinstance(field, ModelField):
-            delimiter = field.field_info.extra.get("delimiter")
-        if not delimiter:
-            delimiter = cls.get_list_delimiter()
-
-        return delimiter
-
-    class Comments(BaseModel, ABC):
-        """Comments defines the comments of an INIBasedModel"""
-
-        class Config:
-            extra = Extra.allow
-            arbitrary_types_allowed = False
-
-    comments: Optional[Comments] = Comments()
-
-    @root_validator(pre=True)
-    def _skip_nones_and_set_header(cls, values):
-        """Drop None fields for known fields."""
-        dropkeys = []
-        for k, v in values.items():
-            if v is None and k in cls.__fields__.keys():
-                dropkeys.append(k)
-
-        logger.info(f"Dropped unset keys: {dropkeys}")
-        for k in dropkeys:
-            values.pop(k)
-
-        if "_header" in values:
-            values["_header"] = cls._header
-
-        return values
-
-    @validator("comments", always=True, allow_reuse=True)
-    def comments_matches_has_comments(cls, v):
-        if not cls._supports_comments() and v is not None:
-            logging.warning(f"Dropped unsupported comments from {cls.__name__} init.")
-            v = None
-        return v
-
-    @classmethod
-    def validate(cls: Type["INIBasedModel"], value: Any) -> "INIBasedModel":
-        if isinstance(value, Section):
-            value = value.flatten(
-                cls._duplicate_keys_as_list(), cls._supports_comments()
-            )
-
-        return super().validate(value)
-
-    @classmethod
-    def _exclude_fields(cls) -> Set:
-        return {"comments", "datablock", "_header"}
-
-    def _convert_value(
-        self,
-        key: str,
-        v: Any,
-        config: INISerializerConfig,
-        save_settings: ModelSaveSettings,
-    ) -> str:
-        if isinstance(v, bool):
-            return str(int(v))
-        elif isinstance(v, list):
-            convert_value = lambda x: self._convert_value(key, x, config, save_settings)
-            return self.__class__.get_list_field_delimiter(key).join(
-                [convert_value(x) for x in v]
-            )
-        elif isinstance(v, Enum):
-            return v.value
-        elif isinstance(v, float):
-            return f"{v:{config.float_format}}"
-        elif isinstance(v, FileModel) and v.filepath is not None:
-            return self._file_path_style_converter.convert_from_os_style(
-                v.filepath, save_settings.path_style
-            )
-        elif v is None:
-            return ""
-        else:
-            return str(v)
-
-    def _to_section(
-        self, config: INISerializerConfig, save_settings: ModelSaveSettings
-    ) -> Section:
-        props = []
-        for key, value in self:
-            if key in self._exclude_fields():
-                continue
-
-            field_key = key
-            if key in self.__fields__:
-                key = self.__fields__[key].alias
-
-            prop = Property(
-                key=key,
-                value=self._convert_value(field_key, value, config, save_settings),
-                comment=getattr(self.comments, field_key, None),
-            )
-            props.append(prop)
-        return Section(header=self._header, content=props)
-
-
-Datablock = List[List[Union[float, str]]]
-
-
-class DataBlockINIBasedModel(INIBasedModel):
-    """DataBlockINIBasedModel defines the base model for ini models with datablocks.
-
-    Attributes:
-        datablock (Datablock): (class attribute) the actual data columns.
-    """
-
-    datablock: Datablock = []
-
-    _make_lists = make_list_validator("datablock")
-
-    def _to_section(
-        self,
-        config: DataBlockINIBasedSerializerConfig,
-        save_settings: ModelSaveSettings,
-    ) -> Section:
-        section = super()._to_section(config, save_settings)
-        section.datablock = self._to_datablock(config)
-        return section
-
-    def _to_datablock(self, config: DataBlockINIBasedSerializerConfig) -> List[List]:
-        converted_datablock = []
-
-        for row in self.datablock:
-            converted_row = (
-                DataBlockINIBasedModel.convert_value(value, config) for value in row
-            )
-            converted_datablock.append(list(converted_row))
-
-        return converted_datablock
-
-    @classmethod
-    def convert_value(
-        cls, value: Union[float, str], config: DataBlockINIBasedSerializerConfig
-    ) -> str:
-        if isinstance(value, float):
-            return f"{value:{config.float_format_datablock}}"
-
-        return value
-
-    @validator("datablock")
-    def _validate_no_nans_are_present(cls, datablock: Datablock) -> Datablock:
-        """Validate that the datablock does not have any NaN values.
-
-        Args:
-            datablock (Datablock): The datablock to verify.
-
-        Raises:
-            ValueError: When a NaN is present in the datablock.
-
-        Returns:
-            Datablock: The validated datablock.
-        """
-        if any(cls._is_float_and_nan(value) for list in datablock for value in list):
-            raise ValueError("NaN is not supported in datablocks.")
-
-        return datablock
-
-    @staticmethod
-    def _is_float_and_nan(value: float) -> bool:
-        return isinstance(value, float) and isnan(value)
-
-
-class INIGeneral(INIBasedModel):
-    _header: Literal["General"] = "General"
-    fileversion: str = Field("3.00", alias="fileVersion")
-    filetype: str = Field(alias="fileType")
-
-    @classmethod
-    def _supports_comments(cls):
-        return True
-
-
-class INIModel(ParsableFileModel):
-    """INI Model representation of a *.ini file.
-
-    Typically subclasses will implement the various sorts of ini files,
-    specifically for their fileType/contents.
-    Child elements of this class associated with chapters/blocks in the
-    ini file will be (sub)class of INIBasedModel.
-    """
-
-    serializer_config: INISerializerConfig = INISerializerConfig()
-
-    general: INIGeneral
-
-    @classmethod
-    def _ext(cls) -> str:
-        return ".ini"
-
-    @classmethod
-    def _filename(cls) -> str:
-        return "fm"
-
-    @classmethod
-    def _get_serializer(cls):
-        pass  # unused in favor of direct _serialize
-
-    @classmethod
-    def _get_parser(cls) -> Callable:
-        return Parser.parse_as_dict
-
-    def _to_document(self, save_settings: ModelSaveSettings) -> Document:
-        header = CommentBlock(lines=[f"written by HYDROLIB-core {version}"])
-        sections = []
-        for key, value in self:
-            if key in self._exclude_fields() or value is None:
-                continue
-            if isinstance(value, list):
-                for v in value:
-                    sections.append(
-                        v._to_section(self.serializer_config, save_settings)
-                    )
-            else:
-                sections.append(
-                    value._to_section(self.serializer_config, save_settings)
-                )
-        return Document(header_comment=[header], sections=sections)
-
-    def _serialize(self, _: dict, save_settings: ModelSaveSettings) -> None:
-        write_ini(
-            self._resolved_filepath,
-            self._to_document(save_settings),
-            config=self.serializer_config,
-        )
+import logging
+from abc import ABC
+from enum import Enum
+from math import isnan
+from typing import Any, Callable, List, Literal, Optional, Set, Type, Union
+
+from pydantic import Extra, Field, root_validator
+from pydantic.class_validators import validator
+from pydantic.fields import ModelField
+
+from hydrolib.core import __version__ as version
+from hydrolib.core.basemodel import (
+    BaseModel,
+    FileModel,
+    FilePathStyleConverter,
+    ModelSaveSettings,
+    ParsableFileModel,
+)
+
+from ..ini.io_models import CommentBlock, Document, Property, Section
+from .parser import Parser
+from .serializer import (
+    DataBlockINIBasedSerializerConfig,
+    INISerializerConfig,
+    write_ini,
+)
+from .util import make_list_validator
+
+logger = logging.getLogger(__name__)
+
+
+class INIBasedModel(BaseModel, ABC):
+    """INIBasedModel defines the base model for blocks/chapters
+    inside an INIModel (*.ini file).
+
+    INIBasedModel instances can be created from Section instances
+    obtained through parsing ini documents. It further supports
+    adding arbitrary fields to it, which will be written to file.
+    Lastly, no arbitrary types are allowed for the defined fields.
+
+    Attributes:
+        comments (Optional[Comments]):
+            Optional Comments if defined by the user, containing
+            descriptions for all data fields.
+    """
+
+    _header: str
+    _file_path_style_converter = FilePathStyleConverter()
+
+    class Config:
+        extra = Extra.ignore
+        arbitrary_types_allowed = False
+
+    @classmethod
+    def _supports_comments(cls):
+        return True
+
+    @classmethod
+    def _duplicate_keys_as_list(cls):
+        return False
+
+    @classmethod
+    def get_list_delimiter(cls) -> str:
+        """List delimiter string that will be used for serializing
+        list field values for any IniBasedModel, **if** that field has
+        no custom list delimiter.
+
+        This function should be overridden by any subclass for a particular
+        filetype that needs a specific/different list separator.
+        """
+        return " "
+
+    @classmethod
+    def get_list_field_delimiter(cls, field_key: str) -> str:
+        """List delimiter string that will be used for serializing
+        the given field's value.
+        The returned delimiter is either the field's custom list delimiter
+        if that was specified using Field(.., delimiter=".."), or the
+        default list delimiter for the model class that this field belongs
+        to.
+
+        Args:
+            field_key (str): the original field key (not its alias).
+        """
+        delimiter = None
+        if (field := cls.__fields__.get(field_key)) and isinstance(field, ModelField):
+            delimiter = field.field_info.extra.get("delimiter")
+        if not delimiter:
+            delimiter = cls.get_list_delimiter()
+
+        return delimiter
+
+    class Comments(BaseModel, ABC):
+        """Comments defines the comments of an INIBasedModel"""
+
+        class Config:
+            extra = Extra.allow
+            arbitrary_types_allowed = False
+
+    comments: Optional[Comments] = Comments()
+
+    @root_validator(pre=True)
+    def _skip_nones_and_set_header(cls, values):
+        """Drop None fields for known fields."""
+        dropkeys = []
+        for k, v in values.items():
+            if v is None and k in cls.__fields__.keys():
+                dropkeys.append(k)
+
+        logger.info(f"Dropped unset keys: {dropkeys}")
+        for k in dropkeys:
+            values.pop(k)
+
+        if "_header" in values:
+            values["_header"] = cls._header
+
+        return values
+
+    @validator("comments", always=True, allow_reuse=True)
+    def comments_matches_has_comments(cls, v):
+        if not cls._supports_comments() and v is not None:
+            logging.warning(f"Dropped unsupported comments from {cls.__name__} init.")
+            v = None
+        return v
+
+    @classmethod
+    def validate(cls: Type["INIBasedModel"], value: Any) -> "INIBasedModel":
+        if isinstance(value, Section):
+            value = value.flatten(
+                cls._duplicate_keys_as_list(), cls._supports_comments()
+            )
+
+        return super().validate(value)
+
+    @classmethod
+    def _exclude_fields(cls) -> Set:
+        return {"comments", "datablock", "_header"}
+
+    def _convert_value(
+        self,
+        key: str,
+        v: Any,
+        config: INISerializerConfig,
+        save_settings: ModelSaveSettings,
+    ) -> str:
+        if isinstance(v, bool):
+            return str(int(v))
+        elif isinstance(v, list):
+            convert_value = lambda x: self._convert_value(key, x, config, save_settings)
+            return self.__class__.get_list_field_delimiter(key).join(
+                [convert_value(x) for x in v]
+            )
+        elif isinstance(v, Enum):
+            return v.value
+        elif isinstance(v, float):
+            return f"{v:{config.float_format}}"
+        elif isinstance(v, FileModel) and v.filepath is not None:
+            return self._file_path_style_converter.convert_from_os_style(
+                v.filepath, save_settings.path_style
+            )
+        elif v is None:
+            return ""
+        else:
+            return str(v)
+
+    def _to_section(
+        self, config: INISerializerConfig, save_settings: ModelSaveSettings
+    ) -> Section:
+        props = []
+        for key, value in self:
+            if key in self._exclude_fields():
+                continue
+
+            field_key = key
+            if key in self.__fields__:
+                key = self.__fields__[key].alias
+
+            prop = Property(
+                key=key,
+                value=self._convert_value(field_key, value, config, save_settings),
+                comment=getattr(self.comments, field_key, None),
+            )
+            props.append(prop)
+        return Section(header=self._header, content=props)
+
+
+Datablock = List[List[Union[float, str]]]
+
+
+class DataBlockINIBasedModel(INIBasedModel):
+    """DataBlockINIBasedModel defines the base model for ini models with datablocks.
+
+    Attributes:
+        datablock (Datablock): (class attribute) the actual data columns.
+    """
+
+    datablock: Datablock = []
+
+    _make_lists = make_list_validator("datablock")
+
+    def _to_section(
+        self,
+        config: DataBlockINIBasedSerializerConfig,
+        save_settings: ModelSaveSettings,
+    ) -> Section:
+        section = super()._to_section(config, save_settings)
+        section.datablock = self._to_datablock(config)
+        return section
+
+    def _to_datablock(self, config: DataBlockINIBasedSerializerConfig) -> List[List]:
+        converted_datablock = []
+
+        for row in self.datablock:
+            converted_row = (
+                DataBlockINIBasedModel.convert_value(value, config) for value in row
+            )
+            converted_datablock.append(list(converted_row))
+
+        return converted_datablock
+
+    @classmethod
+    def convert_value(
+        cls, value: Union[float, str], config: DataBlockINIBasedSerializerConfig
+    ) -> str:
+        if isinstance(value, float):
+            return f"{value:{config.float_format_datablock}}"
+
+        return value
+
+    @validator("datablock")
+    def _validate_no_nans_are_present(cls, datablock: Datablock) -> Datablock:
+        """Validate that the datablock does not have any NaN values.
+
+        Args:
+            datablock (Datablock): The datablock to verify.
+
+        Raises:
+            ValueError: When a NaN is present in the datablock.
+
+        Returns:
+            Datablock: The validated datablock.
+        """
+        if any(cls._is_float_and_nan(value) for list in datablock for value in list):
+            raise ValueError("NaN is not supported in datablocks.")
+
+        return datablock
+
+    @staticmethod
+    def _is_float_and_nan(value: float) -> bool:
+        return isinstance(value, float) and isnan(value)
+
+
+class INIGeneral(INIBasedModel):
+    _header: Literal["General"] = "General"
+    fileversion: str = Field("3.00", alias="fileVersion")
+    filetype: str = Field(alias="fileType")
+
+    @classmethod
+    def _supports_comments(cls):
+        return True
+
+
+class INIModel(ParsableFileModel):
+    """INI Model representation of a *.ini file.
+
+    Typically subclasses will implement the various sorts of ini files,
+    specifically for their fileType/contents.
+    Child elements of this class associated with chapters/blocks in the
+    ini file will be (sub)class of INIBasedModel.
+    """
+
+    serializer_config: INISerializerConfig = INISerializerConfig()
+
+    general: INIGeneral
+
+    @classmethod
+    def _ext(cls) -> str:
+        return ".ini"
+
+    @classmethod
+    def _filename(cls) -> str:
+        return "fm"
+
+    @classmethod
+    def _get_serializer(cls):
+        pass  # unused in favor of direct _serialize
+
+    @classmethod
+    def _get_parser(cls) -> Callable:
+        return Parser.parse_as_dict
+
+    def _to_document(self, save_settings: ModelSaveSettings) -> Document:
+        header = CommentBlock(lines=[f"written by HYDROLIB-core {version}"])
+        sections = []
+        for key, value in self:
+            if key in self._exclude_fields() or value is None:
+                continue
+            if isinstance(value, list):
+                for v in value:
+                    sections.append(
+                        v._to_section(self.serializer_config, save_settings)
+                    )
+            else:
+                sections.append(
+                    value._to_section(self.serializer_config, save_settings)
+                )
+        return Document(header_comment=[header], sections=sections)
+
+    def _serialize(self, _: dict, save_settings: ModelSaveSettings) -> None:
+        write_ini(
+            self._resolved_filepath,
+            self._to_document(save_settings),
+            config=self.serializer_config,
+        )
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/ini/parser.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/ini/parser.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,398 +1,398 @@
-import re
-from enum import IntEnum
-from pathlib import Path
-from typing import Callable, Dict, List, Optional, Tuple, Union
-
-from pydantic import validator
-
-from hydrolib.core.basemodel import BaseModel
-from hydrolib.core.dflowfm.ini.io_models import (
-    CommentBlock,
-    Document,
-    Property,
-    Section,
-)
-
-
-class ParserConfig(BaseModel):
-    """ParserConfig defines the configuration options of the Parser
-
-    Note that we cannot set both allow_only_keywords and parse_datablocks to True
-    because we cannot distinguish between datablocks and key only properties. As
-    such this will lead to a validation error.
-
-    Attributes:
-        allow_only_keywords (bool):
-            Whether to allow properties with only keys (no '=' or value).
-            Defaults to False.
-        parse_datablocks (bool):
-            Whether to allow parsing of datablocks at the bottom of sections.
-            Defaults to False.
-        parse_comments (bool):
-            Whether we allow parsing of comments defined with the comment_delimeter.
-            Defaults to True.
-        comment_delimiter (str):
-            The character or sequence of character used to define a comment.
-            Defaults to '#'.
-    """
-
-    allow_only_keywords: bool = False
-    parse_datablocks: bool = False
-    parse_comments: bool = True
-    comment_delimiter: str = "#"
-
-    @validator("parse_datablocks")
-    def allow_only_keywods_and_parse_datablocks_leads_should_not_both_be_true(
-        cls, parse_datablocks, values
-    ):
-        # if both allow_only_keywords and parse_datablocks is true, we cannot
-        # distinguish between the two, and the parsing will not recognise either
-        # properly
-        if (
-            parse_datablocks
-            and "allow_only_keywords" in values
-            and values["allow_only_keywords"]
-        ):
-            raise ValueError(
-                "Both parse_datablocks and allow_only_keywords should not be both True."
-            )
-        return parse_datablocks
-
-
-class _IntermediateCommentBlock(BaseModel):
-    # _IntermediateCommentBlock used to construct CommentBlock objects within
-    # this Parser.
-    start_line: int
-    lines: List[str] = []
-
-    def add_comment_line(self, line: str) -> None:
-        self.lines.append(line)
-
-    def finalize(self) -> CommentBlock:
-        return CommentBlock(
-            lines=self.lines,
-        )
-
-
-class _IntermediateSection(BaseModel):
-    # _IntermediateSection used to construct section objects within
-    # this Parser.
-    header: str
-    start_line: int
-
-    content: List[Union[Property, CommentBlock]] = []
-    datablock: List[List[str]] = []
-    curr_comment_block: Optional[_IntermediateCommentBlock] = None
-
-    def add_property(self, property: Property) -> None:
-        self._finalize_comment_block()
-        self.content.append(property)
-
-    def add_comment(self, comment: str, line: int) -> None:
-        if self.curr_comment_block is None:
-            self.curr_comment_block = _IntermediateCommentBlock(start_line=line)
-
-        self.curr_comment_block.add_comment_line(comment)
-
-    def add_datarow(self, row: List[str]) -> None:
-        self.datablock.append(row)
-
-    def _finalize_comment_block(self) -> None:
-        if self.curr_comment_block is None:
-            return
-
-        self.content.append(self.curr_comment_block.finalize())
-        self.curr_comment_block = None
-
-    def finalize(self) -> Section:
-        self._finalize_comment_block()
-        return Section(
-            header=self.header,
-            content=self.content,
-            datablock=self.datablock if self.datablock else None,
-        )
-
-
-# TODO: generalise StateBasedLineParser, and let both Ini.Parser and Polyfile.Parser inherit
-class Parser:
-    """Parser defines a generic Parser for Deltares ini files.
-
-    The Parser can be configured with a ParserConfig object.
-    """
-
-    class _StateType(IntEnum):
-        NO_SECTION_FOUND = 0
-        PARSING_PROPERTIES = 1
-        PARSING_DATABLOCK = 2
-
-    def __init__(self, config: ParserConfig) -> None:
-        """Creates a new Parser configured with the provided config
-
-        Args:
-            config (ParserConfig): The configuration of this Parser
-        """
-        self._config = config
-        self._document = Document()
-        self._current_section: Optional[_IntermediateSection] = None
-        self._current_header_block: Optional[_IntermediateCommentBlock] = None
-
-        self._state = self._StateType.NO_SECTION_FOUND
-        self._line_index = 0
-
-        # TODO add invalid blocks
-        self._feed_line: Dict[
-            Parser._StateType, List[Tuple[Callable[[str], bool], Callable[[str], None]]]
-        ] = {
-            Parser._StateType.NO_SECTION_FOUND: [
-                (self._is_comment, self._handle_header_comment),
-                (self._is_section_header, self._handle_next_section_header),
-            ],
-            Parser._StateType.PARSING_PROPERTIES: [
-                (self._is_comment, self._handle_section_comment),
-                (self._is_section_header, self._handle_next_section_header),
-                (self._is_property, self._handle_property),
-                (self._is_datarow, self._handle_new_datarow),
-            ],
-            Parser._StateType.PARSING_DATABLOCK: [
-                (self._is_section_header, self._handle_next_section_header),
-                (self._is_datarow, self._handle_datarow),
-            ],
-        }
-
-        self._handle_emptyline: Dict[Parser._StateType, Callable[[], None]] = {
-            self._StateType.NO_SECTION_FOUND: self._finish_current_header_block,
-            self._StateType.PARSING_PROPERTIES: self._noop,
-            self._StateType.PARSING_DATABLOCK: self._noop,
-        }
-
-    def feed_line(self, line: str) -> None:
-        """Parse the next line with this Parser.
-
-        Args:
-            line (str): The line to parse
-        """
-        if not self._is_empty_line(line):
-            for (is_line_type, handle_line_type) in self._feed_line[self._state]:
-                if is_line_type(line):
-                    handle_line_type(line)
-                    break
-            else:
-                # handle exception
-                pass
-        else:
-            self._handle_emptyline[self._state]()
-
-        self._increment_line()
-
-    def finalize(self) -> Document:
-        """Finalize parsing and return the constructed Document.
-
-        Returns:
-            Document:
-                A Document describing the parsed ini file.
-        """
-        # TODO handle invalid block
-        self._finish_current_header_block()
-        self._finalise_current_section()
-        return self._document
-
-    def _increment_line(self) -> None:
-        self._line_index += 1
-
-    def _handle_next_section_header(self, line: str) -> None:
-        self._finalise_current_section()
-        self._handle_new_section_header(line)
-
-        self._state = Parser._StateType.PARSING_PROPERTIES
-
-    def _handle_new_section_header(self, line: str) -> None:
-        section_header = line.strip()[1:-1].strip()
-        self._current_section = _IntermediateSection(
-            header=section_header, start_line=self._line_index
-        )
-
-    def _finalise_current_section(self) -> None:
-        if self._current_section is not None:
-            self._document.sections.append(self._current_section.finalize())
-
-    def _handle_header_comment(self, line: str) -> None:
-        if self._current_header_block is None:
-            self._current_header_block = _IntermediateCommentBlock(
-                start_line=self._line_index
-            )
-
-        comment = self._convert_to_comment(line)
-        self._current_header_block.add_comment_line(comment)
-
-    def _handle_section_comment(self, line: str) -> None:
-        comment = self._convert_to_comment(line)
-        self._current_section.add_comment(comment, self._line_index)  # type: ignore
-
-    def _handle_property(self, line: str) -> None:
-        key, valuepart = self._retrieve_key_value(line)
-        if valuepart is not None:
-            comment, value = self._retrieve_property_comment(valuepart.strip())
-        else:
-            comment, value = None, None
-
-        prop = Property(key=key, value=value, comment=comment)
-        self._current_section.add_property(prop)  # type: ignore
-
-    def _handle_new_datarow(self, line: str) -> None:
-        self._handle_datarow(line)
-        self._state = Parser._StateType.PARSING_DATABLOCK
-
-    def _handle_datarow(self, line: str) -> None:
-        self._current_section.add_datarow(line.split())  # type: ignore
-
-    def _retrieve_property_comment(self, line: str) -> Tuple[Optional[str], str]:
-        """Retrieve the comment and value part from the valuestring of a key-value pair.
-
-        The comment retrieval is complicated by the fact that in the Deltares-INI
-        dialect, the comment delimiter '#' plays a double role: it may also be used
-        to quote string values (for example if the contain spaces).
-
-        Example lines that are supported:
-        key = valueAndNoComment
-        key = valueA  # and a simple comment
-        key = #valueA with possible spaces#
-        key = #valueA#  # and a simple comment
-        key = #valueA# # and a complicated comment with hashes #1 example
-        key = value # and a complicated comment with hashes #2.
-
-        Keywords arguments:
-            line (str) -- the partial string of the line containing both value and
-                possibly a comment at the end. Note that the "key =" part must already
-                have been split off, for example by _retrieve_key_value
-
-        Returns:
-            Tuple with the comment and string value, respectively. If no comment is
-            present, the first tuple element is None.
-        """
-
-        if self._config.parse_comments and self._config.comment_delimiter in line:
-            line = line.strip()
-            parts = line.split(self._config.comment_delimiter)
-            numhash = line.count(self._config.comment_delimiter)
-            if numhash == 1:
-                # normal value, simple comment: "key =  somevalue # and a comment "
-                comment = parts[-1]
-                value = parts[0]
-            elif line.startswith(self._config.comment_delimiter):
-                # hashed value, possible with comment: "key = #somevalue# ..."
-                comment = (
-                    self._config.comment_delimiter.join(parts[3:])
-                    if numhash >= 3
-                    else ""
-                )
-
-                value = self._config.comment_delimiter.join(parts[0:3])
-            else:
-                # normal value, comment with maybe more hashes: "key = somevalue #This is comment #2, or two "
-                comment = self._config.comment_delimiter.join(parts[1:])
-                value = parts[0]
-        else:
-            comment = ""
-            value = line
-
-        return (
-            comment if len(comment := comment.strip()) > 0 else None,
-            value if len(value := value.strip()) > 0 else None,
-        )
-
-    def _retrieve_key_value(self, line: str) -> Tuple[str, Optional[str]]:
-        if "=" in line:
-            key, value = line.split("=", 1)
-            return key.strip(), value if len(value := value.strip()) > 0 else None
-        else:
-            # if no = exists, due to the previous check we know it will just be a
-            # single value
-            return line, None
-
-    def _finish_current_header_block(self) -> None:
-        if self._current_header_block is not None:
-            self._document.header_comment.append(self._current_header_block.finalize())
-            self._current_header_block = None
-
-    def _noop(self, *_, **__) -> None:
-        # no operation
-        pass
-
-    def _is_empty_line(self, line: str) -> bool:
-        return len(line.strip()) == 0
-
-    def _is_comment(self, line: str) -> bool:
-        return line.strip().startswith(self._config.comment_delimiter)
-
-    def _convert_to_comment(self, line: str) -> str:
-        return line.strip()[1:].strip()
-
-    def _is_section_header(self, line: str) -> bool:
-        # a header is defined as "[ any-value ]"
-        stripped = line.strip()
-        return stripped.startswith("[") and stripped.endswith("]")
-
-    def _is_property(self, line: str) -> bool:
-        # we assume that we already checked wether it is a comment or
-        # a section header.
-        return self._config.allow_only_keywords or "=" in line
-
-    def _is_datarow(self, _: str) -> bool:
-        # we assume that we already checked whether it is either a comment,
-        # section header or a property
-        return self._config.parse_datablocks
-
-    @classmethod
-    def parse_as_dict(cls, filepath: Path, config: ParserConfig = None) -> dict:
-        """
-        Parses an INI file without a specific model type and returns it as a dictionary.
-
-        Args:
-            filepath (Path): File path to the INI-format file.
-            config (ParserConfig, optional): Parser configuration to use. Defaults to None.
-
-        Returns:
-            dict: Representation of the parsed INI-file.
-        """
-        return cls.parse(filepath, config).flatten()
-
-    @classmethod
-    def parse(cls, filepath: Path, config: ParserConfig = None) -> Document:
-        """
-        Parses an INI file without a specific model type and returns it as a Document.
-
-        Args:
-            filepath (Path): File path to the INI-format file.
-            config (ParserConfig, optional): Parser configuration to use. Defaults to None.
-
-        Returns:
-            Document: Representation of the parsed INI-file.
-        """
-        if not config:
-            config = ParserConfig()
-        parser = cls(config)
-
-        progline = re.compile(
-            r"^([^#]*=\s*)([^#]*)(#.*)?"
-        )  # matches whole line: "Field = Value Maybe more # optional comment"
-        progfloat = re.compile(
-            r"([\d.]+)([dD])([+\-]?\d+)"
-        )  # matches a float value: 1d9, 1D-3, 1.D+4, etc.
-
-        with filepath.open(encoding="utf8") as f:
-            for line in f:
-                # Replace Fortran scientific notation for doubles
-                # Match number d/D +/- number (e.g. 1d-05 or 1.23D+01 or 1.d-4)
-                match = progline.match(line)
-                if match:  # Only process value
-                    line = (
-                        match.group(1)
-                        + progfloat.sub(r"\1e\3", match.group(2))
-                        + str(match.group(3) or "")
-                    )
-                else:  # Process full line
-                    line = progfloat.sub(r"\1e\3", line)
-
-                parser.feed_line(line)
-
-        return parser.finalize()
+import re
+from enum import IntEnum
+from pathlib import Path
+from typing import Callable, Dict, List, Optional, Tuple, Union
+
+from pydantic import validator
+
+from hydrolib.core.basemodel import BaseModel
+from hydrolib.core.dflowfm.ini.io_models import (
+    CommentBlock,
+    Document,
+    Property,
+    Section,
+)
+
+
+class ParserConfig(BaseModel):
+    """ParserConfig defines the configuration options of the Parser
+
+    Note that we cannot set both allow_only_keywords and parse_datablocks to True
+    because we cannot distinguish between datablocks and key only properties. As
+    such this will lead to a validation error.
+
+    Attributes:
+        allow_only_keywords (bool):
+            Whether to allow properties with only keys (no '=' or value).
+            Defaults to False.
+        parse_datablocks (bool):
+            Whether to allow parsing of datablocks at the bottom of sections.
+            Defaults to False.
+        parse_comments (bool):
+            Whether we allow parsing of comments defined with the comment_delimeter.
+            Defaults to True.
+        comment_delimiter (str):
+            The character or sequence of character used to define a comment.
+            Defaults to '#'.
+    """
+
+    allow_only_keywords: bool = False
+    parse_datablocks: bool = False
+    parse_comments: bool = True
+    comment_delimiter: str = "#"
+
+    @validator("parse_datablocks")
+    def allow_only_keywods_and_parse_datablocks_leads_should_not_both_be_true(
+        cls, parse_datablocks, values
+    ):
+        # if both allow_only_keywords and parse_datablocks is true, we cannot
+        # distinguish between the two, and the parsing will not recognise either
+        # properly
+        if (
+            parse_datablocks
+            and "allow_only_keywords" in values
+            and values["allow_only_keywords"]
+        ):
+            raise ValueError(
+                "Both parse_datablocks and allow_only_keywords should not be both True."
+            )
+        return parse_datablocks
+
+
+class _IntermediateCommentBlock(BaseModel):
+    # _IntermediateCommentBlock used to construct CommentBlock objects within
+    # this Parser.
+    start_line: int
+    lines: List[str] = []
+
+    def add_comment_line(self, line: str) -> None:
+        self.lines.append(line)
+
+    def finalize(self) -> CommentBlock:
+        return CommentBlock(
+            lines=self.lines,
+        )
+
+
+class _IntermediateSection(BaseModel):
+    # _IntermediateSection used to construct section objects within
+    # this Parser.
+    header: str
+    start_line: int
+
+    content: List[Union[Property, CommentBlock]] = []
+    datablock: List[List[str]] = []
+    curr_comment_block: Optional[_IntermediateCommentBlock] = None
+
+    def add_property(self, property: Property) -> None:
+        self._finalize_comment_block()
+        self.content.append(property)
+
+    def add_comment(self, comment: str, line: int) -> None:
+        if self.curr_comment_block is None:
+            self.curr_comment_block = _IntermediateCommentBlock(start_line=line)
+
+        self.curr_comment_block.add_comment_line(comment)
+
+    def add_datarow(self, row: List[str]) -> None:
+        self.datablock.append(row)
+
+    def _finalize_comment_block(self) -> None:
+        if self.curr_comment_block is None:
+            return
+
+        self.content.append(self.curr_comment_block.finalize())
+        self.curr_comment_block = None
+
+    def finalize(self) -> Section:
+        self._finalize_comment_block()
+        return Section(
+            header=self.header,
+            content=self.content,
+            datablock=self.datablock if self.datablock else None,
+        )
+
+
+# TODO: generalise StateBasedLineParser, and let both Ini.Parser and Polyfile.Parser inherit
+class Parser:
+    """Parser defines a generic Parser for Deltares ini files.
+
+    The Parser can be configured with a ParserConfig object.
+    """
+
+    class _StateType(IntEnum):
+        NO_SECTION_FOUND = 0
+        PARSING_PROPERTIES = 1
+        PARSING_DATABLOCK = 2
+
+    def __init__(self, config: ParserConfig) -> None:
+        """Creates a new Parser configured with the provided config
+
+        Args:
+            config (ParserConfig): The configuration of this Parser
+        """
+        self._config = config
+        self._document = Document()
+        self._current_section: Optional[_IntermediateSection] = None
+        self._current_header_block: Optional[_IntermediateCommentBlock] = None
+
+        self._state = self._StateType.NO_SECTION_FOUND
+        self._line_index = 0
+
+        # TODO add invalid blocks
+        self._feed_line: Dict[
+            Parser._StateType, List[Tuple[Callable[[str], bool], Callable[[str], None]]]
+        ] = {
+            Parser._StateType.NO_SECTION_FOUND: [
+                (self._is_comment, self._handle_header_comment),
+                (self._is_section_header, self._handle_next_section_header),
+            ],
+            Parser._StateType.PARSING_PROPERTIES: [
+                (self._is_comment, self._handle_section_comment),
+                (self._is_section_header, self._handle_next_section_header),
+                (self._is_property, self._handle_property),
+                (self._is_datarow, self._handle_new_datarow),
+            ],
+            Parser._StateType.PARSING_DATABLOCK: [
+                (self._is_section_header, self._handle_next_section_header),
+                (self._is_datarow, self._handle_datarow),
+            ],
+        }
+
+        self._handle_emptyline: Dict[Parser._StateType, Callable[[], None]] = {
+            self._StateType.NO_SECTION_FOUND: self._finish_current_header_block,
+            self._StateType.PARSING_PROPERTIES: self._noop,
+            self._StateType.PARSING_DATABLOCK: self._noop,
+        }
+
+    def feed_line(self, line: str) -> None:
+        """Parse the next line with this Parser.
+
+        Args:
+            line (str): The line to parse
+        """
+        if not self._is_empty_line(line):
+            for (is_line_type, handle_line_type) in self._feed_line[self._state]:
+                if is_line_type(line):
+                    handle_line_type(line)
+                    break
+            else:
+                # handle exception
+                pass
+        else:
+            self._handle_emptyline[self._state]()
+
+        self._increment_line()
+
+    def finalize(self) -> Document:
+        """Finalize parsing and return the constructed Document.
+
+        Returns:
+            Document:
+                A Document describing the parsed ini file.
+        """
+        # TODO handle invalid block
+        self._finish_current_header_block()
+        self._finalise_current_section()
+        return self._document
+
+    def _increment_line(self) -> None:
+        self._line_index += 1
+
+    def _handle_next_section_header(self, line: str) -> None:
+        self._finalise_current_section()
+        self._handle_new_section_header(line)
+
+        self._state = Parser._StateType.PARSING_PROPERTIES
+
+    def _handle_new_section_header(self, line: str) -> None:
+        section_header = line.strip()[1:-1].strip()
+        self._current_section = _IntermediateSection(
+            header=section_header, start_line=self._line_index
+        )
+
+    def _finalise_current_section(self) -> None:
+        if self._current_section is not None:
+            self._document.sections.append(self._current_section.finalize())
+
+    def _handle_header_comment(self, line: str) -> None:
+        if self._current_header_block is None:
+            self._current_header_block = _IntermediateCommentBlock(
+                start_line=self._line_index
+            )
+
+        comment = self._convert_to_comment(line)
+        self._current_header_block.add_comment_line(comment)
+
+    def _handle_section_comment(self, line: str) -> None:
+        comment = self._convert_to_comment(line)
+        self._current_section.add_comment(comment, self._line_index)  # type: ignore
+
+    def _handle_property(self, line: str) -> None:
+        key, valuepart = self._retrieve_key_value(line)
+        if valuepart is not None:
+            comment, value = self._retrieve_property_comment(valuepart.strip())
+        else:
+            comment, value = None, None
+
+        prop = Property(key=key, value=value, comment=comment)
+        self._current_section.add_property(prop)  # type: ignore
+
+    def _handle_new_datarow(self, line: str) -> None:
+        self._handle_datarow(line)
+        self._state = Parser._StateType.PARSING_DATABLOCK
+
+    def _handle_datarow(self, line: str) -> None:
+        self._current_section.add_datarow(line.split())  # type: ignore
+
+    def _retrieve_property_comment(self, line: str) -> Tuple[Optional[str], str]:
+        """Retrieve the comment and value part from the valuestring of a key-value pair.
+
+        The comment retrieval is complicated by the fact that in the Deltares-INI
+        dialect, the comment delimiter '#' plays a double role: it may also be used
+        to quote string values (for example if the contain spaces).
+
+        Example lines that are supported:
+        key = valueAndNoComment
+        key = valueA  # and a simple comment
+        key = #valueA with possible spaces#
+        key = #valueA#  # and a simple comment
+        key = #valueA# # and a complicated comment with hashes #1 example
+        key = value # and a complicated comment with hashes #2.
+
+        Keywords arguments:
+            line (str) -- the partial string of the line containing both value and
+                possibly a comment at the end. Note that the "key =" part must already
+                have been split off, for example by _retrieve_key_value
+
+        Returns:
+            Tuple with the comment and string value, respectively. If no comment is
+            present, the first tuple element is None.
+        """
+
+        if self._config.parse_comments and self._config.comment_delimiter in line:
+            line = line.strip()
+            parts = line.split(self._config.comment_delimiter)
+            numhash = line.count(self._config.comment_delimiter)
+            if numhash == 1:
+                # normal value, simple comment: "key =  somevalue # and a comment "
+                comment = parts[-1]
+                value = parts[0]
+            elif line.startswith(self._config.comment_delimiter):
+                # hashed value, possible with comment: "key = #somevalue# ..."
+                comment = (
+                    self._config.comment_delimiter.join(parts[3:])
+                    if numhash >= 3
+                    else ""
+                )
+
+                value = self._config.comment_delimiter.join(parts[0:3])
+            else:
+                # normal value, comment with maybe more hashes: "key = somevalue #This is comment #2, or two "
+                comment = self._config.comment_delimiter.join(parts[1:])
+                value = parts[0]
+        else:
+            comment = ""
+            value = line
+
+        return (
+            comment if len(comment := comment.strip()) > 0 else None,
+            value if len(value := value.strip()) > 0 else None,
+        )
+
+    def _retrieve_key_value(self, line: str) -> Tuple[str, Optional[str]]:
+        if "=" in line:
+            key, value = line.split("=", 1)
+            return key.strip(), value if len(value := value.strip()) > 0 else None
+        else:
+            # if no = exists, due to the previous check we know it will just be a
+            # single value
+            return line, None
+
+    def _finish_current_header_block(self) -> None:
+        if self._current_header_block is not None:
+            self._document.header_comment.append(self._current_header_block.finalize())
+            self._current_header_block = None
+
+    def _noop(self, *_, **__) -> None:
+        # no operation
+        pass
+
+    def _is_empty_line(self, line: str) -> bool:
+        return len(line.strip()) == 0
+
+    def _is_comment(self, line: str) -> bool:
+        return line.strip().startswith(self._config.comment_delimiter)
+
+    def _convert_to_comment(self, line: str) -> str:
+        return line.strip()[1:].strip()
+
+    def _is_section_header(self, line: str) -> bool:
+        # a header is defined as "[ any-value ]"
+        stripped = line.strip()
+        return stripped.startswith("[") and stripped.endswith("]")
+
+    def _is_property(self, line: str) -> bool:
+        # we assume that we already checked wether it is a comment or
+        # a section header.
+        return self._config.allow_only_keywords or "=" in line
+
+    def _is_datarow(self, _: str) -> bool:
+        # we assume that we already checked whether it is either a comment,
+        # section header or a property
+        return self._config.parse_datablocks
+
+    @classmethod
+    def parse_as_dict(cls, filepath: Path, config: ParserConfig = None) -> dict:
+        """
+        Parses an INI file without a specific model type and returns it as a dictionary.
+
+        Args:
+            filepath (Path): File path to the INI-format file.
+            config (ParserConfig, optional): Parser configuration to use. Defaults to None.
+
+        Returns:
+            dict: Representation of the parsed INI-file.
+        """
+        return cls.parse(filepath, config).flatten()
+
+    @classmethod
+    def parse(cls, filepath: Path, config: ParserConfig = None) -> Document:
+        """
+        Parses an INI file without a specific model type and returns it as a Document.
+
+        Args:
+            filepath (Path): File path to the INI-format file.
+            config (ParserConfig, optional): Parser configuration to use. Defaults to None.
+
+        Returns:
+            Document: Representation of the parsed INI-file.
+        """
+        if not config:
+            config = ParserConfig()
+        parser = cls(config)
+
+        progline = re.compile(
+            r"^([^#]*=\s*)([^#]*)(#.*)?"
+        )  # matches whole line: "Field = Value Maybe more # optional comment"
+        progfloat = re.compile(
+            r"([\d.]+)([dD])([+\-]?\d+)"
+        )  # matches a float value: 1d9, 1D-3, 1.D+4, etc.
+
+        with filepath.open(encoding="utf8") as f:
+            for line in f:
+                # Replace Fortran scientific notation for doubles
+                # Match number d/D +/- number (e.g. 1d-05 or 1.23D+01 or 1.d-4)
+                match = progline.match(line)
+                if match:  # Only process value
+                    line = (
+                        match.group(1)
+                        + progfloat.sub(r"\1e\3", match.group(2))
+                        + str(match.group(3) or "")
+                    )
+                else:  # Process full line
+                    line = progfloat.sub(r"\1e\3", line)
+
+                parser.feed_line(line)
+
+        return parser.finalize()
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/ini/serializer.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/ini/serializer.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,335 +1,335 @@
-from itertools import chain, count, repeat
-from pathlib import Path
-from typing import Any, Iterable, Optional, Sequence
-
-from hydrolib.core.basemodel import BaseModel, SerializerConfig
-from hydrolib.core.dflowfm.ini.io_models import (
-    CommentBlock,
-    ContentElement,
-    Datablock,
-    DatablockRow,
-    Document,
-    Property,
-    Section,
-)
-from hydrolib.core.utils import str_is_empty_or_none
-
-
-class INISerializerConfig(SerializerConfig):
-    """SerializerConfig defines the configuration options of the Serializer
-
-    Attributes:
-        section_indent (int):
-            The number of spaces with which whole sections should be indented.
-            Defaults to 0.
-        property_indent (int):
-            The number of spaces with which properties should be indented relative to
-            the section header (i.e. the full indent equals the section_indent plus
-            property_indent). Defaults to 4.
-        datablock_indent (int):
-            The number of spaces with which datablock rows are indented relative to
-            the section header (i.e. the full indent equals the section_indent plus
-            datablock_indent). Defaults to 8.
-        datablock_spacing (int):
-            The number of spaces between datablock columns. Note that there might be
-            additional offset to ensure . is lined out. Defaults to 2.
-        comment_delimiter (str):
-            The character used to delimit comments. Defaults to '#'.
-        skip_empty_properties (bool):
-            Whether or not to skip properties with a value that is empty or None. Defaults to True.
-    """
-
-    section_indent: int = 0
-    property_indent: int = 0
-    datablock_indent: int = 8
-    datablock_spacing: int = 2
-    comment_delimiter: str = "#"
-    skip_empty_properties: bool = True
-
-    @property
-    def total_property_indent(self) -> int:
-        """The combined property indentation, i.e. section_indent + property_indent"""
-        return self.section_indent + self.property_indent
-
-    @property
-    def total_datablock_indent(self) -> int:
-        """The combined datablock indentation, i.e. section_indent + datablock_indent"""
-        return self.section_indent + self.datablock_indent
-
-
-class DataBlockINIBasedSerializerConfig(INISerializerConfig):
-    """Class that holds the configuration settings for INI files with data blocks serialization."""
-
-    float_format_datablock: str = ""
-    """str: The string format that will be used for float serialization of the datablock. If empty, the original number will be serialized. Defaults to an empty string.
-    
-    Examples:
-        Input value = 123.456
-
-        Format    | Output          | Description
-        -------------------------------------------------------------------------------------------------------------------------------------
-        ".0f"     | 123             | Format float with 0 decimal places.
-        "f"       | 123.456000      | Format float with default (=6) decimal places.
-        ".2f"     | 123.46          | Format float with 2 decimal places.
-        "+.1f"    | +123.5          | Format float with 1 decimal place with a + or  sign.
-        "e"       | 1.234560e+02    | Format scientific notation with the letter 'e' with default (=6) decimal places.
-        "E"       | 1.234560E+02    | Format scientific notation with the letter 'E' with default (=6) decimal places.
-        ".3e"     | 1.235e+02       | Format scientific notation with the letter 'e' with 3 decimal places.
-        "<15"     | 123.456         | Left aligned in space with width 15
-        "^15.0f"  |       123       | Center aligned in space with width 15 with 0 decimal places.
-        ">15.1e"  |         1.2e+02 | Right aligned in space with width 15 with scientific notation with 1 decimal place.
-        "*>15.1f" | **********123.5 | Right aligned in space with width 15 with 1 decimal place and fill empty space with *
-        "%"       | 12345.600000%   | Format percentage with default (=6) decimal places.     
-        ".3%"     | 12345.600%      | Format percentage with 3 decimal places.  
-
-        More information: https://docs.python.org/3/library/string.html#format-specification-mini-language
-    """
-
-
-class MaxLengths(BaseModel):
-    """MaxLengths defines the maxmimum lengths of the parts of a section
-
-    Attributes:
-        key (int):
-            The maximum length of all the keys of the properties within a section.
-            If no properties are present it should be 0.
-        value (int):
-            The maximum length of all the non None values of the properties within a
-            section. If no properties are present, or all values are None, it should
-            be 0.
-        datablock (Optional[Sequence[int]]):
-            The maximum length of the values of each column of the Datablock.
-            If no datablock is present it defaults to None.
-    """
-
-    key: int
-    value: int
-    datablock: Optional[Sequence[int]] = None
-
-    @classmethod
-    def from_section(cls, section: Section) -> "MaxLengths":
-        """Generate a MaxLengths instance from the given Section
-
-        Args:
-            section (Section): The section of which the MaxLengths are calculated
-
-        Returns:
-            MaxLengths: The MaxLengths corresponding with the provided section
-        """
-        properties = list(p for p in section.content if isinstance(p, Property))
-
-        keys = (prop.key for prop in properties)
-        values = (prop.value for prop in properties if prop.value is not None)
-
-        max_key_length = max((len(k) for k in keys), default=0)
-        max_value_length = max((len(v) for v in values), default=0)
-        max_datablock_lengths = MaxLengths._of_datablock(section.datablock)
-
-        return cls(
-            key=max_key_length,
-            value=max_value_length,
-            datablock=max_datablock_lengths,
-        )
-
-    @staticmethod
-    def _of_datablock(datablock: Optional[Datablock]) -> Optional[Sequence[int]]:
-        if datablock is None or len(datablock) < 1:
-            return None
-
-        datablock_columns = map(list, zip(*datablock))
-        datablock_column_lengths = (map(len, column) for column in datablock_columns)  # type: ignore
-        max_lengths = (max(column) for column in datablock_column_lengths)
-
-        return tuple(max_lengths)
-
-
-Lines = Iterable[str]
-
-
-def _serialize_comment_block(
-    block: CommentBlock,
-    delimiter: str = "#",
-    indent_size: int = 0,
-) -> Lines:
-    indent = " " * indent_size
-    return (f"{indent}{delimiter} {l}" for l in block.lines)
-
-
-def _get_offset_whitespace(key: Optional[str], max_length: int) -> str:
-    key_length = len(key) if key is not None else 0
-    return " " * max(max_length - key_length, 0)
-
-
-class SectionSerializer:
-    """SectionSerializer provides the serialize method to serialize a Section
-
-    The entrypoint of this method is the serialize method, which will construct
-    an actual instance and serializes the Section with it.
-    """
-
-    def __init__(self, config: INISerializerConfig, max_length: MaxLengths):
-        """Create a new SectionSerializer
-
-        Args:
-            config (SerializerConfig): The config describing the serialization options
-            max_length (MaxLengths): The max lengths of the section being serialized
-        """
-        self._config = config
-        self._max_length = max_length
-
-    @classmethod
-    def serialize(cls, section: Section, config: INISerializerConfig) -> Lines:
-        """Serialize the provided section with the given config
-
-        Args:
-            section (Section): The section to serialize
-            config (SerializerConfig): The config describing the serialization options
-
-        Returns:
-            Lines: The iterable lines of the serialized section
-        """
-        serializer = cls(config, MaxLengths.from_section(section))
-        return serializer._serialize_section(section)
-
-    @property
-    def config(self) -> INISerializerConfig:
-        """The SerializerConfig used while serializing the section."""
-        return self._config
-
-    @property
-    def max_length(self) -> MaxLengths:
-        """The MaxLengths of the Section being serialized by this SectionSerializer."""
-        return self._max_length
-
-    def _serialize_section(self, section: Section) -> Lines:
-        header_iterable = self._serialize_section_header(section.header)
-        properties = self._serialize_content(section.content)
-        datablock = self._serialize_datablock(section.datablock)
-
-        return chain(header_iterable, properties, datablock)
-
-    def _serialize_section_header(self, section_header: str) -> Lines:
-        indent = " " * (self.config.section_indent)
-        yield f"{indent}[{section_header}]"
-
-    def _serialize_content(self, content: Iterable[ContentElement]) -> Lines:
-        elements = (self._serialize_content_element(elem) for elem in content)
-        return chain.from_iterable(elements)
-
-    def _serialize_content_element(self, elem: ContentElement) -> Lines:
-        if isinstance(elem, Property):
-            return self._serialize_property(elem)
-        else:
-            indent = self.config.total_property_indent
-            delimiter = self.config.comment_delimiter
-            return _serialize_comment_block(elem, delimiter, indent)
-
-    def _serialize_property(self, property: Property) -> Lines:
-        if self.config.skip_empty_properties and str_is_empty_or_none(property.value):
-            return
-
-        indent = " " * (self._config.total_property_indent)
-        key_ws = _get_offset_whitespace(property.key, self.max_length.key)
-        key = f"{property.key}{key_ws} = "
-
-        value_ws = _get_offset_whitespace(property.value, self.max_length.value)
-
-        if property.value is not None:
-            value = f"{property.value}{value_ws}"
-        else:
-            value = value_ws
-
-        comment = (
-            f" # {property.comment}"
-            if not str_is_empty_or_none(property.comment)
-            else ""
-        )
-
-        yield f"{indent}{key}{value}{comment}".rstrip()
-
-    def _serialize_datablock(self, datablock: Optional[Datablock]) -> Lines:
-        if datablock is None or self.max_length.datablock is None:
-            return []
-
-        indent = " " * self._config.total_datablock_indent
-        return (self._serialize_row(row, indent) for row in datablock)
-
-    def _serialize_row(self, row: DatablockRow, indent: str) -> str:
-        elem_spacing = " " * self.config.datablock_spacing
-        elems = (self._serialize_row_element(elem, i) for elem, i in zip(row, count()))
-
-        return indent + elem_spacing.join(elems).rstrip()
-
-    def _serialize_row_element(self, elem: str, index: int) -> str:
-        max_length = self.max_length.datablock[index]  # type: ignore
-        whitespace = _get_offset_whitespace(elem, max_length)
-        return elem + whitespace
-
-
-class Serializer:
-    """Serializer serializes Document to its corresponding lines."""
-
-    def __init__(self, config: INISerializerConfig):
-        """Creates a new Serializer with the provided configuration.
-
-        Args:
-            config (SerializerConfig): The configuration of this Serializer.
-        """
-        self._config = config
-
-    def serialize(self, document: Document) -> Lines:
-        """Serialize the provided document into an iterable of lines.
-
-        Args:
-            document (Document): The Document to serialize.
-
-        Returns:
-            Lines: An iterable returning each line of the serialized Document.
-        """
-        header_iterable = self._serialize_document_header(document.header_comment)
-
-        serialize_section = lambda s: SectionSerializer.serialize(s, self._config)
-        sections = (serialize_section(section) for section in document.sections)
-        sections_with_spacing = Serializer._interweave(sections, [""])
-        sections_iterable = chain.from_iterable(sections_with_spacing)
-
-        return chain(header_iterable, sections_iterable)
-
-    def _serialize_document_header(self, header: Iterable[CommentBlock]) -> Lines:
-        delimiter = self._config.comment_delimiter
-        serialize = lambda cb: _serialize_comment_block(cb, delimiter)
-        blocks = (serialize(block) for block in header)
-        blocks_with_spacing = Serializer._interweave(blocks, [""])
-
-        return chain.from_iterable(blocks_with_spacing)
-
-    @staticmethod
-    def _interweave(iterable: Iterable, val: Any) -> Iterable:
-        # Interweave the provided iterable with the provided value:
-        # iterable_element, val, iterable_element, val, ...
-
-        # Note that this will interweave with val without making copies
-        # as such it is the same object being interweaved.
-        return chain.from_iterable(zip(iterable, repeat(val)))
-
-
-def write_ini(path: Path, document: Document, config: INISerializerConfig) -> None:
-    """Write the provided document to the specified path
-
-    If the provided path already exists, it will be overwritten. If the parent folder
-    do not exist, they will be created.
-
-    Args:
-        path (Path): The path to which the document should be written.
-        document (Document): The document to serialize to the specified path.
-        config (INISerializerConfig): The configuration settings for the serializer.
-    """
-
-    serializer = Serializer(config)
-
-    path.parent.mkdir(parents=True, exist_ok=True)
-
-    with path.open("w", encoding="utf8") as f:
-
-        for line in serializer.serialize(document):
-            f.write(line + "\n")
+from itertools import chain, count, repeat
+from pathlib import Path
+from typing import Any, Iterable, Optional, Sequence
+
+from hydrolib.core.basemodel import BaseModel, SerializerConfig
+from hydrolib.core.dflowfm.ini.io_models import (
+    CommentBlock,
+    ContentElement,
+    Datablock,
+    DatablockRow,
+    Document,
+    Property,
+    Section,
+)
+from hydrolib.core.utils import str_is_empty_or_none
+
+
+class INISerializerConfig(SerializerConfig):
+    """SerializerConfig defines the configuration options of the Serializer
+
+    Attributes:
+        section_indent (int):
+            The number of spaces with which whole sections should be indented.
+            Defaults to 0.
+        property_indent (int):
+            The number of spaces with which properties should be indented relative to
+            the section header (i.e. the full indent equals the section_indent plus
+            property_indent). Defaults to 4.
+        datablock_indent (int):
+            The number of spaces with which datablock rows are indented relative to
+            the section header (i.e. the full indent equals the section_indent plus
+            datablock_indent). Defaults to 8.
+        datablock_spacing (int):
+            The number of spaces between datablock columns. Note that there might be
+            additional offset to ensure . is lined out. Defaults to 2.
+        comment_delimiter (str):
+            The character used to delimit comments. Defaults to '#'.
+        skip_empty_properties (bool):
+            Whether or not to skip properties with a value that is empty or None. Defaults to True.
+    """
+
+    section_indent: int = 0
+    property_indent: int = 0
+    datablock_indent: int = 8
+    datablock_spacing: int = 2
+    comment_delimiter: str = "#"
+    skip_empty_properties: bool = True
+
+    @property
+    def total_property_indent(self) -> int:
+        """The combined property indentation, i.e. section_indent + property_indent"""
+        return self.section_indent + self.property_indent
+
+    @property
+    def total_datablock_indent(self) -> int:
+        """The combined datablock indentation, i.e. section_indent + datablock_indent"""
+        return self.section_indent + self.datablock_indent
+
+
+class DataBlockINIBasedSerializerConfig(INISerializerConfig):
+    """Class that holds the configuration settings for INI files with data blocks serialization."""
+
+    float_format_datablock: str = ""
+    """str: The string format that will be used for float serialization of the datablock. If empty, the original number will be serialized. Defaults to an empty string.
+    
+    Examples:
+        Input value = 123.456
+
+        Format    | Output          | Description
+        -------------------------------------------------------------------------------------------------------------------------------------
+        ".0f"     | 123             | Format float with 0 decimal places.
+        "f"       | 123.456000      | Format float with default (=6) decimal places.
+        ".2f"     | 123.46          | Format float with 2 decimal places.
+        "+.1f"    | +123.5          | Format float with 1 decimal place with a + or  sign.
+        "e"       | 1.234560e+02    | Format scientific notation with the letter 'e' with default (=6) decimal places.
+        "E"       | 1.234560E+02    | Format scientific notation with the letter 'E' with default (=6) decimal places.
+        ".3e"     | 1.235e+02       | Format scientific notation with the letter 'e' with 3 decimal places.
+        "<15"     | 123.456         | Left aligned in space with width 15
+        "^15.0f"  |       123       | Center aligned in space with width 15 with 0 decimal places.
+        ">15.1e"  |         1.2e+02 | Right aligned in space with width 15 with scientific notation with 1 decimal place.
+        "*>15.1f" | **********123.5 | Right aligned in space with width 15 with 1 decimal place and fill empty space with *
+        "%"       | 12345.600000%   | Format percentage with default (=6) decimal places.     
+        ".3%"     | 12345.600%      | Format percentage with 3 decimal places.  
+
+        More information: https://docs.python.org/3/library/string.html#format-specification-mini-language
+    """
+
+
+class MaxLengths(BaseModel):
+    """MaxLengths defines the maxmimum lengths of the parts of a section
+
+    Attributes:
+        key (int):
+            The maximum length of all the keys of the properties within a section.
+            If no properties are present it should be 0.
+        value (int):
+            The maximum length of all the non None values of the properties within a
+            section. If no properties are present, or all values are None, it should
+            be 0.
+        datablock (Optional[Sequence[int]]):
+            The maximum length of the values of each column of the Datablock.
+            If no datablock is present it defaults to None.
+    """
+
+    key: int
+    value: int
+    datablock: Optional[Sequence[int]] = None
+
+    @classmethod
+    def from_section(cls, section: Section) -> "MaxLengths":
+        """Generate a MaxLengths instance from the given Section
+
+        Args:
+            section (Section): The section of which the MaxLengths are calculated
+
+        Returns:
+            MaxLengths: The MaxLengths corresponding with the provided section
+        """
+        properties = list(p for p in section.content if isinstance(p, Property))
+
+        keys = (prop.key for prop in properties)
+        values = (prop.value for prop in properties if prop.value is not None)
+
+        max_key_length = max((len(k) for k in keys), default=0)
+        max_value_length = max((len(v) for v in values), default=0)
+        max_datablock_lengths = MaxLengths._of_datablock(section.datablock)
+
+        return cls(
+            key=max_key_length,
+            value=max_value_length,
+            datablock=max_datablock_lengths,
+        )
+
+    @staticmethod
+    def _of_datablock(datablock: Optional[Datablock]) -> Optional[Sequence[int]]:
+        if datablock is None or len(datablock) < 1:
+            return None
+
+        datablock_columns = map(list, zip(*datablock))
+        datablock_column_lengths = (map(len, column) for column in datablock_columns)  # type: ignore
+        max_lengths = (max(column) for column in datablock_column_lengths)
+
+        return tuple(max_lengths)
+
+
+Lines = Iterable[str]
+
+
+def _serialize_comment_block(
+    block: CommentBlock,
+    delimiter: str = "#",
+    indent_size: int = 0,
+) -> Lines:
+    indent = " " * indent_size
+    return (f"{indent}{delimiter} {l}" for l in block.lines)
+
+
+def _get_offset_whitespace(key: Optional[str], max_length: int) -> str:
+    key_length = len(key) if key is not None else 0
+    return " " * max(max_length - key_length, 0)
+
+
+class SectionSerializer:
+    """SectionSerializer provides the serialize method to serialize a Section
+
+    The entrypoint of this method is the serialize method, which will construct
+    an actual instance and serializes the Section with it.
+    """
+
+    def __init__(self, config: INISerializerConfig, max_length: MaxLengths):
+        """Create a new SectionSerializer
+
+        Args:
+            config (SerializerConfig): The config describing the serialization options
+            max_length (MaxLengths): The max lengths of the section being serialized
+        """
+        self._config = config
+        self._max_length = max_length
+
+    @classmethod
+    def serialize(cls, section: Section, config: INISerializerConfig) -> Lines:
+        """Serialize the provided section with the given config
+
+        Args:
+            section (Section): The section to serialize
+            config (SerializerConfig): The config describing the serialization options
+
+        Returns:
+            Lines: The iterable lines of the serialized section
+        """
+        serializer = cls(config, MaxLengths.from_section(section))
+        return serializer._serialize_section(section)
+
+    @property
+    def config(self) -> INISerializerConfig:
+        """The SerializerConfig used while serializing the section."""
+        return self._config
+
+    @property
+    def max_length(self) -> MaxLengths:
+        """The MaxLengths of the Section being serialized by this SectionSerializer."""
+        return self._max_length
+
+    def _serialize_section(self, section: Section) -> Lines:
+        header_iterable = self._serialize_section_header(section.header)
+        properties = self._serialize_content(section.content)
+        datablock = self._serialize_datablock(section.datablock)
+
+        return chain(header_iterable, properties, datablock)
+
+    def _serialize_section_header(self, section_header: str) -> Lines:
+        indent = " " * (self.config.section_indent)
+        yield f"{indent}[{section_header}]"
+
+    def _serialize_content(self, content: Iterable[ContentElement]) -> Lines:
+        elements = (self._serialize_content_element(elem) for elem in content)
+        return chain.from_iterable(elements)
+
+    def _serialize_content_element(self, elem: ContentElement) -> Lines:
+        if isinstance(elem, Property):
+            return self._serialize_property(elem)
+        else:
+            indent = self.config.total_property_indent
+            delimiter = self.config.comment_delimiter
+            return _serialize_comment_block(elem, delimiter, indent)
+
+    def _serialize_property(self, property: Property) -> Lines:
+        if self.config.skip_empty_properties and str_is_empty_or_none(property.value):
+            return
+
+        indent = " " * (self._config.total_property_indent)
+        key_ws = _get_offset_whitespace(property.key, self.max_length.key)
+        key = f"{property.key}{key_ws} = "
+
+        value_ws = _get_offset_whitespace(property.value, self.max_length.value)
+
+        if property.value is not None:
+            value = f"{property.value}{value_ws}"
+        else:
+            value = value_ws
+
+        comment = (
+            f" # {property.comment}"
+            if not str_is_empty_or_none(property.comment)
+            else ""
+        )
+
+        yield f"{indent}{key}{value}{comment}".rstrip()
+
+    def _serialize_datablock(self, datablock: Optional[Datablock]) -> Lines:
+        if datablock is None or self.max_length.datablock is None:
+            return []
+
+        indent = " " * self._config.total_datablock_indent
+        return (self._serialize_row(row, indent) for row in datablock)
+
+    def _serialize_row(self, row: DatablockRow, indent: str) -> str:
+        elem_spacing = " " * self.config.datablock_spacing
+        elems = (self._serialize_row_element(elem, i) for elem, i in zip(row, count()))
+
+        return indent + elem_spacing.join(elems).rstrip()
+
+    def _serialize_row_element(self, elem: str, index: int) -> str:
+        max_length = self.max_length.datablock[index]  # type: ignore
+        whitespace = _get_offset_whitespace(elem, max_length)
+        return elem + whitespace
+
+
+class Serializer:
+    """Serializer serializes Document to its corresponding lines."""
+
+    def __init__(self, config: INISerializerConfig):
+        """Creates a new Serializer with the provided configuration.
+
+        Args:
+            config (SerializerConfig): The configuration of this Serializer.
+        """
+        self._config = config
+
+    def serialize(self, document: Document) -> Lines:
+        """Serialize the provided document into an iterable of lines.
+
+        Args:
+            document (Document): The Document to serialize.
+
+        Returns:
+            Lines: An iterable returning each line of the serialized Document.
+        """
+        header_iterable = self._serialize_document_header(document.header_comment)
+
+        serialize_section = lambda s: SectionSerializer.serialize(s, self._config)
+        sections = (serialize_section(section) for section in document.sections)
+        sections_with_spacing = Serializer._interweave(sections, [""])
+        sections_iterable = chain.from_iterable(sections_with_spacing)
+
+        return chain(header_iterable, sections_iterable)
+
+    def _serialize_document_header(self, header: Iterable[CommentBlock]) -> Lines:
+        delimiter = self._config.comment_delimiter
+        serialize = lambda cb: _serialize_comment_block(cb, delimiter)
+        blocks = (serialize(block) for block in header)
+        blocks_with_spacing = Serializer._interweave(blocks, [""])
+
+        return chain.from_iterable(blocks_with_spacing)
+
+    @staticmethod
+    def _interweave(iterable: Iterable, val: Any) -> Iterable:
+        # Interweave the provided iterable with the provided value:
+        # iterable_element, val, iterable_element, val, ...
+
+        # Note that this will interweave with val without making copies
+        # as such it is the same object being interweaved.
+        return chain.from_iterable(zip(iterable, repeat(val)))
+
+
+def write_ini(path: Path, document: Document, config: INISerializerConfig) -> None:
+    """Write the provided document to the specified path
+
+    If the provided path already exists, it will be overwritten. If the parent folder
+    do not exist, they will be created.
+
+    Args:
+        path (Path): The path to which the document should be written.
+        document (Document): The document to serialize to the specified path.
+        config (INISerializerConfig): The configuration settings for the serializer.
+    """
+
+    serializer = Serializer(config)
+
+    path.parent.mkdir(parents=True, exist_ok=True)
+
+    with path.open("w", encoding="utf8") as f:
+
+        for line in serializer.serialize(document):
+            f.write(line + "\n")
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/ini/util.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/ini/util.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,624 +1,624 @@
-"""util.py provides additional utility methods related to handling ini files.
-"""
-from datetime import datetime
-from enum import Enum
-from operator import eq
-from typing import Any, Callable, Dict, List, Optional, Type
-
-from pydantic.class_validators import root_validator, validator
-from pydantic.fields import ModelField
-from pydantic.main import BaseModel
-
-from hydrolib.core.dflowfm.common.models import LocationType
-from hydrolib.core.utils import operator_str, str_is_empty_or_none, to_list
-
-
-def get_split_string_on_delimiter_validator(*field_name: str):
-    """Get a validator to split strings passed to the specified field_name.
-
-    Strings are split based on an automatically selected provided delimiter.
-    The delimiter is the field's own delimiter, if that was defined using
-    Field(.., delimiter=".."). Otherwise, the delimiter is the field's parent
-    class's delimiter (which should be (subclass of) INIBasedModel.)
-    The validator splits a string value into a list of substrings before any
-    other validation takes place.
-
-    Returns:
-        the validator which splits strings on the provided delimiter.
-    """
-
-    def split(cls, v: Any, field: ModelField):
-        if isinstance(v, str):
-            v = v.split(cls.get_list_field_delimiter(field.name))
-            v = [item.strip() for item in v if item != ""]
-        return v
-
-    return validator(*field_name, allow_reuse=True, pre=True)(split)
-
-
-def get_enum_validator(
-    *field_name: str,
-    enum: Type[Enum],
-    alternative_enum_values: Optional[Dict[str, List[str]]] = None,
-):
-    """
-    Get a case-insensitive enum validator that will returns the corresponding enum value.
-    If the input is a list, then each list value is checked individually.
-
-    Args:
-        enum (Type[Enum]): The enum type for which to validate.
-        alternative_enum_values (Dict[str, List[str]], optional): Dictionary with alternative
-            allowed values for one or more of the enum keys. Dict key must be a valid current
-            key of enum (case sensitive). Use this to backwards support and convert old enum
-            values in user input. For example: {SomeEnum.current_value: ["old value"]}.
-    """
-
-    def get_enum(v):
-        for entry in enum:
-            if entry.lower() == v.lower():
-                return entry
-            if (
-                alternative_enum_values is not None
-                and (alt_values := alternative_enum_values.get(entry.value)) is not None
-                and v.lower() in (altval.lower() for altval in alt_values)
-            ):
-                return entry
-
-        return v
-
-    return validator(*field_name, allow_reuse=True, pre=True, each_item=True)(get_enum)
-
-
-def make_list_validator(*field_name: str):
-    """Get a validator make a list of object if a single object is passed."""
-
-    def split(v: Any):
-        if not isinstance(v, list):
-            v = [v]
-        return v
-
-    return validator(*field_name, allow_reuse=True, pre=True)(split)
-
-
-def validate_correct_length(
-    values: Dict,
-    *field_names,
-    length_name: str,
-    length_incr: int = 0,
-    list_required_with_length: bool = False,
-    min_length: int = 0,
-) -> Dict:
-    """
-    Validate the correct length (and presence) of several list fields in an object.
-
-    Args:
-        values (Dict): dictionary of values to validate.
-        *field_names (str): names of the instance variables that are a list and need checking.
-        length_name (str): name of the instance variable that stores the expected length.
-        length_incr (int): Optional extra increment of length value (e.g., to have +1 extra value in lists).
-        list_required_with_length (obj:`bool`, optional): Whether each list *must* be present if the length
-            attribute is present (and > 0) in the input values. Default: False. If False, list length is only
-            checked for the lists that are not None.
-        min_length (int): minimum for list length value, overrides length_name value if that is smaller.
-            For example, to require list length 1 when length value is given as 0.
-
-    Raises:
-        ValueError: When the number of values for any of the given field_names is not as expected.
-
-    Returns:
-        Dict: Dictionary of validated values.
-    """
-
-    def _get_incorrect_length_validation_message() -> str:
-        """Make a string with a validation message, ready to be format()ed with
-        field name and length name."""
-        incrstring = f" + {length_incr}" if length_incr != 0 else ""
-        minstring = f" (and at least {min_length})" if min_length > 0 else ""
-
-        return (
-            "Number of values for {} should be equal to the {} value"
-            + incrstring
-            + minstring
-            + "."
-        )
-
-    def _validate_listfield_length(
-        field_name: str, field: Optional[List[Any]], requiredlength: int
-    ):
-        """Validate the length of a single field, which should be a list."""
-
-        if field is not None and len(field) != requiredlength:
-            raise ValueError(
-                _get_incorrect_length_validation_message().format(
-                    field_name, length_name
-                )
-            )
-        if field is None and list_required_with_length and requiredlength > 0:
-            raise ValueError(
-                f"List {field_name} cannot be missing if {length_name} is given."
-            )
-
-        return field
-
-    length = values.get(length_name)
-    if length is None:
-        # length attribute not present, possibly defer validation to a subclass.
-        return values
-
-    requiredlength = max(length + length_incr, min_length)
-
-    for field_name in field_names:
-        field = values.get(field_name)
-        values[field_name] = _validate_listfield_length(
-            field_name,
-            field,
-            requiredlength,
-        )
-
-    return values
-
-
-def validate_forbidden_fields(
-    values: Dict,
-    *field_names,
-    conditional_field_name: str,
-    conditional_value: Any,
-    comparison_func: Callable[[Any, Any], bool] = eq,
-) -> Dict:
-    """
-    Validates whether certain fields are *not* provided, if `conditional_field_name` is equal to `conditional_value`.
-    The equality check can be overridden with another comparison operator function.
-
-    Args:
-        values (Dict): Dictionary of input class fields.
-        *field_names (str): Names of the instance variables that need to be validated.
-        conditional_field_name (str): Name of the instance variable on which the fields are dependent.
-        conditional_value (Any): Value that the conditional field should contain to perform this validation.
-        comparison_func (Callable): binary operator function, used to override the default "eq" check for the conditional field value.
-
-    Raises:
-        ValueError: When a forbidden field is provided.
-
-    Returns:
-        Dict: Validated dictionary of input class fields.
-    """
-    if (val := values.get(conditional_field_name)) is None or not comparison_func(
-        val, conditional_value
-    ):
-        return values
-
-    for field in field_names:
-        if values.get(field) != None:
-            raise ValueError(
-                f"{field} is forbidden when {conditional_field_name} {operator_str(comparison_func)} {conditional_value}"
-            )
-
-    return values
-
-
-def validate_required_fields(
-    values: Dict,
-    *field_names,
-    conditional_field_name: str,
-    conditional_value: Any,
-    comparison_func: Callable[[Any, Any], bool] = eq,
-) -> Dict:
-    """
-    Validates whether the specified fields are provided, if `conditional_field_name` is equal to `conditional_value`.
-    The equality check can be overridden with another comparison operator function.
-
-    Args:
-        values (Dict): Dictionary of input class fields.
-        *field_names (str): Names of the instance variables that need to be validated.
-        conditional_field_name (str): Name of the instance variable on which the fields are dependent.
-        conditional_value (Any): Value that the conditional field should contain to perform this validation.
-        comparison_func (Callable): binary operator function, used to override the default "eq" check for the conditional field value.
-
-    Raises:
-        ValueError: When a required field is not provided under the given conditions.
-
-    Returns:
-        Dict: Validated dictionary of input class fields.
-    """
-
-    if (val := values.get(conditional_field_name)) is None or not comparison_func(
-        val, conditional_value
-    ):
-        return values
-
-    for field in field_names:
-        if values.get(field) == None:
-            raise ValueError(
-                f"{field} should be provided when {conditional_field_name} {operator_str(comparison_func)} {conditional_value}"
-            )
-
-    return values
-
-
-def validate_conditionally(
-    cls,
-    values: Dict,
-    root_vldt: classmethod,
-    conditional_field_name: str,
-    conditional_value: Any,
-    comparison_func: Callable[[Any, Any], bool] = eq,
-) -> Dict:
-    """
-    Validate whether certain fields are *not* provided, if `conditional_field_name` is equal to `conditional_value`.
-    The equality check can be overridden with another comparison operator function.
-
-    Args:
-        cls: Reference to a class.
-        values (Dict): Dictionary of input class fields.
-        root_vldt (classmethod): A root validator that is to be called *if* the condition is satisfied.
-        conditional_field_name (str): Name of the instance variable that determines whether the root validator must be called or not.
-        conditional_value (Any): Value that the conditional field should be compared with to perform this validation.
-        comparison_func (Callable): Binary operator function, used to override the default "eq" check for the conditional field value.
-
-    Returns:
-        Dict: Validated dictionary of input class fields.
-    """
-    if (val := values.get(conditional_field_name)) is not None and comparison_func(
-        val, conditional_value
-    ):
-        # Condition is met: call the actual root validator, passing on the attribute values.
-        root_vldt.__func__(cls, values)
-
-    return values
-
-
-def validate_datetime_string(
-    field_value: Optional[str], field: ModelField
-) -> Optional[str]:
-    """Validate that a field value matches the YYYYmmddHHMMSS datetime format.
-
-    Args:
-        field_value (Optional[str]): value of a Pydantic field, may be optional.
-        field (ModelField): the underlying Pydantic ModelField, used in error
-            message.
-
-    Returns:
-        Optional[str]: the original input value, if valid.
-
-    Raises:
-        ValueError: if a non-empty input string does not have valid format.
-    """
-    if (
-        field_value is not None
-        and len(field_value.strip()) > 0
-        and field_value != "yyyymmddhhmmss"
-    ):
-        try:
-            _ = datetime.strptime(field_value, r"%Y%m%d%H%M%S")
-        except ValueError:
-            raise ValueError(
-                f"Invalid datetime string for {field.alias}: '{field_value}', expecting 'YYYYmmddHHMMSS'."
-            )
-
-    return field_value  # this is the value written to the class field
-
-
-def get_from_subclass_defaults(cls: Type[BaseModel], fieldname: str, value: str) -> str:
-    """
-    Gets a value that corresponds with the default field value of one of the subclasses.
-    If the subclass doesn't have the specified field, it will look into its own subclasses
-    recursively for the specified fieldname.
-
-    Args:
-        cls (Type[BaseModel]): The parent model type.
-        fieldname (str): The field name for which retrieve the default for.
-        value (str): The value to compare with.
-
-    Returns:
-        str: The field default that corresponds to the value. If nothing is found return the input value.
-    """
-    # Immediately check in direct subclasses, not in base cls itself:
-    for c in cls.__subclasses__():
-        default = _try_get_default_value(c, fieldname, value)
-        if default is not None:
-            return default
-
-    # No matching default was found, return input value:
-    return value
-
-
-def _try_get_default_value(
-    c: Type[BaseModel], fieldname: str, value: str
-) -> Optional[str]:
-    """Helper subroutine to get the default value for a particular field in
-    the given class or any of its descendant classes, if it matches the input
-    value (case insensitive).
-
-    This method recurses depth-first topdown into the class'es subclasses.
-
-        c (Type[BaseModel]): The base model type where the search starts.
-        fieldname (str): The field name for which retrieve the default for.
-        value (str): The value to compare with.
-
-    Returns:
-        Optional[str]: The field default that corresponds to the value. If nothing is found return None.
-    """
-    if (field := c.__fields__.get(fieldname)) is None:
-        return None
-
-    default = field.default
-
-    if default is not None and default.lower() == value.lower():
-        # If this class's default matches, directly return it to end the recursion.
-        return default
-
-    for sc in c.__subclasses__():
-        default = _try_get_default_value(sc, fieldname, value)
-        if default is not None:
-            return default
-
-    # Nothing found under c, return None to caller (e.g., to continue recursion).
-    return None
-
-
-def get_type_based_on_subclass_default_value(
-    cls: Type, fieldname: str, value: str
-) -> Optional[Type]:
-    """
-    Gets the type of the first subclass where the default value of the fieldname is equal
-    to the provided value. If there is no match in the subclass, it will recursively search
-    in the subclasses of the subclass.
-
-    Args:
-        cls (Type): The base type.
-        fieldname (str): The field name for which retrieve the default for.
-        value (str): The value to compare with.
-
-    Returns:
-        [type]: The type of the first subclass that has a default value for the provided fieldname
-        equal to the provided value. Returns None if the fieldname is not found in the subclasses
-        or if no match was found.
-    """
-    for c in cls.__subclasses__():
-        subclass_type = _get_type_based_on_default_value(c, fieldname, value)
-        if subclass_type is not None:
-            return subclass_type
-    return None
-
-
-def _get_type_based_on_default_value(cls, fieldname, value) -> Optional[Type]:
-    if (field := cls.__fields__.get(fieldname)) is None:
-        return None
-
-    default = field.default
-    if default is not None and default.lower() == value.lower():
-        return cls
-
-    for sc in cls.__subclasses__():
-        subclass_type = _get_type_based_on_default_value(sc, fieldname, value)
-        if subclass_type is not None:
-            return subclass_type
-
-    return None
-
-
-class LocationValidationConfiguration(BaseModel):
-    """Class that holds the various configuration settings needed for location validation."""
-
-    validate_node: bool = True
-    """bool, optional: Whether or not node location specification should be validated. Defaults to True."""
-
-    validate_coordinates: bool = True
-    """bool, optional: Whether or not coordinate location specification should be validated. Defaults to True."""
-
-    validate_branch: bool = True
-    """bool, optional: Whether or not branch location specification should be validated. Defaults to True."""
-
-    validate_num_coordinates: bool = True
-    """bool, optional: Whether or not the number of coordinates should be validated or not. This option is only relevant when `validate_coordinates` is True. Defaults to True."""
-
-    minimum_num_coordinates: int = 0
-    """int, optional: The minimum required number of coordinates. This option is only relevant when `validate_coordinates` is True. Defaults to 0."""
-
-
-class LocationValidationFieldNames(BaseModel):
-    """Class that holds the various field names needed for location validation."""
-
-    node_id: str = "nodeId"
-    """str, optional: The node id field name. Defaults to `nodeId`."""
-
-    branch_id: str = "branchId"
-    """str, optional: The branch id field name. Defaults to `branchId`."""
-
-    chainage: str = "chainage"
-    """str, optional: The chainage field name. Defaults to `chainage`."""
-
-    x_coordinates: str = "xCoordinates"
-    """str, optional: The x-coordinates field name. Defaults to `xCoordinates`."""
-
-    y_coordinates: str = "yCoordinates"
-    """str, optional: The y-coordinates field name. Defaults to `yCoordinates`."""
-
-    num_coordinates: str = "numCoordinates"
-    """str, optional: The number of coordinates field name. Defaults to `numCoordinates`."""
-
-    location_type: str = "locationType"
-    """str, optional: The location type field name. Defaults to `locationType`."""
-
-
-def validate_location_specification(
-    values: Dict,
-    config: Optional[LocationValidationConfiguration] = None,
-    fields: Optional[LocationValidationFieldNames] = None,
-) -> Dict:
-    """
-    Validates whether the correct location specification is given in
-    typical 1D2D input in an IniBasedModel class.
-
-    Validates for presence of at least one of: nodeId, branchId with chainage,
-    xCoordinates with yCoordinates, or xCoordinates with yCoordinates and numCoordinates.
-    Validates for the locationType for nodeId and branchId.
-
-    Args:
-        values (Dict): Dictionary of object's validated fields.
-        config (LocationValidationConfiguration, optional): Configuration for the location validation. Default is None.
-        field (LocationValidationFieldNames, optional): Fields names that should be used for the location validation. Default is None.
-
-    Raises:
-        ValueError: When exactly one of the following combinations were not given:
-            - nodeId
-            - branchId with chainage
-            - xCoordinates with yCoordinates
-            - xCoordinates with yCoordinates and numCoordinates.
-        ValueError: When numCoordinates does not meet the requirement minimum amount or does not match the amount of xCoordinates or yCoordinates.
-        ValueError: When locationType should be 1d but other was specified.
-
-    Returns:
-        Dict: Validated dictionary of input class fields.
-    """
-
-    if config is None:
-        config = LocationValidationConfiguration()
-
-    if fields is None:
-        fields = LocationValidationFieldNames()
-
-    has_node_id = not str_is_empty_or_none(values.get(fields.node_id.lower()))
-    has_branch_id = not str_is_empty_or_none(values.get(fields.branch_id.lower()))
-    has_chainage = values.get(fields.chainage.lower()) is not None
-    has_x_coordinates = values.get(fields.x_coordinates.lower()) is not None
-    has_y_coordinates = values.get(fields.y_coordinates.lower()) is not None
-    has_num_coordinates = values.get(fields.num_coordinates.lower()) is not None
-
-    # ----- Local validation functions
-    def get_length(field: str):
-        value = values[field.lower()]
-        return len(to_list(value))
-
-    def validate_location_type(expected_location_type: LocationType) -> None:
-        location_type = values.get(fields.location_type.lower(), None)
-        if str_is_empty_or_none(location_type):
-            values[fields.location_type.lower()] = expected_location_type
-        elif location_type != expected_location_type:
-            raise ValueError(
-                f"{fields.location_type} should be {expected_location_type} but was {location_type}"
-            )
-
-    def validate_coordinates_with_num_coordinates() -> None:
-        length_x_coordinates = get_length(fields.x_coordinates)
-        length_y_coordinates = get_length(fields.y_coordinates)
-        num_coordinates = values[fields.num_coordinates.lower()]
-
-        if not num_coordinates == length_x_coordinates == length_y_coordinates:
-            raise ValueError(
-                f"{fields.num_coordinates} should be equal to the amount of {fields.x_coordinates} and {fields.y_coordinates}"
-            )
-
-        validate_minimum_num_coordinates(num_coordinates)
-
-    def validate_coordinates() -> None:
-        len_x_coordinates = get_length(fields.x_coordinates)
-        len_y_coordinates = get_length(fields.y_coordinates)
-
-        if len_x_coordinates != len_y_coordinates:
-            raise ValueError(
-                f"{fields.x_coordinates} and {fields.y_coordinates} should have an equal amount of coordinates"
-            )
-
-        validate_minimum_num_coordinates(len_x_coordinates)
-
-    def validate_minimum_num_coordinates(actual_num: int) -> None:
-        if actual_num < config.minimum_num_coordinates:
-            raise ValueError(
-                f"{fields.x_coordinates} and {fields.y_coordinates} should have at least {config.minimum_num_coordinates} coordinate(s)"
-            )
-
-    def is_valid_node_specification() -> bool:
-        has_other = (
-            has_branch_id
-            or has_chainage
-            or has_x_coordinates
-            or has_y_coordinates
-            or has_num_coordinates
-        )
-        return has_node_id and not has_other
-
-    def is_valid_branch_specification() -> bool:
-        has_other = (
-            has_node_id or has_x_coordinates or has_y_coordinates or has_num_coordinates
-        )
-        return has_branch_id and has_chainage and not has_other
-
-    def is_valid_coordinates_specification() -> bool:
-        has_other = has_node_id or has_branch_id or has_chainage or has_num_coordinates
-        return has_x_coordinates and has_y_coordinates and not has_other
-
-    def is_valid_coordinates_with_num_coordinates_specification() -> bool:
-        has_other = has_node_id or has_branch_id or has_chainage
-        return (
-            has_x_coordinates
-            and has_y_coordinates
-            and has_num_coordinates
-            and not has_other
-        )
-
-    # -----
-
-    error_parts: List[str] = []
-
-    if config.validate_node:
-        if is_valid_node_specification():
-            validate_location_type(LocationType.oned)
-            return values
-
-        error_parts.append(fields.node_id)
-
-    if config.validate_branch:
-        if is_valid_branch_specification():
-            validate_location_type(LocationType.oned)
-            return values
-
-        error_parts.append(f"{fields.branch_id} and {fields.chainage}")
-
-    if config.validate_coordinates:
-        if config.validate_num_coordinates:
-            if is_valid_coordinates_with_num_coordinates_specification():
-                validate_coordinates_with_num_coordinates()
-                return values
-
-            error_parts.append(
-                f"{fields.x_coordinates}, {fields.y_coordinates} and {fields.num_coordinates}"
-            )
-
-        else:
-            if is_valid_coordinates_specification():
-                validate_coordinates()
-                return values
-
-            error_parts.append(f"{fields.x_coordinates} and {fields.y_coordinates}")
-
-    error = " or ".join(error_parts) + " should be provided"
-    raise ValueError(error)
-
-
-def rename_keys_for_backwards_compatibility(
-    values: Dict, keys_to_rename: Dict[str, List[str]]
-) -> Dict:
-    """
-    Renames the provided keys to support backwards compatibility.
-
-    Args:
-
-        values (Dict): Dictionary of input class fields.
-        keys_to_rename (Dict[str, List[str]]): Dictionary of keys and a list of old keys that
-        should be converted to the current key.
-
-    Returns:
-        Dict: Dictionary where the provided keys are renamed.
-    """
-    for current_keyword, old_keywords in keys_to_rename.items():
-        if current_keyword in values:
-            continue
-
-        for old_keyword in old_keywords:
-            if (value := values.get(old_keyword)) is not None:
-                values[current_keyword] = value
-                del values[old_keyword]
-                break
-
-    return values
+"""util.py provides additional utility methods related to handling ini files.
+"""
+from datetime import datetime
+from enum import Enum
+from operator import eq
+from typing import Any, Callable, Dict, List, Optional, Type
+
+from pydantic.class_validators import root_validator, validator
+from pydantic.fields import ModelField
+from pydantic.main import BaseModel
+
+from hydrolib.core.dflowfm.common.models import LocationType
+from hydrolib.core.utils import operator_str, str_is_empty_or_none, to_list
+
+
+def get_split_string_on_delimiter_validator(*field_name: str):
+    """Get a validator to split strings passed to the specified field_name.
+
+    Strings are split based on an automatically selected provided delimiter.
+    The delimiter is the field's own delimiter, if that was defined using
+    Field(.., delimiter=".."). Otherwise, the delimiter is the field's parent
+    class's delimiter (which should be (subclass of) INIBasedModel.)
+    The validator splits a string value into a list of substrings before any
+    other validation takes place.
+
+    Returns:
+        the validator which splits strings on the provided delimiter.
+    """
+
+    def split(cls, v: Any, field: ModelField):
+        if isinstance(v, str):
+            v = v.split(cls.get_list_field_delimiter(field.name))
+            v = [item.strip() for item in v if item != ""]
+        return v
+
+    return validator(*field_name, allow_reuse=True, pre=True)(split)
+
+
+def get_enum_validator(
+    *field_name: str,
+    enum: Type[Enum],
+    alternative_enum_values: Optional[Dict[str, List[str]]] = None,
+):
+    """
+    Get a case-insensitive enum validator that will returns the corresponding enum value.
+    If the input is a list, then each list value is checked individually.
+
+    Args:
+        enum (Type[Enum]): The enum type for which to validate.
+        alternative_enum_values (Dict[str, List[str]], optional): Dictionary with alternative
+            allowed values for one or more of the enum keys. Dict key must be a valid current
+            key of enum (case sensitive). Use this to backwards support and convert old enum
+            values in user input. For example: {SomeEnum.current_value: ["old value"]}.
+    """
+
+    def get_enum(v):
+        for entry in enum:
+            if entry.lower() == v.lower():
+                return entry
+            if (
+                alternative_enum_values is not None
+                and (alt_values := alternative_enum_values.get(entry.value)) is not None
+                and v.lower() in (altval.lower() for altval in alt_values)
+            ):
+                return entry
+
+        return v
+
+    return validator(*field_name, allow_reuse=True, pre=True, each_item=True)(get_enum)
+
+
+def make_list_validator(*field_name: str):
+    """Get a validator make a list of object if a single object is passed."""
+
+    def split(v: Any):
+        if not isinstance(v, list):
+            v = [v]
+        return v
+
+    return validator(*field_name, allow_reuse=True, pre=True)(split)
+
+
+def validate_correct_length(
+    values: Dict,
+    *field_names,
+    length_name: str,
+    length_incr: int = 0,
+    list_required_with_length: bool = False,
+    min_length: int = 0,
+) -> Dict:
+    """
+    Validate the correct length (and presence) of several list fields in an object.
+
+    Args:
+        values (Dict): dictionary of values to validate.
+        *field_names (str): names of the instance variables that are a list and need checking.
+        length_name (str): name of the instance variable that stores the expected length.
+        length_incr (int): Optional extra increment of length value (e.g., to have +1 extra value in lists).
+        list_required_with_length (obj:`bool`, optional): Whether each list *must* be present if the length
+            attribute is present (and > 0) in the input values. Default: False. If False, list length is only
+            checked for the lists that are not None.
+        min_length (int): minimum for list length value, overrides length_name value if that is smaller.
+            For example, to require list length 1 when length value is given as 0.
+
+    Raises:
+        ValueError: When the number of values for any of the given field_names is not as expected.
+
+    Returns:
+        Dict: Dictionary of validated values.
+    """
+
+    def _get_incorrect_length_validation_message() -> str:
+        """Make a string with a validation message, ready to be format()ed with
+        field name and length name."""
+        incrstring = f" + {length_incr}" if length_incr != 0 else ""
+        minstring = f" (and at least {min_length})" if min_length > 0 else ""
+
+        return (
+            "Number of values for {} should be equal to the {} value"
+            + incrstring
+            + minstring
+            + "."
+        )
+
+    def _validate_listfield_length(
+        field_name: str, field: Optional[List[Any]], requiredlength: int
+    ):
+        """Validate the length of a single field, which should be a list."""
+
+        if field is not None and len(field) != requiredlength:
+            raise ValueError(
+                _get_incorrect_length_validation_message().format(
+                    field_name, length_name
+                )
+            )
+        if field is None and list_required_with_length and requiredlength > 0:
+            raise ValueError(
+                f"List {field_name} cannot be missing if {length_name} is given."
+            )
+
+        return field
+
+    length = values.get(length_name)
+    if length is None:
+        # length attribute not present, possibly defer validation to a subclass.
+        return values
+
+    requiredlength = max(length + length_incr, min_length)
+
+    for field_name in field_names:
+        field = values.get(field_name)
+        values[field_name] = _validate_listfield_length(
+            field_name,
+            field,
+            requiredlength,
+        )
+
+    return values
+
+
+def validate_forbidden_fields(
+    values: Dict,
+    *field_names,
+    conditional_field_name: str,
+    conditional_value: Any,
+    comparison_func: Callable[[Any, Any], bool] = eq,
+) -> Dict:
+    """
+    Validates whether certain fields are *not* provided, if `conditional_field_name` is equal to `conditional_value`.
+    The equality check can be overridden with another comparison operator function.
+
+    Args:
+        values (Dict): Dictionary of input class fields.
+        *field_names (str): Names of the instance variables that need to be validated.
+        conditional_field_name (str): Name of the instance variable on which the fields are dependent.
+        conditional_value (Any): Value that the conditional field should contain to perform this validation.
+        comparison_func (Callable): binary operator function, used to override the default "eq" check for the conditional field value.
+
+    Raises:
+        ValueError: When a forbidden field is provided.
+
+    Returns:
+        Dict: Validated dictionary of input class fields.
+    """
+    if (val := values.get(conditional_field_name)) is None or not comparison_func(
+        val, conditional_value
+    ):
+        return values
+
+    for field in field_names:
+        if values.get(field) != None:
+            raise ValueError(
+                f"{field} is forbidden when {conditional_field_name} {operator_str(comparison_func)} {conditional_value}"
+            )
+
+    return values
+
+
+def validate_required_fields(
+    values: Dict,
+    *field_names,
+    conditional_field_name: str,
+    conditional_value: Any,
+    comparison_func: Callable[[Any, Any], bool] = eq,
+) -> Dict:
+    """
+    Validates whether the specified fields are provided, if `conditional_field_name` is equal to `conditional_value`.
+    The equality check can be overridden with another comparison operator function.
+
+    Args:
+        values (Dict): Dictionary of input class fields.
+        *field_names (str): Names of the instance variables that need to be validated.
+        conditional_field_name (str): Name of the instance variable on which the fields are dependent.
+        conditional_value (Any): Value that the conditional field should contain to perform this validation.
+        comparison_func (Callable): binary operator function, used to override the default "eq" check for the conditional field value.
+
+    Raises:
+        ValueError: When a required field is not provided under the given conditions.
+
+    Returns:
+        Dict: Validated dictionary of input class fields.
+    """
+
+    if (val := values.get(conditional_field_name)) is None or not comparison_func(
+        val, conditional_value
+    ):
+        return values
+
+    for field in field_names:
+        if values.get(field) == None:
+            raise ValueError(
+                f"{field} should be provided when {conditional_field_name} {operator_str(comparison_func)} {conditional_value}"
+            )
+
+    return values
+
+
+def validate_conditionally(
+    cls,
+    values: Dict,
+    root_vldt: classmethod,
+    conditional_field_name: str,
+    conditional_value: Any,
+    comparison_func: Callable[[Any, Any], bool] = eq,
+) -> Dict:
+    """
+    Validate whether certain fields are *not* provided, if `conditional_field_name` is equal to `conditional_value`.
+    The equality check can be overridden with another comparison operator function.
+
+    Args:
+        cls: Reference to a class.
+        values (Dict): Dictionary of input class fields.
+        root_vldt (classmethod): A root validator that is to be called *if* the condition is satisfied.
+        conditional_field_name (str): Name of the instance variable that determines whether the root validator must be called or not.
+        conditional_value (Any): Value that the conditional field should be compared with to perform this validation.
+        comparison_func (Callable): Binary operator function, used to override the default "eq" check for the conditional field value.
+
+    Returns:
+        Dict: Validated dictionary of input class fields.
+    """
+    if (val := values.get(conditional_field_name)) is not None and comparison_func(
+        val, conditional_value
+    ):
+        # Condition is met: call the actual root validator, passing on the attribute values.
+        root_vldt.__func__(cls, values)
+
+    return values
+
+
+def validate_datetime_string(
+    field_value: Optional[str], field: ModelField
+) -> Optional[str]:
+    """Validate that a field value matches the YYYYmmddHHMMSS datetime format.
+
+    Args:
+        field_value (Optional[str]): value of a Pydantic field, may be optional.
+        field (ModelField): the underlying Pydantic ModelField, used in error
+            message.
+
+    Returns:
+        Optional[str]: the original input value, if valid.
+
+    Raises:
+        ValueError: if a non-empty input string does not have valid format.
+    """
+    if (
+        field_value is not None
+        and len(field_value.strip()) > 0
+        and field_value != "yyyymmddhhmmss"
+    ):
+        try:
+            _ = datetime.strptime(field_value, r"%Y%m%d%H%M%S")
+        except ValueError:
+            raise ValueError(
+                f"Invalid datetime string for {field.alias}: '{field_value}', expecting 'YYYYmmddHHMMSS'."
+            )
+
+    return field_value  # this is the value written to the class field
+
+
+def get_from_subclass_defaults(cls: Type[BaseModel], fieldname: str, value: str) -> str:
+    """
+    Gets a value that corresponds with the default field value of one of the subclasses.
+    If the subclass doesn't have the specified field, it will look into its own subclasses
+    recursively for the specified fieldname.
+
+    Args:
+        cls (Type[BaseModel]): The parent model type.
+        fieldname (str): The field name for which retrieve the default for.
+        value (str): The value to compare with.
+
+    Returns:
+        str: The field default that corresponds to the value. If nothing is found return the input value.
+    """
+    # Immediately check in direct subclasses, not in base cls itself:
+    for c in cls.__subclasses__():
+        default = _try_get_default_value(c, fieldname, value)
+        if default is not None:
+            return default
+
+    # No matching default was found, return input value:
+    return value
+
+
+def _try_get_default_value(
+    c: Type[BaseModel], fieldname: str, value: str
+) -> Optional[str]:
+    """Helper subroutine to get the default value for a particular field in
+    the given class or any of its descendant classes, if it matches the input
+    value (case insensitive).
+
+    This method recurses depth-first topdown into the class'es subclasses.
+
+        c (Type[BaseModel]): The base model type where the search starts.
+        fieldname (str): The field name for which retrieve the default for.
+        value (str): The value to compare with.
+
+    Returns:
+        Optional[str]: The field default that corresponds to the value. If nothing is found return None.
+    """
+    if (field := c.__fields__.get(fieldname)) is None:
+        return None
+
+    default = field.default
+
+    if default is not None and default.lower() == value.lower():
+        # If this class's default matches, directly return it to end the recursion.
+        return default
+
+    for sc in c.__subclasses__():
+        default = _try_get_default_value(sc, fieldname, value)
+        if default is not None:
+            return default
+
+    # Nothing found under c, return None to caller (e.g., to continue recursion).
+    return None
+
+
+def get_type_based_on_subclass_default_value(
+    cls: Type, fieldname: str, value: str
+) -> Optional[Type]:
+    """
+    Gets the type of the first subclass where the default value of the fieldname is equal
+    to the provided value. If there is no match in the subclass, it will recursively search
+    in the subclasses of the subclass.
+
+    Args:
+        cls (Type): The base type.
+        fieldname (str): The field name for which retrieve the default for.
+        value (str): The value to compare with.
+
+    Returns:
+        [type]: The type of the first subclass that has a default value for the provided fieldname
+        equal to the provided value. Returns None if the fieldname is not found in the subclasses
+        or if no match was found.
+    """
+    for c in cls.__subclasses__():
+        subclass_type = _get_type_based_on_default_value(c, fieldname, value)
+        if subclass_type is not None:
+            return subclass_type
+    return None
+
+
+def _get_type_based_on_default_value(cls, fieldname, value) -> Optional[Type]:
+    if (field := cls.__fields__.get(fieldname)) is None:
+        return None
+
+    default = field.default
+    if default is not None and default.lower() == value.lower():
+        return cls
+
+    for sc in cls.__subclasses__():
+        subclass_type = _get_type_based_on_default_value(sc, fieldname, value)
+        if subclass_type is not None:
+            return subclass_type
+
+    return None
+
+
+class LocationValidationConfiguration(BaseModel):
+    """Class that holds the various configuration settings needed for location validation."""
+
+    validate_node: bool = True
+    """bool, optional: Whether or not node location specification should be validated. Defaults to True."""
+
+    validate_coordinates: bool = True
+    """bool, optional: Whether or not coordinate location specification should be validated. Defaults to True."""
+
+    validate_branch: bool = True
+    """bool, optional: Whether or not branch location specification should be validated. Defaults to True."""
+
+    validate_num_coordinates: bool = True
+    """bool, optional: Whether or not the number of coordinates should be validated or not. This option is only relevant when `validate_coordinates` is True. Defaults to True."""
+
+    minimum_num_coordinates: int = 0
+    """int, optional: The minimum required number of coordinates. This option is only relevant when `validate_coordinates` is True. Defaults to 0."""
+
+
+class LocationValidationFieldNames(BaseModel):
+    """Class that holds the various field names needed for location validation."""
+
+    node_id: str = "nodeId"
+    """str, optional: The node id field name. Defaults to `nodeId`."""
+
+    branch_id: str = "branchId"
+    """str, optional: The branch id field name. Defaults to `branchId`."""
+
+    chainage: str = "chainage"
+    """str, optional: The chainage field name. Defaults to `chainage`."""
+
+    x_coordinates: str = "xCoordinates"
+    """str, optional: The x-coordinates field name. Defaults to `xCoordinates`."""
+
+    y_coordinates: str = "yCoordinates"
+    """str, optional: The y-coordinates field name. Defaults to `yCoordinates`."""
+
+    num_coordinates: str = "numCoordinates"
+    """str, optional: The number of coordinates field name. Defaults to `numCoordinates`."""
+
+    location_type: str = "locationType"
+    """str, optional: The location type field name. Defaults to `locationType`."""
+
+
+def validate_location_specification(
+    values: Dict,
+    config: Optional[LocationValidationConfiguration] = None,
+    fields: Optional[LocationValidationFieldNames] = None,
+) -> Dict:
+    """
+    Validates whether the correct location specification is given in
+    typical 1D2D input in an IniBasedModel class.
+
+    Validates for presence of at least one of: nodeId, branchId with chainage,
+    xCoordinates with yCoordinates, or xCoordinates with yCoordinates and numCoordinates.
+    Validates for the locationType for nodeId and branchId.
+
+    Args:
+        values (Dict): Dictionary of object's validated fields.
+        config (LocationValidationConfiguration, optional): Configuration for the location validation. Default is None.
+        field (LocationValidationFieldNames, optional): Fields names that should be used for the location validation. Default is None.
+
+    Raises:
+        ValueError: When exactly one of the following combinations were not given:
+            - nodeId
+            - branchId with chainage
+            - xCoordinates with yCoordinates
+            - xCoordinates with yCoordinates and numCoordinates.
+        ValueError: When numCoordinates does not meet the requirement minimum amount or does not match the amount of xCoordinates or yCoordinates.
+        ValueError: When locationType should be 1d but other was specified.
+
+    Returns:
+        Dict: Validated dictionary of input class fields.
+    """
+
+    if config is None:
+        config = LocationValidationConfiguration()
+
+    if fields is None:
+        fields = LocationValidationFieldNames()
+
+    has_node_id = not str_is_empty_or_none(values.get(fields.node_id.lower()))
+    has_branch_id = not str_is_empty_or_none(values.get(fields.branch_id.lower()))
+    has_chainage = values.get(fields.chainage.lower()) is not None
+    has_x_coordinates = values.get(fields.x_coordinates.lower()) is not None
+    has_y_coordinates = values.get(fields.y_coordinates.lower()) is not None
+    has_num_coordinates = values.get(fields.num_coordinates.lower()) is not None
+
+    # ----- Local validation functions
+    def get_length(field: str):
+        value = values[field.lower()]
+        return len(to_list(value))
+
+    def validate_location_type(expected_location_type: LocationType) -> None:
+        location_type = values.get(fields.location_type.lower(), None)
+        if str_is_empty_or_none(location_type):
+            values[fields.location_type.lower()] = expected_location_type
+        elif location_type != expected_location_type:
+            raise ValueError(
+                f"{fields.location_type} should be {expected_location_type} but was {location_type}"
+            )
+
+    def validate_coordinates_with_num_coordinates() -> None:
+        length_x_coordinates = get_length(fields.x_coordinates)
+        length_y_coordinates = get_length(fields.y_coordinates)
+        num_coordinates = values[fields.num_coordinates.lower()]
+
+        if not num_coordinates == length_x_coordinates == length_y_coordinates:
+            raise ValueError(
+                f"{fields.num_coordinates} should be equal to the amount of {fields.x_coordinates} and {fields.y_coordinates}"
+            )
+
+        validate_minimum_num_coordinates(num_coordinates)
+
+    def validate_coordinates() -> None:
+        len_x_coordinates = get_length(fields.x_coordinates)
+        len_y_coordinates = get_length(fields.y_coordinates)
+
+        if len_x_coordinates != len_y_coordinates:
+            raise ValueError(
+                f"{fields.x_coordinates} and {fields.y_coordinates} should have an equal amount of coordinates"
+            )
+
+        validate_minimum_num_coordinates(len_x_coordinates)
+
+    def validate_minimum_num_coordinates(actual_num: int) -> None:
+        if actual_num < config.minimum_num_coordinates:
+            raise ValueError(
+                f"{fields.x_coordinates} and {fields.y_coordinates} should have at least {config.minimum_num_coordinates} coordinate(s)"
+            )
+
+    def is_valid_node_specification() -> bool:
+        has_other = (
+            has_branch_id
+            or has_chainage
+            or has_x_coordinates
+            or has_y_coordinates
+            or has_num_coordinates
+        )
+        return has_node_id and not has_other
+
+    def is_valid_branch_specification() -> bool:
+        has_other = (
+            has_node_id or has_x_coordinates or has_y_coordinates or has_num_coordinates
+        )
+        return has_branch_id and has_chainage and not has_other
+
+    def is_valid_coordinates_specification() -> bool:
+        has_other = has_node_id or has_branch_id or has_chainage or has_num_coordinates
+        return has_x_coordinates and has_y_coordinates and not has_other
+
+    def is_valid_coordinates_with_num_coordinates_specification() -> bool:
+        has_other = has_node_id or has_branch_id or has_chainage
+        return (
+            has_x_coordinates
+            and has_y_coordinates
+            and has_num_coordinates
+            and not has_other
+        )
+
+    # -----
+
+    error_parts: List[str] = []
+
+    if config.validate_node:
+        if is_valid_node_specification():
+            validate_location_type(LocationType.oned)
+            return values
+
+        error_parts.append(fields.node_id)
+
+    if config.validate_branch:
+        if is_valid_branch_specification():
+            validate_location_type(LocationType.oned)
+            return values
+
+        error_parts.append(f"{fields.branch_id} and {fields.chainage}")
+
+    if config.validate_coordinates:
+        if config.validate_num_coordinates:
+            if is_valid_coordinates_with_num_coordinates_specification():
+                validate_coordinates_with_num_coordinates()
+                return values
+
+            error_parts.append(
+                f"{fields.x_coordinates}, {fields.y_coordinates} and {fields.num_coordinates}"
+            )
+
+        else:
+            if is_valid_coordinates_specification():
+                validate_coordinates()
+                return values
+
+            error_parts.append(f"{fields.x_coordinates} and {fields.y_coordinates}")
+
+    error = " or ".join(error_parts) + " should be provided"
+    raise ValueError(error)
+
+
+def rename_keys_for_backwards_compatibility(
+    values: Dict, keys_to_rename: Dict[str, List[str]]
+) -> Dict:
+    """
+    Renames the provided keys to support backwards compatibility.
+
+    Args:
+
+        values (Dict): Dictionary of input class fields.
+        keys_to_rename (Dict[str, List[str]]): Dictionary of keys and a list of old keys that
+        should be converted to the current key.
+
+    Returns:
+        Dict: Dictionary where the provided keys are renamed.
+    """
+    for current_keyword, old_keywords in keys_to_rename.items():
+        if current_keyword in values:
+            continue
+
+        for old_keyword in old_keywords:
+            if (value := values.get(old_keyword)) is not None:
+                values[current_keyword] = value
+                del values[old_keyword]
+                break
+
+    return values
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/inifield/models.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/inifield/models.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,230 +1,230 @@
-import logging
-from abc import ABC
-from enum import Enum
-from typing import Dict, List, Literal, Optional
-
-from pydantic import Field
-from pydantic.class_validators import root_validator, validator
-from pydantic.types import NonNegativeFloat, PositiveInt
-
-from hydrolib.core.basemodel import DiskOnlyFileModel
-from hydrolib.core.dflowfm.common import LocationType
-from hydrolib.core.dflowfm.common.models import Operand
-from hydrolib.core.dflowfm.ini.models import INIBasedModel, INIGeneral, INIModel
-from hydrolib.core.dflowfm.ini.util import (
-    get_enum_validator,
-    make_list_validator,
-    validate_required_fields,
-)
-
-logger = logging.getLogger(__name__)
-
-
-class DataFileType(str, Enum):
-    """
-    Enum class containing the valid values for the dataFileType
-    attribute in several subclasses of AbstractIniField.
-    """
-
-    arcinfo = "arcinfo"
-    geotiff = "GeoTIFF"
-    sample = "sample"
-    onedfield = "1dField"
-    polygon = "polygon"
-
-    allowedvaluestext = "Possible values: arcinfo, GeoTIFF, sample, 1dField, polygon."
-
-
-class InterpolationMethod(str, Enum):
-    """
-    Enum class containing the valid values for the interpolationMethod
-    attribute in several subclasses of AbstractIniField.
-    """
-
-    constant = "constant"  # only with dataFileType=polygon .
-    triangulation = "triangulation"  # Delaunay triangulation+linear interpolation.
-    averaging = "averaging"  # grid cell averaging.
-
-    allowedvaluestext = "Possible values: constant, triangulation, averaging."
-
-
-class AveragingType(str, Enum):
-    """
-    Enum class containing the valid values for the averagingType
-    attribute in several subclasses of AbstractIniField.
-    """
-
-    mean = "mean"  # simple average
-    nearestnb = "nearestNb"  # nearest neighbour value
-    max = "max"  # highest
-    min = "min"  # lowest
-    invdist = "invDist"  # inverse-weighted distance average
-    minabs = "minAbs"  # smallest absolute value
-
-    allowedvaluestext = "Possible values: mean, nearestNb, max, min, invDist, minAbs."
-
-
-class IniFieldGeneral(INIGeneral):
-    """The initial field file's `[General]` section with file meta data."""
-
-    class Comments(INIBasedModel.Comments):
-        fileversion: Optional[str] = Field(
-            "File version. Do not edit this.", alias="fileVersion"
-        )
-        filetype: Optional[str] = Field(
-            "File type. Should be 'iniField'. Do not edit this.",
-            alias="fileType",
-        )
-
-    comments: Comments = Comments()
-    _header: Literal["General"] = "General"
-    fileversion: str = Field("2.00", alias="fileVersion")
-    filetype: Literal["iniField"] = Field("iniField", alias="fileType")
-
-
-class AbstractSpatialField(INIBasedModel, ABC):
-    """
-    Abstract base class for `[Initial]` and `[Parameter]` block data in
-    inifield files.
-
-    Defines all common fields. Used via subclasses InitialField and ParameterField.
-    """
-
-    class Comments(INIBasedModel.Comments):
-        quantity: Optional[str] = Field(
-            "Name of the quantity. See UM Table D.2.", alias="quantity"
-        )
-        datafile: Optional[str] = Field(
-            "Name of file containing field data values.", alias="dataFile"
-        )
-        datafiletype: Optional[str] = Field("Type of dataFile.", alias="dataFileType")
-        interpolationmethod: Optional[str] = Field(
-            "Type of (spatial) interpolation.", alias="interpolationmethod"
-        )
-        operand: Optional[str] = Field(
-            "How this data is combined with previous data for the same quantity (if any).",
-            alias="operand",
-        )
-        averagingtype: Optional[str] = Field(
-            "Type of averaging, if interpolationMethod=averaging .",
-            alias="averagingtype",
-        )
-        averagingrelsize: Optional[str] = Field(
-            "Relative search cell size for averaging.", alias="averagingrelsize"
-        )
-        averagingnummin: Optional[str] = Field(
-            "Minimum number of points in averaging. Must be â‰¥ 1.",
-            alias="averagingnummin",
-        )
-        averagingpercentile: Optional[str] = Field(
-            "Percentile value for which data values to include in averaging. 0.0 means off.",
-            alias="averagingpercentile",
-        )
-        extrapolationmethod: Optional[str] = Field(
-            "Option for (spatial) extrapolation.", alias="extrapolationmethod"
-        )
-        locationtype: Optional[str] = Field(
-            "Target location of interpolation.", alias="locationtype"
-        )
-        value: Optional[str] = Field(
-            "Only for dataFileType=polygon. The constant value to be set inside for all model points inside the polygon."
-        )
-
-    comments: Comments = Comments()
-
-    quantity: str = Field(alias="quantity")
-    datafile: DiskOnlyFileModel = Field(alias="dataFile")
-
-    datafiletype: DataFileType = Field(alias="dataFileType")
-    interpolationmethod: Optional[InterpolationMethod] = Field(
-        alias="interpolationMethod"
-    )
-    operand: Optional[Operand] = Field(Operand.override.value, alias="operand")
-    averagingtype: Optional[AveragingType] = Field(
-        AveragingType.mean.value, alias="averagingType"
-    )
-    averagingrelsize: Optional[NonNegativeFloat] = Field(1.01, alias="averagingRelSize")
-    averagingnummin: Optional[PositiveInt] = Field(1, alias="averagingNumMin")
-    averagingpercentile: Optional[NonNegativeFloat] = Field(
-        0, alias="averagingPercentile"
-    )
-    extrapolationmethod: Optional[bool] = Field(False, alias="extrapolationMethod")
-    locationtype: Optional[LocationType] = Field(
-        LocationType.all.value, alias="locationType"
-    )
-    value: Optional[float] = Field(alias="value")
-
-    datafiletype_validator = get_enum_validator("datafiletype", enum=DataFileType)
-    interpolationmethod_validator = get_enum_validator(
-        "interpolationmethod", enum=InterpolationMethod
-    )
-    operand_validator = get_enum_validator("operand", enum=Operand)
-    averagingtype_validator = get_enum_validator("averagingtype", enum=AveragingType)
-    locationtype_validator = get_enum_validator("locationtype", enum=LocationType)
-
-    @root_validator(allow_reuse=True)
-    def validate_that_value_is_present_for_polygons(cls, values: Dict) -> Dict:
-        """Validates that the value is provided when dealing with polygons."""
-        return validate_required_fields(
-            values,
-            "value",
-            conditional_field_name="datafiletype",
-            conditional_value=DataFileType.polygon,
-        )
-
-    @validator("value", always=True)
-    @classmethod
-    def _validate_value_and_filetype(cls, v, values: dict):
-        if v is not None and values.get("datafiletype") != DataFileType.polygon:
-            raise ValueError(
-                f"When value={v} is given, dataFileType={DataFileType.polygon} is required."
-            )
-
-        return v
-
-
-class InitialField(AbstractSpatialField):
-    """
-    Initial condition field definition, represents an `[Initial]` block in
-    an inifield file.
-    Typically inside the definition list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.inifieldfile.initial[..]`
-    """
-
-    _header: Literal["Initial"] = "Initial"
-
-
-class ParameterField(AbstractSpatialField):
-    """
-    Parameter field definition, represents a `[Parameter]` block in
-    an inifield file.
-    Typically inside the definition list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.inifieldfile.parameter[..]`
-    """
-
-    _header: Literal["Parameter"] = "Parameter"
-
-
-class IniFieldModel(INIModel):
-    """
-    The overall inifield model that contains the contents of one initial field and parameter file.
-
-    This model is typically referenced under a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.inifieldfile[..]`.
-
-    Attributes:
-        general (IniFieldGeneral): `[General]` block with file metadata.
-        initial (List[InitialField]): List of `[Initial]` blocks with initial condition definitions.
-        parameter (List[ParameterField]): List of `[Parameter]` blocks with spatial parameter definitions.
-    """
-
-    general: IniFieldGeneral = IniFieldGeneral()
-    initial: List[InitialField] = []
-    parameter: List[ParameterField] = []
-
-    _split_to_list = make_list_validator("initial", "parameter")
-
-    @classmethod
-    def _ext(cls) -> str:
-        return ".ini"
-
-    @classmethod
-    def _filename(cls) -> str:
-        return "fieldFile"
+import logging
+from abc import ABC
+from enum import Enum
+from typing import Dict, List, Literal, Optional
+
+from pydantic import Field
+from pydantic.class_validators import root_validator, validator
+from pydantic.types import NonNegativeFloat, PositiveInt
+
+from hydrolib.core.basemodel import DiskOnlyFileModel
+from hydrolib.core.dflowfm.common import LocationType
+from hydrolib.core.dflowfm.common.models import Operand
+from hydrolib.core.dflowfm.ini.models import INIBasedModel, INIGeneral, INIModel
+from hydrolib.core.dflowfm.ini.util import (
+    get_enum_validator,
+    make_list_validator,
+    validate_required_fields,
+)
+
+logger = logging.getLogger(__name__)
+
+
+class DataFileType(str, Enum):
+    """
+    Enum class containing the valid values for the dataFileType
+    attribute in several subclasses of AbstractIniField.
+    """
+
+    arcinfo = "arcinfo"
+    geotiff = "GeoTIFF"
+    sample = "sample"
+    onedfield = "1dField"
+    polygon = "polygon"
+
+    allowedvaluestext = "Possible values: arcinfo, GeoTIFF, sample, 1dField, polygon."
+
+
+class InterpolationMethod(str, Enum):
+    """
+    Enum class containing the valid values for the interpolationMethod
+    attribute in several subclasses of AbstractIniField.
+    """
+
+    constant = "constant"  # only with dataFileType=polygon .
+    triangulation = "triangulation"  # Delaunay triangulation+linear interpolation.
+    averaging = "averaging"  # grid cell averaging.
+
+    allowedvaluestext = "Possible values: constant, triangulation, averaging."
+
+
+class AveragingType(str, Enum):
+    """
+    Enum class containing the valid values for the averagingType
+    attribute in several subclasses of AbstractIniField.
+    """
+
+    mean = "mean"  # simple average
+    nearestnb = "nearestNb"  # nearest neighbour value
+    max = "max"  # highest
+    min = "min"  # lowest
+    invdist = "invDist"  # inverse-weighted distance average
+    minabs = "minAbs"  # smallest absolute value
+
+    allowedvaluestext = "Possible values: mean, nearestNb, max, min, invDist, minAbs."
+
+
+class IniFieldGeneral(INIGeneral):
+    """The initial field file's `[General]` section with file meta data."""
+
+    class Comments(INIBasedModel.Comments):
+        fileversion: Optional[str] = Field(
+            "File version. Do not edit this.", alias="fileVersion"
+        )
+        filetype: Optional[str] = Field(
+            "File type. Should be 'iniField'. Do not edit this.",
+            alias="fileType",
+        )
+
+    comments: Comments = Comments()
+    _header: Literal["General"] = "General"
+    fileversion: str = Field("2.00", alias="fileVersion")
+    filetype: Literal["iniField"] = Field("iniField", alias="fileType")
+
+
+class AbstractSpatialField(INIBasedModel, ABC):
+    """
+    Abstract base class for `[Initial]` and `[Parameter]` block data in
+    inifield files.
+
+    Defines all common fields. Used via subclasses InitialField and ParameterField.
+    """
+
+    class Comments(INIBasedModel.Comments):
+        quantity: Optional[str] = Field(
+            "Name of the quantity. See UM Table D.2.", alias="quantity"
+        )
+        datafile: Optional[str] = Field(
+            "Name of file containing field data values.", alias="dataFile"
+        )
+        datafiletype: Optional[str] = Field("Type of dataFile.", alias="dataFileType")
+        interpolationmethod: Optional[str] = Field(
+            "Type of (spatial) interpolation.", alias="interpolationmethod"
+        )
+        operand: Optional[str] = Field(
+            "How this data is combined with previous data for the same quantity (if any).",
+            alias="operand",
+        )
+        averagingtype: Optional[str] = Field(
+            "Type of averaging, if interpolationMethod=averaging .",
+            alias="averagingtype",
+        )
+        averagingrelsize: Optional[str] = Field(
+            "Relative search cell size for averaging.", alias="averagingrelsize"
+        )
+        averagingnummin: Optional[str] = Field(
+            "Minimum number of points in averaging. Must be â‰¥ 1.",
+            alias="averagingnummin",
+        )
+        averagingpercentile: Optional[str] = Field(
+            "Percentile value for which data values to include in averaging. 0.0 means off.",
+            alias="averagingpercentile",
+        )
+        extrapolationmethod: Optional[str] = Field(
+            "Option for (spatial) extrapolation.", alias="extrapolationmethod"
+        )
+        locationtype: Optional[str] = Field(
+            "Target location of interpolation.", alias="locationtype"
+        )
+        value: Optional[str] = Field(
+            "Only for dataFileType=polygon. The constant value to be set inside for all model points inside the polygon."
+        )
+
+    comments: Comments = Comments()
+
+    quantity: str = Field(alias="quantity")
+    datafile: DiskOnlyFileModel = Field(alias="dataFile")
+
+    datafiletype: DataFileType = Field(alias="dataFileType")
+    interpolationmethod: Optional[InterpolationMethod] = Field(
+        alias="interpolationMethod"
+    )
+    operand: Optional[Operand] = Field(Operand.override.value, alias="operand")
+    averagingtype: Optional[AveragingType] = Field(
+        AveragingType.mean.value, alias="averagingType"
+    )
+    averagingrelsize: Optional[NonNegativeFloat] = Field(1.01, alias="averagingRelSize")
+    averagingnummin: Optional[PositiveInt] = Field(1, alias="averagingNumMin")
+    averagingpercentile: Optional[NonNegativeFloat] = Field(
+        0, alias="averagingPercentile"
+    )
+    extrapolationmethod: Optional[bool] = Field(False, alias="extrapolationMethod")
+    locationtype: Optional[LocationType] = Field(
+        LocationType.all.value, alias="locationType"
+    )
+    value: Optional[float] = Field(alias="value")
+
+    datafiletype_validator = get_enum_validator("datafiletype", enum=DataFileType)
+    interpolationmethod_validator = get_enum_validator(
+        "interpolationmethod", enum=InterpolationMethod
+    )
+    operand_validator = get_enum_validator("operand", enum=Operand)
+    averagingtype_validator = get_enum_validator("averagingtype", enum=AveragingType)
+    locationtype_validator = get_enum_validator("locationtype", enum=LocationType)
+
+    @root_validator(allow_reuse=True)
+    def validate_that_value_is_present_for_polygons(cls, values: Dict) -> Dict:
+        """Validates that the value is provided when dealing with polygons."""
+        return validate_required_fields(
+            values,
+            "value",
+            conditional_field_name="datafiletype",
+            conditional_value=DataFileType.polygon,
+        )
+
+    @validator("value", always=True)
+    @classmethod
+    def _validate_value_and_filetype(cls, v, values: dict):
+        if v is not None and values.get("datafiletype") != DataFileType.polygon:
+            raise ValueError(
+                f"When value={v} is given, dataFileType={DataFileType.polygon} is required."
+            )
+
+        return v
+
+
+class InitialField(AbstractSpatialField):
+    """
+    Initial condition field definition, represents an `[Initial]` block in
+    an inifield file.
+    Typically inside the definition list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.inifieldfile.initial[..]`
+    """
+
+    _header: Literal["Initial"] = "Initial"
+
+
+class ParameterField(AbstractSpatialField):
+    """
+    Parameter field definition, represents a `[Parameter]` block in
+    an inifield file.
+    Typically inside the definition list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.inifieldfile.parameter[..]`
+    """
+
+    _header: Literal["Parameter"] = "Parameter"
+
+
+class IniFieldModel(INIModel):
+    """
+    The overall inifield model that contains the contents of one initial field and parameter file.
+
+    This model is typically referenced under a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.inifieldfile[..]`.
+
+    Attributes:
+        general (IniFieldGeneral): `[General]` block with file metadata.
+        initial (List[InitialField]): List of `[Initial]` blocks with initial condition definitions.
+        parameter (List[ParameterField]): List of `[Parameter]` blocks with spatial parameter definitions.
+    """
+
+    general: IniFieldGeneral = IniFieldGeneral()
+    initial: List[InitialField] = []
+    parameter: List[ParameterField] = []
+
+    _split_to_list = make_list_validator("initial", "parameter")
+
+    @classmethod
+    def _ext(cls) -> str:
+        return ".ini"
+
+    @classmethod
+    def _filename(cls) -> str:
+        return "fieldFile"
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/mdu/models.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/mdu/models.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,2116 +1,2116 @@
-from enum import IntEnum
-from pathlib import Path
-from typing import Any, Dict, List, Literal, Optional, Union
-
-from pydantic import Field, validator
-
-from hydrolib.core.basemodel import (
-    DiskOnlyFileModel,
-    FileModel,
-    ResolveRelativeMode,
-    validator_set_default_disk_only_file_model_when_none,
-)
-from hydrolib.core.dflowfm.crosssection.models import CrossDefModel, CrossLocModel
-from hydrolib.core.dflowfm.ext.models import ExtModel
-from hydrolib.core.dflowfm.extold.models import ExtOldModel
-from hydrolib.core.dflowfm.friction.models import FrictionModel
-from hydrolib.core.dflowfm.ini.models import INIBasedModel, INIGeneral, INIModel
-from hydrolib.core.dflowfm.ini.serializer import INISerializerConfig
-from hydrolib.core.dflowfm.ini.util import (
-    get_split_string_on_delimiter_validator,
-    validate_datetime_string,
-)
-from hydrolib.core.dflowfm.inifield.models import IniFieldModel
-from hydrolib.core.dflowfm.net.models import NetworkModel
-from hydrolib.core.dflowfm.obs.models import ObservationPointModel
-from hydrolib.core.dflowfm.obscrosssection.models import ObservationCrossSectionModel
-from hydrolib.core.dflowfm.polyfile.models import PolyFile
-from hydrolib.core.dflowfm.storagenode.models import StorageNodeModel
-from hydrolib.core.dflowfm.structure.models import StructureModel
-from hydrolib.core.dflowfm.xyn.models import XYNModel
-from hydrolib.core.dflowfm.xyz.models import XYZModel
-
-
-class AutoStartOption(IntEnum):
-    """
-    Enum class containing the valid values for the AutoStart
-    attribute in the [General][hydrolib.core.dflowfm.mdu.models.General] class.
-    """
-
-    no = 0
-    autostart = 1
-    autostartstop = 2
-
-
-class General(INIGeneral):
-    class Comments(INIBasedModel.Comments):
-        program: Optional[str] = Field("Program.", alias="program")
-        version: Optional[str] = Field(
-            "Version number of computational kernel", alias="version"
-        )
-        filetype: Optional[str] = Field("File type. Do not edit this", alias="fileType")
-        fileversion: Optional[str] = Field(
-            "File version. Do not edit this.", alias="fileVersion"
-        )
-        autostart: Optional[str] = Field(
-            "Autostart simulation after loading MDU or not (0=no, 1=autostart, 2=autostartstop).",
-            alias="autoStart",
-        )
-        pathsrelativetoparent: Optional[str] = Field(
-            "Whether or not (1/0) to resolve file names (e.g. inside the *.ext file) relative to their direct parent, instead of to the toplevel MDU working dir",
-            alias="pathsRelativeToParent",
-        )
-
-    comments: Comments = Comments()
-    _header: Literal["General"] = "General"
-    program: str = Field("D-Flow FM", alias="program")
-    version: str = Field("1.2.94.66079M", alias="version")
-    filetype: Literal["modelDef"] = Field("modelDef", alias="fileType")
-    fileversion: str = Field("1.09", alias="fileVersion")
-    autostart: Optional[AutoStartOption] = Field(AutoStartOption.no, alias="autoStart")
-    pathsrelativetoparent: bool = Field(False, alias="pathsRelativeToParent")
-
-
-class Numerics(INIBasedModel):
-    """
-    The `[Numerics]` section in an MDU file.
-
-    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.numerics`.
-
-    All lowercased attributes match with the [Numerics] input as described in
-    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
-    """
-
-    class Comments(INIBasedModel.Comments):
-        cflmax: Optional[str] = Field("Maximum Courant nr.", alias="CFLMax")
-        epsmaxlev: Optional[str] = Field(
-            "Stop criterium for non linear iteration", alias="EpsMaxlev"
-        )
-        epsmaxlevm: Optional[str] = Field(
-            "Stop criterium for Nested Newton loop in non linear iteration",
-            alias="EpsMaxlevM",
-        )
-        advectype: Optional[str] = Field(
-            "Adv type, 0=no, 33=Perot q(uio-u) fast, 3=Perot q(uio-u).",
-            alias="advecType",
-        )
-        timesteptype: Optional[str] = Field(
-            "0=only transport, 1=transport + velocity update, 2=full implicit step_reduce, 3=step_jacobi, 4=explicit.",
-            alias="timeStepType",
-        )
-        limtyphu: Optional[str] = Field(
-            "Limiter type for waterdepth in continuity eq., 0=no, 1=minmod,2=vanLeer,3=Koren,4=Monotone Central.",
-            alias="limTypHu",
-        )
-        limtypmom: Optional[str] = Field(
-            "Limiter type for cell center advection velocity, 0=no, 1=minmod,2=vanLeer,4=Monotone Central.",
-            alias="limTypMom",
-        )
-        limtypsa: Optional[str] = Field(
-            "Limiter type for salinity transport,           0=no, 1=minmod,2=vanLeer,4=Monotone Central.",
-            alias="limTypSa",
-        )
-        icgsolver: Optional[str] = Field(
-            "Solver type, 4 = sobekGS + Saad-ILUD (default sequential), 6 = PETSc (default parallel), 7= CG+MILU (parallel).",
-            alias="icgSolver",
-        )
-        maxdegree: Optional[str] = Field(
-            "Maximum degree in Gauss elimination.", alias="maxDegree"
-        )
-        fixedweirscheme: Optional[str] = Field(
-            "6 = semi-subgrid scheme, 8 = Tabellenboek, 9 = Villemonte (default).",
-            alias="fixedWeirScheme",
-        )
-        fixedweircontraction: Optional[str] = Field(
-            "flow width = flow width*fixedWeirContraction.",
-            alias="fixedWeirContraction",
-        )
-        izbndpos: Optional[str] = Field(
-            "Position of z boundary, 0=mirroring of closest cell (as in Delft3D-FLOW), 1=on net boundary.",
-            alias="izBndPos",
-        )
-        tlfsmo: Optional[str] = Field(
-            "Fourier smoothing time on water level boundaries [s].", alias="tlfSmo"
-        )
-        keepstbndonoutflow: Optional[str] = Field(
-            "Keep salinity and temperature signals on boundary also at outflow, 1=yes, 0=no. Default=0: copy inside value on outflow.",
-            alias="keepSTBndOnOutflow",
-        )
-        slopedrop2d: Optional[str] = Field(
-            "Apply droplosses only if local bottom slope > Slopedrop2D, <=0 =no droplosses.",
-            alias="slopeDrop2D",
-        )
-        drop1d: Optional[str] = Field(
-            "Limit the downstream water level in the momentum equation to the downstream invert level, BOBdown (Î¶*down = max(BOBdown, Î¶down)).",
-            alias="drop1D",
-        )
-        chkadvd: Optional[str] = Field(
-            "Check advection terms if depth < chkadvdp.", alias="chkAdvd"
-        )
-        teta0: Optional[str] = Field(
-            "Theta (implicitness) of time integration, 0.5 < Theta < 1.0.",
-            alias="teta0",
-        )
-        qhrelax: Optional[str] = Field(None, alias="qhRelax")
-        cstbnd: Optional[str] = Field(
-            "Delft3D-FLOW type velocity treatment near boundaries for small coastal models (1) or not (0).",
-            alias="cstBnd",
-        )
-        maxitverticalforestersal: Optional[str] = Field(
-            "Forester iterations for salinity (0: no vertical filter for salinity, > 0: max nr of iterations).",
-            alias="maxitVerticalForesterSal",
-        )
-        maxitverticalforestertem: Optional[str] = Field(
-            "Forester iterations for temperature (0: no vertical filter for temperature, > 0: max nr of iterations).",
-            alias="maxitVerticalForesterTem",
-        )
-        turbulencemodel: Optional[str] = Field(
-            "0=no, 1 = constant, 2 = algebraic, 3 = k-epsilon, 4 = k-tau.",
-            alias="turbulenceModel",
-        )
-        turbulenceadvection: Optional[str] = Field(
-            "Turbulence advection (0=no, 3 = horizontal explicit vertical implicit).",
-            alias="turbulenceAdvection",
-        )
-        anticreep: Optional[str] = Field(
-            "Include anti-creep calculation (0: no, 1: yes).", alias="antiCreep"
-        )
-        baroczlaybed: Optional[str] = Field(
-            "Use fix in baroclinic pressure for zlaybed (1: yes, 0: no)",
-            alias="barocZLayBed",
-        )
-        barocponbnd: Optional[str] = Field(
-            "Use baroclinic pressure correction on open boundaries (1: yes, 0: no)",
-            alias="barocPOnBnd",
-        )
-        maxwaterleveldiff: Optional[str] = Field(
-            "Upper bound [m] on water level changes, (<= 0: no bounds). Run will abort when violated.",
-            alias="maxWaterLevelDiff",
-        )
-        maxvelocitydiff: Optional[str] = Field(
-            "Upper bound [m/s] on velocity changes, (<= 0: no bounds). Run will abort when violated.",
-            alias="maxVelocityDiff",
-        )
-        mintimestepbreak: Optional[str] = Field(
-            "Smallest allowed timestep (in s), checked on a sliding average of several timesteps. Run will abort when violated.",
-            alias="minTimestepBreak",
-        )
-        epshu: Optional[str] = Field(
-            "Threshold water depth for wetting and drying [m].", alias="epsHu"
-        )
-
-    comments: Comments = Comments()
-
-    _header: Literal["Numerics"] = "Numerics"
-    cflmax: float = Field(0.7, alias="CFLMax")
-    epsmaxlev: float = Field(1e-8, alias="EpsMaxlev")
-    epsmaxlevm: float = Field(1e-8, alias="EpsMaxlevM")
-    advectype: int = Field(33, alias="advecType")
-    timesteptype: int = Field(2, alias="timeStepType")
-    limtyphu: int = Field(0, alias="limTypHu")
-    limtypmom: int = Field(4, alias="limTypMom")
-    limtypsa: int = Field(4, alias="limTypSa")
-    icgsolver: int = Field(4, alias="icgSolver")
-    maxdegree: int = Field(6, alias="maxDegree")
-    fixedweirscheme: int = Field(9, alias="fixedWeirScheme")
-    fixedweircontraction: float = Field(1.0, alias="fixedWeirContraction")
-    izbndpos: int = Field(0, alias="izBndPos")
-    tlfsmo: float = Field(0.0, alias="tlfSmo")
-    keepstbndonoutflow: bool = Field(False, alias="keepSTBndOnOutflow")
-    slopedrop2d: float = Field(0.0, alias="slopeDrop2D")
-    drop1d: bool = Field(False, alias="drop1D")
-    chkadvd: float = Field(0.1, alias="chkAdvd")
-    teta0: float = Field(0.55, alias="teta0")
-    qhrelax: float = Field(0.01, alias="qhRelax")
-    cstbnd: bool = Field(False, alias="cstBnd")
-    maxitverticalforestersal: int = Field(0, alias="maxitVerticalForesterSal")
-    maxitverticalforestertem: int = Field(0, alias="maxitVerticalForesterTem")
-    turbulencemodel: int = Field(3, alias="turbulenceModel")
-    turbulenceadvection: int = Field(3, alias="turbulenceAdvection")
-    anticreep: bool = Field(False, alias="antiCreep")
-    baroczlaybed: bool = Field(False, alias="barocZLayBed")
-    barocponbnd: bool = Field(False, alias="barocPOnBnd")
-    maxwaterleveldiff: float = Field(0.0, alias="maxWaterLevelDiff")
-    maxvelocitydiff: float = Field(0.0, alias="maxVelocityDiff")
-    mintimestepbreak: float = Field(0.0, alias="minTimestepBreak")
-    epshu: float = Field(0.0001, alias="epsHu")
-
-
-class VolumeTables(INIBasedModel):
-    """
-    The `[VolumeTables]` section in an MDU file.
-
-    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.volumetables`.
-
-    All lowercased attributes match with the [VolumeTables] input as described in
-    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
-    """
-
-    class Comments(INIBasedModel.Comments):
-        usevolumetables: Optional[str] = Field(
-            "Use volume tables for 1D grid cells (1: yes, 0 = no).",
-            alias="useVolumeTables",
-        )
-        increment: Optional[str] = Field(
-            "The height increment for the volume tables [m].", alias="increment"
-        )
-        usevolumetablefile: Optional[str] = Field(
-            "Read and write the volume table from/to file (1: yes, 0= no).",
-            alias="useVolumeTableFile",
-        )
-
-    comments: Comments = Comments()
-
-    _header: Literal["VolumeTables"] = "VolumeTables"
-    usevolumetables: bool = Field(False, alias="useVolumeTables")
-    increment: float = Field(0.2, alias="increment")
-    usevolumetablefile: bool = Field(False, alias="useVolumeTableFile")
-
-
-class Physics(INIBasedModel):
-    """
-    The `[Physics]` section in an MDU file.
-
-    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.physics`.
-
-    All lowercased attributes match with the [Physics] input as described in
-    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
-    """
-
-    class Comments(INIBasedModel.Comments):
-        uniffrictcoef: Optional[str] = Field(
-            "Uniform friction coefficient (0: no friction).", alias="unifFrictCoef"
-        )
-        uniffricttype: Optional[str] = Field(
-            "Uniform friction type (0: Chezy, 1: Manning, 2: White-Colebrook, 3: idem, WAQUA style).",
-            alias="unifFrictType",
-        )
-        uniffrictcoef1d: Optional[str] = Field(
-            "Uniform friction coefficient in 1D links (0: no friction).",
-            alias="unifFrictCoef1D",
-        )
-        uniffrictcoeflin: Optional[str] = Field(
-            "Uniform linear friction coefficient (0: no friction).",
-            alias="unifFrictCoefLin",
-        )
-        vicouv: Optional[str] = Field(
-            "Uniform horizontal eddy viscosity [m2/s].", alias="vicouv"
-        )
-        dicouv: Optional[str] = Field(
-            "Uniform horizontal eddy diffusivity [m2/s].", alias="dicouv"
-        )
-        vicoww: Optional[str] = Field(
-            "Background vertical eddy viscosity [m2/s].", alias="vicoww"
-        )
-        dicoww: Optional[str] = Field(
-            "Background vertical eddy diffusivity [m2/s].", alias="dicoww"
-        )
-        vicwminb: Optional[str] = Field(
-            "Minimum viscosity in production and buoyancy term [m2/s].",
-            alias="vicwminb",
-        )
-        xlozmidov: Optional[str] = Field(
-            "Ozmidov length scale [m], default=0.0, no contribution of internal waves to vertical diffusion.",
-            alias="xlozmidov",
-        )
-        smagorinsky: Optional[str] = Field(
-            "Add Smagorinsky horizontal turbulence: vicu = vicu + ( (Smagorinsky*dx)**2)*S.",
-            alias="smagorinsky",
-        )
-        elder: Optional[str] = Field(
-            "Add Elder contribution: vicu = vicu + Elder*kappa*ustar*H/6); e.g. 1.0.",
-            alias="elder",
-        )
-        irov: Optional[str] = Field(
-            "Wall friction, 0=free slip, 1 = partial slip using wall_ks.", alias="irov"
-        )
-        wall_ks: Optional[str] = Field(
-            "Nikuradse roughness [m] for side walls, wall_z0=wall_ks/30.",
-            alias="wall_ks",
-        )
-        rhomean: Optional[str] = Field(
-            "Average water density [kg/m3].", alias="rhomean"
-        )
-        idensform: Optional[str] = Field(
-            "Density calulation (0: uniform, 1: Eckart, 2: Unesco, 3=Unesco83, 13=3+pressure).",
-            alias="idensform",
-        )
-        ag: Optional[str] = Field("Gravitational acceleration [m/s2].", alias="ag")
-        tidalforcing: Optional[str] = Field(
-            "Tidal forcing, if jsferic=1 (0: no, 1: yes).", alias="tidalForcing"
-        )
-        itcap: Optional[str] = Field(
-            "Upper limit on internal tides dissipation (W/m^2)", alias="ITcap"
-        )
-        doodsonstart: Optional[str] = Field(
-            "Doodson start time for tidal forcing [s].", alias="doodsonStart"
-        )
-        doodsonstop: Optional[str] = Field(
-            "Doodson stop time for tidal forcing [s].", alias="doodsonStop"
-        )
-        doodsoneps: Optional[str] = Field(
-            "Doodson tolerance level for tidal forcing [s].", alias="doodsonEps"
-        )
-        villemontecd1: Optional[str] = Field(
-            "Calibration coefficient for Villemonte. Default = 1.0.",
-            alias="villemonteCD1",
-        )
-        villemontecd2: Optional[str] = Field(
-            "Calibration coefficient for Villemonte. Default = 10.0.",
-            alias="villemonteCD2",
-        )
-        salinity: Optional[str] = Field(
-            "Include salinity, (0: no, 1: yes).", alias="salinity"
-        )
-        initialsalinity: Optional[str] = Field(
-            "Initial salinity concentration [ppt].", alias="initialSalinity"
-        )
-        sal0abovezlev: Optional[str] = Field(
-            "Salinity 0 above level [m].", alias="sal0AboveZLev"
-        )
-        deltasalinity: Optional[str] = Field(
-            "uniform initial salinity [ppt].", alias="deltaSalinity"
-        )
-        backgroundsalinity: Optional[str] = Field(
-            "Background salinity for eqn. of state if salinity not computed [psu].",
-            alias="backgroundSalinity",
-        )
-        temperature: Optional[str] = Field(
-            "Include temperature (0: no, 1: only transport, 3: excess model of D3D, 5: composite (ocean) model).",
-            alias="temperature",
-        )
-        initialtemperature: Optional[str] = Field(
-            "Initial temperature [â—¦C].", alias="initialTemperature"
-        )
-        backgroundwatertemperature: Optional[str] = Field(
-            "Background water temperature for eqn. of state if temperature not computed [â—¦C].",
-            alias="backgroundWaterTemperature",
-        )
-        secchidepth: Optional[str] = Field(
-            "Water clarity parameter [m].", alias="secchiDepth"
-        )
-        stanton: Optional[str] = Field(
-            "Coefficient for convective heat flux ( ), if negative, then Cd wind is used.",
-            alias="stanton",
-        )
-        dalton: Optional[str] = Field(
-            "Coefficient for evaporative heat flux ( ), if negative, then Cd wind is used.",
-            alias="dalton",
-        )
-        tempmax: Optional[str] = Field(
-            "Limit the temperature to max value [Â°C]", alias="tempMax"
-        )
-        tempmin: Optional[str] = Field(
-            "Limit the temperature to min value [Â°C]", alias="tempMin"
-        )
-        salimax: Optional[str] = Field(
-            "Limit for salinity to max value [ppt]", alias="saliMax"
-        )
-        salimin: Optional[str] = Field(
-            "Limit for salinity to min value [ppt]", alias="saliMin"
-        )
-        heat_eachstep: Optional[str] = Field(
-            "'1=heat each timestep, 0=heat each usertimestep", alias="heat_eachStep"
-        )
-        rhoairrhowater: Optional[str] = Field(
-            "'windstress rhoa/rhow: 0=Rhoair/Rhomean, 1=Rhoair/rhow(), 2=rhoa0()/rhow(), 3=rhoa10()/Rhow()",
-            alias="rhoAirRhoWater",
-        )
-        nudgetimeuni: Optional[str] = Field(
-            "Uniform nudge relaxation time [s]", alias="nudgeTimeUni"
-        )
-        iniwithnudge: Optional[str] = Field(
-            "Initialize salinity and temperature with nudge variables (0: no, 1: yes, 2: only initialize, no nudging)",
-            alias="iniWithNudge",
-        )
-        secondaryflow: Optional[str] = Field(
-            "Secondary flow (0: no, 1: yes).", alias="secondaryFlow"
-        )
-        betaspiral: Optional[str] = Field(
-            "Weight factor of the spiral flow intensity on flow dispersion stresses (0d0 = disabled).",
-            alias="betaSpiral",
-        )
-
-    comments: Comments = Comments()
-
-    _header: Literal["Physics"] = "Physics"
-    uniffrictcoef: float = Field(0.023, alias="unifFrictCoef")
-    uniffricttype: int = Field(1, alias="unifFrictType")
-    uniffrictcoef1d: float = Field(0.023, alias="unifFrictCoef1D")
-    uniffrictcoeflin: float = Field(0.0, alias="unifFrictCoefLin")
-    vicouv: float = Field(0.1, alias="vicouv")
-    dicouv: float = Field(0.1, alias="dicouv")
-    vicoww: float = Field(5e-05, alias="vicoww")
-    dicoww: float = Field(5e-05, alias="dicoww")
-    vicwminb: float = Field(0.0, alias="vicwminb")
-    xlozmidov: float = Field(0.0, alias="xlozmidov")
-    smagorinsky: float = Field(0.2, alias="smagorinsky")
-    elder: float = Field(0.0, alias="elder")
-    irov: int = Field(0, alias="irov")
-    wall_ks: float = Field(0.0, alias="wall_ks")
-    rhomean: float = Field(1000, alias="rhomean")
-    idensform: int = Field(2, alias="idensform")
-    ag: float = Field(9.81, alias="ag")
-    tidalforcing: bool = Field(False, alias="tidalForcing")
-    itcap: Optional[float] = Field(None, alias="ITcap")
-    doodsonstart: float = Field(55.565, alias="doodsonStart")
-    doodsonstop: float = Field(375.575, alias="doodsonStop")
-    doodsoneps: float = Field(0.0, alias="doodsonEps")
-    villemontecd1: float = Field(1.0, alias="villemonteCD1")
-    villemontecd2: float = Field(10.0, alias="villemonteCD2")
-    salinity: bool = Field(False, alias="salinity")
-    initialsalinity: float = Field(0.0, alias="initialSalinity")
-    sal0abovezlev: float = Field(-999.0, alias="sal0AboveZLev")
-    deltasalinity: float = Field(-999.0, alias="deltaSalinity")
-    backgroundsalinity: float = Field(30.0, alias="backgroundSalinity")
-    temperature: int = Field(0, alias="temperature")
-    initialtemperature: float = Field(6.0, alias="initialTemperature")
-    backgroundwatertemperature: float = Field(6.0, alias="backgroundWaterTemperature")
-    secchidepth: float = Field(2.0, alias="secchiDepth")
-    stanton: float = Field(0.0013, alias="stanton")
-    dalton: float = Field(0.0013, alias="dalton")
-    tempmax: float = Field(-999.0, alias="tempMax")
-    tempmin: float = Field(0.0, alias="tempMin")
-    salimax: float = Field(-999.0, alias="saliMax")
-    salimin: float = Field(0.0, alias="saliMin")
-    heat_eachstep: bool = Field(False, alias="heat_eachStep")
-    rhoairrhowater: int = Field(0, alias="rhoAirRhoWater")
-    nudgetimeuni: float = Field(3600.0, alias="nudgeTimeUni")
-    iniwithnudge: int = Field(0, alias="iniWithNudge")
-    secondaryflow: bool = Field(False, alias="secondaryFlow")
-    betaspiral: float = Field(0.0, alias="betaSpiral")
-
-
-class Sediment(INIBasedModel):
-    class Comments(INIBasedModel.Comments):
-        sedimentmodelnr: Optional[str] = Field(
-            "Sediment model nr, (0=no, 1=Krone, 2=SvR2007, 3=E-H, 4=MorphologyModule).",
-            alias="Sedimentmodelnr",
-        )
-        morfile: Optional[str] = Field(
-            "Morphology settings file (*.mor)", alias="MorFile"
-        )
-        sedfile: Optional[str] = Field(
-            "Sediment characteristics file (*.sed)", alias="SedFile"
-        )
-
-    comments: Comments = Comments()
-
-    _disk_only_file_model_should_not_be_none = (
-        validator_set_default_disk_only_file_model_when_none()
-    )
-
-    _header: Literal["Sediment"] = "Sediment"
-    sedimentmodelnr: Optional[int] = Field(alias="Sedimentmodelnr")
-    morfile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="MorFile"
-    )
-    sedfile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="SedFile"
-    )
-
-
-class Wind(INIBasedModel):
-    """
-    The `[Wind]` section in an MDU file.
-
-    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.wind`.
-
-    All lowercased attributes match with the [Wind] input as described in
-    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
-    """
-
-    class Comments(INIBasedModel.Comments):
-        icdtyp: Optional[str] = Field(
-            "Wind drag coefficient type (1: Const, 2: Smith&Banke (2 pts), 3: S&B (3 pts), 4: Charnock 1955, 5: Hwang 2005, 6: Wuest 2005, 7: Hersbach 2010 (2 pts), 8: 4+viscous).",
-            alias="iCdTyp",
-        )
-        cdbreakpoints: Optional[str] = Field(
-            "Wind drag breakpoints, e.g. 0.00063 0.00723.", alias="CdBreakpoints"
-        )
-        windspeedbreakpoints: Optional[str] = Field(
-            "Wind speed breakpoints [m/s], e.g. 0.0 100.0.",
-            alias="windSpeedBreakpoints",
-        )
-        rhoair: Optional[str] = Field("Air density [kg/m3].", alias="rhoAir")
-        relativewind: Optional[str] = Field(
-            "Wind speed [kg/m3] relative to top-layer water speed*relativewind (0d0=no relative wind, 1d0=using full top layer speed).",
-            alias="relativeWind",
-        )
-        windpartialdry: Optional[str] = Field(
-            "Reduce windstress on water if link partially dry, only for bedlevtyp=3, 0=no, 1=yes (default).",
-            alias="windPartialDry",
-        )
-        pavbnd: Optional[str] = Field(
-            "Average air pressure on open boundaries [N/m2], only applied if value > 0.",
-            alias="pavBnd",
-        )
-        pavini: Optional[str] = Field(
-            "Initial air pressure [N/m2], only applied if value > 0.", alias="pavIni"
-        )
-
-    comments: Comments = Comments()
-
-    _header: Literal["Wind"] = "Wind"
-    icdtyp: int = Field(2, alias="iCdTyp")
-    cdbreakpoints: List[float] = Field([0.00063, 0.00723], alias="CdBreakpoints")
-    windspeedbreakpoints: List[float] = Field(
-        [0.0, 100.0], alias="windSpeedBreakpoints"
-    )
-    rhoair: float = Field(1.205, alias="rhoAir")
-    relativewind: float = Field(0.0, alias="relativeWind")
-    windpartialdry: bool = Field(True, alias="windPartialDry")
-    pavbnd: float = Field(0.0, alias="pavBnd")
-    pavini: float = Field(0.0, alias="pavIni")
-
-    @classmethod
-    def list_delimiter(cls) -> str:
-        return " "
-
-    _split_to_list = get_split_string_on_delimiter_validator(
-        "cdbreakpoints",
-        "windspeedbreakpoints",
-    )
-
-
-class Waves(INIBasedModel):
-    """
-    The `[Waves]` section in an MDU file.
-
-    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.waves`.
-
-    All lowercased attributes match with the [Waves] input as described in
-    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
-    """
-
-    class Comments(INIBasedModel.Comments):
-        wavemodelnr: Optional[str] = Field(
-            "Wave model nr. (0: none, 1: fetch/depth limited hurdlestive, 2: Young-Verhagen, 3: SWAN, 5: uniform, 6: SWAN-NetCDF",
-            alias="waveModelNr",
-        )
-        rouwav: Optional[str] = Field(
-            "Friction model for wave induced shear stress: FR84 (default) or: MS90, HT91, GM79, DS88, BK67, CJ85, OY88, VR04.",
-            alias="rouWav",
-        )
-        gammax: Optional[str] = Field(
-            "Maximum wave height/water depth ratio", alias="gammaX"
-        )
-
-    comments: Comments = Comments()
-
-    _header: Literal["Waves"] = "Waves"
-    wavemodelnr: int = Field(3, alias="waveModelNr")
-    rouwav: str = Field("FR84", alias="rouWav")
-    gammax: float = Field(0.5, alias="gammaX")
-
-
-class Time(INIBasedModel):
-    """
-    The `[Time]` section in an MDU file.
-
-    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.time`.
-
-    All lowercased attributes match with the [Time] input as described in
-    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
-    """
-
-    class Comments(INIBasedModel.Comments):
-        refdate: Optional[str] = Field("Reference date [yyyymmdd].", alias="refDate")
-        tzone: Optional[str] = Field(
-            "Data Sources in GMT are interrogated with time in minutes since refdat-Tzone*60 [min].",
-            alias="tZone",
-        )
-        tunit: Optional[str] = Field("Time units in MDU [D, H, M or S].", alias="tUnit")
-        dtuser: Optional[str] = Field(
-            "User timestep in seconds [s] (interval for external forcing update & his/map output).",
-            alias="dtUser",
-        )
-        dtnodal: Optional[str] = Field(
-            "Time interval [s] for updating nodal factors in astronomical boundary conditions.",
-            alias="dtNodal",
-        )
-        dtmax: Optional[str] = Field("Max timestep in seconds [s].", alias="dtMax")
-        dtinit: Optional[str] = Field(
-            "Initial timestep in seconds [s].", alias="dtInit"
-        )
-        autotimestep: Optional[str] = Field(
-            "0 = no, 1 = 2D (hor. out), 3=3D (hor. out), 5 = 3D (hor. inout + ver. inout), smallest dt",
-            alias="autoTimestep",
-        )
-        autotimestepnostruct: Optional[str] = Field(
-            "Exclude structure links (and neighbours) from time step limitation (0 = no, 1 = yes).",
-            alias="autoTimestepNoStruct",
-        )
-        autotimestepnoqout: Optional[str] = Field(
-            "Exclude negative qin terms from time step limitation (0 = no, 1 = yes).",
-            alias="autoTimestepNoQout",
-        )
-        tstart: Optional[str] = Field(
-            "Start time w.r.t. RefDate [TUnit].", alias="tStart"
-        )
-        tstop: Optional[str] = Field("Stop time w.r.t. RefDate [TUnit].", alias="tStop")
-        startdatetime: Optional[str] = Field(
-            "Computation Startdatetime (yyyymmddhhmmss), when specified, overrides tStart",
-            alias="startDateTime",
-        )
-        stopdatetime: Optional[str] = Field(
-            "Computation Stopdatetime  (yyyymmddhhmmss), when specified, overrides tStop",
-            alias="stopDateTime",
-        )
-        updateroughnessinterval: Optional[str] = Field(
-            "Update interval for time dependent roughness parameters [s].",
-            alias="updateRoughnessInterval",
-        )
-
-    comments: Comments = Comments()
-
-    _header: Literal["Time"] = "Time"
-    refdate: int = Field(20200101, alias="refDate")  # TODO Convert to datetime
-    tzone: float = Field(0.0, alias="tZone")
-    tunit: str = Field("S", alias="tUnit")  # DHMS
-    dtuser: float = Field(300.0, alias="dtUser")
-    dtnodal: float = Field(21600.0, alias="dtNodal")
-    dtmax: float = Field(30.0, alias="dtMax")
-    dtinit: float = Field(1.0, alias="dtInit")
-    autotimestep: Optional[int] = Field(None, alias="autoTimestep")
-    autotimestepnostruct: bool = Field(False, alias="autoTimestepNoStruct")
-    autotimestepnoqout: bool = Field(True, alias="autoTimestepNoQout")
-    tstart: float = Field(0.0, alias="tStart")
-    tstop: float = Field(86400.0, alias="tStop")
-    startdatetime: Optional[str] = Field(None, alias="startDateTime")
-    stopdatetime: Optional[str] = Field(None, alias="stopDateTime")
-    updateroughnessinterval: float = Field(86400.0, alias="updateRoughnessInterval")
-
-    @validator("startdatetime", "stopdatetime")
-    def _validate_datetime(cls, value, field):
-        return validate_datetime_string(value, field)
-
-
-class Restart(INIBasedModel):
-    """
-    The `[Restart]` section in an MDU file.
-
-    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.restart`.
-
-    All lowercased attributes match with the [Restart] input as described in
-    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
-    """
-
-    class Comments(INIBasedModel.Comments):
-        restartfile: Optional[str] = Field(
-            "Restart file, only from netCDF-file, hence: either *_rst.nc or *_map.nc.",
-            alias="restartFile",
-        )
-        restartdatetime: Optional[str] = Field(
-            "Restart time [YYYYMMDDHHMMSS], only relevant in case of restart from *_map.nc.",
-            alias="restartDateTime",
-        )
-
-    comments: Comments = Comments()
-
-    _disk_only_file_model_should_not_be_none = (
-        validator_set_default_disk_only_file_model_when_none()
-    )
-
-    _header: Literal["Restart"] = "Restart"
-    restartfile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="restartFile"
-    )
-    restartdatetime: Optional[str] = Field(None, alias="restartDateTime")
-
-    @validator("restartdatetime")
-    def _validate_datetime(cls, value, field):
-        return validate_datetime_string(value, field)
-
-
-class ExternalForcing(INIBasedModel):
-    """
-    The `[External Forcing]` section in an MDU file.
-
-    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.external_forcing`.
-
-    All lowercased attributes match with the [External Forcing] input as described in
-    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
-    """
-
-    class Comments(INIBasedModel.Comments):
-        extforcefile: Optional[str] = Field(
-            "Old format for external forcings file *.ext, link with tim/cmp-format boundary conditions specification.",
-            alias="extForceFile",
-        )
-        extforcefilenew: Optional[str] = Field(
-            "New format for external forcings file *.ext, link with bcformat boundary conditions specification.",
-            alias="extForceFileNew",
-        )
-        rainfall: Optional[str] = Field(
-            "Include rainfall, (0=no, 1=yes).", alias="rainfall"
-        )
-        qext: Optional[str] = Field(
-            "Include user Qin/out, externally provided, (0=no, 1=yes).", alias="qExt"
-        )
-        evaporation: Optional[str] = Field(
-            "Include evaporation in water balance, (0=no, 1=yes).", alias="evaporation"
-        )
-        windext: Optional[str] = Field(
-            "Include wind, externally provided, (0=no, 1=reserved for EC, 2=yes).",
-            alias="windExt",
-        )
-
-    comments: Comments = Comments()
-
-    _disk_only_file_model_should_not_be_none = (
-        validator_set_default_disk_only_file_model_when_none()
-    )
-
-    _header: Literal["External Forcing"] = "External Forcing"
-    extforcefile: Optional[ExtOldModel] = Field(None, alias="extForceFile")
-    extforcefilenew: Optional[ExtModel] = Field(None, alias="extForceFileNew")
-    rainfall: Optional[bool] = Field(None, alias="rainfall")
-    qext: Optional[bool] = Field(None, alias="qExt")
-    evaporation: Optional[bool] = Field(None, alias="evaporation")
-    windext: Optional[int] = Field(None, alias="windExt")
-
-    def is_intermediate_link(self) -> bool:
-        return True
-
-
-class Hydrology(INIBasedModel):
-    """
-    The `[Hydrology]` section in an MDU file.
-
-    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.hydrology`.
-
-    All lowercased attributes match with the [Hydrology] input as described in
-    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
-    """
-
-    class Comments(INIBasedModel.Comments):
-        interceptionmodel: Optional[str] = Field(
-            "Interception model (0: none, 1: on, via layer thickness).",
-            alias="interceptionModel",
-        )
-
-    comments: Comments = Comments()
-
-    _header: Literal["Hydrology"] = "Hydrology"
-    interceptionmodel: bool = Field(False, alias="interceptionModel")
-
-
-class Trachytopes(INIBasedModel):
-    """
-    The `[Trachytopes]` section in an MDU file.
-
-    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.trachytopes`.
-
-    All lowercased attributes match with the [Trachytopes] input as described in
-    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
-    """
-
-    class Comments(INIBasedModel.Comments):
-        trtrou: Optional[str] = Field(
-            "Flag for trachytopes (Y=on, N=off).", alias="trtRou"
-        )
-        trtdef: Optional[str] = Field(
-            "File (*.ttd) including trachytope definitions.", alias="trtDef"
-        )
-        trtl: Optional[str] = Field(
-            "File (*.arl) including distribution of trachytope definitions.",
-            alias="trtL",
-        )
-        dttrt: Optional[str] = Field(
-            "Interval for updating of bottom roughness due to trachytopes in seconds [s].",
-            alias="dtTrt",
-        )
-        trtmxr: Optional[str] = Field(
-            "Maximum recursion level for composite trachytope definitions",
-            alias="trtMxR",
-        )
-
-    comments: Comments = Comments()
-
-    _header: Literal["Trachytopes"] = "Trachytopes"
-    trtrou: str = Field("N", alias="trtRou")  # TODO bool
-    trtdef: Optional[Path] = Field(None, alias="trtDef")
-    trtl: Optional[Path] = Field(None, alias="trtL")
-    dttrt: float = Field(60.0, alias="dtTrt")
-    trtmxr: Optional[int] = Field(None, alias="trtMxR")
-
-
-ObsFile = Union[XYNModel, ObservationPointModel]
-ObsCrsFile = Union[PolyFile, ObservationCrossSectionModel]
-
-
-class Output(INIBasedModel):
-    """
-    The `[Output]` section in an MDU file.
-
-    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.output`.
-
-    All lowercased attributes match with the [Output] input as described in
-    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
-    """
-
-    class Comments(INIBasedModel.Comments):
-        wrishp_crs: Optional[str] = Field(
-            "Writing cross sections to shape file (0=no, 1=yes).", alias="wrishp_crs"
-        )
-        wrishp_weir: Optional[str] = Field(
-            "Writing weirs to shape file (0=no, 1=yes).", alias="wrishp_weir"
-        )
-        wrishp_gate: Optional[str] = Field(
-            "Writing gates to shape file (0=no, 1=yes).", alias="wrishp_gate"
-        )
-        wrishp_fxw: Optional[str] = Field(
-            "Writing fixed weirs to shape file (0=no, 1=yes).", alias="wrishp_fxw"
-        )
-        wrishp_thd: Optional[str] = Field(
-            "Writing thin dams to shape file (0=no, 1=yes).", alias="wrishp_thd"
-        )
-        wrishp_obs: Optional[str] = Field(
-            "Writing observation points to shape file (0=no, 1=yes).",
-            alias="wrishp_obs",
-        )
-        wrishp_emb: Optional[str] = Field(
-            "Writing embankments file (0=no, 1=yes).", alias="wrishp_emb"
-        )
-        wrishp_dryarea: Optional[str] = Field(
-            "Writing dry areas to shape file (0=no, 1=yes).", alias="wrishp_dryArea"
-        )
-        wrishp_enc: Optional[str] = Field(
-            "Writing enclosures to shape file (0=no, 1=yes).", alias="wrishp_enc"
-        )
-        wrishp_src: Optional[str] = Field(
-            "Writing sources and sinks to shape file (0=no, 1=yes).", alias="wrishp_src"
-        )
-        wrishp_pump: Optional[str] = Field(
-            "Writing pumps to shape file (0=no, 1=yes).", alias="wrishp_pump"
-        )
-        outputdir: Optional[str] = Field(
-            "Output directory of map-, his-, rst-, dat- and timingsfiles, default: DFM_OUTPUT_<modelname>. Set to . for no dir/current dir.",
-            alias="outputDir",
-        )
-        waqoutputdir: Optional[str] = Field(
-            "Output directory of Water Quality files.", alias="waqOutputDir"
-        )
-        flowgeomfile: Optional[str] = Field(
-            "*_flowgeom.nc Flow geometry file in netCDF format.", alias="flowGeomFile"
-        )
-        obsfile: Optional[str] = Field(
-            "Space separated list of files, containing information about observation points.",
-            alias="obsFile",
-        )
-        crsfile: Optional[str] = Field(
-            "Space separated list of files, containing information about observation cross sections.",
-            alias="crsFile",
-        )
-        foufile: Optional[str] = Field(
-            "Fourier analysis input file *.fou", alias="fouFile"
-        )
-        fouupdatestep: Optional[str] = Field(
-            "Fourier update step type: 0=every user time step, 1=every computational timestep, 2=same as history output.",
-            alias="fouUpdateStep",
-        )
-        hisfile: Optional[str] = Field(
-            "*_his.nc History file in netCDF format.", alias="hisFile"
-        )
-        hisinterval: Optional[str] = Field(
-            "History output, given as 'interval' 'start period' 'end period' [s].",
-            alias="hisInterval",
-        )
-        xlsinterval: Optional[str] = Field(
-            "Interval between XLS history [s].", alias="xlsInterval"
-        )
-        mapfile: Optional[str] = Field(
-            "*_map.nc Map file in netCDF format.", alias="mapFile"
-        )
-        mapinterval: Optional[str] = Field(
-            "Map file output, given as 'interval' 'start period' 'end period' [s].",
-            alias="mapInterval",
-        )
-        rstinterval: Optional[str] = Field(
-            "Restart file output, given as 'interval' 'start period' 'end period' [s].",
-            alias="rstInterval",
-        )
-        mapformat: Optional[str] = Field(
-            "Map file format, 1: netCDF, 2: Tecplot, 3: NetCFD and Tecplot, 4: netCDF UGRID.",
-            alias="mapFormat",
-        )
-        ncformat: Optional[str] = Field(
-            "Format for all NetCDF output files (3: classic, 4: NetCDF4+HDF5).",
-            alias="ncFormat",
-        )
-        ncnounlimited: Optional[str] = Field(
-            "Write full-length time-dimension instead of unlimited dimension (1: yes, 0: no). (Might require NcFormat=4.)",
-            alias="ncNoUnlimited",
-        )
-        ncnoforcedflush: Optional[str] = Field(
-            "Do not force flushing of map-like files every output timestep (1: yes, 0: no).",
-            alias="ncNoForcedFlush",
-        )
-        ncwritelatlon: Optional[str] = Field(
-            "Write extra lat-lon coordinates for all projected coordinate variables in each NetCDF file (for CF-compliancy) (1: yes, 0: no).",
-            alias="ncWriteLatLon",
-        )
-        wrihis_balance: Optional[str] = Field(
-            "Write mass balance totals to his file, (1: yes, 0: no).",
-            alias="wrihis_balance",
-        )
-        wrihis_sourcesink: Optional[str] = Field(
-            "Write sources-sinks statistics to his file, (1: yes, 0: no).",
-            alias="wrihis_sourceSink",
-        )
-        wrihis_structure_gen: Optional[str] = Field(
-            "Write general structure parameters to his file, (1: yes, 0: no).",
-            alias="wrihis_structure_gen",
-        )
-        wrihis_structure_dam: Optional[str] = Field(
-            "Write dam parameters to his file, (1: yes, 0: no).",
-            alias="wrihis_structure_dam",
-        )
-        wrihis_structure_pump: Optional[str] = Field(
-            "Write pump parameters to his file, (1: yes, 0: no).",
-            alias="wrihis_structure_pump",
-        )
-        wrihis_structure_gate: Optional[str] = Field(
-            "Write gate parameters to his file, (1: yes, 0: no).",
-            alias="wrihis_structure_gate",
-        )
-        wrihis_structure_weir: Optional[str] = Field(
-            "Write weir parameters to his file, (1: yes, 0: no).",
-            alias="wrihis_structure_weir",
-        )
-        wrihis_structure_orifice: Optional[str] = Field(
-            "Write orifice parameters to his file, (1: yes, 0: no).",
-            alias="wrihis_structure_orifice",
-        )
-        wrihis_structure_bridge: Optional[str] = Field(
-            "Write bridge parameters to his file, (1: yes, 0: no).",
-            alias="wrihis_structure_bridge",
-        )
-        wrihis_structure_culvert: Optional[str] = Field(
-            "Write culvert parameters to his file, (1: yes, 0: no).",
-            alias="wrihis_structure_culvert",
-        )
-        wrihis_structure_longculvert: Optional[str] = Field(
-            "Write long culvert parameters to his file, (1: yes, 0: no).",
-            alias="wrihis_structure_longCulvert",
-        )
-        wrihis_structure_dambreak: Optional[str] = Field(
-            "Write dam break parameters to his file, (1: yes, 0: no).",
-            alias="wrihis_structure_damBreak",
-        )
-        wrihis_structure_uniweir: Optional[str] = Field(
-            "Write universal weir parameters to his file, (1: yes, 0: no).",
-            alias="wrihis_structure_uniWeir",
-        )
-        wrihis_structure_compound: Optional[str] = Field(
-            "Write compound structure parameters to his file, (1: yes, 0: no).",
-            alias="wrihis_structure_compound",
-        )
-        wrihis_turbulence: Optional[str] = Field(
-            "Write k, eps and vicww to his file (1: yes, 0: no)'",
-            alias="wrihis_turbulence",
-        )
-        wrihis_wind: Optional[str] = Field(
-            "Write wind velocities to his file (1: yes, 0: no)'", alias="wrihis_wind"
-        )
-        wrihis_rain: Optional[str] = Field(
-            "Write precipitation to his file (1: yes, 0: no)'", alias="wrihis_rain"
-        )
-        wrihis_infiltration: Optional[str] = Field(
-            "Write infiltration to his file (1: yes, 0: no)'",
-            alias="wrihis_infiltration",
-        )
-        wrihis_temperature: Optional[str] = Field(
-            "Write temperature to his file (1: yes, 0: no)'", alias="wrihis_temperature"
-        )
-        wrihis_waves: Optional[str] = Field(
-            "Write wave data to his file (1: yes, 0: no)'", alias="wrihis_waves"
-        )
-        wrihis_heat_fluxes: Optional[str] = Field(
-            "Write heat fluxes to his file (1: yes, 0: no)'", alias="wrihis_heat_fluxes"
-        )
-        wrihis_salinity: Optional[str] = Field(
-            "Write salinity to his file (1: yes, 0: no)'", alias="wrihis_salinity"
-        )
-        wrihis_density: Optional[str] = Field(
-            "Write density to his file (1: yes, 0: no)'", alias="wrihis_density"
-        )
-        wrihis_waterlevel_s1: Optional[str] = Field(
-            "Write water level to his file (1: yes, 0: no)'",
-            alias="wrihis_waterlevel_s1",
-        )
-        wrihis_bedlevel: Optional[str] = Field(
-            "Write bed level to his file (1: yes, 0: no)'", alias="wrihis_bedlevel"
-        )
-        wrihis_waterdepth: Optional[str] = Field(
-            "Write water depth to his file (1: yes, 0: no)'", alias="wrihis_waterdepth"
-        )
-        wrihis_velocity_vector: Optional[str] = Field(
-            "Write velocity vectors to his file (1: yes, 0: no)'",
-            alias="wrihis_velocity_vector",
-        )
-        wrihis_upward_velocity_component: Optional[str] = Field(
-            "Write upward velocity to his file (1: yes, 0: no)'",
-            alias="wrihis_upward_velocity_component",
-        )
-        wrihis_velocity: Optional[str] = Field(
-            "Write velocity magnitude in observation point to his file, (1: yes, 0: no).",
-            alias="wrihis_velocity",
-        )
-        wrihis_discharge: Optional[str] = Field(
-            "Write discharge magnitude in observation point to his file, (1: yes, 0: no).",
-            alias="wrihis_discharge",
-        )
-        wrihis_sediment: Optional[str] = Field(
-            "Write sediment transport to his file (1: yes, 0: no)'",
-            alias="wrihis_sediment",
-        )
-        wrihis_constituents: Optional[str] = Field(
-            "Write tracers to his file (1: yes, 0: no)'", alias="wrihis_constituents"
-        )
-        wrihis_zcor: Optional[str] = Field(
-            "Write vertical coordinates to his file (1: yes, 0: no)'",
-            alias="wrihis_zcor",
-        )
-        wrihis_lateral: Optional[str] = Field(
-            "Write lateral data to his file, (1: yes, 0: no).", alias="wrihis_lateral"
-        )
-        wrihis_taucurrent: Optional[str] = Field(
-            "Write mean bed shear stress to his file (1: yes, 0: no)'",
-            alias="wrihis_taucurrent",
-        )
-        wrimap_waterlevel_s0: Optional[str] = Field(
-            "Write water levels at old time level to map file, (1: yes, 0: no).",
-            alias="wrimap_waterLevel_s0",
-        )
-        wrimap_waterlevel_s1: Optional[str] = Field(
-            "Write water levels at new time level to map file, (1: yes, 0: no).",
-            alias="wrimap_waterLevel_s1",
-        )
-        wrimap_evaporation: Optional[str] = Field(
-            "Write evaporation to map file, (1: yes, 0: no).",
-            alias="wrimap_evaporation",
-        )
-        wrimap_waterdepth: Optional[str] = Field(
-            "Write water depths to map file (1: yes, 0: no).",
-            alias="wrimap_waterdepth",
-        )
-        wrimap_velocity_component_u0: Optional[str] = Field(
-            "Write velocities at old time level to map file, (1: yes, 0: no).",
-            alias="wrimap_velocity_component_u0",
-        )
-        wrimap_velocity_component_u1: Optional[str] = Field(
-            "Write velocities at new time level to map file, (1: yes, 0: no).",
-            alias="wrimap_velocity_component_u1",
-        )
-        wrimap_velocity_vector: Optional[str] = Field(
-            "Write cell-center velocity vectors to map file, (1: yes, 0: no).",
-            alias="wrimap_velocity_vector",
-        )
-        wrimap_velocity_magnitude: Optional[str] = Field(
-            "Write cell-center velocity vector magnitude to map file (1: yes, 0: no).",
-            alias="wrimap_velocity_magnitude",
-        )
-        wrimap_upward_velocity_component: Optional[str] = Field(
-            "Write upward velocity component to map file, (1: yes, 0: no).",
-            alias="wrimap_upward_velocity_component",
-        )
-        wrimap_density_rho: Optional[str] = Field(
-            "Write density to map file, (1: yes, 0: no).", alias="wrimap_density_rho"
-        )
-        wrimap_horizontal_viscosity_viu: Optional[str] = Field(
-            "Write horizontal viscosity to map file, (1: yes, 0: no).",
-            alias="wrimap_horizontal_viscosity_viu",
-        )
-        wrimap_horizontal_diffusivity_diu: Optional[str] = Field(
-            "Write horizontal diffusivity to map file, (1: yes, 0: no).",
-            alias="wrimap_horizontal_diffusivity_diu",
-        )
-        wrimap_flow_flux_q1: Optional[str] = Field(
-            "Write fluxes to map file, (1: yes, 0: no).", alias="wrimap_flow_flux_q1"
-        )
-        wrimap_spiral_flow: Optional[str] = Field(
-            "Write spiral flow to map file, (1: yes, 0: no).",
-            alias="wrimap_spiral_flow",
-        )
-        wrimap_numlimdt: Optional[str] = Field(
-            "Write numlimdt to map file, (1: yes, 0: no).", alias="wrimap_numLimdt"
-        )
-        wrimap_taucurrent: Optional[str] = Field(
-            "Write bottom friction to map file, (1: yes, 0: no).",
-            alias="wrimap_tauCurrent",
-        )
-        wrimap_chezy: Optional[str] = Field(
-            "Write chezy values to map file, (1: yes, 0: no).", alias="wrimap_chezy"
-        )
-        wrimap_turbulence: Optional[str] = Field(
-            "Write turbulence to map file, (1: yes, 0: no).", alias="wrimap_turbulence"
-        )
-        wrimap_rain: Optional[str] = Field(
-            "Write rainfall rate to map file, (1: yes, 0: no).", alias="wrimap_rain"
-        )
-        wrimap_wind: Optional[str] = Field(
-            "Write winds to map file, (1: yes, 0: no).", alias="wrimap_wind"
-        )
-        writek_cdwind: Optional[str] = Field(
-            "Write wind friction coefficients to tek file (1: yes, 0: no).",
-            alias="writek_CdWind",
-        )
-        wrimap_heat_fluxes: Optional[str] = Field(
-            "Write heat fluxes to map file, (1: yes, 0: no).",
-            alias="wrimap_heat_fluxes",
-        )
-        wrimap_wet_waterdepth_threshold: Optional[str] = Field(
-            "Waterdepth threshold above which a grid point counts as 'wet'. Defaults to 0.2Â·Epshu. It is used for Wrimap_time_water_on_ground, Wrimap_waterdepth_on_ground and Wrimap_volume_on_ground.",
-            alias="wrimap_wet_waterDepth_threshold",
-        )
-        wrimap_time_water_on_ground: Optional[str] = Field(
-            "Write cumulative time when water is above ground level (only for 1D nodes) to map file, (1: yes, 0: no).",
-            alias="wrimap_time_water_on_ground",
-        )
-        wrimap_freeboard: Optional[str] = Field(
-            "Write freeboard (only for 1D nodes) to map file, (1: yes, 0: no).",
-            alias="wrimap_freeboard",
-        )
-        wrimap_waterdepth_on_ground: Optional[str] = Field(
-            "Write waterdepth that is above ground level to map file (only for 1D nodes) (1: yes, 0: no).",
-            alias="wrimap_waterDepth_on_ground",
-        )
-        wrimap_volume_on_ground: Optional[str] = Field(
-            "Write volume that is above ground level to map file (only for 1D nodes) (1: yes, 0: no).",
-            alias="wrimap_volume_on_ground",
-        )
-        wrimap_total_net_inflow_1d2d: Optional[str] = Field(
-            "Write current total 1D2D net inflow (discharge) and cumulative total 1D2D net inflow (volume) to map file (only for 1D nodes) (1:yes, 0:no).",
-            alias="wrimap_total_net_inflow_1d2d",
-        )
-        wrimap_total_net_inflow_lateral: Optional[str] = Field(
-            "Write current total lateral net inflow (discharge) and cumulative total lateral net inflow (volume) to map file (only for 1D nodes) (1:yes, 0:no).",
-            alias="wrimap_total_net_inflow_lateral",
-        )
-        wrimap_water_level_gradient: Optional[str] = Field(
-            "Write water level gradient to map file (only for 1D links) (1:yes, 0:no).",
-            alias="wrimap_water_level_gradient",
-        )
-        wrimap_tidal_potential: Optional[str] = Field(
-            "Write tidal potential to map file (1: yes, 0: no)",
-            alias="wrimap_tidal_potential",
-        )
-        wrimap_sal_potential: Optional[str] = Field(
-            "Write self attraction and loading potential to map file (1: yes, 0: no)",
-            alias="wrimap_SAL_potential",
-        )
-        wrimap_internal_tides_dissipation: Optional[str] = Field(
-            "Write internal tides dissipation to map file (1: yes, 0: no)",
-            alias="wrimap_internal_tides_dissipation",
-        )
-        wrimap_flow_analysis: Optional[str] = Field(
-            "Write flow analysis data to the map file (1:yes, 0:no).",
-            alias="wrimap_flow_analysis",
-        )
-        mapoutputtimevector: Optional[str] = Field(
-            "File (.mpt) containing fixed map output times (s) w.r.t. RefDate.",
-            alias="mapOutputTimeVector",
-        )
-        fullgridoutput: Optional[str] = Field(
-            "Full grid output mode for layer positions (0: compact, 1: full time-varying grid layer data).",
-            alias="fullGridOutput",
-        )
-        eulervelocities: Optional[str] = Field(
-            "Write Eulerian velocities, (1: yes, 0: no).", alias="eulerVelocities"
-        )
-        classmapfile: Optional[str] = Field(
-            "Name of class map file.", alias="classMapFile"
-        )
-        waterlevelclasses: Optional[str] = Field(
-            "Series of values between which water level classes are computed.",
-            alias="waterLevelClasses",
-        )
-        waterdepthclasses: Optional[str] = Field(
-            "Series of values between which water depth classes are computed.",
-            alias="waterDepthClasses",
-        )
-        classmapinterval: Optional[str] = Field(
-            "Interval [s] between class map file outputs.", alias="classMapInterval"
-        )
-        waqinterval: Optional[str] = Field(
-            "Interval [s] between DELWAQ file outputs.", alias="waqInterval"
-        )
-        statsinterval: Optional[str] = Field(
-            "Interval [s] between screen step outputs in seconds simulation time, if negative in seconds wall clock time.",
-            alias="statsInterval",
-        )
-        timingsinterval: Optional[str] = Field(
-            "Timings output interval TimingsInterval.", alias="timingsInterval"
-        )
-        richardsononoutput: Optional[str] = Field(
-            "Write Richardson number, (1: yes, 0: no).", alias="richardsonOnOutput"
-        )
-
-    comments: Comments = Comments()
-
-    _disk_only_file_model_should_not_be_none = (
-        validator_set_default_disk_only_file_model_when_none()
-    )
-
-    _header: Literal["Output"] = "Output"
-    wrishp_crs: bool = Field(False, alias="wrishp_crs")
-    wrishp_weir: bool = Field(False, alias="wrishp_weir")
-    wrishp_gate: bool = Field(False, alias="wrishp_gate")
-    wrishp_fxw: bool = Field(False, alias="wrishp_fxw")
-    wrishp_thd: bool = Field(False, alias="wrishp_thd")
-    wrishp_obs: bool = Field(False, alias="wrishp_obs")
-    wrishp_emb: bool = Field(False, alias="wrishp_emb")
-    wrishp_dryarea: bool = Field(False, alias="wrishp_dryArea")
-    wrishp_enc: bool = Field(False, alias="wrishp_enc")
-    wrishp_src: bool = Field(False, alias="wrishp_src")
-    wrishp_pump: bool = Field(False, alias="wrishp_pump")
-    outputdir: Optional[Path] = Field(None, alias="outputDir")
-    waqoutputdir: Optional[Path] = Field(None, alias="waqOutputDir")
-    flowgeomfile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="flowGeomFile"
-    )
-    obsfile: Optional[List[ObsFile]] = Field(None, alias="obsFile")
-    crsfile: Optional[List[ObsCrsFile]] = Field(None, alias="crsFile")
-    foufile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="fouFile"
-    )
-    fouupdatestep: int = Field(0, alias="fouUpdateStep")
-    hisfile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="hisFile"
-    )
-    hisinterval: List[float] = Field([300.0], alias="hisInterval")
-    xlsinterval: List[float] = Field([0.0], alias="xlsInterval")
-    mapfile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="mapFile"
-    )
-    mapinterval: List[float] = Field([1200.0], alias="mapInterval")
-    rstinterval: List[float] = Field([0.0], alias="rstInterval")
-    mapformat: int = Field(4, alias="mapFormat")
-    ncformat: int = Field(3, alias="ncFormat")
-    ncnounlimited: bool = Field(False, alias="ncNoUnlimited")
-    ncnoforcedflush: bool = Field(False, alias="ncNoForcedFlush")
-    ncwritelatlon: bool = Field(False, alias="ncWriteLatLon")
-
-    # his file
-    wrihis_balance: bool = Field(True, alias="wrihis_balance")
-    wrihis_sourcesink: bool = Field(True, alias="wrihis_sourceSink")
-    wrihis_structure_gen: bool = Field(True, alias="wrihis_structure_gen")
-    wrihis_structure_dam: bool = Field(True, alias="wrihis_structure_dam")
-    wrihis_structure_pump: bool = Field(True, alias="wrihis_structure_pump")
-    wrihis_structure_gate: bool = Field(True, alias="wrihis_structure_gate")
-    wrihis_structure_weir: bool = Field(True, alias="wrihis_structure_weir")
-    wrihis_structure_orifice: bool = Field(True, alias="wrihis_structure_orifice")
-    wrihis_structure_bridge: bool = Field(True, alias="wrihis_structure_bridge")
-    wrihis_structure_culvert: bool = Field(True, alias="wrihis_structure_culvert")
-    wrihis_structure_longculvert: bool = Field(
-        True, alias="wrihis_structure_longCulvert"
-    )
-    wrihis_structure_dambreak: bool = Field(True, alias="wrihis_structure_damBreak")
-    wrihis_structure_uniweir: bool = Field(True, alias="wrihis_structure_uniWeir")
-    wrihis_structure_compound: bool = Field(True, alias="wrihis_structure_compound")
-    wrihis_turbulence: bool = Field(True, alias="wrihis_turbulence")
-    wrihis_wind: bool = Field(True, alias="wrihis_wind")
-    wrihis_rain: bool = Field(True, alias="wrihis_rain")
-    wrihis_infiltration: bool = Field(True, alias="wrihis_infiltration")
-    wrihis_temperature: bool = Field(True, alias="wrihis_temperature")
-    wrihis_waves: bool = Field(True, alias="wrihis_waves")
-    wrihis_heat_fluxes: bool = Field(True, alias="wrihis_heat_fluxes")
-    wrihis_salinity: bool = Field(True, alias="wrihis_salinity")
-    wrihis_density: bool = Field(True, alias="wrihis_density")
-    wrihis_waterlevel_s1: bool = Field(True, alias="wrihis_waterlevel_s1")
-    wrihis_bedlevel: bool = Field(True, alias="wrihis_bedlevel")
-    wrihis_waterdepth: bool = Field(False, alias="wrihis_waterdepth")
-    wrihis_velocity_vector: bool = Field(True, alias="wrihis_velocity_vector")
-    wrihis_upward_velocity_component: bool = Field(
-        False, alias="wrihis_upward_velocity_component"
-    )
-    wrihis_velocity: bool = Field(False, alias="wrihis_velocity")
-    wrihis_discharge: bool = Field(False, alias="wrihis_discharge")
-    wrihis_sediment: bool = Field(True, alias="wrihis_sediment")
-    wrihis_constituents: bool = Field(True, alias="wrihis_constituents")
-    wrihis_zcor: bool = Field(True, alias="wrihis_zcor")
-    wrihis_lateral: bool = Field(True, alias="wrihis_lateral")
-    wrihis_taucurrent: bool = Field(True, alias="wrihis_taucurrent")
-
-    # Map file
-    wrimap_waterlevel_s0: bool = Field(True, alias="wrimap_waterLevel_s0")
-    wrimap_waterlevel_s1: bool = Field(True, alias="wrimap_waterLevel_s1")
-    wrimap_evaporation: bool = Field(False, alias="wrimap_evaporation")
-    wrimap_waterdepth: bool = Field(True, alias="wrimap_waterdepth")
-    wrimap_velocity_component_u0: bool = Field(
-        True, alias="wrimap_velocity_component_u0"
-    )
-    wrimap_velocity_component_u1: bool = Field(
-        True, alias="wrimap_velocity_component_u1"
-    )
-    wrimap_velocity_vector: bool = Field(True, alias="wrimap_velocity_vector")
-    wrimap_velocity_magnitude: bool = Field(True, alias="wrimap_velocity_magnitude")
-    wrimap_upward_velocity_component: bool = Field(
-        False, alias="wrimap_upward_velocity_component"
-    )
-    wrimap_density_rho: bool = Field(True, alias="wrimap_density_rho")
-    wrimap_horizontal_viscosity_viu: bool = Field(
-        True, alias="wrimap_horizontal_viscosity_viu"
-    )
-    wrimap_horizontal_diffusivity_diu: bool = Field(
-        True, alias="wrimap_horizontal_diffusivity_diu"
-    )
-    wrimap_flow_flux_q1: bool = Field(True, alias="wrimap_flow_flux_q1")
-    wrimap_spiral_flow: bool = Field(True, alias="wrimap_spiral_flow")
-    wrimap_numlimdt: bool = Field(True, alias="wrimap_numLimdt")
-    wrimap_taucurrent: bool = Field(True, alias="wrimap_tauCurrent")
-    wrimap_chezy: bool = Field(True, alias="wrimap_chezy")
-    wrimap_turbulence: bool = Field(True, alias="wrimap_turbulence")
-    wrimap_rain: bool = Field(False, alias="wrimap_rain")
-    wrimap_wind: bool = Field(True, alias="wrimap_wind")
-    writek_cdwind: bool = Field(False, alias="writek_CdWind")
-    wrimap_heat_fluxes: bool = Field(False, alias="wrimap_heat_fluxes")
-    wrimap_wet_waterdepth_threshold: float = Field(
-        2e-5, alias="wrimap_wet_waterDepth_threshold"
-    )
-    wrimap_time_water_on_ground: bool = Field(
-        False, alias="wrimap_time_water_on_ground"
-    )
-    wrimap_freeboard: bool = Field(False, alias="wrimap_freeboard")
-    wrimap_waterdepth_on_ground: bool = Field(
-        False, alias="wrimap_waterDepth_on_ground"
-    )
-    wrimap_volume_on_ground: bool = Field(False, alias="wrimap_volume_on_ground")
-    wrimap_total_net_inflow_1d2d: bool = Field(
-        False, alias="wrimap_total_net_inflow_1d2d"
-    )
-    wrimap_total_net_inflow_lateral: bool = Field(
-        False, alias="wrimap_total_net_inflow_lateral"
-    )
-    wrimap_water_level_gradient: bool = Field(
-        False, alias="wrimap_water_level_gradient"
-    )
-    wrimap_tidal_potential: bool = Field(True, alias="wrimap_tidal_potential")
-    wrimap_sal_potential: bool = Field(True, alias="wrimap_SAL_potential")
-    wrimap_internal_tides_dissipation: bool = Field(
-        True, alias="wrimap_internal_tides_dissipation"
-    )
-    wrimap_flow_analysis: bool = Field(False, alias="wrimap_flow_analysis")
-    mapoutputtimevector: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="mapOutputTimeVector"
-    )
-    fullgridoutput: bool = Field(False, alias="fullGridOutput")
-    eulervelocities: bool = Field(False, alias="eulerVelocities")
-    classmapfile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="classMapFile"
-    )
-    waterlevelclasses: List[float] = Field([0.0], alias="waterLevelClasses")
-    waterdepthclasses: List[float] = Field([0.0], alias="waterDepthClasses")
-    classmapinterval: List[float] = Field([0.0], alias="classMapInterval")
-    waqinterval: List[float] = Field([0.0], alias="waqInterval")
-    statsinterval: List[float] = Field([-60.0], alias="statsInterval")
-    timingsinterval: List[float] = Field([0.0], alias="timingsInterval")
-    richardsononoutput: bool = Field(False, alias="richardsonOnOutput")
-
-    _split_to_list = get_split_string_on_delimiter_validator(
-        "waterlevelclasses",
-        "waterdepthclasses",
-        "crsfile",
-        "obsfile",
-        "hisinterval",
-        "xlsinterval",
-        "mapinterval",
-        "rstinterval",
-        "classmapinterval",
-        "waqinterval",
-        "statsinterval",
-        "timingsinterval",
-    )
-
-    def is_intermediate_link(self) -> bool:
-        return True
-
-
-class Geometry(INIBasedModel):
-    """
-    The `[Geometry]` section in an MDU file.
-
-    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry`.
-
-    All lowercased attributes match with the [Geometry] input as described in
-    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
-    """
-
-    class Comments(INIBasedModel.Comments):
-        netfile: Optional[str] = Field("The net file <*_net.nc>", alias="netFile")
-        bathymetryfile: Optional[str] = Field(
-            "Removed since March 2022. See [geometry] keyword BedLevelFile.",
-            alias="bathymetryFile",
-        )
-        drypointsfile: Optional[str] = Field(
-            "Dry points file <*.xyz>, third column dummy z values, or polygon file <*.pol>.",
-            alias="dryPointsFile",
-        )
-        structurefile: Optional[str] = Field(
-            "File <*.ini> containing list of hydraulic structures.",
-            alias="structureFile",
-        )
-        inifieldfile: Optional[str] = Field(
-            "Initial and parameter field file <*.ini>.",
-            alias="iniFieldFile",
-        )
-        waterlevinifile: Optional[str] = Field(
-            "Initial water levels sample file <*.xyz>.", alias="waterLevIniFile"
-        )
-        landboundaryfile: Optional[str] = Field(
-            "Only for plotting.", alias="landBoundaryFile"
-        )
-        thindamfile: Optional[str] = Field(
-            "<*_thd.pli>, Polyline(s) for tracing thin dams.", alias="thinDamFile"
-        )
-        fixedweirfile: Optional[str] = Field(
-            "<*_fxw.pliz>, Polyline(s) x, y, z, z = fixed weir top levels (formerly fixed weir).",
-            alias="fixedWeirFile",
-        )
-        pillarfile: Optional[str] = Field(
-            "<*_pillar.pliz>, Polyline file containing four colums with x, y, diameter and Cd coefficient for bridge pillars.",
-            alias="pillarFile",
-        )
-        usecaching: Optional[str] = Field(
-            "Use caching for geometrical/network-related items (0: no, 1: yes) (section C.19).",
-            alias="useCaching",
-        )
-        vertplizfile: Optional[str] = Field(
-            "<*_vlay.pliz>), = pliz with x, y, Z, first Z = nr of layers, second Z = laytyp.",
-            alias="vertPlizFile",
-        )
-        frictfile: Optional[str] = Field(
-            "Location of the files with roughness data for 1D.",
-            alias="frictFile",
-        )
-        crossdeffile: Optional[str] = Field(
-            "Cross section definitions for all cross section shapes.",
-            alias="crossDefFile",
-        )
-        crosslocfile: Optional[str] = Field(
-            "Location definitions of the cross sections on a 1D network.",
-            alias="crossLocFile",
-        )
-        storagenodefile: Optional[str] = Field(
-            "File containing the specification of storage nodes and/or manholes to add extra storage to 1D models.",
-            alias="storageNodeFile",
-        )
-        oned2dlinkfile: Optional[str] = Field(
-            "File containing the custom parameterization of 1D-2D links.",
-            alias="1d2dLinkFile",
-        )
-        proflocfile: Optional[str] = Field(
-            "<*_proflocation.xyz>) x, y, z, z = profile refnumber.", alias="profLocFile"
-        )
-        profdeffile: Optional[str] = Field(
-            "<*_profdefinition.def>) definition for all profile nrs.",
-            alias="profDefFile",
-        )
-        profdefxyzfile: Optional[str] = Field(
-            "<*_profdefinition.def>) definition for all profile nrs.",
-            alias="profDefXyzFile",
-        )
-        manholefile: Optional[str] = Field(
-            "File containing manholes (e.g. <*.dat>).", alias="manholeFile"
-        )
-        partitionfile: Optional[str] = Field(
-            "<*_part.pol>, polyline(s) x, y.", alias="partitionFile"
-        )
-        uniformwidth1d: Optional[str] = Field(None, alias="uniformWidth1D")
-        dxwuimin2d: Optional[str] = Field(
-            "Smallest fraction dx/wu , set dx > Dxwuimin2D*wu",
-            alias="dxWuiMin2D",
-        )
-        waterlevini: Optional[str] = Field("Initial water level.", alias="waterLevIni")
-        bedlevuni: Optional[str] = Field(
-            "Uniform bed level [m], (only if bedlevtype>=3), used at missing z values in netfile.",
-            alias="bedLevUni",
-        )
-        bedslope: Optional[str] = Field(
-            "Bed slope inclination, sets zk = bedlevuni + x*bedslope ans sets zbndz = xbndz*bedslope.",
-            alias="bedSlope",
-        )
-        bedlevtype: Optional[str] = Field(
-            "1: at cell center (tiles xz,yz,bl,bob=max(bl)), 2: at face (tiles xu,yu,blu,bob=blu), 3: at face (using mean node values), 4: at face (using min node values), 5: at face (using max node values), 6: with bl based on node values.",
-            alias="bedLevType",
-        )
-        blmeanbelow: Optional[str] = Field(
-            "if not -999d0, below this level [m] the cell centre bedlevel is the mean of surrouding netnodes.",
-            alias="blMeanBelow",
-        )
-        blminabove: Optional[str] = Field(
-            "if not -999d0, above this level [m] the cell centre bedlevel is the min of surrouding netnodes.",
-            alias="blMinAbove",
-        )
-        anglat: Optional[str] = Field(
-            "Angle of latitude S-N [deg], 0=no Coriolis.", alias="angLat"
-        )
-        anglon: Optional[str] = Field(
-            "Angle of longitude E-W [deg], 0=Greenwich Mean Time.", alias="angLon"
-        )
-        conveyance2d: Optional[str] = Field(
-            "-1:R=HU, 0:R=H, 1:R=A/P, 2:K=analytic-1D conv, 3:K=analytic-2D conv.",
-            alias="conveyance2D",
-        )
-        nonlin1d: Optional[str] = Field(
-            "Non-linear 1D volumes, applicable for models with closed cross sections. 1=treat closed sections as partially open by using a Preissmann slot, 2=Nested Newton approach, 3=Partial Nested Newton approach.",
-            alias="nonlin1D",
-        )
-        nonlin2d: Optional[str] = Field(
-            "Non-linear 2D volumes, only i.c.m. ibedlevtype = 3 and Conveyance2D>=1.",
-            alias="nonlin2D",
-        )
-        sillheightmin: Optional[str] = Field(
-            "Fixed weir only active if both ground heights are larger than this value [m].",
-            alias="sillHeightMin",
-        )
-        makeorthocenters: Optional[str] = Field(
-            "(1: yes, 0: no) switch from circumcentres to orthocentres in geominit.",
-            alias="makeOrthoCenters",
-        )
-        dcenterinside: Optional[str] = Field(
-            "limit cell center; 1.0:in cell <-> 0.0:on c/g.", alias="dCenterInside"
-        )
-        bamin: Optional[str] = Field(
-            "Minimum grid cell area [m2], i.c.m. cutcells.", alias="baMin"
-        )
-        openboundarytolerance: Optional[str] = Field(
-            "Search tolerance factor between boundary polyline and grid cells. [Unit: in cell size units (i.e., not meters)].",
-            alias="openBoundaryTolerance",
-        )
-        renumberflownodes: Optional[str] = Field(
-            "Renumber the flow nodes (1: yes, 0: no).", alias="renumberFlowNodes"
-        )
-        kmx: Optional[str] = Field("Number of vertical layers.", alias="kmx")
-        layertype: Optional[str] = Field(
-            "Number of vertical layers.", alias="layerType"
-        )
-        numtopsig: Optional[str] = Field(
-            "Number of sigma-layers on top of z-layers.", alias="numTopSig"
-        )
-        numtopsiguniform: Optional[str] = Field(
-            "Spatially constant number of sigma layers above z-layers in a z-sigma model (1: yes, 0: no, spatially varying)",
-            alias="numTopSigUniform",
-        )
-        dztop: Optional[str] = Field(
-            "Z-layer thickness of layers above level Dztopuniabovez", alias="dzTop"
-        )
-        floorlevtoplay: Optional[str] = Field(
-            "Floor level of top layer", alias="floorLevTopLay"
-        )
-        dztopuniabovez: Optional[str] = Field(
-            "Above this level layers will have uniform dzTop, below we use sigmaGrowthFactor",
-            alias="dzTopUniAboveZ",
-        )
-        keepzlayeringatbed: Optional[str] = Field(
-            "0:possibly very thin layer at bed, 1:bedlayerthickness == zlayerthickness, 2=equal thickness first two layers",
-            alias="keepZLayeringAtBed",
-        )
-        sigmagrowthfactor: Optional[str] = Field(
-            "layer thickness growth factor from bed up.", alias="sigmaGrowthFactor"
-        )
-        dxdoubleat1dendnodes: Optional[str] = Field(
-            "Whether a 1D grid cell at the end of a network has to be extended with 0.5Î”x.",
-            alias="dxDoubleAt1DEndNodes",
-        )
-        changevelocityatstructures: Optional[str] = Field(
-            "Ignore structure dimensions for the velocity at hydraulic structures, when calculating the surrounding cell centered flow velocities.",
-            alias="changeVelocityAtStructures",
-        )
-        changestructuredimensions: Optional[str] = Field(
-            "Change the structure dimensions in case these are inconsistent with the channel dimensions.",
-            alias="changeStructureDimensions",
-        )
-
-    comments: Comments = Comments()
-
-    _disk_only_file_model_should_not_be_none = (
-        validator_set_default_disk_only_file_model_when_none()
-    )
-
-    _header: Literal["Geometry"] = "Geometry"
-    netfile: Optional[NetworkModel] = Field(
-        default_factory=NetworkModel, alias="netFile"
-    )
-    bathymetryfile: Optional[XYZModel] = Field(None, alias="bathymetryFile")
-    drypointsfile: Optional[List[Union[XYZModel, PolyFile]]] = Field(
-        None, alias="dryPointsFile"
-    )  # TODO Fix, this will always try XYZ first, alias="]")
-    structurefile: Optional[List[StructureModel]] = Field(
-        None, alias="structureFile", delimiter=";"
-    )
-    inifieldfile: Optional[IniFieldModel] = Field(None, alias="iniFieldFile")
-    waterlevinifile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="waterLevIniFile"
-    )
-    landboundaryfile: Optional[List[DiskOnlyFileModel]] = Field(
-        None, alias="landBoundaryFile"
-    )
-    thindamfile: Optional[List[PolyFile]] = Field(None, alias="thinDamFile")
-    fixedweirfile: Optional[List[PolyFile]] = Field(None, alias="fixedWeirFile")
-    pillarfile: Optional[List[PolyFile]] = Field(None, alias="pillarFile")
-    usecaching: bool = Field(False, alias="useCaching")
-    vertplizfile: Optional[PolyFile] = Field(None, alias="vertPlizFile")
-    frictfile: Optional[List[FrictionModel]] = Field(
-        None, alias="frictFile", delimiter=";"
-    )
-    crossdeffile: Optional[CrossDefModel] = Field(None, alias="crossDefFile")
-    crosslocfile: Optional[CrossLocModel] = Field(None, alias="crossLocFile")
-    storagenodefile: Optional[StorageNodeModel] = Field(None, alias="storageNodeFile")
-    oned2dlinkfile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="1d2dLinkFile"
-    )
-    proflocfile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="profLocFile"
-    )
-    profdeffile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="profDefFile"
-    )
-    profdefxyzfile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="profDefXyzFile"
-    )
-    manholefile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="manholeFile"
-    )
-    partitionfile: Optional[PolyFile] = Field(None, alias="partitionFile")
-    uniformwidth1d: float = Field(2.0, alias="uniformWidth1D")
-    dxwuimin2d: float = Field(0.0, alias="dxWuiMin2D")
-    waterlevini: float = Field(0.0, alias="waterLevIni")
-    bedlevuni: float = Field(-5.0, alias="bedLevUni")
-    bedslope: float = Field(0.0, alias="bedSlope")
-    bedlevtype: int = Field(3, alias="bedLevType")
-    blmeanbelow: float = Field(-999.0, alias="blMeanBelow")
-    blminabove: float = Field(-999.0, alias="blMinAbove")
-    anglat: float = Field(0.0, alias="angLat")
-    anglon: float = Field(0.0, alias="angLon")
-    conveyance2d: int = Field(-1, alias="conveyance2D")
-    nonlin1d: int = Field(1, alias="nonlin1D")
-    nonlin2d: int = Field(0, alias="nonlin2D")
-    sillheightmin: float = Field(0.0, alias="sillHeightMin")
-    makeorthocenters: bool = Field(False, alias="makeOrthoCenters")
-    dcenterinside: float = Field(1.0, alias="dCenterInside")
-    bamin: float = Field(1e-06, alias="baMin")
-    openboundarytolerance: float = Field(3.0, alias="openBoundaryTolerance")
-    renumberflownodes: bool = Field(True, alias="renumberFlowNodes")
-    kmx: int = Field(0, alias="kmx")
-    layertype: int = Field(1, alias="layerType")
-    numtopsig: int = Field(0, alias="numTopSig")
-    numtopsiguniform: bool = Field(True, alias="numTopSigUniform")
-    sigmagrowthfactor: float = Field(1.0, alias="sigmaGrowthFactor")
-    dztop: Optional[float] = Field(None, alias="dzTop")
-    floorlevtoplay: Optional[float] = Field(None, alias="floorLevTopLay")
-    dztopuniabovez: Optional[float] = Field(None, alias="dzTopUniAboveZ")
-    keepzlayeringatbed: int = Field(2, alias="keepZLayeringAtBed")
-    dxdoubleat1dendnodes: bool = Field(True, alias="dxDoubleAt1DEndNodes")
-    changevelocityatstructures: bool = Field(False, alias="changeVelocityAtStructures")
-    changestructuredimensions: bool = Field(True, alias="changeStructureDimensions")
-
-    _split_to_list = get_split_string_on_delimiter_validator(
-        "frictfile",
-        "structurefile",
-        "drypointsfile",
-        "landboundaryfile",
-        "thindamfile",
-        "fixedweirfile",
-        "pillarfile",
-    )
-
-    def is_intermediate_link(self) -> bool:
-        return True
-
-
-class Calibration(INIBasedModel):
-    """
-    The `[Calibration]` section in an MDU file.
-
-    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.calibration`.
-
-    All lowercased attributes match with the [Calibration] input as described in
-    [UM Sec.A.3](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.3).
-    """
-
-    class Comments(INIBasedModel.Comments):
-        usecalibration: Optional[str] = Field(
-            "Activate calibration factor friction multiplier (0: no, 1: yes).",
-            alias="UseCalibration",
-        )
-        definitionfile: Optional[str] = Field(
-            "File (*.cld) containing calibration definitions.",
-            alias="DefinitionFile",
-        )
-        areafile: Optional[str] = Field(
-            "File (*.cll) containing area distribution of calibration definitions.",
-            alias="AreaFile",
-        )
-
-    comments: Comments = Comments()
-
-    _disk_only_file_model_should_not_be_none = (
-        validator_set_default_disk_only_file_model_when_none()
-    )
-
-    _header: Literal["Calibration"] = "Calibration"
-    usecalibration: bool = Field(False, alias="UseCalibration")
-    definitionfile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="DefinitionFile"
-    )
-    areafile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="AreaFile"
-    )
-
-
-class InfiltrationMethod(IntEnum):
-    """
-    Enum class containing the valid values for the Infiltrationmodel
-    attribute in the [Groundwater][hydrolib.core.dflowfm.mdu.models.Groundwater] class.
-    """
-
-    NoInfiltration = 0
-    InterceptionLayer = 1
-    ConstantInfiltrationCapacity = 2
-    ModelUnsaturatedSaturated = 3
-    Horton = 4
-
-
-class GroundWater(INIBasedModel):
-    """
-    The `[Grw]` section in an MDU file.
-
-    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.grw`.
-
-    All lowercased attributes match with the [Grw] input as described in
-    [UM Sec.A.3](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.3).
-    """
-
-    class Comments(INIBasedModel.Comments):
-        groundwater: Optional[str] = Field(None, alias="GroundWater")
-        infiltrationmodel: Optional[str] = Field(
-            "Infiltration method (0: No infiltration, 1: Interception layer, 2: Constant infiltration capacity, 3: model unsaturated/saturated (with grw), 4: Horton).",
-            alias="Infiltrationmodel",
-        )
-        hinterceptionlayer: Optional[str] = Field(None, alias="Hinterceptionlayer")
-        unifinfiltrationcapacity: Optional[str] = Field(
-            "Uniform maximum infiltration capacity [m/s].",
-            alias="UnifInfiltrationCapacity",
-        )
-        conductivity: Optional[str] = Field(
-            "Non-dimensionless K conductivity   saturated (m/s), Q = K*A*i (m3/s)",
-            alias="Conductivity",
-        )
-        h_aquiferuni: Optional[str] = Field(
-            "bgrw = bl - h_aquiferuni (m), if negative, bgrw = bgrwuni.",
-            alias="h_aquiferuni",
-        )
-        bgrwuni: Optional[str] = Field(
-            "uniform level of impervious layer, only used if h_aquiferuni is negative.",
-            alias="bgrwuni",
-        )
-        h_unsatini: Optional[str] = Field(
-            "initial level groundwater is bedlevel - h_unsatini (m), if negative, sgrw = sgrwini.",
-            alias="h_unsatini",
-        )
-        sgrwini: Optional[str] = Field(
-            "Initial groundwater level, if h_unsatini < 0.", alias="sgrwini"
-        )
-
-    comments: Comments = Comments()
-    _header: Literal["Grw"] = "Grw"
-
-    groundwater: Optional[bool] = Field(False, alias="GroundWater")
-    infiltrationmodel: Optional[InfiltrationMethod] = Field(
-        InfiltrationMethod.NoInfiltration, alias="Infiltrationmodel"
-    )
-    hinterceptionlayer: Optional[float] = Field(None, alias="Hinterceptionlayer")
-    unifinfiltrationcapacity: Optional[float] = Field(
-        0.0, alias="UnifInfiltrationCapacity"
-    )
-    conductivity: Optional[float] = Field(0.0, alias="Conductivity")
-    h_aquiferuni: Optional[float] = Field(20.0, alias="h_aquiferuni")
-    bgrwuni: Optional[float] = Field(None, alias="bgrwuni")
-    h_unsatini: Optional[float] = Field(0.2, alias="h_unsatini")
-    sgrwini: Optional[float] = Field(None, alias="sgrwini")
-
-
-class ProcessFluxIntegration(IntEnum):
-    """
-    Enum class containing the valid values for the ProcessFluxIntegration
-    attribute in the [Processes][hydrolib.core.dflowfm.mdu.models.Processes] class.
-    """
-
-    WAQ = 1
-    DFlowFM = 2
-
-
-class Processes(INIBasedModel):
-    """
-    The `[Processes]` section in an MDU file.
-
-    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.processes`.
-
-    All lowercased attributes match with the [Processes] input as described in
-    [UM Sec.A.3](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.3).
-    """
-
-    class Comments(INIBasedModel.Comments):
-        substancefile: Optional[str] = Field(
-            "Substance file name.", alias="SubstanceFile"
-        )
-        additionalhistoryoutputfile: Optional[str] = Field(
-            "Extra history output filename.",
-            alias="AdditionalHistoryOutputFile",
-        )
-        statisticsfile: Optional[str] = Field(
-            "Statistics definition file.",
-            alias="StatisticsFile",
-        )
-        thetavertical: Optional[str] = Field(
-            "Theta value for vertical transport of water quality substances [-].",
-            alias="ThetaVertical",
-        )
-        dtprocesses: Optional[str] = Field(
-            "Waq processes time step [s]. Must be a multiple of DtUser. If DtProcesses is negative, water quality processes are calculated with every hydrodynamic time step.",
-            alias="DtProcesses",
-        )
-        dtmassbalance: Optional[str] = Field(None, alias="DtMassBalance")
-        processfluxintegration: Optional[str] = Field(
-            "Process fluxes integration option (1: WAQ, 2: D-Flow FM).",
-            alias="ProcessFluxIntegration",
-        )
-        wriwaqbot3doutput: Optional[str] = Field(
-            "Write 3D water quality bottom variables (0: no, 1: yes).",
-            alias="Wriwaqbot3Doutput",
-        )
-        volumedrythreshold: Optional[str] = Field(
-            "Volume [m3] below which segments are marked as dry.",
-            alias="VolumeDryThreshold",
-        )
-        depthdrythreshold: Optional[str] = Field(
-            "Water depth [m] below which segments are marked as dry.",
-            alias="DepthDryThreshold",
-        )
-
-    comments: Comments = Comments()
-
-    _disk_only_file_model_should_not_be_none = (
-        validator_set_default_disk_only_file_model_when_none()
-    )
-
-    _header: Literal["Processes"] = "Processes"
-
-    substancefile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="SubstanceFile"
-    )
-    additionalhistoryoutputfile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None),
-        alias="AdditionalHistoryOutputFile",
-    )
-    statisticsfile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="StatisticsFile"
-    )
-    thetavertical: Optional[float] = Field(0.0, alias="ThetaVertical")
-    dtprocesses: Optional[float] = Field(0.0, alias="DtProcesses")
-    dtmassbalance: Optional[float] = Field(0.0, alias="DtMassBalance")
-    processfluxintegration: Optional[ProcessFluxIntegration] = Field(
-        ProcessFluxIntegration.WAQ, alias="ProcessFluxIntegration"
-    )
-    wriwaqbot3doutput: Optional[bool] = Field(False, alias="Wriwaqbot3Doutput")
-    volumedrythreshold: Optional[float] = Field(1e-3, alias="VolumeDryThreshold")
-    depthdrythreshold: Optional[float] = Field(1e-3, alias="DepthDryThreshold")
-
-
-class ParticlesThreeDType(IntEnum):
-    """
-    Enum class containing the valid values for the 3Dtype
-    attribute in the `Particles` class.
-    """
-
-    DepthAveraged = 0
-    FreeSurface = 1
-
-
-class Particles(INIBasedModel):
-    """
-    The `[Particles]` section in an MDU file.
-
-    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.particles`.
-
-    All lowercased attributes match with the [Particles] input as described in
-    [UM Sec.A.3](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.3).
-    """
-
-    class Comments(INIBasedModel.Comments):
-        particlesfile: Optional[str] = Field(
-            "Initial particle locations file (*.xyz).", alias="ParticlesFile"
-        )
-        particlesreleasefile: Optional[str] = Field(
-            "Particles release file (*.tim, 4 column).", alias="ParticlesReleaseFile"
-        )
-        addtracer: Optional[str] = Field(
-            "Add tracer or not (0: no, 1: yes).", alias="AddTracer"
-        )
-        starttime: Optional[str] = Field("Start time (if > 0) [s]", alias="StartTime")
-        timestep: Optional[str] = Field(
-            "Time step (if > 0) or every computational time step [s].", alias="TimeStep"
-        )
-        threedtype: Optional[str] = Field(
-            "3D velocity type (0: depth averaged velocities, 1: free surface/top layer velocities).",
-            alias="3Dtype",
-        )
-
-    comments: Comments = Comments()
-    _disk_only_file_model_should_not_be_none = (
-        validator_set_default_disk_only_file_model_when_none()
-    )
-
-    _header: Literal["Particles"] = "Particles"
-
-    particlesfile: Optional[XYZModel] = Field(None, alias="ParticlesFile")
-    particlesreleasefile: DiskOnlyFileModel = Field(
-        default_factory=lambda: DiskOnlyFileModel(None), alias="ParticlesReleaseFile"
-    )
-    addtracer: Optional[bool] = Field(False, alias="AddTracer")
-    starttime: Optional[float] = Field(0.0, alias="StartTime")
-    timestep: Optional[float] = Field(0.0, alias="TimeStep")
-    threedtype: Optional[ParticlesThreeDType] = Field(
-        ParticlesThreeDType.DepthAveraged, alias="3Dtype"
-    )
-
-
-class VegetationModelNr(IntEnum):
-    """
-    Enum class containing the valid values for the VegetationModelNr
-    attribute in the [Vegetation][hydrolib.core.dflowfm.mdu.models.Vegetation] class.
-    """
-
-    No = 0
-    BaptistDFM = 1
-
-
-class Vegetation(INIBasedModel):
-    """
-    The `[Veg]` section in an MDU file.
-
-    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.veg`.
-
-    All lowercased attributes match with the [Veg] input as described in
-    [UM Sec.A.3](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.3).
-    """
-
-    class Comments(INIBasedModel.Comments):
-        vegetationmodelnr: Optional[str] = Field(
-            "Vegetation model nr, (0: no, 1: Baptist DFM).", alias="Vegetationmodelnr"
-        )
-        clveg: Optional[str] = Field("Stem distance factor [-].", alias="Clveg")
-        cdveg: Optional[str] = Field("Stem Cd coefficient [-].", alias="Cdveg")
-        cbveg: Optional[str] = Field("Stem stiffness coefficient [-].", alias="Cbveg")
-        rhoveg: Optional[str] = Field(
-            "Stem Rho, if > 0, bouyant stick procedure [kg/m3].", alias="Rhoveg"
-        )
-        stemheightstd: Optional[str] = Field(
-            "Stem height standard deviation fraction, e.g. 0.1 [-].",
-            alias="Stemheightstd",
-        )
-        densvegminbap: Optional[str] = Field(
-            "Minimum vegetation density in Baptist formula. Only in 2D. [1/m2].",
-            alias="Densvegminbap",
-        )
-
-    comments: Comments = Comments()
-    _header: Literal["Veg"] = "Veg"
-
-    vegetationmodelnr: Optional[VegetationModelNr] = Field(
-        VegetationModelNr.No, alias="Vegetationmodelnr"
-    )
-    clveg: Optional[float] = Field(0.8, alias="Clveg")
-    cdveg: Optional[float] = Field(0.7, alias="Cdveg")
-    cbveg: Optional[float] = Field(0.0, alias="Cbveg")
-    rhoveg: Optional[float] = Field(0.0, alias="Rhoveg")
-    stemheightstd: Optional[float] = Field(0.0, alias="Stemheightstd")
-    densvegminbap: Optional[float] = Field(0.0, alias="Densvegminbap")
-
-
-class FMModel(INIModel):
-    """
-    The overall FM model that contains the contents of the toplevel MDU file.
-
-    All lowercased attributes match with the supported "[section]"s as described in
-    [UM Sec.A](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#appendix.A).
-
-    Each of these class attributes refers to an underlying model class for that particular section.
-    """
-
-    general: General = Field(default_factory=General)
-    geometry: Geometry = Field(default_factory=Geometry)
-    volumetables: VolumeTables = Field(default_factory=VolumeTables)
-    numerics: Numerics = Field(default_factory=Numerics)
-    physics: Physics = Field(default_factory=Physics)
-    sediment: Sediment = Field(default_factory=Sediment)
-    wind: Wind = Field(default_factory=Wind)
-    waves: Optional[Waves] = None
-    time: Time = Field(default_factory=Time)
-    restart: Restart = Field(default_factory=Restart)
-    external_forcing: ExternalForcing = Field(default_factory=ExternalForcing)
-    hydrology: Hydrology = Field(default_factory=Hydrology)
-    trachytopes: Trachytopes = Field(default_factory=Trachytopes)
-    output: Output = Field(default_factory=Output)
-    calibration: Optional[Calibration] = Field(None)
-    grw: Optional[GroundWater] = Field(None)
-    processes: Optional[Processes] = Field(None)
-    particles: Optional[Particles] = Field(None)
-    veg: Optional[Vegetation] = Field(None)
-
-    serializer_config: INISerializerConfig = INISerializerConfig(
-        skip_empty_properties=False
-    )
-
-    @classmethod
-    def _ext(cls) -> str:
-        return ".mdu"
-
-    @classmethod
-    def _filename(cls) -> str:
-        return "fm"
-
-    @FileModel._relative_mode.getter
-    def _relative_mode(self) -> ResolveRelativeMode:
-        # This method overrides the _relative_mode property of the FileModel:
-        # The FMModel has a "special" feature which determines how relative filepaths
-        # should be resolved. When the field "pathsRelativeToParent" is set to False
-        # all relative paths should be resolved in respect to the parent directory of
-        # the mdu file. As such we need to explicitly set the resolve mode to ToAnchor
-        # when this attribute is set.
-
-        if not hasattr(self, "general") or self.general is None:
-            return ResolveRelativeMode.ToParent
-
-        if self.general.pathsrelativetoparent:
-            return ResolveRelativeMode.ToParent
-        else:
-            return ResolveRelativeMode.ToAnchor
-
-    @classmethod
-    def _get_relative_mode_from_data(cls, data: Dict[str, Any]) -> ResolveRelativeMode:
-        """Gets the ResolveRelativeMode of this FileModel based on the provided data.
-
-        The ResolveRelativeMode of the FMModel is determined by the
-        'pathsRelativeToParent' property of the 'General' category.
-
-        Args:
-            data (Dict[str, Any]):
-                The unvalidated/parsed data which is fed to the pydantic base model,
-                used to determine the ResolveRelativeMode.
-
-        Returns:
-            ResolveRelativeMode: The ResolveRelativeMode of this FileModel
-        """
-        if not (general := data.get("general", None)):
-            return ResolveRelativeMode.ToParent
-        if not (relative_to_parent := general.get("pathsrelativetoparent", None)):
-            return ResolveRelativeMode.ToParent
-
-        if relative_to_parent == "0":
-            return ResolveRelativeMode.ToAnchor
-        else:
-            return ResolveRelativeMode.ToParent
+from enum import IntEnum
+from pathlib import Path
+from typing import Any, Dict, List, Literal, Optional, Union
+
+from pydantic import Field, validator
+
+from hydrolib.core.basemodel import (
+    DiskOnlyFileModel,
+    FileModel,
+    ResolveRelativeMode,
+    validator_set_default_disk_only_file_model_when_none,
+)
+from hydrolib.core.dflowfm.crosssection.models import CrossDefModel, CrossLocModel
+from hydrolib.core.dflowfm.ext.models import ExtModel
+from hydrolib.core.dflowfm.extold.models import ExtOldModel
+from hydrolib.core.dflowfm.friction.models import FrictionModel
+from hydrolib.core.dflowfm.ini.models import INIBasedModel, INIGeneral, INIModel
+from hydrolib.core.dflowfm.ini.serializer import INISerializerConfig
+from hydrolib.core.dflowfm.ini.util import (
+    get_split_string_on_delimiter_validator,
+    validate_datetime_string,
+)
+from hydrolib.core.dflowfm.inifield.models import IniFieldModel
+from hydrolib.core.dflowfm.net.models import NetworkModel
+from hydrolib.core.dflowfm.obs.models import ObservationPointModel
+from hydrolib.core.dflowfm.obscrosssection.models import ObservationCrossSectionModel
+from hydrolib.core.dflowfm.polyfile.models import PolyFile
+from hydrolib.core.dflowfm.storagenode.models import StorageNodeModel
+from hydrolib.core.dflowfm.structure.models import StructureModel
+from hydrolib.core.dflowfm.xyn.models import XYNModel
+from hydrolib.core.dflowfm.xyz.models import XYZModel
+
+
+class AutoStartOption(IntEnum):
+    """
+    Enum class containing the valid values for the AutoStart
+    attribute in the [General][hydrolib.core.dflowfm.mdu.models.General] class.
+    """
+
+    no = 0
+    autostart = 1
+    autostartstop = 2
+
+
+class General(INIGeneral):
+    class Comments(INIBasedModel.Comments):
+        program: Optional[str] = Field("Program.", alias="program")
+        version: Optional[str] = Field(
+            "Version number of computational kernel", alias="version"
+        )
+        filetype: Optional[str] = Field("File type. Do not edit this", alias="fileType")
+        fileversion: Optional[str] = Field(
+            "File version. Do not edit this.", alias="fileVersion"
+        )
+        autostart: Optional[str] = Field(
+            "Autostart simulation after loading MDU or not (0=no, 1=autostart, 2=autostartstop).",
+            alias="autoStart",
+        )
+        pathsrelativetoparent: Optional[str] = Field(
+            "Whether or not (1/0) to resolve file names (e.g. inside the *.ext file) relative to their direct parent, instead of to the toplevel MDU working dir",
+            alias="pathsRelativeToParent",
+        )
+
+    comments: Comments = Comments()
+    _header: Literal["General"] = "General"
+    program: str = Field("D-Flow FM", alias="program")
+    version: str = Field("1.2.94.66079M", alias="version")
+    filetype: Literal["modelDef"] = Field("modelDef", alias="fileType")
+    fileversion: str = Field("1.09", alias="fileVersion")
+    autostart: Optional[AutoStartOption] = Field(AutoStartOption.no, alias="autoStart")
+    pathsrelativetoparent: bool = Field(False, alias="pathsRelativeToParent")
+
+
+class Numerics(INIBasedModel):
+    """
+    The `[Numerics]` section in an MDU file.
+
+    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.numerics`.
+
+    All lowercased attributes match with the [Numerics] input as described in
+    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
+    """
+
+    class Comments(INIBasedModel.Comments):
+        cflmax: Optional[str] = Field("Maximum Courant nr.", alias="CFLMax")
+        epsmaxlev: Optional[str] = Field(
+            "Stop criterium for non linear iteration", alias="EpsMaxlev"
+        )
+        epsmaxlevm: Optional[str] = Field(
+            "Stop criterium for Nested Newton loop in non linear iteration",
+            alias="EpsMaxlevM",
+        )
+        advectype: Optional[str] = Field(
+            "Adv type, 0=no, 33=Perot q(uio-u) fast, 3=Perot q(uio-u).",
+            alias="advecType",
+        )
+        timesteptype: Optional[str] = Field(
+            "0=only transport, 1=transport + velocity update, 2=full implicit step_reduce, 3=step_jacobi, 4=explicit.",
+            alias="timeStepType",
+        )
+        limtyphu: Optional[str] = Field(
+            "Limiter type for waterdepth in continuity eq., 0=no, 1=minmod,2=vanLeer,3=Koren,4=Monotone Central.",
+            alias="limTypHu",
+        )
+        limtypmom: Optional[str] = Field(
+            "Limiter type for cell center advection velocity, 0=no, 1=minmod,2=vanLeer,4=Monotone Central.",
+            alias="limTypMom",
+        )
+        limtypsa: Optional[str] = Field(
+            "Limiter type for salinity transport,           0=no, 1=minmod,2=vanLeer,4=Monotone Central.",
+            alias="limTypSa",
+        )
+        icgsolver: Optional[str] = Field(
+            "Solver type, 4 = sobekGS + Saad-ILUD (default sequential), 6 = PETSc (default parallel), 7= CG+MILU (parallel).",
+            alias="icgSolver",
+        )
+        maxdegree: Optional[str] = Field(
+            "Maximum degree in Gauss elimination.", alias="maxDegree"
+        )
+        fixedweirscheme: Optional[str] = Field(
+            "6 = semi-subgrid scheme, 8 = Tabellenboek, 9 = Villemonte (default).",
+            alias="fixedWeirScheme",
+        )
+        fixedweircontraction: Optional[str] = Field(
+            "flow width = flow width*fixedWeirContraction.",
+            alias="fixedWeirContraction",
+        )
+        izbndpos: Optional[str] = Field(
+            "Position of z boundary, 0=mirroring of closest cell (as in Delft3D-FLOW), 1=on net boundary.",
+            alias="izBndPos",
+        )
+        tlfsmo: Optional[str] = Field(
+            "Fourier smoothing time on water level boundaries [s].", alias="tlfSmo"
+        )
+        keepstbndonoutflow: Optional[str] = Field(
+            "Keep salinity and temperature signals on boundary also at outflow, 1=yes, 0=no. Default=0: copy inside value on outflow.",
+            alias="keepSTBndOnOutflow",
+        )
+        slopedrop2d: Optional[str] = Field(
+            "Apply droplosses only if local bottom slope > Slopedrop2D, <=0 =no droplosses.",
+            alias="slopeDrop2D",
+        )
+        drop1d: Optional[str] = Field(
+            "Limit the downstream water level in the momentum equation to the downstream invert level, BOBdown (Î¶*down = max(BOBdown, Î¶down)).",
+            alias="drop1D",
+        )
+        chkadvd: Optional[str] = Field(
+            "Check advection terms if depth < chkadvdp.", alias="chkAdvd"
+        )
+        teta0: Optional[str] = Field(
+            "Theta (implicitness) of time integration, 0.5 < Theta < 1.0.",
+            alias="teta0",
+        )
+        qhrelax: Optional[str] = Field(None, alias="qhRelax")
+        cstbnd: Optional[str] = Field(
+            "Delft3D-FLOW type velocity treatment near boundaries for small coastal models (1) or not (0).",
+            alias="cstBnd",
+        )
+        maxitverticalforestersal: Optional[str] = Field(
+            "Forester iterations for salinity (0: no vertical filter for salinity, > 0: max nr of iterations).",
+            alias="maxitVerticalForesterSal",
+        )
+        maxitverticalforestertem: Optional[str] = Field(
+            "Forester iterations for temperature (0: no vertical filter for temperature, > 0: max nr of iterations).",
+            alias="maxitVerticalForesterTem",
+        )
+        turbulencemodel: Optional[str] = Field(
+            "0=no, 1 = constant, 2 = algebraic, 3 = k-epsilon, 4 = k-tau.",
+            alias="turbulenceModel",
+        )
+        turbulenceadvection: Optional[str] = Field(
+            "Turbulence advection (0=no, 3 = horizontal explicit vertical implicit).",
+            alias="turbulenceAdvection",
+        )
+        anticreep: Optional[str] = Field(
+            "Include anti-creep calculation (0: no, 1: yes).", alias="antiCreep"
+        )
+        baroczlaybed: Optional[str] = Field(
+            "Use fix in baroclinic pressure for zlaybed (1: yes, 0: no)",
+            alias="barocZLayBed",
+        )
+        barocponbnd: Optional[str] = Field(
+            "Use baroclinic pressure correction on open boundaries (1: yes, 0: no)",
+            alias="barocPOnBnd",
+        )
+        maxwaterleveldiff: Optional[str] = Field(
+            "Upper bound [m] on water level changes, (<= 0: no bounds). Run will abort when violated.",
+            alias="maxWaterLevelDiff",
+        )
+        maxvelocitydiff: Optional[str] = Field(
+            "Upper bound [m/s] on velocity changes, (<= 0: no bounds). Run will abort when violated.",
+            alias="maxVelocityDiff",
+        )
+        mintimestepbreak: Optional[str] = Field(
+            "Smallest allowed timestep (in s), checked on a sliding average of several timesteps. Run will abort when violated.",
+            alias="minTimestepBreak",
+        )
+        epshu: Optional[str] = Field(
+            "Threshold water depth for wetting and drying [m].", alias="epsHu"
+        )
+
+    comments: Comments = Comments()
+
+    _header: Literal["Numerics"] = "Numerics"
+    cflmax: float = Field(0.7, alias="CFLMax")
+    epsmaxlev: float = Field(1e-8, alias="EpsMaxlev")
+    epsmaxlevm: float = Field(1e-8, alias="EpsMaxlevM")
+    advectype: int = Field(33, alias="advecType")
+    timesteptype: int = Field(2, alias="timeStepType")
+    limtyphu: int = Field(0, alias="limTypHu")
+    limtypmom: int = Field(4, alias="limTypMom")
+    limtypsa: int = Field(4, alias="limTypSa")
+    icgsolver: int = Field(4, alias="icgSolver")
+    maxdegree: int = Field(6, alias="maxDegree")
+    fixedweirscheme: int = Field(9, alias="fixedWeirScheme")
+    fixedweircontraction: float = Field(1.0, alias="fixedWeirContraction")
+    izbndpos: int = Field(0, alias="izBndPos")
+    tlfsmo: float = Field(0.0, alias="tlfSmo")
+    keepstbndonoutflow: bool = Field(False, alias="keepSTBndOnOutflow")
+    slopedrop2d: float = Field(0.0, alias="slopeDrop2D")
+    drop1d: bool = Field(False, alias="drop1D")
+    chkadvd: float = Field(0.1, alias="chkAdvd")
+    teta0: float = Field(0.55, alias="teta0")
+    qhrelax: float = Field(0.01, alias="qhRelax")
+    cstbnd: bool = Field(False, alias="cstBnd")
+    maxitverticalforestersal: int = Field(0, alias="maxitVerticalForesterSal")
+    maxitverticalforestertem: int = Field(0, alias="maxitVerticalForesterTem")
+    turbulencemodel: int = Field(3, alias="turbulenceModel")
+    turbulenceadvection: int = Field(3, alias="turbulenceAdvection")
+    anticreep: bool = Field(False, alias="antiCreep")
+    baroczlaybed: bool = Field(False, alias="barocZLayBed")
+    barocponbnd: bool = Field(False, alias="barocPOnBnd")
+    maxwaterleveldiff: float = Field(0.0, alias="maxWaterLevelDiff")
+    maxvelocitydiff: float = Field(0.0, alias="maxVelocityDiff")
+    mintimestepbreak: float = Field(0.0, alias="minTimestepBreak")
+    epshu: float = Field(0.0001, alias="epsHu")
+
+
+class VolumeTables(INIBasedModel):
+    """
+    The `[VolumeTables]` section in an MDU file.
+
+    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.volumetables`.
+
+    All lowercased attributes match with the [VolumeTables] input as described in
+    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
+    """
+
+    class Comments(INIBasedModel.Comments):
+        usevolumetables: Optional[str] = Field(
+            "Use volume tables for 1D grid cells (1: yes, 0 = no).",
+            alias="useVolumeTables",
+        )
+        increment: Optional[str] = Field(
+            "The height increment for the volume tables [m].", alias="increment"
+        )
+        usevolumetablefile: Optional[str] = Field(
+            "Read and write the volume table from/to file (1: yes, 0= no).",
+            alias="useVolumeTableFile",
+        )
+
+    comments: Comments = Comments()
+
+    _header: Literal["VolumeTables"] = "VolumeTables"
+    usevolumetables: bool = Field(False, alias="useVolumeTables")
+    increment: float = Field(0.2, alias="increment")
+    usevolumetablefile: bool = Field(False, alias="useVolumeTableFile")
+
+
+class Physics(INIBasedModel):
+    """
+    The `[Physics]` section in an MDU file.
+
+    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.physics`.
+
+    All lowercased attributes match with the [Physics] input as described in
+    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
+    """
+
+    class Comments(INIBasedModel.Comments):
+        uniffrictcoef: Optional[str] = Field(
+            "Uniform friction coefficient (0: no friction).", alias="unifFrictCoef"
+        )
+        uniffricttype: Optional[str] = Field(
+            "Uniform friction type (0: Chezy, 1: Manning, 2: White-Colebrook, 3: idem, WAQUA style).",
+            alias="unifFrictType",
+        )
+        uniffrictcoef1d: Optional[str] = Field(
+            "Uniform friction coefficient in 1D links (0: no friction).",
+            alias="unifFrictCoef1D",
+        )
+        uniffrictcoeflin: Optional[str] = Field(
+            "Uniform linear friction coefficient (0: no friction).",
+            alias="unifFrictCoefLin",
+        )
+        vicouv: Optional[str] = Field(
+            "Uniform horizontal eddy viscosity [m2/s].", alias="vicouv"
+        )
+        dicouv: Optional[str] = Field(
+            "Uniform horizontal eddy diffusivity [m2/s].", alias="dicouv"
+        )
+        vicoww: Optional[str] = Field(
+            "Background vertical eddy viscosity [m2/s].", alias="vicoww"
+        )
+        dicoww: Optional[str] = Field(
+            "Background vertical eddy diffusivity [m2/s].", alias="dicoww"
+        )
+        vicwminb: Optional[str] = Field(
+            "Minimum viscosity in production and buoyancy term [m2/s].",
+            alias="vicwminb",
+        )
+        xlozmidov: Optional[str] = Field(
+            "Ozmidov length scale [m], default=0.0, no contribution of internal waves to vertical diffusion.",
+            alias="xlozmidov",
+        )
+        smagorinsky: Optional[str] = Field(
+            "Add Smagorinsky horizontal turbulence: vicu = vicu + ( (Smagorinsky*dx)**2)*S.",
+            alias="smagorinsky",
+        )
+        elder: Optional[str] = Field(
+            "Add Elder contribution: vicu = vicu + Elder*kappa*ustar*H/6); e.g. 1.0.",
+            alias="elder",
+        )
+        irov: Optional[str] = Field(
+            "Wall friction, 0=free slip, 1 = partial slip using wall_ks.", alias="irov"
+        )
+        wall_ks: Optional[str] = Field(
+            "Nikuradse roughness [m] for side walls, wall_z0=wall_ks/30.",
+            alias="wall_ks",
+        )
+        rhomean: Optional[str] = Field(
+            "Average water density [kg/m3].", alias="rhomean"
+        )
+        idensform: Optional[str] = Field(
+            "Density calulation (0: uniform, 1: Eckart, 2: Unesco, 3=Unesco83, 13=3+pressure).",
+            alias="idensform",
+        )
+        ag: Optional[str] = Field("Gravitational acceleration [m/s2].", alias="ag")
+        tidalforcing: Optional[str] = Field(
+            "Tidal forcing, if jsferic=1 (0: no, 1: yes).", alias="tidalForcing"
+        )
+        itcap: Optional[str] = Field(
+            "Upper limit on internal tides dissipation (W/m^2)", alias="ITcap"
+        )
+        doodsonstart: Optional[str] = Field(
+            "Doodson start time for tidal forcing [s].", alias="doodsonStart"
+        )
+        doodsonstop: Optional[str] = Field(
+            "Doodson stop time for tidal forcing [s].", alias="doodsonStop"
+        )
+        doodsoneps: Optional[str] = Field(
+            "Doodson tolerance level for tidal forcing [s].", alias="doodsonEps"
+        )
+        villemontecd1: Optional[str] = Field(
+            "Calibration coefficient for Villemonte. Default = 1.0.",
+            alias="villemonteCD1",
+        )
+        villemontecd2: Optional[str] = Field(
+            "Calibration coefficient for Villemonte. Default = 10.0.",
+            alias="villemonteCD2",
+        )
+        salinity: Optional[str] = Field(
+            "Include salinity, (0: no, 1: yes).", alias="salinity"
+        )
+        initialsalinity: Optional[str] = Field(
+            "Initial salinity concentration [ppt].", alias="initialSalinity"
+        )
+        sal0abovezlev: Optional[str] = Field(
+            "Salinity 0 above level [m].", alias="sal0AboveZLev"
+        )
+        deltasalinity: Optional[str] = Field(
+            "uniform initial salinity [ppt].", alias="deltaSalinity"
+        )
+        backgroundsalinity: Optional[str] = Field(
+            "Background salinity for eqn. of state if salinity not computed [psu].",
+            alias="backgroundSalinity",
+        )
+        temperature: Optional[str] = Field(
+            "Include temperature (0: no, 1: only transport, 3: excess model of D3D, 5: composite (ocean) model).",
+            alias="temperature",
+        )
+        initialtemperature: Optional[str] = Field(
+            "Initial temperature [â—¦C].", alias="initialTemperature"
+        )
+        backgroundwatertemperature: Optional[str] = Field(
+            "Background water temperature for eqn. of state if temperature not computed [â—¦C].",
+            alias="backgroundWaterTemperature",
+        )
+        secchidepth: Optional[str] = Field(
+            "Water clarity parameter [m].", alias="secchiDepth"
+        )
+        stanton: Optional[str] = Field(
+            "Coefficient for convective heat flux ( ), if negative, then Cd wind is used.",
+            alias="stanton",
+        )
+        dalton: Optional[str] = Field(
+            "Coefficient for evaporative heat flux ( ), if negative, then Cd wind is used.",
+            alias="dalton",
+        )
+        tempmax: Optional[str] = Field(
+            "Limit the temperature to max value [Â°C]", alias="tempMax"
+        )
+        tempmin: Optional[str] = Field(
+            "Limit the temperature to min value [Â°C]", alias="tempMin"
+        )
+        salimax: Optional[str] = Field(
+            "Limit for salinity to max value [ppt]", alias="saliMax"
+        )
+        salimin: Optional[str] = Field(
+            "Limit for salinity to min value [ppt]", alias="saliMin"
+        )
+        heat_eachstep: Optional[str] = Field(
+            "'1=heat each timestep, 0=heat each usertimestep", alias="heat_eachStep"
+        )
+        rhoairrhowater: Optional[str] = Field(
+            "'windstress rhoa/rhow: 0=Rhoair/Rhomean, 1=Rhoair/rhow(), 2=rhoa0()/rhow(), 3=rhoa10()/Rhow()",
+            alias="rhoAirRhoWater",
+        )
+        nudgetimeuni: Optional[str] = Field(
+            "Uniform nudge relaxation time [s]", alias="nudgeTimeUni"
+        )
+        iniwithnudge: Optional[str] = Field(
+            "Initialize salinity and temperature with nudge variables (0: no, 1: yes, 2: only initialize, no nudging)",
+            alias="iniWithNudge",
+        )
+        secondaryflow: Optional[str] = Field(
+            "Secondary flow (0: no, 1: yes).", alias="secondaryFlow"
+        )
+        betaspiral: Optional[str] = Field(
+            "Weight factor of the spiral flow intensity on flow dispersion stresses (0d0 = disabled).",
+            alias="betaSpiral",
+        )
+
+    comments: Comments = Comments()
+
+    _header: Literal["Physics"] = "Physics"
+    uniffrictcoef: float = Field(0.023, alias="unifFrictCoef")
+    uniffricttype: int = Field(1, alias="unifFrictType")
+    uniffrictcoef1d: float = Field(0.023, alias="unifFrictCoef1D")
+    uniffrictcoeflin: float = Field(0.0, alias="unifFrictCoefLin")
+    vicouv: float = Field(0.1, alias="vicouv")
+    dicouv: float = Field(0.1, alias="dicouv")
+    vicoww: float = Field(5e-05, alias="vicoww")
+    dicoww: float = Field(5e-05, alias="dicoww")
+    vicwminb: float = Field(0.0, alias="vicwminb")
+    xlozmidov: float = Field(0.0, alias="xlozmidov")
+    smagorinsky: float = Field(0.2, alias="smagorinsky")
+    elder: float = Field(0.0, alias="elder")
+    irov: int = Field(0, alias="irov")
+    wall_ks: float = Field(0.0, alias="wall_ks")
+    rhomean: float = Field(1000, alias="rhomean")
+    idensform: int = Field(2, alias="idensform")
+    ag: float = Field(9.81, alias="ag")
+    tidalforcing: bool = Field(False, alias="tidalForcing")
+    itcap: Optional[float] = Field(None, alias="ITcap")
+    doodsonstart: float = Field(55.565, alias="doodsonStart")
+    doodsonstop: float = Field(375.575, alias="doodsonStop")
+    doodsoneps: float = Field(0.0, alias="doodsonEps")
+    villemontecd1: float = Field(1.0, alias="villemonteCD1")
+    villemontecd2: float = Field(10.0, alias="villemonteCD2")
+    salinity: bool = Field(False, alias="salinity")
+    initialsalinity: float = Field(0.0, alias="initialSalinity")
+    sal0abovezlev: float = Field(-999.0, alias="sal0AboveZLev")
+    deltasalinity: float = Field(-999.0, alias="deltaSalinity")
+    backgroundsalinity: float = Field(30.0, alias="backgroundSalinity")
+    temperature: int = Field(0, alias="temperature")
+    initialtemperature: float = Field(6.0, alias="initialTemperature")
+    backgroundwatertemperature: float = Field(6.0, alias="backgroundWaterTemperature")
+    secchidepth: float = Field(2.0, alias="secchiDepth")
+    stanton: float = Field(0.0013, alias="stanton")
+    dalton: float = Field(0.0013, alias="dalton")
+    tempmax: float = Field(-999.0, alias="tempMax")
+    tempmin: float = Field(0.0, alias="tempMin")
+    salimax: float = Field(-999.0, alias="saliMax")
+    salimin: float = Field(0.0, alias="saliMin")
+    heat_eachstep: bool = Field(False, alias="heat_eachStep")
+    rhoairrhowater: int = Field(0, alias="rhoAirRhoWater")
+    nudgetimeuni: float = Field(3600.0, alias="nudgeTimeUni")
+    iniwithnudge: int = Field(0, alias="iniWithNudge")
+    secondaryflow: bool = Field(False, alias="secondaryFlow")
+    betaspiral: float = Field(0.0, alias="betaSpiral")
+
+
+class Sediment(INIBasedModel):
+    class Comments(INIBasedModel.Comments):
+        sedimentmodelnr: Optional[str] = Field(
+            "Sediment model nr, (0=no, 1=Krone, 2=SvR2007, 3=E-H, 4=MorphologyModule).",
+            alias="Sedimentmodelnr",
+        )
+        morfile: Optional[str] = Field(
+            "Morphology settings file (*.mor)", alias="MorFile"
+        )
+        sedfile: Optional[str] = Field(
+            "Sediment characteristics file (*.sed)", alias="SedFile"
+        )
+
+    comments: Comments = Comments()
+
+    _disk_only_file_model_should_not_be_none = (
+        validator_set_default_disk_only_file_model_when_none()
+    )
+
+    _header: Literal["Sediment"] = "Sediment"
+    sedimentmodelnr: Optional[int] = Field(alias="Sedimentmodelnr")
+    morfile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="MorFile"
+    )
+    sedfile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="SedFile"
+    )
+
+
+class Wind(INIBasedModel):
+    """
+    The `[Wind]` section in an MDU file.
+
+    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.wind`.
+
+    All lowercased attributes match with the [Wind] input as described in
+    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
+    """
+
+    class Comments(INIBasedModel.Comments):
+        icdtyp: Optional[str] = Field(
+            "Wind drag coefficient type (1: Const, 2: Smith&Banke (2 pts), 3: S&B (3 pts), 4: Charnock 1955, 5: Hwang 2005, 6: Wuest 2005, 7: Hersbach 2010 (2 pts), 8: 4+viscous).",
+            alias="iCdTyp",
+        )
+        cdbreakpoints: Optional[str] = Field(
+            "Wind drag breakpoints, e.g. 0.00063 0.00723.", alias="CdBreakpoints"
+        )
+        windspeedbreakpoints: Optional[str] = Field(
+            "Wind speed breakpoints [m/s], e.g. 0.0 100.0.",
+            alias="windSpeedBreakpoints",
+        )
+        rhoair: Optional[str] = Field("Air density [kg/m3].", alias="rhoAir")
+        relativewind: Optional[str] = Field(
+            "Wind speed [kg/m3] relative to top-layer water speed*relativewind (0d0=no relative wind, 1d0=using full top layer speed).",
+            alias="relativeWind",
+        )
+        windpartialdry: Optional[str] = Field(
+            "Reduce windstress on water if link partially dry, only for bedlevtyp=3, 0=no, 1=yes (default).",
+            alias="windPartialDry",
+        )
+        pavbnd: Optional[str] = Field(
+            "Average air pressure on open boundaries [N/m2], only applied if value > 0.",
+            alias="pavBnd",
+        )
+        pavini: Optional[str] = Field(
+            "Initial air pressure [N/m2], only applied if value > 0.", alias="pavIni"
+        )
+
+    comments: Comments = Comments()
+
+    _header: Literal["Wind"] = "Wind"
+    icdtyp: int = Field(2, alias="iCdTyp")
+    cdbreakpoints: List[float] = Field([0.00063, 0.00723], alias="CdBreakpoints")
+    windspeedbreakpoints: List[float] = Field(
+        [0.0, 100.0], alias="windSpeedBreakpoints"
+    )
+    rhoair: float = Field(1.205, alias="rhoAir")
+    relativewind: float = Field(0.0, alias="relativeWind")
+    windpartialdry: bool = Field(True, alias="windPartialDry")
+    pavbnd: float = Field(0.0, alias="pavBnd")
+    pavini: float = Field(0.0, alias="pavIni")
+
+    @classmethod
+    def list_delimiter(cls) -> str:
+        return " "
+
+    _split_to_list = get_split_string_on_delimiter_validator(
+        "cdbreakpoints",
+        "windspeedbreakpoints",
+    )
+
+
+class Waves(INIBasedModel):
+    """
+    The `[Waves]` section in an MDU file.
+
+    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.waves`.
+
+    All lowercased attributes match with the [Waves] input as described in
+    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
+    """
+
+    class Comments(INIBasedModel.Comments):
+        wavemodelnr: Optional[str] = Field(
+            "Wave model nr. (0: none, 1: fetch/depth limited hurdlestive, 2: Young-Verhagen, 3: SWAN, 5: uniform, 6: SWAN-NetCDF",
+            alias="waveModelNr",
+        )
+        rouwav: Optional[str] = Field(
+            "Friction model for wave induced shear stress: FR84 (default) or: MS90, HT91, GM79, DS88, BK67, CJ85, OY88, VR04.",
+            alias="rouWav",
+        )
+        gammax: Optional[str] = Field(
+            "Maximum wave height/water depth ratio", alias="gammaX"
+        )
+
+    comments: Comments = Comments()
+
+    _header: Literal["Waves"] = "Waves"
+    wavemodelnr: int = Field(3, alias="waveModelNr")
+    rouwav: str = Field("FR84", alias="rouWav")
+    gammax: float = Field(0.5, alias="gammaX")
+
+
+class Time(INIBasedModel):
+    """
+    The `[Time]` section in an MDU file.
+
+    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.time`.
+
+    All lowercased attributes match with the [Time] input as described in
+    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
+    """
+
+    class Comments(INIBasedModel.Comments):
+        refdate: Optional[str] = Field("Reference date [yyyymmdd].", alias="refDate")
+        tzone: Optional[str] = Field(
+            "Data Sources in GMT are interrogated with time in minutes since refdat-Tzone*60 [min].",
+            alias="tZone",
+        )
+        tunit: Optional[str] = Field("Time units in MDU [D, H, M or S].", alias="tUnit")
+        dtuser: Optional[str] = Field(
+            "User timestep in seconds [s] (interval for external forcing update & his/map output).",
+            alias="dtUser",
+        )
+        dtnodal: Optional[str] = Field(
+            "Time interval [s] for updating nodal factors in astronomical boundary conditions.",
+            alias="dtNodal",
+        )
+        dtmax: Optional[str] = Field("Max timestep in seconds [s].", alias="dtMax")
+        dtinit: Optional[str] = Field(
+            "Initial timestep in seconds [s].", alias="dtInit"
+        )
+        autotimestep: Optional[str] = Field(
+            "0 = no, 1 = 2D (hor. out), 3=3D (hor. out), 5 = 3D (hor. inout + ver. inout), smallest dt",
+            alias="autoTimestep",
+        )
+        autotimestepnostruct: Optional[str] = Field(
+            "Exclude structure links (and neighbours) from time step limitation (0 = no, 1 = yes).",
+            alias="autoTimestepNoStruct",
+        )
+        autotimestepnoqout: Optional[str] = Field(
+            "Exclude negative qin terms from time step limitation (0 = no, 1 = yes).",
+            alias="autoTimestepNoQout",
+        )
+        tstart: Optional[str] = Field(
+            "Start time w.r.t. RefDate [TUnit].", alias="tStart"
+        )
+        tstop: Optional[str] = Field("Stop time w.r.t. RefDate [TUnit].", alias="tStop")
+        startdatetime: Optional[str] = Field(
+            "Computation Startdatetime (yyyymmddhhmmss), when specified, overrides tStart",
+            alias="startDateTime",
+        )
+        stopdatetime: Optional[str] = Field(
+            "Computation Stopdatetime  (yyyymmddhhmmss), when specified, overrides tStop",
+            alias="stopDateTime",
+        )
+        updateroughnessinterval: Optional[str] = Field(
+            "Update interval for time dependent roughness parameters [s].",
+            alias="updateRoughnessInterval",
+        )
+
+    comments: Comments = Comments()
+
+    _header: Literal["Time"] = "Time"
+    refdate: int = Field(20200101, alias="refDate")  # TODO Convert to datetime
+    tzone: float = Field(0.0, alias="tZone")
+    tunit: str = Field("S", alias="tUnit")  # DHMS
+    dtuser: float = Field(300.0, alias="dtUser")
+    dtnodal: float = Field(21600.0, alias="dtNodal")
+    dtmax: float = Field(30.0, alias="dtMax")
+    dtinit: float = Field(1.0, alias="dtInit")
+    autotimestep: Optional[int] = Field(None, alias="autoTimestep")
+    autotimestepnostruct: bool = Field(False, alias="autoTimestepNoStruct")
+    autotimestepnoqout: bool = Field(True, alias="autoTimestepNoQout")
+    tstart: float = Field(0.0, alias="tStart")
+    tstop: float = Field(86400.0, alias="tStop")
+    startdatetime: Optional[str] = Field(None, alias="startDateTime")
+    stopdatetime: Optional[str] = Field(None, alias="stopDateTime")
+    updateroughnessinterval: float = Field(86400.0, alias="updateRoughnessInterval")
+
+    @validator("startdatetime", "stopdatetime")
+    def _validate_datetime(cls, value, field):
+        return validate_datetime_string(value, field)
+
+
+class Restart(INIBasedModel):
+    """
+    The `[Restart]` section in an MDU file.
+
+    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.restart`.
+
+    All lowercased attributes match with the [Restart] input as described in
+    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
+    """
+
+    class Comments(INIBasedModel.Comments):
+        restartfile: Optional[str] = Field(
+            "Restart file, only from netCDF-file, hence: either *_rst.nc or *_map.nc.",
+            alias="restartFile",
+        )
+        restartdatetime: Optional[str] = Field(
+            "Restart time [YYYYMMDDHHMMSS], only relevant in case of restart from *_map.nc.",
+            alias="restartDateTime",
+        )
+
+    comments: Comments = Comments()
+
+    _disk_only_file_model_should_not_be_none = (
+        validator_set_default_disk_only_file_model_when_none()
+    )
+
+    _header: Literal["Restart"] = "Restart"
+    restartfile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="restartFile"
+    )
+    restartdatetime: Optional[str] = Field(None, alias="restartDateTime")
+
+    @validator("restartdatetime")
+    def _validate_datetime(cls, value, field):
+        return validate_datetime_string(value, field)
+
+
+class ExternalForcing(INIBasedModel):
+    """
+    The `[External Forcing]` section in an MDU file.
+
+    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.external_forcing`.
+
+    All lowercased attributes match with the [External Forcing] input as described in
+    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
+    """
+
+    class Comments(INIBasedModel.Comments):
+        extforcefile: Optional[str] = Field(
+            "Old format for external forcings file *.ext, link with tim/cmp-format boundary conditions specification.",
+            alias="extForceFile",
+        )
+        extforcefilenew: Optional[str] = Field(
+            "New format for external forcings file *.ext, link with bcformat boundary conditions specification.",
+            alias="extForceFileNew",
+        )
+        rainfall: Optional[str] = Field(
+            "Include rainfall, (0=no, 1=yes).", alias="rainfall"
+        )
+        qext: Optional[str] = Field(
+            "Include user Qin/out, externally provided, (0=no, 1=yes).", alias="qExt"
+        )
+        evaporation: Optional[str] = Field(
+            "Include evaporation in water balance, (0=no, 1=yes).", alias="evaporation"
+        )
+        windext: Optional[str] = Field(
+            "Include wind, externally provided, (0=no, 1=reserved for EC, 2=yes).",
+            alias="windExt",
+        )
+
+    comments: Comments = Comments()
+
+    _disk_only_file_model_should_not_be_none = (
+        validator_set_default_disk_only_file_model_when_none()
+    )
+
+    _header: Literal["External Forcing"] = "External Forcing"
+    extforcefile: Optional[ExtOldModel] = Field(None, alias="extForceFile")
+    extforcefilenew: Optional[ExtModel] = Field(None, alias="extForceFileNew")
+    rainfall: Optional[bool] = Field(None, alias="rainfall")
+    qext: Optional[bool] = Field(None, alias="qExt")
+    evaporation: Optional[bool] = Field(None, alias="evaporation")
+    windext: Optional[int] = Field(None, alias="windExt")
+
+    def is_intermediate_link(self) -> bool:
+        return True
+
+
+class Hydrology(INIBasedModel):
+    """
+    The `[Hydrology]` section in an MDU file.
+
+    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.hydrology`.
+
+    All lowercased attributes match with the [Hydrology] input as described in
+    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
+    """
+
+    class Comments(INIBasedModel.Comments):
+        interceptionmodel: Optional[str] = Field(
+            "Interception model (0: none, 1: on, via layer thickness).",
+            alias="interceptionModel",
+        )
+
+    comments: Comments = Comments()
+
+    _header: Literal["Hydrology"] = "Hydrology"
+    interceptionmodel: bool = Field(False, alias="interceptionModel")
+
+
+class Trachytopes(INIBasedModel):
+    """
+    The `[Trachytopes]` section in an MDU file.
+
+    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.trachytopes`.
+
+    All lowercased attributes match with the [Trachytopes] input as described in
+    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
+    """
+
+    class Comments(INIBasedModel.Comments):
+        trtrou: Optional[str] = Field(
+            "Flag for trachytopes (Y=on, N=off).", alias="trtRou"
+        )
+        trtdef: Optional[str] = Field(
+            "File (*.ttd) including trachytope definitions.", alias="trtDef"
+        )
+        trtl: Optional[str] = Field(
+            "File (*.arl) including distribution of trachytope definitions.",
+            alias="trtL",
+        )
+        dttrt: Optional[str] = Field(
+            "Interval for updating of bottom roughness due to trachytopes in seconds [s].",
+            alias="dtTrt",
+        )
+        trtmxr: Optional[str] = Field(
+            "Maximum recursion level for composite trachytope definitions",
+            alias="trtMxR",
+        )
+
+    comments: Comments = Comments()
+
+    _header: Literal["Trachytopes"] = "Trachytopes"
+    trtrou: str = Field("N", alias="trtRou")  # TODO bool
+    trtdef: Optional[Path] = Field(None, alias="trtDef")
+    trtl: Optional[Path] = Field(None, alias="trtL")
+    dttrt: float = Field(60.0, alias="dtTrt")
+    trtmxr: Optional[int] = Field(None, alias="trtMxR")
+
+
+ObsFile = Union[XYNModel, ObservationPointModel]
+ObsCrsFile = Union[PolyFile, ObservationCrossSectionModel]
+
+
+class Output(INIBasedModel):
+    """
+    The `[Output]` section in an MDU file.
+
+    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.output`.
+
+    All lowercased attributes match with the [Output] input as described in
+    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
+    """
+
+    class Comments(INIBasedModel.Comments):
+        wrishp_crs: Optional[str] = Field(
+            "Writing cross sections to shape file (0=no, 1=yes).", alias="wrishp_crs"
+        )
+        wrishp_weir: Optional[str] = Field(
+            "Writing weirs to shape file (0=no, 1=yes).", alias="wrishp_weir"
+        )
+        wrishp_gate: Optional[str] = Field(
+            "Writing gates to shape file (0=no, 1=yes).", alias="wrishp_gate"
+        )
+        wrishp_fxw: Optional[str] = Field(
+            "Writing fixed weirs to shape file (0=no, 1=yes).", alias="wrishp_fxw"
+        )
+        wrishp_thd: Optional[str] = Field(
+            "Writing thin dams to shape file (0=no, 1=yes).", alias="wrishp_thd"
+        )
+        wrishp_obs: Optional[str] = Field(
+            "Writing observation points to shape file (0=no, 1=yes).",
+            alias="wrishp_obs",
+        )
+        wrishp_emb: Optional[str] = Field(
+            "Writing embankments file (0=no, 1=yes).", alias="wrishp_emb"
+        )
+        wrishp_dryarea: Optional[str] = Field(
+            "Writing dry areas to shape file (0=no, 1=yes).", alias="wrishp_dryArea"
+        )
+        wrishp_enc: Optional[str] = Field(
+            "Writing enclosures to shape file (0=no, 1=yes).", alias="wrishp_enc"
+        )
+        wrishp_src: Optional[str] = Field(
+            "Writing sources and sinks to shape file (0=no, 1=yes).", alias="wrishp_src"
+        )
+        wrishp_pump: Optional[str] = Field(
+            "Writing pumps to shape file (0=no, 1=yes).", alias="wrishp_pump"
+        )
+        outputdir: Optional[str] = Field(
+            "Output directory of map-, his-, rst-, dat- and timingsfiles, default: DFM_OUTPUT_<modelname>. Set to . for no dir/current dir.",
+            alias="outputDir",
+        )
+        waqoutputdir: Optional[str] = Field(
+            "Output directory of Water Quality files.", alias="waqOutputDir"
+        )
+        flowgeomfile: Optional[str] = Field(
+            "*_flowgeom.nc Flow geometry file in netCDF format.", alias="flowGeomFile"
+        )
+        obsfile: Optional[str] = Field(
+            "Space separated list of files, containing information about observation points.",
+            alias="obsFile",
+        )
+        crsfile: Optional[str] = Field(
+            "Space separated list of files, containing information about observation cross sections.",
+            alias="crsFile",
+        )
+        foufile: Optional[str] = Field(
+            "Fourier analysis input file *.fou", alias="fouFile"
+        )
+        fouupdatestep: Optional[str] = Field(
+            "Fourier update step type: 0=every user time step, 1=every computational timestep, 2=same as history output.",
+            alias="fouUpdateStep",
+        )
+        hisfile: Optional[str] = Field(
+            "*_his.nc History file in netCDF format.", alias="hisFile"
+        )
+        hisinterval: Optional[str] = Field(
+            "History output, given as 'interval' 'start period' 'end period' [s].",
+            alias="hisInterval",
+        )
+        xlsinterval: Optional[str] = Field(
+            "Interval between XLS history [s].", alias="xlsInterval"
+        )
+        mapfile: Optional[str] = Field(
+            "*_map.nc Map file in netCDF format.", alias="mapFile"
+        )
+        mapinterval: Optional[str] = Field(
+            "Map file output, given as 'interval' 'start period' 'end period' [s].",
+            alias="mapInterval",
+        )
+        rstinterval: Optional[str] = Field(
+            "Restart file output, given as 'interval' 'start period' 'end period' [s].",
+            alias="rstInterval",
+        )
+        mapformat: Optional[str] = Field(
+            "Map file format, 1: netCDF, 2: Tecplot, 3: NetCFD and Tecplot, 4: netCDF UGRID.",
+            alias="mapFormat",
+        )
+        ncformat: Optional[str] = Field(
+            "Format for all NetCDF output files (3: classic, 4: NetCDF4+HDF5).",
+            alias="ncFormat",
+        )
+        ncnounlimited: Optional[str] = Field(
+            "Write full-length time-dimension instead of unlimited dimension (1: yes, 0: no). (Might require NcFormat=4.)",
+            alias="ncNoUnlimited",
+        )
+        ncnoforcedflush: Optional[str] = Field(
+            "Do not force flushing of map-like files every output timestep (1: yes, 0: no).",
+            alias="ncNoForcedFlush",
+        )
+        ncwritelatlon: Optional[str] = Field(
+            "Write extra lat-lon coordinates for all projected coordinate variables in each NetCDF file (for CF-compliancy) (1: yes, 0: no).",
+            alias="ncWriteLatLon",
+        )
+        wrihis_balance: Optional[str] = Field(
+            "Write mass balance totals to his file, (1: yes, 0: no).",
+            alias="wrihis_balance",
+        )
+        wrihis_sourcesink: Optional[str] = Field(
+            "Write sources-sinks statistics to his file, (1: yes, 0: no).",
+            alias="wrihis_sourceSink",
+        )
+        wrihis_structure_gen: Optional[str] = Field(
+            "Write general structure parameters to his file, (1: yes, 0: no).",
+            alias="wrihis_structure_gen",
+        )
+        wrihis_structure_dam: Optional[str] = Field(
+            "Write dam parameters to his file, (1: yes, 0: no).",
+            alias="wrihis_structure_dam",
+        )
+        wrihis_structure_pump: Optional[str] = Field(
+            "Write pump parameters to his file, (1: yes, 0: no).",
+            alias="wrihis_structure_pump",
+        )
+        wrihis_structure_gate: Optional[str] = Field(
+            "Write gate parameters to his file, (1: yes, 0: no).",
+            alias="wrihis_structure_gate",
+        )
+        wrihis_structure_weir: Optional[str] = Field(
+            "Write weir parameters to his file, (1: yes, 0: no).",
+            alias="wrihis_structure_weir",
+        )
+        wrihis_structure_orifice: Optional[str] = Field(
+            "Write orifice parameters to his file, (1: yes, 0: no).",
+            alias="wrihis_structure_orifice",
+        )
+        wrihis_structure_bridge: Optional[str] = Field(
+            "Write bridge parameters to his file, (1: yes, 0: no).",
+            alias="wrihis_structure_bridge",
+        )
+        wrihis_structure_culvert: Optional[str] = Field(
+            "Write culvert parameters to his file, (1: yes, 0: no).",
+            alias="wrihis_structure_culvert",
+        )
+        wrihis_structure_longculvert: Optional[str] = Field(
+            "Write long culvert parameters to his file, (1: yes, 0: no).",
+            alias="wrihis_structure_longCulvert",
+        )
+        wrihis_structure_dambreak: Optional[str] = Field(
+            "Write dam break parameters to his file, (1: yes, 0: no).",
+            alias="wrihis_structure_damBreak",
+        )
+        wrihis_structure_uniweir: Optional[str] = Field(
+            "Write universal weir parameters to his file, (1: yes, 0: no).",
+            alias="wrihis_structure_uniWeir",
+        )
+        wrihis_structure_compound: Optional[str] = Field(
+            "Write compound structure parameters to his file, (1: yes, 0: no).",
+            alias="wrihis_structure_compound",
+        )
+        wrihis_turbulence: Optional[str] = Field(
+            "Write k, eps and vicww to his file (1: yes, 0: no)'",
+            alias="wrihis_turbulence",
+        )
+        wrihis_wind: Optional[str] = Field(
+            "Write wind velocities to his file (1: yes, 0: no)'", alias="wrihis_wind"
+        )
+        wrihis_rain: Optional[str] = Field(
+            "Write precipitation to his file (1: yes, 0: no)'", alias="wrihis_rain"
+        )
+        wrihis_infiltration: Optional[str] = Field(
+            "Write infiltration to his file (1: yes, 0: no)'",
+            alias="wrihis_infiltration",
+        )
+        wrihis_temperature: Optional[str] = Field(
+            "Write temperature to his file (1: yes, 0: no)'", alias="wrihis_temperature"
+        )
+        wrihis_waves: Optional[str] = Field(
+            "Write wave data to his file (1: yes, 0: no)'", alias="wrihis_waves"
+        )
+        wrihis_heat_fluxes: Optional[str] = Field(
+            "Write heat fluxes to his file (1: yes, 0: no)'", alias="wrihis_heat_fluxes"
+        )
+        wrihis_salinity: Optional[str] = Field(
+            "Write salinity to his file (1: yes, 0: no)'", alias="wrihis_salinity"
+        )
+        wrihis_density: Optional[str] = Field(
+            "Write density to his file (1: yes, 0: no)'", alias="wrihis_density"
+        )
+        wrihis_waterlevel_s1: Optional[str] = Field(
+            "Write water level to his file (1: yes, 0: no)'",
+            alias="wrihis_waterlevel_s1",
+        )
+        wrihis_bedlevel: Optional[str] = Field(
+            "Write bed level to his file (1: yes, 0: no)'", alias="wrihis_bedlevel"
+        )
+        wrihis_waterdepth: Optional[str] = Field(
+            "Write water depth to his file (1: yes, 0: no)'", alias="wrihis_waterdepth"
+        )
+        wrihis_velocity_vector: Optional[str] = Field(
+            "Write velocity vectors to his file (1: yes, 0: no)'",
+            alias="wrihis_velocity_vector",
+        )
+        wrihis_upward_velocity_component: Optional[str] = Field(
+            "Write upward velocity to his file (1: yes, 0: no)'",
+            alias="wrihis_upward_velocity_component",
+        )
+        wrihis_velocity: Optional[str] = Field(
+            "Write velocity magnitude in observation point to his file, (1: yes, 0: no).",
+            alias="wrihis_velocity",
+        )
+        wrihis_discharge: Optional[str] = Field(
+            "Write discharge magnitude in observation point to his file, (1: yes, 0: no).",
+            alias="wrihis_discharge",
+        )
+        wrihis_sediment: Optional[str] = Field(
+            "Write sediment transport to his file (1: yes, 0: no)'",
+            alias="wrihis_sediment",
+        )
+        wrihis_constituents: Optional[str] = Field(
+            "Write tracers to his file (1: yes, 0: no)'", alias="wrihis_constituents"
+        )
+        wrihis_zcor: Optional[str] = Field(
+            "Write vertical coordinates to his file (1: yes, 0: no)'",
+            alias="wrihis_zcor",
+        )
+        wrihis_lateral: Optional[str] = Field(
+            "Write lateral data to his file, (1: yes, 0: no).", alias="wrihis_lateral"
+        )
+        wrihis_taucurrent: Optional[str] = Field(
+            "Write mean bed shear stress to his file (1: yes, 0: no)'",
+            alias="wrihis_taucurrent",
+        )
+        wrimap_waterlevel_s0: Optional[str] = Field(
+            "Write water levels at old time level to map file, (1: yes, 0: no).",
+            alias="wrimap_waterLevel_s0",
+        )
+        wrimap_waterlevel_s1: Optional[str] = Field(
+            "Write water levels at new time level to map file, (1: yes, 0: no).",
+            alias="wrimap_waterLevel_s1",
+        )
+        wrimap_evaporation: Optional[str] = Field(
+            "Write evaporation to map file, (1: yes, 0: no).",
+            alias="wrimap_evaporation",
+        )
+        wrimap_waterdepth: Optional[str] = Field(
+            "Write water depths to map file (1: yes, 0: no).",
+            alias="wrimap_waterdepth",
+        )
+        wrimap_velocity_component_u0: Optional[str] = Field(
+            "Write velocities at old time level to map file, (1: yes, 0: no).",
+            alias="wrimap_velocity_component_u0",
+        )
+        wrimap_velocity_component_u1: Optional[str] = Field(
+            "Write velocities at new time level to map file, (1: yes, 0: no).",
+            alias="wrimap_velocity_component_u1",
+        )
+        wrimap_velocity_vector: Optional[str] = Field(
+            "Write cell-center velocity vectors to map file, (1: yes, 0: no).",
+            alias="wrimap_velocity_vector",
+        )
+        wrimap_velocity_magnitude: Optional[str] = Field(
+            "Write cell-center velocity vector magnitude to map file (1: yes, 0: no).",
+            alias="wrimap_velocity_magnitude",
+        )
+        wrimap_upward_velocity_component: Optional[str] = Field(
+            "Write upward velocity component to map file, (1: yes, 0: no).",
+            alias="wrimap_upward_velocity_component",
+        )
+        wrimap_density_rho: Optional[str] = Field(
+            "Write density to map file, (1: yes, 0: no).", alias="wrimap_density_rho"
+        )
+        wrimap_horizontal_viscosity_viu: Optional[str] = Field(
+            "Write horizontal viscosity to map file, (1: yes, 0: no).",
+            alias="wrimap_horizontal_viscosity_viu",
+        )
+        wrimap_horizontal_diffusivity_diu: Optional[str] = Field(
+            "Write horizontal diffusivity to map file, (1: yes, 0: no).",
+            alias="wrimap_horizontal_diffusivity_diu",
+        )
+        wrimap_flow_flux_q1: Optional[str] = Field(
+            "Write fluxes to map file, (1: yes, 0: no).", alias="wrimap_flow_flux_q1"
+        )
+        wrimap_spiral_flow: Optional[str] = Field(
+            "Write spiral flow to map file, (1: yes, 0: no).",
+            alias="wrimap_spiral_flow",
+        )
+        wrimap_numlimdt: Optional[str] = Field(
+            "Write numlimdt to map file, (1: yes, 0: no).", alias="wrimap_numLimdt"
+        )
+        wrimap_taucurrent: Optional[str] = Field(
+            "Write bottom friction to map file, (1: yes, 0: no).",
+            alias="wrimap_tauCurrent",
+        )
+        wrimap_chezy: Optional[str] = Field(
+            "Write chezy values to map file, (1: yes, 0: no).", alias="wrimap_chezy"
+        )
+        wrimap_turbulence: Optional[str] = Field(
+            "Write turbulence to map file, (1: yes, 0: no).", alias="wrimap_turbulence"
+        )
+        wrimap_rain: Optional[str] = Field(
+            "Write rainfall rate to map file, (1: yes, 0: no).", alias="wrimap_rain"
+        )
+        wrimap_wind: Optional[str] = Field(
+            "Write winds to map file, (1: yes, 0: no).", alias="wrimap_wind"
+        )
+        writek_cdwind: Optional[str] = Field(
+            "Write wind friction coefficients to tek file (1: yes, 0: no).",
+            alias="writek_CdWind",
+        )
+        wrimap_heat_fluxes: Optional[str] = Field(
+            "Write heat fluxes to map file, (1: yes, 0: no).",
+            alias="wrimap_heat_fluxes",
+        )
+        wrimap_wet_waterdepth_threshold: Optional[str] = Field(
+            "Waterdepth threshold above which a grid point counts as 'wet'. Defaults to 0.2Â·Epshu. It is used for Wrimap_time_water_on_ground, Wrimap_waterdepth_on_ground and Wrimap_volume_on_ground.",
+            alias="wrimap_wet_waterDepth_threshold",
+        )
+        wrimap_time_water_on_ground: Optional[str] = Field(
+            "Write cumulative time when water is above ground level (only for 1D nodes) to map file, (1: yes, 0: no).",
+            alias="wrimap_time_water_on_ground",
+        )
+        wrimap_freeboard: Optional[str] = Field(
+            "Write freeboard (only for 1D nodes) to map file, (1: yes, 0: no).",
+            alias="wrimap_freeboard",
+        )
+        wrimap_waterdepth_on_ground: Optional[str] = Field(
+            "Write waterdepth that is above ground level to map file (only for 1D nodes) (1: yes, 0: no).",
+            alias="wrimap_waterDepth_on_ground",
+        )
+        wrimap_volume_on_ground: Optional[str] = Field(
+            "Write volume that is above ground level to map file (only for 1D nodes) (1: yes, 0: no).",
+            alias="wrimap_volume_on_ground",
+        )
+        wrimap_total_net_inflow_1d2d: Optional[str] = Field(
+            "Write current total 1D2D net inflow (discharge) and cumulative total 1D2D net inflow (volume) to map file (only for 1D nodes) (1:yes, 0:no).",
+            alias="wrimap_total_net_inflow_1d2d",
+        )
+        wrimap_total_net_inflow_lateral: Optional[str] = Field(
+            "Write current total lateral net inflow (discharge) and cumulative total lateral net inflow (volume) to map file (only for 1D nodes) (1:yes, 0:no).",
+            alias="wrimap_total_net_inflow_lateral",
+        )
+        wrimap_water_level_gradient: Optional[str] = Field(
+            "Write water level gradient to map file (only for 1D links) (1:yes, 0:no).",
+            alias="wrimap_water_level_gradient",
+        )
+        wrimap_tidal_potential: Optional[str] = Field(
+            "Write tidal potential to map file (1: yes, 0: no)",
+            alias="wrimap_tidal_potential",
+        )
+        wrimap_sal_potential: Optional[str] = Field(
+            "Write self attraction and loading potential to map file (1: yes, 0: no)",
+            alias="wrimap_SAL_potential",
+        )
+        wrimap_internal_tides_dissipation: Optional[str] = Field(
+            "Write internal tides dissipation to map file (1: yes, 0: no)",
+            alias="wrimap_internal_tides_dissipation",
+        )
+        wrimap_flow_analysis: Optional[str] = Field(
+            "Write flow analysis data to the map file (1:yes, 0:no).",
+            alias="wrimap_flow_analysis",
+        )
+        mapoutputtimevector: Optional[str] = Field(
+            "File (.mpt) containing fixed map output times (s) w.r.t. RefDate.",
+            alias="mapOutputTimeVector",
+        )
+        fullgridoutput: Optional[str] = Field(
+            "Full grid output mode for layer positions (0: compact, 1: full time-varying grid layer data).",
+            alias="fullGridOutput",
+        )
+        eulervelocities: Optional[str] = Field(
+            "Write Eulerian velocities, (1: yes, 0: no).", alias="eulerVelocities"
+        )
+        classmapfile: Optional[str] = Field(
+            "Name of class map file.", alias="classMapFile"
+        )
+        waterlevelclasses: Optional[str] = Field(
+            "Series of values between which water level classes are computed.",
+            alias="waterLevelClasses",
+        )
+        waterdepthclasses: Optional[str] = Field(
+            "Series of values between which water depth classes are computed.",
+            alias="waterDepthClasses",
+        )
+        classmapinterval: Optional[str] = Field(
+            "Interval [s] between class map file outputs.", alias="classMapInterval"
+        )
+        waqinterval: Optional[str] = Field(
+            "Interval [s] between DELWAQ file outputs.", alias="waqInterval"
+        )
+        statsinterval: Optional[str] = Field(
+            "Interval [s] between screen step outputs in seconds simulation time, if negative in seconds wall clock time.",
+            alias="statsInterval",
+        )
+        timingsinterval: Optional[str] = Field(
+            "Timings output interval TimingsInterval.", alias="timingsInterval"
+        )
+        richardsononoutput: Optional[str] = Field(
+            "Write Richardson number, (1: yes, 0: no).", alias="richardsonOnOutput"
+        )
+
+    comments: Comments = Comments()
+
+    _disk_only_file_model_should_not_be_none = (
+        validator_set_default_disk_only_file_model_when_none()
+    )
+
+    _header: Literal["Output"] = "Output"
+    wrishp_crs: bool = Field(False, alias="wrishp_crs")
+    wrishp_weir: bool = Field(False, alias="wrishp_weir")
+    wrishp_gate: bool = Field(False, alias="wrishp_gate")
+    wrishp_fxw: bool = Field(False, alias="wrishp_fxw")
+    wrishp_thd: bool = Field(False, alias="wrishp_thd")
+    wrishp_obs: bool = Field(False, alias="wrishp_obs")
+    wrishp_emb: bool = Field(False, alias="wrishp_emb")
+    wrishp_dryarea: bool = Field(False, alias="wrishp_dryArea")
+    wrishp_enc: bool = Field(False, alias="wrishp_enc")
+    wrishp_src: bool = Field(False, alias="wrishp_src")
+    wrishp_pump: bool = Field(False, alias="wrishp_pump")
+    outputdir: Optional[Path] = Field(None, alias="outputDir")
+    waqoutputdir: Optional[Path] = Field(None, alias="waqOutputDir")
+    flowgeomfile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="flowGeomFile"
+    )
+    obsfile: Optional[List[ObsFile]] = Field(None, alias="obsFile")
+    crsfile: Optional[List[ObsCrsFile]] = Field(None, alias="crsFile")
+    foufile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="fouFile"
+    )
+    fouupdatestep: int = Field(0, alias="fouUpdateStep")
+    hisfile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="hisFile"
+    )
+    hisinterval: List[float] = Field([300.0], alias="hisInterval")
+    xlsinterval: List[float] = Field([0.0], alias="xlsInterval")
+    mapfile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="mapFile"
+    )
+    mapinterval: List[float] = Field([1200.0], alias="mapInterval")
+    rstinterval: List[float] = Field([0.0], alias="rstInterval")
+    mapformat: int = Field(4, alias="mapFormat")
+    ncformat: int = Field(3, alias="ncFormat")
+    ncnounlimited: bool = Field(False, alias="ncNoUnlimited")
+    ncnoforcedflush: bool = Field(False, alias="ncNoForcedFlush")
+    ncwritelatlon: bool = Field(False, alias="ncWriteLatLon")
+
+    # his file
+    wrihis_balance: bool = Field(True, alias="wrihis_balance")
+    wrihis_sourcesink: bool = Field(True, alias="wrihis_sourceSink")
+    wrihis_structure_gen: bool = Field(True, alias="wrihis_structure_gen")
+    wrihis_structure_dam: bool = Field(True, alias="wrihis_structure_dam")
+    wrihis_structure_pump: bool = Field(True, alias="wrihis_structure_pump")
+    wrihis_structure_gate: bool = Field(True, alias="wrihis_structure_gate")
+    wrihis_structure_weir: bool = Field(True, alias="wrihis_structure_weir")
+    wrihis_structure_orifice: bool = Field(True, alias="wrihis_structure_orifice")
+    wrihis_structure_bridge: bool = Field(True, alias="wrihis_structure_bridge")
+    wrihis_structure_culvert: bool = Field(True, alias="wrihis_structure_culvert")
+    wrihis_structure_longculvert: bool = Field(
+        True, alias="wrihis_structure_longCulvert"
+    )
+    wrihis_structure_dambreak: bool = Field(True, alias="wrihis_structure_damBreak")
+    wrihis_structure_uniweir: bool = Field(True, alias="wrihis_structure_uniWeir")
+    wrihis_structure_compound: bool = Field(True, alias="wrihis_structure_compound")
+    wrihis_turbulence: bool = Field(True, alias="wrihis_turbulence")
+    wrihis_wind: bool = Field(True, alias="wrihis_wind")
+    wrihis_rain: bool = Field(True, alias="wrihis_rain")
+    wrihis_infiltration: bool = Field(True, alias="wrihis_infiltration")
+    wrihis_temperature: bool = Field(True, alias="wrihis_temperature")
+    wrihis_waves: bool = Field(True, alias="wrihis_waves")
+    wrihis_heat_fluxes: bool = Field(True, alias="wrihis_heat_fluxes")
+    wrihis_salinity: bool = Field(True, alias="wrihis_salinity")
+    wrihis_density: bool = Field(True, alias="wrihis_density")
+    wrihis_waterlevel_s1: bool = Field(True, alias="wrihis_waterlevel_s1")
+    wrihis_bedlevel: bool = Field(True, alias="wrihis_bedlevel")
+    wrihis_waterdepth: bool = Field(False, alias="wrihis_waterdepth")
+    wrihis_velocity_vector: bool = Field(True, alias="wrihis_velocity_vector")
+    wrihis_upward_velocity_component: bool = Field(
+        False, alias="wrihis_upward_velocity_component"
+    )
+    wrihis_velocity: bool = Field(False, alias="wrihis_velocity")
+    wrihis_discharge: bool = Field(False, alias="wrihis_discharge")
+    wrihis_sediment: bool = Field(True, alias="wrihis_sediment")
+    wrihis_constituents: bool = Field(True, alias="wrihis_constituents")
+    wrihis_zcor: bool = Field(True, alias="wrihis_zcor")
+    wrihis_lateral: bool = Field(True, alias="wrihis_lateral")
+    wrihis_taucurrent: bool = Field(True, alias="wrihis_taucurrent")
+
+    # Map file
+    wrimap_waterlevel_s0: bool = Field(True, alias="wrimap_waterLevel_s0")
+    wrimap_waterlevel_s1: bool = Field(True, alias="wrimap_waterLevel_s1")
+    wrimap_evaporation: bool = Field(False, alias="wrimap_evaporation")
+    wrimap_waterdepth: bool = Field(True, alias="wrimap_waterdepth")
+    wrimap_velocity_component_u0: bool = Field(
+        True, alias="wrimap_velocity_component_u0"
+    )
+    wrimap_velocity_component_u1: bool = Field(
+        True, alias="wrimap_velocity_component_u1"
+    )
+    wrimap_velocity_vector: bool = Field(True, alias="wrimap_velocity_vector")
+    wrimap_velocity_magnitude: bool = Field(True, alias="wrimap_velocity_magnitude")
+    wrimap_upward_velocity_component: bool = Field(
+        False, alias="wrimap_upward_velocity_component"
+    )
+    wrimap_density_rho: bool = Field(True, alias="wrimap_density_rho")
+    wrimap_horizontal_viscosity_viu: bool = Field(
+        True, alias="wrimap_horizontal_viscosity_viu"
+    )
+    wrimap_horizontal_diffusivity_diu: bool = Field(
+        True, alias="wrimap_horizontal_diffusivity_diu"
+    )
+    wrimap_flow_flux_q1: bool = Field(True, alias="wrimap_flow_flux_q1")
+    wrimap_spiral_flow: bool = Field(True, alias="wrimap_spiral_flow")
+    wrimap_numlimdt: bool = Field(True, alias="wrimap_numLimdt")
+    wrimap_taucurrent: bool = Field(True, alias="wrimap_tauCurrent")
+    wrimap_chezy: bool = Field(True, alias="wrimap_chezy")
+    wrimap_turbulence: bool = Field(True, alias="wrimap_turbulence")
+    wrimap_rain: bool = Field(False, alias="wrimap_rain")
+    wrimap_wind: bool = Field(True, alias="wrimap_wind")
+    writek_cdwind: bool = Field(False, alias="writek_CdWind")
+    wrimap_heat_fluxes: bool = Field(False, alias="wrimap_heat_fluxes")
+    wrimap_wet_waterdepth_threshold: float = Field(
+        2e-5, alias="wrimap_wet_waterDepth_threshold"
+    )
+    wrimap_time_water_on_ground: bool = Field(
+        False, alias="wrimap_time_water_on_ground"
+    )
+    wrimap_freeboard: bool = Field(False, alias="wrimap_freeboard")
+    wrimap_waterdepth_on_ground: bool = Field(
+        False, alias="wrimap_waterDepth_on_ground"
+    )
+    wrimap_volume_on_ground: bool = Field(False, alias="wrimap_volume_on_ground")
+    wrimap_total_net_inflow_1d2d: bool = Field(
+        False, alias="wrimap_total_net_inflow_1d2d"
+    )
+    wrimap_total_net_inflow_lateral: bool = Field(
+        False, alias="wrimap_total_net_inflow_lateral"
+    )
+    wrimap_water_level_gradient: bool = Field(
+        False, alias="wrimap_water_level_gradient"
+    )
+    wrimap_tidal_potential: bool = Field(True, alias="wrimap_tidal_potential")
+    wrimap_sal_potential: bool = Field(True, alias="wrimap_SAL_potential")
+    wrimap_internal_tides_dissipation: bool = Field(
+        True, alias="wrimap_internal_tides_dissipation"
+    )
+    wrimap_flow_analysis: bool = Field(False, alias="wrimap_flow_analysis")
+    mapoutputtimevector: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="mapOutputTimeVector"
+    )
+    fullgridoutput: bool = Field(False, alias="fullGridOutput")
+    eulervelocities: bool = Field(False, alias="eulerVelocities")
+    classmapfile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="classMapFile"
+    )
+    waterlevelclasses: List[float] = Field([0.0], alias="waterLevelClasses")
+    waterdepthclasses: List[float] = Field([0.0], alias="waterDepthClasses")
+    classmapinterval: List[float] = Field([0.0], alias="classMapInterval")
+    waqinterval: List[float] = Field([0.0], alias="waqInterval")
+    statsinterval: List[float] = Field([-60.0], alias="statsInterval")
+    timingsinterval: List[float] = Field([0.0], alias="timingsInterval")
+    richardsononoutput: bool = Field(False, alias="richardsonOnOutput")
+
+    _split_to_list = get_split_string_on_delimiter_validator(
+        "waterlevelclasses",
+        "waterdepthclasses",
+        "crsfile",
+        "obsfile",
+        "hisinterval",
+        "xlsinterval",
+        "mapinterval",
+        "rstinterval",
+        "classmapinterval",
+        "waqinterval",
+        "statsinterval",
+        "timingsinterval",
+    )
+
+    def is_intermediate_link(self) -> bool:
+        return True
+
+
+class Geometry(INIBasedModel):
+    """
+    The `[Geometry]` section in an MDU file.
+
+    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry`.
+
+    All lowercased attributes match with the [Geometry] input as described in
+    [UM Sec.A.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.1).
+    """
+
+    class Comments(INIBasedModel.Comments):
+        netfile: Optional[str] = Field("The net file <*_net.nc>", alias="netFile")
+        bathymetryfile: Optional[str] = Field(
+            "Removed since March 2022. See [geometry] keyword BedLevelFile.",
+            alias="bathymetryFile",
+        )
+        drypointsfile: Optional[str] = Field(
+            "Dry points file <*.xyz>, third column dummy z values, or polygon file <*.pol>.",
+            alias="dryPointsFile",
+        )
+        structurefile: Optional[str] = Field(
+            "File <*.ini> containing list of hydraulic structures.",
+            alias="structureFile",
+        )
+        inifieldfile: Optional[str] = Field(
+            "Initial and parameter field file <*.ini>.",
+            alias="iniFieldFile",
+        )
+        waterlevinifile: Optional[str] = Field(
+            "Initial water levels sample file <*.xyz>.", alias="waterLevIniFile"
+        )
+        landboundaryfile: Optional[str] = Field(
+            "Only for plotting.", alias="landBoundaryFile"
+        )
+        thindamfile: Optional[str] = Field(
+            "<*_thd.pli>, Polyline(s) for tracing thin dams.", alias="thinDamFile"
+        )
+        fixedweirfile: Optional[str] = Field(
+            "<*_fxw.pliz>, Polyline(s) x, y, z, z = fixed weir top levels (formerly fixed weir).",
+            alias="fixedWeirFile",
+        )
+        pillarfile: Optional[str] = Field(
+            "<*_pillar.pliz>, Polyline file containing four colums with x, y, diameter and Cd coefficient for bridge pillars.",
+            alias="pillarFile",
+        )
+        usecaching: Optional[str] = Field(
+            "Use caching for geometrical/network-related items (0: no, 1: yes) (section C.19).",
+            alias="useCaching",
+        )
+        vertplizfile: Optional[str] = Field(
+            "<*_vlay.pliz>), = pliz with x, y, Z, first Z = nr of layers, second Z = laytyp.",
+            alias="vertPlizFile",
+        )
+        frictfile: Optional[str] = Field(
+            "Location of the files with roughness data for 1D.",
+            alias="frictFile",
+        )
+        crossdeffile: Optional[str] = Field(
+            "Cross section definitions for all cross section shapes.",
+            alias="crossDefFile",
+        )
+        crosslocfile: Optional[str] = Field(
+            "Location definitions of the cross sections on a 1D network.",
+            alias="crossLocFile",
+        )
+        storagenodefile: Optional[str] = Field(
+            "File containing the specification of storage nodes and/or manholes to add extra storage to 1D models.",
+            alias="storageNodeFile",
+        )
+        oned2dlinkfile: Optional[str] = Field(
+            "File containing the custom parameterization of 1D-2D links.",
+            alias="1d2dLinkFile",
+        )
+        proflocfile: Optional[str] = Field(
+            "<*_proflocation.xyz>) x, y, z, z = profile refnumber.", alias="profLocFile"
+        )
+        profdeffile: Optional[str] = Field(
+            "<*_profdefinition.def>) definition for all profile nrs.",
+            alias="profDefFile",
+        )
+        profdefxyzfile: Optional[str] = Field(
+            "<*_profdefinition.def>) definition for all profile nrs.",
+            alias="profDefXyzFile",
+        )
+        manholefile: Optional[str] = Field(
+            "File containing manholes (e.g. <*.dat>).", alias="manholeFile"
+        )
+        partitionfile: Optional[str] = Field(
+            "<*_part.pol>, polyline(s) x, y.", alias="partitionFile"
+        )
+        uniformwidth1d: Optional[str] = Field(None, alias="uniformWidth1D")
+        dxwuimin2d: Optional[str] = Field(
+            "Smallest fraction dx/wu , set dx > Dxwuimin2D*wu",
+            alias="dxWuiMin2D",
+        )
+        waterlevini: Optional[str] = Field("Initial water level.", alias="waterLevIni")
+        bedlevuni: Optional[str] = Field(
+            "Uniform bed level [m], (only if bedlevtype>=3), used at missing z values in netfile.",
+            alias="bedLevUni",
+        )
+        bedslope: Optional[str] = Field(
+            "Bed slope inclination, sets zk = bedlevuni + x*bedslope ans sets zbndz = xbndz*bedslope.",
+            alias="bedSlope",
+        )
+        bedlevtype: Optional[str] = Field(
+            "1: at cell center (tiles xz,yz,bl,bob=max(bl)), 2: at face (tiles xu,yu,blu,bob=blu), 3: at face (using mean node values), 4: at face (using min node values), 5: at face (using max node values), 6: with bl based on node values.",
+            alias="bedLevType",
+        )
+        blmeanbelow: Optional[str] = Field(
+            "if not -999d0, below this level [m] the cell centre bedlevel is the mean of surrouding netnodes.",
+            alias="blMeanBelow",
+        )
+        blminabove: Optional[str] = Field(
+            "if not -999d0, above this level [m] the cell centre bedlevel is the min of surrouding netnodes.",
+            alias="blMinAbove",
+        )
+        anglat: Optional[str] = Field(
+            "Angle of latitude S-N [deg], 0=no Coriolis.", alias="angLat"
+        )
+        anglon: Optional[str] = Field(
+            "Angle of longitude E-W [deg], 0=Greenwich Mean Time.", alias="angLon"
+        )
+        conveyance2d: Optional[str] = Field(
+            "-1:R=HU, 0:R=H, 1:R=A/P, 2:K=analytic-1D conv, 3:K=analytic-2D conv.",
+            alias="conveyance2D",
+        )
+        nonlin1d: Optional[str] = Field(
+            "Non-linear 1D volumes, applicable for models with closed cross sections. 1=treat closed sections as partially open by using a Preissmann slot, 2=Nested Newton approach, 3=Partial Nested Newton approach.",
+            alias="nonlin1D",
+        )
+        nonlin2d: Optional[str] = Field(
+            "Non-linear 2D volumes, only i.c.m. ibedlevtype = 3 and Conveyance2D>=1.",
+            alias="nonlin2D",
+        )
+        sillheightmin: Optional[str] = Field(
+            "Fixed weir only active if both ground heights are larger than this value [m].",
+            alias="sillHeightMin",
+        )
+        makeorthocenters: Optional[str] = Field(
+            "(1: yes, 0: no) switch from circumcentres to orthocentres in geominit.",
+            alias="makeOrthoCenters",
+        )
+        dcenterinside: Optional[str] = Field(
+            "limit cell center; 1.0:in cell <-> 0.0:on c/g.", alias="dCenterInside"
+        )
+        bamin: Optional[str] = Field(
+            "Minimum grid cell area [m2], i.c.m. cutcells.", alias="baMin"
+        )
+        openboundarytolerance: Optional[str] = Field(
+            "Search tolerance factor between boundary polyline and grid cells. [Unit: in cell size units (i.e., not meters)].",
+            alias="openBoundaryTolerance",
+        )
+        renumberflownodes: Optional[str] = Field(
+            "Renumber the flow nodes (1: yes, 0: no).", alias="renumberFlowNodes"
+        )
+        kmx: Optional[str] = Field("Number of vertical layers.", alias="kmx")
+        layertype: Optional[str] = Field(
+            "Number of vertical layers.", alias="layerType"
+        )
+        numtopsig: Optional[str] = Field(
+            "Number of sigma-layers on top of z-layers.", alias="numTopSig"
+        )
+        numtopsiguniform: Optional[str] = Field(
+            "Spatially constant number of sigma layers above z-layers in a z-sigma model (1: yes, 0: no, spatially varying)",
+            alias="numTopSigUniform",
+        )
+        dztop: Optional[str] = Field(
+            "Z-layer thickness of layers above level Dztopuniabovez", alias="dzTop"
+        )
+        floorlevtoplay: Optional[str] = Field(
+            "Floor level of top layer", alias="floorLevTopLay"
+        )
+        dztopuniabovez: Optional[str] = Field(
+            "Above this level layers will have uniform dzTop, below we use sigmaGrowthFactor",
+            alias="dzTopUniAboveZ",
+        )
+        keepzlayeringatbed: Optional[str] = Field(
+            "0:possibly very thin layer at bed, 1:bedlayerthickness == zlayerthickness, 2=equal thickness first two layers",
+            alias="keepZLayeringAtBed",
+        )
+        sigmagrowthfactor: Optional[str] = Field(
+            "layer thickness growth factor from bed up.", alias="sigmaGrowthFactor"
+        )
+        dxdoubleat1dendnodes: Optional[str] = Field(
+            "Whether a 1D grid cell at the end of a network has to be extended with 0.5Î”x.",
+            alias="dxDoubleAt1DEndNodes",
+        )
+        changevelocityatstructures: Optional[str] = Field(
+            "Ignore structure dimensions for the velocity at hydraulic structures, when calculating the surrounding cell centered flow velocities.",
+            alias="changeVelocityAtStructures",
+        )
+        changestructuredimensions: Optional[str] = Field(
+            "Change the structure dimensions in case these are inconsistent with the channel dimensions.",
+            alias="changeStructureDimensions",
+        )
+
+    comments: Comments = Comments()
+
+    _disk_only_file_model_should_not_be_none = (
+        validator_set_default_disk_only_file_model_when_none()
+    )
+
+    _header: Literal["Geometry"] = "Geometry"
+    netfile: Optional[NetworkModel] = Field(
+        default_factory=NetworkModel, alias="netFile"
+    )
+    bathymetryfile: Optional[XYZModel] = Field(None, alias="bathymetryFile")
+    drypointsfile: Optional[List[Union[XYZModel, PolyFile]]] = Field(
+        None, alias="dryPointsFile"
+    )  # TODO Fix, this will always try XYZ first, alias="]")
+    structurefile: Optional[List[StructureModel]] = Field(
+        None, alias="structureFile", delimiter=";"
+    )
+    inifieldfile: Optional[IniFieldModel] = Field(None, alias="iniFieldFile")
+    waterlevinifile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="waterLevIniFile"
+    )
+    landboundaryfile: Optional[List[DiskOnlyFileModel]] = Field(
+        None, alias="landBoundaryFile"
+    )
+    thindamfile: Optional[List[PolyFile]] = Field(None, alias="thinDamFile")
+    fixedweirfile: Optional[List[PolyFile]] = Field(None, alias="fixedWeirFile")
+    pillarfile: Optional[List[PolyFile]] = Field(None, alias="pillarFile")
+    usecaching: bool = Field(False, alias="useCaching")
+    vertplizfile: Optional[PolyFile] = Field(None, alias="vertPlizFile")
+    frictfile: Optional[List[FrictionModel]] = Field(
+        None, alias="frictFile", delimiter=";"
+    )
+    crossdeffile: Optional[CrossDefModel] = Field(None, alias="crossDefFile")
+    crosslocfile: Optional[CrossLocModel] = Field(None, alias="crossLocFile")
+    storagenodefile: Optional[StorageNodeModel] = Field(None, alias="storageNodeFile")
+    oned2dlinkfile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="1d2dLinkFile"
+    )
+    proflocfile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="profLocFile"
+    )
+    profdeffile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="profDefFile"
+    )
+    profdefxyzfile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="profDefXyzFile"
+    )
+    manholefile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="manholeFile"
+    )
+    partitionfile: Optional[PolyFile] = Field(None, alias="partitionFile")
+    uniformwidth1d: float = Field(2.0, alias="uniformWidth1D")
+    dxwuimin2d: float = Field(0.0, alias="dxWuiMin2D")
+    waterlevini: float = Field(0.0, alias="waterLevIni")
+    bedlevuni: float = Field(-5.0, alias="bedLevUni")
+    bedslope: float = Field(0.0, alias="bedSlope")
+    bedlevtype: int = Field(3, alias="bedLevType")
+    blmeanbelow: float = Field(-999.0, alias="blMeanBelow")
+    blminabove: float = Field(-999.0, alias="blMinAbove")
+    anglat: float = Field(0.0, alias="angLat")
+    anglon: float = Field(0.0, alias="angLon")
+    conveyance2d: int = Field(-1, alias="conveyance2D")
+    nonlin1d: int = Field(1, alias="nonlin1D")
+    nonlin2d: int = Field(0, alias="nonlin2D")
+    sillheightmin: float = Field(0.0, alias="sillHeightMin")
+    makeorthocenters: bool = Field(False, alias="makeOrthoCenters")
+    dcenterinside: float = Field(1.0, alias="dCenterInside")
+    bamin: float = Field(1e-06, alias="baMin")
+    openboundarytolerance: float = Field(3.0, alias="openBoundaryTolerance")
+    renumberflownodes: bool = Field(True, alias="renumberFlowNodes")
+    kmx: int = Field(0, alias="kmx")
+    layertype: int = Field(1, alias="layerType")
+    numtopsig: int = Field(0, alias="numTopSig")
+    numtopsiguniform: bool = Field(True, alias="numTopSigUniform")
+    sigmagrowthfactor: float = Field(1.0, alias="sigmaGrowthFactor")
+    dztop: Optional[float] = Field(None, alias="dzTop")
+    floorlevtoplay: Optional[float] = Field(None, alias="floorLevTopLay")
+    dztopuniabovez: Optional[float] = Field(None, alias="dzTopUniAboveZ")
+    keepzlayeringatbed: int = Field(2, alias="keepZLayeringAtBed")
+    dxdoubleat1dendnodes: bool = Field(True, alias="dxDoubleAt1DEndNodes")
+    changevelocityatstructures: bool = Field(False, alias="changeVelocityAtStructures")
+    changestructuredimensions: bool = Field(True, alias="changeStructureDimensions")
+
+    _split_to_list = get_split_string_on_delimiter_validator(
+        "frictfile",
+        "structurefile",
+        "drypointsfile",
+        "landboundaryfile",
+        "thindamfile",
+        "fixedweirfile",
+        "pillarfile",
+    )
+
+    def is_intermediate_link(self) -> bool:
+        return True
+
+
+class Calibration(INIBasedModel):
+    """
+    The `[Calibration]` section in an MDU file.
+
+    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.calibration`.
+
+    All lowercased attributes match with the [Calibration] input as described in
+    [UM Sec.A.3](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.3).
+    """
+
+    class Comments(INIBasedModel.Comments):
+        usecalibration: Optional[str] = Field(
+            "Activate calibration factor friction multiplier (0: no, 1: yes).",
+            alias="UseCalibration",
+        )
+        definitionfile: Optional[str] = Field(
+            "File (*.cld) containing calibration definitions.",
+            alias="DefinitionFile",
+        )
+        areafile: Optional[str] = Field(
+            "File (*.cll) containing area distribution of calibration definitions.",
+            alias="AreaFile",
+        )
+
+    comments: Comments = Comments()
+
+    _disk_only_file_model_should_not_be_none = (
+        validator_set_default_disk_only_file_model_when_none()
+    )
+
+    _header: Literal["Calibration"] = "Calibration"
+    usecalibration: bool = Field(False, alias="UseCalibration")
+    definitionfile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="DefinitionFile"
+    )
+    areafile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="AreaFile"
+    )
+
+
+class InfiltrationMethod(IntEnum):
+    """
+    Enum class containing the valid values for the Infiltrationmodel
+    attribute in the [Groundwater][hydrolib.core.dflowfm.mdu.models.Groundwater] class.
+    """
+
+    NoInfiltration = 0
+    InterceptionLayer = 1
+    ConstantInfiltrationCapacity = 2
+    ModelUnsaturatedSaturated = 3
+    Horton = 4
+
+
+class GroundWater(INIBasedModel):
+    """
+    The `[Grw]` section in an MDU file.
+
+    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.grw`.
+
+    All lowercased attributes match with the [Grw] input as described in
+    [UM Sec.A.3](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.3).
+    """
+
+    class Comments(INIBasedModel.Comments):
+        groundwater: Optional[str] = Field(None, alias="GroundWater")
+        infiltrationmodel: Optional[str] = Field(
+            "Infiltration method (0: No infiltration, 1: Interception layer, 2: Constant infiltration capacity, 3: model unsaturated/saturated (with grw), 4: Horton).",
+            alias="Infiltrationmodel",
+        )
+        hinterceptionlayer: Optional[str] = Field(None, alias="Hinterceptionlayer")
+        unifinfiltrationcapacity: Optional[str] = Field(
+            "Uniform maximum infiltration capacity [m/s].",
+            alias="UnifInfiltrationCapacity",
+        )
+        conductivity: Optional[str] = Field(
+            "Non-dimensionless K conductivity   saturated (m/s), Q = K*A*i (m3/s)",
+            alias="Conductivity",
+        )
+        h_aquiferuni: Optional[str] = Field(
+            "bgrw = bl - h_aquiferuni (m), if negative, bgrw = bgrwuni.",
+            alias="h_aquiferuni",
+        )
+        bgrwuni: Optional[str] = Field(
+            "uniform level of impervious layer, only used if h_aquiferuni is negative.",
+            alias="bgrwuni",
+        )
+        h_unsatini: Optional[str] = Field(
+            "initial level groundwater is bedlevel - h_unsatini (m), if negative, sgrw = sgrwini.",
+            alias="h_unsatini",
+        )
+        sgrwini: Optional[str] = Field(
+            "Initial groundwater level, if h_unsatini < 0.", alias="sgrwini"
+        )
+
+    comments: Comments = Comments()
+    _header: Literal["Grw"] = "Grw"
+
+    groundwater: Optional[bool] = Field(False, alias="GroundWater")
+    infiltrationmodel: Optional[InfiltrationMethod] = Field(
+        InfiltrationMethod.NoInfiltration, alias="Infiltrationmodel"
+    )
+    hinterceptionlayer: Optional[float] = Field(None, alias="Hinterceptionlayer")
+    unifinfiltrationcapacity: Optional[float] = Field(
+        0.0, alias="UnifInfiltrationCapacity"
+    )
+    conductivity: Optional[float] = Field(0.0, alias="Conductivity")
+    h_aquiferuni: Optional[float] = Field(20.0, alias="h_aquiferuni")
+    bgrwuni: Optional[float] = Field(None, alias="bgrwuni")
+    h_unsatini: Optional[float] = Field(0.2, alias="h_unsatini")
+    sgrwini: Optional[float] = Field(None, alias="sgrwini")
+
+
+class ProcessFluxIntegration(IntEnum):
+    """
+    Enum class containing the valid values for the ProcessFluxIntegration
+    attribute in the [Processes][hydrolib.core.dflowfm.mdu.models.Processes] class.
+    """
+
+    WAQ = 1
+    DFlowFM = 2
+
+
+class Processes(INIBasedModel):
+    """
+    The `[Processes]` section in an MDU file.
+
+    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.processes`.
+
+    All lowercased attributes match with the [Processes] input as described in
+    [UM Sec.A.3](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.3).
+    """
+
+    class Comments(INIBasedModel.Comments):
+        substancefile: Optional[str] = Field(
+            "Substance file name.", alias="SubstanceFile"
+        )
+        additionalhistoryoutputfile: Optional[str] = Field(
+            "Extra history output filename.",
+            alias="AdditionalHistoryOutputFile",
+        )
+        statisticsfile: Optional[str] = Field(
+            "Statistics definition file.",
+            alias="StatisticsFile",
+        )
+        thetavertical: Optional[str] = Field(
+            "Theta value for vertical transport of water quality substances [-].",
+            alias="ThetaVertical",
+        )
+        dtprocesses: Optional[str] = Field(
+            "Waq processes time step [s]. Must be a multiple of DtUser. If DtProcesses is negative, water quality processes are calculated with every hydrodynamic time step.",
+            alias="DtProcesses",
+        )
+        dtmassbalance: Optional[str] = Field(None, alias="DtMassBalance")
+        processfluxintegration: Optional[str] = Field(
+            "Process fluxes integration option (1: WAQ, 2: D-Flow FM).",
+            alias="ProcessFluxIntegration",
+        )
+        wriwaqbot3doutput: Optional[str] = Field(
+            "Write 3D water quality bottom variables (0: no, 1: yes).",
+            alias="Wriwaqbot3Doutput",
+        )
+        volumedrythreshold: Optional[str] = Field(
+            "Volume [m3] below which segments are marked as dry.",
+            alias="VolumeDryThreshold",
+        )
+        depthdrythreshold: Optional[str] = Field(
+            "Water depth [m] below which segments are marked as dry.",
+            alias="DepthDryThreshold",
+        )
+
+    comments: Comments = Comments()
+
+    _disk_only_file_model_should_not_be_none = (
+        validator_set_default_disk_only_file_model_when_none()
+    )
+
+    _header: Literal["Processes"] = "Processes"
+
+    substancefile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="SubstanceFile"
+    )
+    additionalhistoryoutputfile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None),
+        alias="AdditionalHistoryOutputFile",
+    )
+    statisticsfile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="StatisticsFile"
+    )
+    thetavertical: Optional[float] = Field(0.0, alias="ThetaVertical")
+    dtprocesses: Optional[float] = Field(0.0, alias="DtProcesses")
+    dtmassbalance: Optional[float] = Field(0.0, alias="DtMassBalance")
+    processfluxintegration: Optional[ProcessFluxIntegration] = Field(
+        ProcessFluxIntegration.WAQ, alias="ProcessFluxIntegration"
+    )
+    wriwaqbot3doutput: Optional[bool] = Field(False, alias="Wriwaqbot3Doutput")
+    volumedrythreshold: Optional[float] = Field(1e-3, alias="VolumeDryThreshold")
+    depthdrythreshold: Optional[float] = Field(1e-3, alias="DepthDryThreshold")
+
+
+class ParticlesThreeDType(IntEnum):
+    """
+    Enum class containing the valid values for the 3Dtype
+    attribute in the `Particles` class.
+    """
+
+    DepthAveraged = 0
+    FreeSurface = 1
+
+
+class Particles(INIBasedModel):
+    """
+    The `[Particles]` section in an MDU file.
+
+    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.particles`.
+
+    All lowercased attributes match with the [Particles] input as described in
+    [UM Sec.A.3](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.3).
+    """
+
+    class Comments(INIBasedModel.Comments):
+        particlesfile: Optional[str] = Field(
+            "Initial particle locations file (*.xyz).", alias="ParticlesFile"
+        )
+        particlesreleasefile: Optional[str] = Field(
+            "Particles release file (*.tim, 4 column).", alias="ParticlesReleaseFile"
+        )
+        addtracer: Optional[str] = Field(
+            "Add tracer or not (0: no, 1: yes).", alias="AddTracer"
+        )
+        starttime: Optional[str] = Field("Start time (if > 0) [s]", alias="StartTime")
+        timestep: Optional[str] = Field(
+            "Time step (if > 0) or every computational time step [s].", alias="TimeStep"
+        )
+        threedtype: Optional[str] = Field(
+            "3D velocity type (0: depth averaged velocities, 1: free surface/top layer velocities).",
+            alias="3Dtype",
+        )
+
+    comments: Comments = Comments()
+    _disk_only_file_model_should_not_be_none = (
+        validator_set_default_disk_only_file_model_when_none()
+    )
+
+    _header: Literal["Particles"] = "Particles"
+
+    particlesfile: Optional[XYZModel] = Field(None, alias="ParticlesFile")
+    particlesreleasefile: DiskOnlyFileModel = Field(
+        default_factory=lambda: DiskOnlyFileModel(None), alias="ParticlesReleaseFile"
+    )
+    addtracer: Optional[bool] = Field(False, alias="AddTracer")
+    starttime: Optional[float] = Field(0.0, alias="StartTime")
+    timestep: Optional[float] = Field(0.0, alias="TimeStep")
+    threedtype: Optional[ParticlesThreeDType] = Field(
+        ParticlesThreeDType.DepthAveraged, alias="3Dtype"
+    )
+
+
+class VegetationModelNr(IntEnum):
+    """
+    Enum class containing the valid values for the VegetationModelNr
+    attribute in the [Vegetation][hydrolib.core.dflowfm.mdu.models.Vegetation] class.
+    """
+
+    No = 0
+    BaptistDFM = 1
+
+
+class Vegetation(INIBasedModel):
+    """
+    The `[Veg]` section in an MDU file.
+
+    This model is typically referenced under [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.veg`.
+
+    All lowercased attributes match with the [Veg] input as described in
+    [UM Sec.A.3](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#section.A.3).
+    """
+
+    class Comments(INIBasedModel.Comments):
+        vegetationmodelnr: Optional[str] = Field(
+            "Vegetation model nr, (0: no, 1: Baptist DFM).", alias="Vegetationmodelnr"
+        )
+        clveg: Optional[str] = Field("Stem distance factor [-].", alias="Clveg")
+        cdveg: Optional[str] = Field("Stem Cd coefficient [-].", alias="Cdveg")
+        cbveg: Optional[str] = Field("Stem stiffness coefficient [-].", alias="Cbveg")
+        rhoveg: Optional[str] = Field(
+            "Stem Rho, if > 0, bouyant stick procedure [kg/m3].", alias="Rhoveg"
+        )
+        stemheightstd: Optional[str] = Field(
+            "Stem height standard deviation fraction, e.g. 0.1 [-].",
+            alias="Stemheightstd",
+        )
+        densvegminbap: Optional[str] = Field(
+            "Minimum vegetation density in Baptist formula. Only in 2D. [1/m2].",
+            alias="Densvegminbap",
+        )
+
+    comments: Comments = Comments()
+    _header: Literal["Veg"] = "Veg"
+
+    vegetationmodelnr: Optional[VegetationModelNr] = Field(
+        VegetationModelNr.No, alias="Vegetationmodelnr"
+    )
+    clveg: Optional[float] = Field(0.8, alias="Clveg")
+    cdveg: Optional[float] = Field(0.7, alias="Cdveg")
+    cbveg: Optional[float] = Field(0.0, alias="Cbveg")
+    rhoveg: Optional[float] = Field(0.0, alias="Rhoveg")
+    stemheightstd: Optional[float] = Field(0.0, alias="Stemheightstd")
+    densvegminbap: Optional[float] = Field(0.0, alias="Densvegminbap")
+
+
+class FMModel(INIModel):
+    """
+    The overall FM model that contains the contents of the toplevel MDU file.
+
+    All lowercased attributes match with the supported "[section]"s as described in
+    [UM Sec.A](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#appendix.A).
+
+    Each of these class attributes refers to an underlying model class for that particular section.
+    """
+
+    general: General = Field(default_factory=General)
+    geometry: Geometry = Field(default_factory=Geometry)
+    volumetables: VolumeTables = Field(default_factory=VolumeTables)
+    numerics: Numerics = Field(default_factory=Numerics)
+    physics: Physics = Field(default_factory=Physics)
+    sediment: Sediment = Field(default_factory=Sediment)
+    wind: Wind = Field(default_factory=Wind)
+    waves: Optional[Waves] = None
+    time: Time = Field(default_factory=Time)
+    restart: Restart = Field(default_factory=Restart)
+    external_forcing: ExternalForcing = Field(default_factory=ExternalForcing)
+    hydrology: Hydrology = Field(default_factory=Hydrology)
+    trachytopes: Trachytopes = Field(default_factory=Trachytopes)
+    output: Output = Field(default_factory=Output)
+    calibration: Optional[Calibration] = Field(None)
+    grw: Optional[GroundWater] = Field(None)
+    processes: Optional[Processes] = Field(None)
+    particles: Optional[Particles] = Field(None)
+    veg: Optional[Vegetation] = Field(None)
+
+    serializer_config: INISerializerConfig = INISerializerConfig(
+        skip_empty_properties=False
+    )
+
+    @classmethod
+    def _ext(cls) -> str:
+        return ".mdu"
+
+    @classmethod
+    def _filename(cls) -> str:
+        return "fm"
+
+    @FileModel._relative_mode.getter
+    def _relative_mode(self) -> ResolveRelativeMode:
+        # This method overrides the _relative_mode property of the FileModel:
+        # The FMModel has a "special" feature which determines how relative filepaths
+        # should be resolved. When the field "pathsRelativeToParent" is set to False
+        # all relative paths should be resolved in respect to the parent directory of
+        # the mdu file. As such we need to explicitly set the resolve mode to ToAnchor
+        # when this attribute is set.
+
+        if not hasattr(self, "general") or self.general is None:
+            return ResolveRelativeMode.ToParent
+
+        if self.general.pathsrelativetoparent:
+            return ResolveRelativeMode.ToParent
+        else:
+            return ResolveRelativeMode.ToAnchor
+
+    @classmethod
+    def _get_relative_mode_from_data(cls, data: Dict[str, Any]) -> ResolveRelativeMode:
+        """Gets the ResolveRelativeMode of this FileModel based on the provided data.
+
+        The ResolveRelativeMode of the FMModel is determined by the
+        'pathsRelativeToParent' property of the 'General' category.
+
+        Args:
+            data (Dict[str, Any]):
+                The unvalidated/parsed data which is fed to the pydantic base model,
+                used to determine the ResolveRelativeMode.
+
+        Returns:
+            ResolveRelativeMode: The ResolveRelativeMode of this FileModel
+        """
+        if not (general := data.get("general", None)):
+            return ResolveRelativeMode.ToParent
+        if not (relative_to_parent := general.get("pathsrelativetoparent", None)):
+            return ResolveRelativeMode.ToParent
+
+        if relative_to_parent == "0":
+            return ResolveRelativeMode.ToAnchor
+        else:
+            return ResolveRelativeMode.ToParent
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/net/models.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/net/models.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,1280 +1,1280 @@
-from __future__ import annotations
-
-import logging
-from pathlib import Path
-from typing import Dict, List, Optional, Union
-
-import meshkernel as mk
-import netCDF4 as nc
-import numpy as np
-import numpy.typing as npt
-from meshkernel.py_structures import GeometryList
-from pydantic import Field
-
-from hydrolib.core import __version__
-from hydrolib.core.basemodel import (
-    BaseModel,
-    ModelSaveSettings,
-    ParsableFileModel,
-    file_load_context,
-)
-from hydrolib.core.dflowfm.net.reader import UgridReader
-from hydrolib.core.dflowfm.net.writer import UgridWriter
-
-logger = logging.getLogger(__name__)
-
-
-def split_by(gl: mk.GeometryList, by: float) -> list:
-    """Function to split mk.GeometryList by seperator.
-
-    Args:
-        gl (mk.GeometryList): The geometry list to split.
-        by (float): The value by which to split the gl.
-
-    Returns:
-        list: The split lists.
-    """
-    x, y = gl.x_coordinates.copy(), gl.y_coordinates.copy()
-    idx = np.where(x == by)[0]
-
-    xparts = np.split(x, idx)
-    yparts = np.split(y, idx)
-
-    lists = [
-        mk.GeometryList(xp[min(i, 1) :], yp[min(i, 1) :])
-        for i, (xp, yp) in enumerate(zip(xparts, yparts))
-    ]
-
-    return lists
-
-
-class Mesh2d(BaseModel):
-    """Mesh2d defines a single two dimensional grid.
-
-    Attributes:
-        meshkernel (mk.MeshKernel):
-            The meshkernel used to manimpulate this Mesh2d.
-        mesh2d_node_x (np.ndarray):
-            The node positions on the x-axis. Defaults to np.empty(0, dtype=np.double).
-        mesh2d_node_y (np.ndarray):
-            The node positions on the y-axis. Defaults to np.empty(0, dtype=np.double).
-        mesh2d_node_z (np.ndarray):
-            The node positions on the z-axis. Defaults to np.empty(0, dtype=np.double).
-        mesh2d_edge_x (np.ndarray):
-            The edge positions on the x-axis. Defaults to np.empty(0, dtype=np.double).
-        mesh2d_edge_y (np.ndarray):
-            The edge positions on the y-axis. Defaults to np.empty(0, dtype=np.double).
-        mesh2d_edge_z (np.ndarray):
-            The edge positions on the z-axis. Defaults to np.empty(0, dtype=np.double).
-        mesh2d_edge_nodes (np.ndarray):
-            The mapping of edges to node indices. Defaults to
-            np.empty((0, 2), dtype=np.int32).
-
-
-        mesh2d_face_x (np.ndarray):
-            The face positions on the x-axis. Defaults to np.empty(0, dtype=np.double).
-        mesh2d_face_y (np.ndarray):
-            The face positions on the y-axis. Defaults to np.empty(0, dtype=np.double).
-        mesh2d_face_z (np.ndarray):
-            The face positions on the z-axis. Defaults to np.empty(0, dtype=np.double).
-        mesh2d_face_nodes (np.ndarray):
-            The mapping of faces to node indices. Defaults to
-            np.empty((0, 0), dtype=np.int32)
-    """
-
-    meshkernel: mk.MeshKernel = Field(default_factory=mk.MeshKernel)
-
-    mesh2d_node_x: np.ndarray = Field(
-        default_factory=lambda: np.empty(0, dtype=np.double)
-    )
-    mesh2d_node_y: np.ndarray = Field(
-        default_factory=lambda: np.empty(0, dtype=np.double)
-    )
-    mesh2d_node_z: np.ndarray = Field(
-        default_factory=lambda: np.empty(0, dtype=np.double)
-    )
-
-    mesh2d_edge_x: np.ndarray = Field(
-        default_factory=lambda: np.empty(0, dtype=np.double)
-    )
-    mesh2d_edge_y: np.ndarray = Field(
-        default_factory=lambda: np.empty(0, dtype=np.double)
-    )
-    mesh2d_edge_z: np.ndarray = Field(
-        default_factory=lambda: np.empty(0, dtype=np.double)
-    )
-    mesh2d_edge_nodes: np.ndarray = Field(
-        default_factory=lambda: np.empty((0, 2), dtype=np.int32)
-    )
-
-    mesh2d_face_x: np.ndarray = Field(
-        default_factory=lambda: np.empty(0, dtype=np.double)
-    )
-    mesh2d_face_y: np.ndarray = Field(
-        default_factory=lambda: np.empty(0, dtype=np.double)
-    )
-    mesh2d_face_z: np.ndarray = Field(
-        default_factory=lambda: np.empty(0, dtype=np.double)
-    )
-    mesh2d_face_nodes: np.ndarray = Field(
-        default_factory=lambda: np.empty((0, 0), dtype=np.int32)
-    )
-
-    def is_empty(self) -> bool:
-        """Determine whether this Mesh2d is empty.
-
-        Returns:
-            (bool): Whether this Mesh2d is empty.
-        """
-        return self.mesh2d_node_x.size == 0
-
-    def read_file(self, file_path: Path) -> None:
-        """Read the Mesh2d from the file at file_path.
-
-        Args:
-            file_path (Path): Path to the file to be read.
-        """
-        reader = UgridReader(file_path)
-        reader.read_mesh2d(self)
-
-    def _set_mesh2d(self) -> None:
-        mesh2d = mk.Mesh2d(
-            node_x=self.mesh2d_node_x,
-            node_y=self.mesh2d_node_y,
-            edge_nodes=self.mesh2d_edge_nodes.ravel(),
-        )
-
-        self.meshkernel.mesh2d_set(mesh2d)
-
-    def get_mesh2d(self) -> mk.Mesh2d:
-        """Get the mesh2d as represented in the MeshKernel
-
-        Returns:
-            (mk.Mesh2d): The mesh2d as represented in the MeshKernel
-        """
-        return self.meshkernel.mesh2d_get()
-
-    def create_rectilinear(self, extent: tuple, dx: float, dy: float) -> None:
-        """Create a rectilinear mesh within a polygon. A rectangular grid is generated within the polygon bounds
-
-        Args:
-            extent (tuple): Bounding box of mesh (left, bottom, right, top)
-            dx (float): Horizontal distance
-            dy (float): Vertical distance
-
-        Raises:
-            NotImplementedError: MultiPolygons
-        """
-
-        xmin, ymin, xmax, ymax = extent
-
-        # Generate mesh
-        mesh2d_input = mk.Mesh2dFactory.create(
-            rows=int((ymax - ymin) / dy),
-            columns=int((xmax - xmin) / dx),
-            origin_x=xmin,
-            origin_y=ymin,
-            spacing_x=dx,
-            spacing_y=dy,
-        )
-
-        # Process
-        self._process(mesh2d_input)
-
-    def create_triangular(self, geometry_list: mk.GeometryList) -> None:
-        """Create triangular grid within GeometryList object
-
-        Args:
-            geometry_list (mk.GeometryList): GeometryList represeting a polygon within which the mesh is generated.
-        """
-        # Call meshkernel
-        self.meshkernel.mesh2d_make_mesh_from_polygon(geometry_list)
-
-        # Process new mesh
-        self._process(self.get_mesh2d())
-
-    def _process(self, mesh2d_input) -> None:
-        # Add input
-        self.meshkernel.mesh2d_set(mesh2d_input)
-        # Get output
-        mesh2d_output = self.meshkernel.mesh2d_get()
-        # Add to mesh2d variables
-        self.mesh2d_node_x = mesh2d_output.node_x
-        self.mesh2d_node_y = mesh2d_output.node_y
-
-        self.mesh2d_edge_x = mesh2d_output.edge_x
-        self.mesh2d_edge_y = mesh2d_output.edge_y
-        self.mesh2d_edge_nodes = mesh2d_output.edge_nodes.reshape((-1, 2))
-
-        self.mesh2d_face_x = mesh2d_output.face_x
-        self.mesh2d_face_y = mesh2d_output.face_y
-        npf = mesh2d_output.nodes_per_face
-        self.mesh2d_face_nodes = np.full(
-            (len(self.mesh2d_face_x), max(npf)), np.iinfo(np.int32).min
-        )
-        idx = (
-            np.ones_like(self.mesh2d_face_nodes) * np.arange(max(npf))[None, :]
-        ) < npf[:, None]
-        self.mesh2d_face_nodes[idx] = mesh2d_output.face_nodes
-
-    def clip(
-        self,
-        geometrylist: mk.GeometryList,
-        deletemeshoption: int = 1,
-        inside=False,
-    ) -> None:
-        """Clip the 2D mesh by a polygon. Both outside the exterior and inside the interiors is clipped
-
-        Args:
-            geometrylist (GeometryList): Polygon stored as GeometryList
-            deletemeshoption (int, optional): [description]. Defaults to 1.
-        """
-
-        # Add current mesh to Mesh2d instance
-        self._set_mesh2d()
-
-        deletemeshoption = mk.DeleteMeshOption(deletemeshoption)
-
-        # For clipping outside
-        if not inside:
-            # Check if a multipolygon was provided when clipping outside
-            if geometrylist.geometry_separator in geometrylist.x_coordinates:
-                raise NotImplementedError(
-                    "Deleting outside more than a single exterior (MultiPolygon) is not implemented."
-                )
-
-            # Get exterior and interiors
-            parts = split_by(geometrylist, geometrylist.inner_outer_separator)
-
-            exteriors = [parts[0]]
-            interiors = parts[1:]
-
-        # Inside
-        else:
-            # Check if any polygon contains holes, when clipping inside
-            if geometrylist.inner_outer_separator in geometrylist.x_coordinates:
-                raise NotImplementedError(
-                    "Deleting inside a (Multi)Polygon with holes is not implemented."
-                )
-
-            # Get exterior and interiors
-            parts = split_by(geometrylist, geometrylist.geometry_separator)
-
-            exteriors = parts[:]
-            interiors = []
-
-        # Check if parts are closed
-        for part in exteriors + interiors:
-            if (part.x_coordinates[0], part.y_coordinates[0]) != (
-                part.x_coordinates[-1],
-                part.y_coordinates[-1],
-            ):
-                raise ValueError(
-                    "First and last coordinate of each GeometryList part should match."
-                )
-
-        # Delete everything outside the (Multi)Polygon
-        for exterior in exteriors:
-            self.meshkernel.mesh2d_delete(
-                geometry_list=exterior,
-                delete_option=deletemeshoption,
-                invert_deletion=not inside,
-            )
-
-        # Delete all holes.
-        for interior in interiors:
-            self.meshkernel.mesh2d_delete(
-                geometry_list=interior,
-                delete_option=deletemeshoption,
-                invert_deletion=inside,
-            )
-
-        # Process
-        self._process(self.meshkernel.mesh2d_get())
-
-    def refine(self, polygon: mk.GeometryList, level: int):
-        """Refine the mesh within a polygon, by a number of steps (level)
-
-        Args:
-            polygon (GeometryList): Polygon in which to refine
-            level (int): Number of refinement steps
-        """
-        # Add current mesh to Mesh2d instance
-        mesh2d_input = mk.Mesh2d(
-            node_x=self.mesh2d_node_x,
-            node_y=self.mesh2d_node_y,
-            edge_nodes=self.mesh2d_edge_nodes.ravel(),
-        )
-        self.meshkernel.mesh2d_set(mesh2d_input)
-
-        # Check if parts are closed
-        # if not (polygon.x_coordinates[0], polygon.y_coordinates[0]) == (
-        #     polygon.x_coordinates[-1],
-        #     polygon.y_coordinates[-1],
-        # ):
-        #     raise ValueError("First and last coordinate of each GeometryList part should match.")
-
-        parameters = mk.MeshRefinementParameters(
-            refine_intersected=True,
-            use_mass_center_when_refining=False,
-            min_face_size=10.0,  # Does nothing?
-            refinement_type=1,  # No effect?
-            connect_hanging_nodes=True,
-            account_for_samples_outside_face=False,
-            max_refinement_iterations=level,
-        )
-        self.meshkernel.mesh2d_refine_based_on_polygon(polygon, parameters)
-
-        # Process
-        self._process(self.meshkernel.mesh2d_get())
-
-
-class Branch:
-    def __init__(
-        self,
-        geometry: np.ndarray,
-        branch_offsets: np.ndarray = None,
-        mask: np.ndarray = None,
-    ) -> None:
-        # Check that the array has two collumns (x and y)
-        assert geometry.shape[1] == 2
-
-        # Split in x and y
-        self.geometry = geometry
-        self._x_coordinates = geometry[:, 0]
-        self._y_coordinates = geometry[:, 1]
-
-        # Calculate distance of coordinates along line
-        segment_distances = np.hypot(
-            np.diff(self._x_coordinates), np.diff(self._y_coordinates)
-        )
-        self._distance = np.concatenate([[0], np.cumsum(segment_distances)])
-        self.length = segment_distances.sum()
-
-        # Check if mask and branch offsets (if both given) have same shape
-        if (
-            mask is not None
-            and branch_offsets is not None
-            and branch_offsets.shape != mask.shape
-        ):
-            raise ValueError("Mask and branch offset have different shape.")
-
-        # Set branch offsets
-        self.branch_offsets = branch_offsets
-        # Calculate node positions
-        if branch_offsets is not None:
-            self.node_xy = self.interpolate(branch_offsets)
-
-        # Set which of the nodes are present
-        if (mask is None) and (branch_offsets is not None):
-            self.mask = np.full(branch_offsets.shape, False)
-        else:
-            self.mask = mask
-
-    def generate_nodes(
-        self,
-        mesh1d_edge_length: float,
-        structure_chainage: Optional[List[float]] = None,
-        max_dist_to_struc: Optional[float] = None,
-    ):
-        """Generate the branch offsets and the nodes.
-
-        Args:
-            mesh1d_edge_length (float): The edge length of the 1d mesh.
-            structure_chainage (Optional[List[float]], optional): A list with the structure chainages. If not specified, calculation will not take it into account. Defaults to None.
-            max_dist_to_struc (Optional[float], optional): The maximum distance from a node to a structure. If not specified, calculation will not take it into account. Defaults to None.
-
-        Raises:
-            ValueError: Raised when any of the structure offsets, if specified, is smaller than zero.
-            ValueError: Raised when any of the structure offsets, if specified, is greater than the branch length.
-        """
-        # Generate offsets
-        self.branch_offsets = self._generate_offsets(
-            mesh1d_edge_length, structure_chainage, max_dist_to_struc
-        )
-        # Calculate node positions
-        self.node_xy = self.interpolate(self.branch_offsets)
-        # Add mask (all False)
-        self.mask = np.full(self.branch_offsets.shape, False)
-
-    def _generate_offsets(
-        self,
-        mesh1d_edge_length: float,
-        structure_offsets: Optional[List[float]] = None,
-        max_dist_to_struc: Optional[float] = None,
-    ) -> np.ndarray:
-        """Generate the branch offsets.
-
-        Args:
-            mesh1d_edge_length (float): The edge length of the 1d mesh.
-            structure_chainage (Optional[List[float]], optional): A list with the structure chainages. If not specified, calculation will not take it into account. Defaults to None.
-            max_dist_to_struc (Optional[float], optional): The maximum distance from a node to a structure. If not specified, calculation will not take it into account. Defaults to None.
-
-        Raises:
-            ValueError: Raised when any of the structure offsets, if specified, is smaller than zero.
-            ValueError: Raised when any of the structure offsets, if specified, is greater than the branch length.
-
-        Returns:
-            np.ndarray: The generated branch offsets.
-        """
-        # Generate initial offsets
-        anchor_pts = [0.0, self.length]
-        offsets = self._generate_1d_spacing(anchor_pts, mesh1d_edge_length)
-
-        if structure_offsets is None:
-            return offsets
-
-        # Check the limits
-        if (excess := min(structure_offsets)) < 0.0 or (
-            excess := max(structure_offsets)
-        ) > self.length:
-            raise ValueError(
-                f"Distance {excess} is outside the branch range (0.0 - {self.length})."
-            )
-
-        # Merge limits with start and end of branch
-        limits = [-1e-3] + list(sorted(structure_offsets)) + [self.length + 1e-3]
-
-        # if requested, check if the calculation point are close enough to the structures
-        if max_dist_to_struc is not None:
-            limits = self._generate_extended_limits(max_dist_to_struc, limits)
-
-        offsets = self._add_nodes_to_segments(
-            offsets, anchor_pts, limits, mesh1d_edge_length
-        )
-
-        return offsets
-
-    def _generate_extended_limits(
-        self, max_dist_to_struc: float, limits: List[float]
-    ) -> List[float]:
-        """Generate extended limits by taking into account the maximum distance to a structure.
-
-        Args:
-            max_dist_to_struc (float): The maximum distance from a node to a structure.
-            limits (List[float]): The limits.
-
-        Returns:
-            List[float]: A list with the updated limits.
-        """
-
-        additional = []
-
-        # Skip the first and the last, these are no structures
-        for i in range(1, len(limits) - 1):
-            # if the distance between two limits is large than twice the max distance to structure,
-            # the mesh point will be too far away. Add a limit on the minimum of half the length and
-            # two times the max distance
-            dist_to_prev_limit = limits[i] - (
-                max(additional[-1], limits[i - 1]) if any(additional) else limits[i - 1]
-            )
-            if dist_to_prev_limit > 2 * max_dist_to_struc:
-                additional.append(
-                    limits[i] - min(2 * max_dist_to_struc, dist_to_prev_limit / 2)
-                )
-
-            dist_to_next_limit = limits[i + 1] - limits[i]
-            if dist_to_next_limit > 2 * max_dist_to_struc:
-                additional.append(
-                    limits[i] + min(2 * max_dist_to_struc, dist_to_next_limit / 2)
-                )
-
-        # Join the limits
-        return sorted(limits + additional)
-
-    def _add_nodes_to_segments(
-        self,
-        offsets: np.ndarray,
-        anchor_pts: List[float],
-        limits: List[float],
-        mesh1d_edge_length: float,
-    ) -> np.ndarray:
-        """Add nodes to segments that are missing a mesh node.
-
-        Args:
-            offsets (np.ndarray): The branch offsets.
-            anchor_pts (List[float]): The anchor points.
-            limits (List[float]): The limits.
-            mesh1d_edge_length (float): The edge length of the 1d mesh.
-
-        Returns:
-            np.ndarray: The array with branch offsets.
-        """
-        # Get upper and lower limits
-        upper_limits = limits[1:]
-        lower_limits = limits[:-1]
-
-        def in_range():
-            return [
-                ((offsets > lower) & (offsets < upper)).any()
-                for lower, upper in zip(lower_limits, upper_limits)
-            ]
-
-        # Determine the segments that are missing a mesh node
-        # Anchor points are added on these segments, such that they will get a mesh node
-        nodes_in_range = in_range()
-
-        while not all(nodes_in_range):
-            # Get the index of the first segment without grid point
-            i = nodes_in_range.index(False)
-
-            # Add it to the anchor pts
-            anchor_pts.append((lower_limits[i] + upper_limits[i]) / 2.0)
-            anchor_pts = sorted(anchor_pts)
-
-            # Generate new offsets
-            offsets = self._generate_1d_spacing(anchor_pts, mesh1d_edge_length)
-
-            # Determine the segments that are missing a grid point
-            nodes_in_range = in_range()
-
-        if len(anchor_pts) > 2:
-            logger.info(
-                f"Added 1d mesh nodes on branch at: {anchor_pts}, due to the structures at {limits}."
-            )
-
-        return offsets
-
-    @staticmethod
-    def _generate_1d_spacing(
-        anchor_pts: List[float], mesh1d_edge_length: float
-    ) -> np.ndarray:
-        """
-        Generates 1d distances, called by function generate offsets
-        """
-        offsets = []
-        # Loop through anchor point pairs
-        for i in range(len(anchor_pts) - 1):
-            # Determine section length between anchor point
-            section_length = anchor_pts[i + 1] - anchor_pts[i]
-            if section_length <= 0.0:
-                raise ValueError("Section length must be larger than 0.0")
-            # Determine number of nodes
-            nnodes = max(2, int(round(section_length / mesh1d_edge_length) + 1)) - 1
-            # Add nodes
-            offsets.extend(
-                np.linspace(
-                    anchor_pts[i], anchor_pts[i + 1], nnodes, endpoint=False
-                ).tolist()
-            )
-        # Add last node
-        offsets.append(anchor_pts[-1])
-
-        return np.asarray(offsets)
-
-    def interpolate(self, distance: npt.ArrayLike) -> np.ndarray:
-        """Interpolate coordinates along branch by length
-
-        Args:
-            distance (npt.ArrayLike): Length
-        """
-        intpcoords = np.stack(
-            [
-                np.interp(distance, self._distance, self._x_coordinates),
-                np.interp(distance, self._distance, self._y_coordinates),
-            ],
-            axis=1,
-        )
-
-        return intpcoords
-
-
-class Link1d2d(BaseModel):
-    """Link1d2d defines the 1D2D Links of a model network.
-
-    Attributes:
-        meshkernel (Optional[mk.MeshKernel]):
-            The MeshKernel used to interact with this Link1d2d
-        link1d2d_id (np.ndarray):
-            The id of this Link1d2d
-        link1d2d_long_name (np.ndarray):
-            The long name of this Link1d2d
-        link1d2d_contact_type (np.ndarray):
-            The contact type of this Link1d2d
-        link1d2d (np.ndarray):
-            The underlying data object of this Link1d2d
-    """
-
-    meshkernel: mk.MeshKernel = Field(default_factory=mk.MeshKernel)
-
-    link1d2d_id: np.ndarray = Field(default_factory=lambda: np.empty(0, object))
-    link1d2d_long_name: np.ndarray = Field(default_factory=lambda: np.empty(0, object))
-    link1d2d_contact_type: np.ndarray = Field(
-        default_factory=lambda: np.empty(0, np.int32)
-    )
-    link1d2d: np.ndarray = Field(default_factory=lambda: np.empty((0, 2), np.int32))
-
-    def is_empty(self) -> bool:
-        """Whether this Link1d2d is currently empty.
-
-        Returns:
-            bool: True if the Link1d2d is currently empty; False otherwise.
-        """
-        return self.link1d2d.size == 0
-
-    def read_file(self, file_path: Path) -> None:
-        """Read the Link1d2d data from the specified netCDF file at file_path into this
-
-        Args:
-            file_path (Path): Path to the netCDF file.
-        """
-
-        reader = UgridReader(file_path)
-        reader.read_link1d2d(self)
-
-    def clear(self) -> None:
-        """Remove all saved links from the links administration"""
-        self.link1d2d_id = np.empty(0, object)
-        self.link1d2d_long_name = np.empty(0, object)
-        self.link1d2d_contact_type = np.empty(0, np.int32)
-        self.link1d2d = np.empty((0, 2), np.int32)
-        # The meshkernel object needs to be resetted
-        self.meshkernel._deallocate_state()
-        self.meshkernel._allocate_state(self.meshkernel.is_geographic)
-        self.meshkernel.contacts_get()
-
-    def _process(self) -> None:
-        """
-        Get links from meshkernel and add to the array with link administration
-        """
-        contacts = self.meshkernel.contacts_get()
-
-        self.link1d2d = np.append(
-            self.link1d2d,
-            np.stack([contacts.mesh1d_indices, contacts.mesh2d_indices], axis=1),
-            axis=0,
-        )
-        self.link1d2d_contact_type = np.append(
-            self.link1d2d_contact_type, np.full(contacts.mesh1d_indices.size, 3)
-        )
-        self.link1d2d_id = np.append(
-            self.link1d2d_id,
-            np.array([f"{n1d:d}_{f2d:d}" for n1d, f2d in self.link1d2d]),
-        )
-        self.link1d2d_long_name = np.append(
-            self.link1d2d_long_name,
-            np.array([f"{n1d:d}_{f2d:d}" for n1d, f2d in self.link1d2d]),
-        )
-
-    def _link_from_1d_to_2d(
-        self, node_mask: np.ndarray, polygon: mk.GeometryList = None
-    ):
-        """Connect 1d nodes to 2d face circumcenters. A list of branchid's can be given
-        to indicate where the 1d-side of the connections should be made. A polygon can
-        be given to indicate where the 2d-side of the connections should be made.
-
-        Note that the links are added to the already existing links. To remove these, use the method "clear".
-
-        Args:
-            node_mask (np.ndarray): Array indicating what 1d nodes should be connected. Defaults to None.
-            polygon (mk.GeometryList): Coordinates of the area within which the 2d side of the links are connected.
-        """
-
-        # Computes Mesh1d-Mesh2d contacts, where each single Mesh1d node is connected to one Mesh2d face circumcenter.
-        # The boundary nodes of Mesh1d (those sharing only one Mesh1d edge) are not connected to any Mesh2d face.
-        self.meshkernel.contacts_compute_single(node_mask=node_mask, polygons=polygon)
-        self._process()
-
-        # Note that the function "contacts_compute_multiple" also computes the connections, but does not take into account
-        # a bounding polygon or the end points of the 1d mesh.
-
-    def _link_from_2d_to_1d_embedded(
-        self, node_mask: np.ndarray, points: mk.GeometryList
-    ):
-        """"""
-        self.meshkernel.contacts_compute_with_points(node_mask=node_mask, points=points)
-        self._process()
-
-    def _link_from_2d_to_1d_lateral(
-        self,
-        node_mask: np.ndarray,
-        # boundary_face_xy: np.ndarray,
-        polygon: mk.GeometryList = None,
-        search_radius: float = None,
-    ):
-        # TODO: Missing value double for search radius?
-
-        # Computes Mesh1d-Mesh2d contacts, where Mesh1d nodes are connected to the closest Mesh2d faces at the boundary
-        self.meshkernel.contacts_compute_boundary(
-            node_mask=node_mask, polygons=polygon, search_radius=search_radius
-        )
-        self._process()
-
-
-class Mesh1d(BaseModel):
-    """"""
-
-    meshkernel: mk.MeshKernel = Field(default_factory=mk.MeshKernel)
-
-    branches: Dict[str, Branch] = {}
-
-    network1d_node_id: np.ndarray = Field(default_factory=lambda: np.empty(0, object))
-    network1d_node_long_name: np.ndarray = Field(
-        default_factory=lambda: np.empty(0, object)
-    )
-    network1d_node_x: np.ndarray = Field(default_factory=lambda: np.empty(0, np.double))
-    network1d_node_y: np.ndarray = Field(default_factory=lambda: np.empty(0, np.double))
-    network1d_branch_id: np.ndarray = Field(default_factory=lambda: np.empty(0, object))
-    network1d_branch_long_name: np.ndarray = Field(
-        default_factory=lambda: np.empty(0, object)
-    )
-    network1d_branch_length: np.ndarray = Field(
-        default_factory=lambda: np.empty(0, np.double)
-    )
-    network1d_branch_order: np.ndarray = Field(
-        default_factory=lambda: np.empty(0, np.int32)
-    )
-    network1d_edge_nodes: np.ndarray = Field(
-        default_factory=lambda: np.empty((0, 2), np.int32)
-    )
-    network1d_geom_x: np.ndarray = Field(default_factory=lambda: np.empty(0, np.double))
-    network1d_geom_y: np.ndarray = Field(default_factory=lambda: np.empty(0, np.double))
-    network1d_part_node_count: np.ndarray = Field(
-        default_factory=lambda: np.empty(0, np.int32)
-    )
-
-    mesh1d_node_x: np.ndarray = Field(default_factory=lambda: np.empty(0, np.double))
-    mesh1d_node_y: np.ndarray = Field(default_factory=lambda: np.empty(0, np.double))
-    mesh1d_node_id: np.ndarray = Field(default_factory=lambda: np.empty(0, object))
-    mesh1d_node_long_name: np.ndarray = Field(
-        default_factory=lambda: np.empty(0, object)
-    )
-    mesh1d_node_branch_id: np.ndarray = Field(
-        default_factory=lambda: np.empty(0, np.int32)
-    )
-    mesh1d_node_branch_offset: np.ndarray = Field(
-        default_factory=lambda: np.empty(0, np.double)
-    )
-
-    mesh1d_edge_nodes: np.ndarray = Field(
-        default_factory=lambda: np.empty((0, 2), np.int32)
-    )
-    mesh1d_edge_x: np.ndarray = Field(default_factory=lambda: np.empty(0, np.double))
-    mesh1d_edge_y: np.ndarray = Field(default_factory=lambda: np.empty(0, np.double))
-    mesh1d_edge_branch_id: np.ndarray = Field(
-        default_factory=lambda: np.empty(0, np.int32)
-    )
-    mesh1d_edge_branch_offset: np.ndarray = Field(
-        default_factory=lambda: np.empty(0, np.double)
-    )
-
-    def is_empty(self) -> bool:
-        return self.mesh1d_node_x.size == 0
-
-    def _get_mesh1d(self) -> mk.Mesh1d:
-        """Return mesh1d from meshkernel. Note that the meshkernel.Mesh1d instance
-        does not contain all mesh attributes that are contained in this class"""
-        return self.meshkernel.mesh1d_get()
-
-    def _set_mesh1d(self) -> None:
-        self.meshkernel.mesh1d_set(
-            mk.Mesh1d(
-                node_x=self.mesh1d_node_x.astype(np.float64),
-                node_y=self.mesh1d_node_y.astype(np.float64),
-                edge_nodes=self.mesh1d_edge_nodes.ravel().astype(np.int32),
-            )
-        )
-
-    def _process_network1d(self) -> None:
-        """
-        Determine x, y locations of mesh1d nodes based on the network1d
-        """
-        # Create a list of coordinates to create the branches from
-        ngeom = list(zip(self.network1d_geom_x, self.network1d_geom_y))
-
-        self.branches.clear()
-
-        for i, (name, nnodes) in enumerate(
-            zip(self.network1d_branch_id, self.network1d_part_node_count)
-        ):
-
-            # Create network branch
-            # Get geometry of branch from network geometry
-            geometry = np.array([ngeom.pop(0) for _ in range(nnodes)])
-            # Get branch offsets
-            idx = self.mesh1d_node_branch_id == i
-            branch_offsets = self.mesh1d_node_branch_offset[idx]
-            mask = np.full(branch_offsets.shape, False)
-
-            # Determine if a start or end coordinate needs to be added for constructing a complete branch
-            # As nodes are re-used, the last and first branch_offsets are often missing. However, they are still used
-            # for determining the length along the discretized branch.
-            if branch_offsets.size == 0 or not np.isclose(branch_offsets[0], 0.0):
-                branch_offsets = np.concatenate([[0], branch_offsets])
-                mask = np.concatenate([[True], mask])
-            length = np.hypot(*np.diff(geometry, axis=0).T).sum()
-            if not np.isclose(branch_offsets[-1], length):
-                branch_offsets = np.concatenate([branch_offsets, [length]])
-                mask = np.concatenate([mask, [True]])
-
-            # Create instance of branch object and add to dictionary
-            geo_branch = Branch(geometry, branch_offsets=branch_offsets, mask=mask)
-            self.branches[name.strip()] = geo_branch
-
-        # Convert list with all coordinates (except the appended ones for the schematized branches) to arrays
-        node_x, node_y = np.vstack(
-            [branch.node_xy[~branch.mask] for branch in self.branches.values()]
-        ).T
-
-        # Add to variables
-        self.mesh1d_node_x = node_x
-        self.mesh1d_node_y = node_y
-
-        # Calculate edge coordinates
-        edge_x, edge_y = np.vstack(
-            [
-                branch.interpolate(
-                    self.mesh1d_edge_branch_offset[self.mesh1d_edge_branch_id == i]
-                )
-                for i, branch in enumerate(self.branches.values())
-            ]
-        ).T
-
-        # Add to variables
-        self.mesh1d_edge_x = edge_x
-        self.mesh1d_edge_y = edge_y
-
-    def _network1d_node_position(self, x: float, y: float) -> Union[np.int32, None]:
-        """Determine the position (index) of a x, y coordinate in the network nodes
-
-        Args:
-            x (float): x-coordinate
-            y (float): y-coordinate
-
-        Returns:
-            Union[np.int32, None]: The index of the coordinate. None if not found
-        """
-        return self._node_position(self.network1d_node_x, self.network1d_node_y, x, y)
-
-    def _mesh1d_node_position(self, x: float, y: float) -> Union[np.int32, None]:
-        """Determine the position (index) of a x, y coordinate in the mesh nodes
-
-        Args:
-            x (float): x-coordinate
-            y (float): y-coordinate
-
-        Returns:
-            Union[np.int32, None]: The index of the coordinate. None if not found
-        """
-        return self._node_position(self.mesh1d_node_x, self.mesh1d_node_y, x, y)
-
-    def _node_position(
-        self, arrx: np.ndarray, arry: np.ndarray, x: float, y: float
-    ) -> Union[np.int32, None]:
-        """Determine the position (index) of a x, y coordinate in a given x and y array
-
-        Args:
-            arrx (np.ndarray): x-coordinates in which the position is sought
-            arry (np.ndarray): y-coordiantes in which the position is sought
-            x (float): x-coordinate to be sought
-            y (float): y-coordinate to be sought
-
-        Raises:
-            ValueError: If multiple positions are found for the coordinate
-
-        Returns:
-            Union[np.int32, None]: The index of the coordinate. None if not found
-        """
-        pos = np.where(np.isclose(arrx, x, rtol=0.0) & np.isclose(arry, y, rtol=0.0))[0]
-        if pos.size == 0:
-            return None
-        elif pos.size == 1:
-            return np.int32(pos[0])
-        else:
-            # Find the nearest
-            distance = np.hypot(arrx[pos] - x, arry[pos] - y)
-            if np.unique(distance).size == 1:
-                raise ValueError("Multiple nodes were found at the same position.")
-            else:
-                return np.int32(pos[np.argmin(distance)])
-
-    def _add_branch(
-        self,
-        branch: Branch,
-        name: str = None,
-        branch_order: int = -1,
-        long_name: str = None,
-        force_midpoint: bool = True,
-    ):
-        """Add the branch to mesh1d
-
-        Args:
-            branch (Branch): branch to add to the mesh1d
-            name (str): id of the branch
-            branch_order (int): interpolation order of the branch
-            long_name (str): long name of the branch
-            force_midpoint(bool): argument to control if a midpoint will be forced on the branch, use False for pipes
-
-        Returns:
-            Str: name of the branch.
-        """
-
-        # Check if branch had coordinate discretization
-        if branch.branch_offsets.size == 0:
-            raise ValueError(
-                'Branch has no mesh discretization. Use the function "generate_nodes" solve generate a 1d mesh on the branch.'
-            )
-
-        if name in self.network1d_branch_id:
-            raise KeyError(f'The branch name "{name}" is already used.')
-        if long_name in self.network1d_branch_long_name:
-            raise KeyError(f'The branch long name "{long_name}" is already used.')
-
-        branch_nr = len(self.network1d_branch_id)
-        if name is None:
-            name = f"br{branch_nr:05d}"
-        if long_name is None:
-            long_name = name
-
-        self.branches[name] = branch
-
-        # Add branch administration
-        self.network1d_branch_order = np.append(
-            self.network1d_branch_order, branch_order
-        )
-        self.network1d_branch_length = np.append(
-            self.network1d_branch_length, branch.length
-        )
-        self.network1d_branch_id = np.append(self.network1d_branch_id, name)
-        self.network1d_branch_long_name = np.append(
-            self.network1d_branch_long_name, long_name
-        )
-
-        # Add branch geometry coordinates
-        self.network1d_part_node_count = np.append(
-            self.network1d_part_node_count, len(branch.geometry)
-        )
-        self.network1d_geom_x = np.append(self.network1d_geom_x, branch._x_coordinates)
-        self.network1d_geom_y = np.append(self.network1d_geom_y, branch._y_coordinates)
-
-        # Network edge node administration
-        # -------------------------------
-
-        first_point = branch.geometry[0]
-        last_point = branch.geometry[-1]
-
-        # Get offsets from dictionary
-        offsets = branch.branch_offsets[:]
-        # The number of links on the branch
-        nlinks = len(offsets) - 1
-
-        # Check if the first and last point of the branch are already in the set
-        first_present = self._network1d_node_position(*first_point) is not None
-        if first_present:
-            # If present, remove from branch offsets
-            offsets = offsets[1:]
-            branch.mask[0] = True
-        else:
-            # If not present, add to network nodes
-            self.network1d_node_x = np.append(self.network1d_node_x, first_point[0])
-            self.network1d_node_y = np.append(self.network1d_node_y, first_point[1])
-
-            self.network1d_node_id = np.append(
-                self.network1d_node_id, "{:.6f}_{:.6f}".format(*first_point)
-            )
-            self.network1d_node_long_name = np.append(
-                self.network1d_node_long_name, "x={:.6f}_y={:.6f}".format(*first_point)
-            )
-
-        last_present = self._network1d_node_position(*last_point) is not None
-        if last_present:
-            # If present, remove from branch offsets
-            offsets = offsets[:-1]
-            branch.mask[-1] = True
-        else:
-            # If not present, add to network nodes
-            self.network1d_node_x = np.append(self.network1d_node_x, last_point[0])
-            self.network1d_node_y = np.append(self.network1d_node_y, last_point[1])
-
-            self.network1d_node_id = np.append(
-                self.network1d_node_id, "{:.6f}_{:.6f}".format(*last_point)
-            )
-            self.network1d_node_long_name = np.append(
-                self.network1d_node_long_name, "x={:.6f}_y={:.6f}".format(*last_point)
-            )
-
-        # If no points remain, add an extra halfway: each branch should have at least 1 node
-        # Adjust the branch object as well, by adding the extra point
-        if len(offsets) == 0 and force_midpoint:
-            # Add extra offset
-            extra_offset = branch.length / 2.0
-            offsets = np.array([extra_offset])
-            nlinks += 1
-            # Adjust branch object
-            branch.branch_offsets = np.insert(branch.branch_offsets, 1, extra_offset)
-            branch.node_xy = np.insert(
-                branch.node_xy, 1, branch.interpolate(offsets), axis=0
-            )
-            branch.mask = np.insert(branch.mask, 1, False)
-
-        # Get the index of the first and last node, add as edge_nodes
-        i_from = self._network1d_node_position(first_point[0], first_point[1])
-        i_to = self._network1d_node_position(last_point[0], last_point[1])
-        if i_from == i_to:
-            raise ValueError(
-                "Start and end node are the same. Ring geometries are not accepted."
-            )
-
-        self.network1d_edge_nodes = np.append(
-            self.network1d_edge_nodes,
-            np.array([[i_from, i_to]], dtype=np.int32),
-            axis=0,
-        )
-
-        # Mesh1d edge node administration
-
-        # -------------------------------
-        # First determine the start index. This is equal to the number of already present points
-        start_index = len(self.mesh1d_node_branch_id)
-        # For each link, create a new edge node connection
-        # If the first node is already present, subtract 1, since the first number will be substitud with the present node
-        if first_present:
-            start_index -= 1
-        new_edge_nodes = (
-            np.stack([np.arange(nlinks), np.arange(nlinks) + 1], axis=1) + start_index
-        ).astype(np.int32)
-
-        # If the first node is present, change the first point of the first edge to the existing point
-        if first_present:
-            new_edge_nodes[0, 0] = self._mesh1d_node_position(*first_point)
-        # If the last node is present, change the last point of the last edge too
-        if last_present:
-            new_edge_nodes[-1, 1] = self._mesh1d_node_position(*last_point)
-
-        # Add to variables
-        self.mesh1d_node_x = np.append(
-            self.mesh1d_node_x, branch.node_xy[~branch.mask, 0]
-        )
-        self.mesh1d_node_y = np.append(
-            self.mesh1d_node_y, branch.node_xy[~branch.mask, 1]
-        )
-
-        # Add to edge_nodes
-        self.mesh1d_edge_nodes = np.append(
-            self.mesh1d_edge_nodes, new_edge_nodes, axis=0
-        )
-        edge_coords = np.stack([self.mesh1d_node_x, self.mesh1d_node_y], axis=1)[
-            new_edge_nodes
-        ].mean(1)
-        edge_offsets = (branch.branch_offsets[:-1] + branch.branch_offsets[1:]) / 2
-
-        self.mesh1d_edge_branch_id = np.append(
-            self.mesh1d_edge_branch_id, np.full(len(edge_coords), branch_nr)
-        )
-        self.mesh1d_edge_branch_offset = np.append(
-            self.mesh1d_edge_branch_offset, edge_offsets
-        )
-
-        self.mesh1d_edge_x = np.append(self.mesh1d_edge_x, edge_coords[:, 0])
-        self.mesh1d_edge_y = np.append(self.mesh1d_edge_y, edge_coords[:, 1])
-
-        # Update names of nodes
-        mesh_point_names = np.array(
-            [f"{name}_{offset:.2f}" for offset in offsets], dtype=object
-        )
-        self.mesh1d_node_id = np.append(self.mesh1d_node_id, mesh_point_names)
-        self.mesh1d_node_long_name = np.append(
-            self.mesh1d_node_long_name, mesh_point_names
-        )
-
-        # Add mesh1d nodes
-        self.mesh1d_node_branch_id = np.append(
-            self.mesh1d_node_branch_id, np.full(len(offsets), branch_nr)
-        )
-        self.mesh1d_node_branch_offset = np.append(
-            self.mesh1d_node_branch_offset, offsets
-        )
-        return name
-
-    def get_node_mask(self, branchids: List[str] = None):
-        """Get node mask, give a mask with True for each node that is in the given branchid list"""
-
-        mask = np.full(self.mesh1d_node_id.shape, False, dtype=bool)
-        if branchids is None:
-            mask[:] = True
-            return mask
-
-        # Get number (index) of given branches
-        idx = np.where(np.isin(self.network1d_branch_id, branchids))[0]
-        if idx.size == 0:
-            raise KeyError("No branches corresponding to the given keys were found.")
-
-        mask[np.isin(self.mesh1d_node_branch_id, idx)] = True
-
-        return mask
-
-
-class Network:
-    def __init__(self, is_geographic: bool = False) -> None:
-        self.meshkernel = mk.MeshKernel(is_geographic=is_geographic)
-        # Monkeypatch the meshkernel object, because the "is_geographic" is not saved
-        # otherwise, and needed for reinitializing the meshkernel
-        self.meshkernel.is_geographic = is_geographic
-
-        self._mesh1d = Mesh1d(meshkernel=self.meshkernel)
-        self._mesh2d = Mesh2d(meshkernel=self.meshkernel)
-        self._link1d2d = Link1d2d(meshkernel=self.meshkernel)
-
-        # Spatial index (rtree)
-        # self._idx = index.Index()
-
-    @classmethod
-    def from_file(cls, file_path: Path) -> Network:
-        """Read network from file. This classmethod checks what mesh components (mesh1d & network1d, mesh2d, link1d2d) are
-        present, and loads them one by one.
-
-        Args:
-            file_path (Path): path to netcdf file with network data
-
-        Returns:
-            Network: The instance of the class itself that is returned
-        """
-
-        network = cls()
-        ds = nc.Dataset(file_path)  # type: ignore[import]
-
-        reader = UgridReader(file_path)
-
-        reader.read_mesh1d_network1d(network._mesh1d)
-        reader.read_mesh2d(network._mesh2d)
-        reader.read_link1d2d(network._link1d2d)
-
-        ds.close()
-
-        return network
-
-    def to_file(self, file: Path) -> None:
-        """Write network to file
-
-        Args:
-            file (Path): File where _net.nc is written to.
-        """
-
-        writer = UgridWriter()
-        writer.write(self, file)
-
-    def link1d2d_from_1d_to_2d(
-        self, branchids: List[str] = None, polygon: GeometryList = None
-    ) -> None:
-        self._mesh1d._set_mesh1d()
-        self._mesh2d._set_mesh2d()
-
-        node_mask = self._mesh1d.get_node_mask(branchids)
-        if polygon is None:
-            polygon = self.meshkernel.mesh2d_get_mesh_boundaries_as_polygons()
-
-        self._link1d2d._link_from_1d_to_2d(node_mask, polygon=polygon)
-
-    def mesh2d_create_rectilinear_within_extent(
-        self, extent: tuple, dx: float, dy: float
-    ) -> None:
-        self._mesh2d.create_rectilinear(extent=extent, dx=dx, dy=dy)
-
-    def mesh2d_create_triangular_within_polygon(self, polygon: mk.GeometryList) -> None:
-        """Create triangular grid within GeometryList object. Calls _mesh2d.create_triangular
-        directly, but is easier accessible for users.
-
-        Args:
-            polygon (mk.GeometryList): GeometryList representing a polygon within which the mesh is generated.
-        """
-        self._mesh2d.create_triangular(geometry_list=polygon)
-
-    def mesh2d_clip_mesh(
-        self,
-        geometrylist: mk.GeometryList,
-        deletemeshoption: mk.DeleteMeshOption = mk.DeleteMeshOption.ALL_FACE_CIRCUMCENTERS,
-        inside=True,
-    ) -> None:
-        self._mesh2d.clip(
-            geometrylist=geometrylist,
-            deletemeshoption=deletemeshoption,
-            inside=inside,
-        )
-
-    def mesh2d_refine_mesh(self, polygon: mk.GeometryList, level: int = 1) -> None:
-        self._mesh2d.refine(polygon=polygon, level=level)
-
-    def mesh1d_add_branch(
-        self,
-        branch: Branch,
-        name: str = None,
-        branch_order: int = -1,
-        long_name: str = None,
-        force_midpoint: bool = True,
-    ) -> None:
-        name = self._mesh1d._add_branch(
-            branch=branch,
-            name=name,
-            branch_order=branch_order,
-            long_name=long_name,
-            force_midpoint=force_midpoint,
-        )
-        return name
-
-
-class NetworkModel(ParsableFileModel):
-    """Network model representation."""
-
-    network: Network = Field(default_factory=Network)
-
-    def _post_init_load(self) -> None:
-        """
-        Load the network file if the filepath exists relative to the
-        current FileLoadContext.
-        """
-        super()._post_init_load()
-
-        if self.filepath is None:
-            return
-
-        with file_load_context() as context:
-            network_path = context.resolve(self.filepath)
-
-            if network_path.is_file():
-                self.network = Network.from_file(network_path)
-
-    @property
-    def _mesh1d(self):
-        return self.network._mesh1d
-
-    @property
-    def _mesh2d(self):
-        return self.network._mesh2d
-
-    @property
-    def _link1d2d(self):
-        return self.network._link1d2d
-
-    @classmethod
-    def _ext(cls) -> str:
-        return ".nc"
-
-    @classmethod
-    def _filename(cls) -> str:
-        return "network"
-
-    def _save(self, save_settings: ModelSaveSettings):
-        with file_load_context() as context:
-            write_path = context.resolve(self.filepath)  # type: ignore[arg-type]
-
-            write_path.parent.mkdir(parents=True, exist_ok=True)
-            self.network.to_file(write_path)
-
-    def _export(self, folder: Path) -> None:
-        filename = Path(self.filepath.name) if self.filepath else self._generate_name()
-        self.filepath = folder / filename
-        folder.mkdir(parents=True, exist_ok=True)
-        self.network.to_file(self.filepath)
-
-    def _parse(self, _):
-        return {}
-
-    @classmethod
-    def _get_serializer(cls):
-        # Unused, but requires abstract implementation
-        pass
-
-    @classmethod
-    def _get_parser(cls):
-        # Unused, but requires abstract implementation
-        pass
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+from typing import Dict, List, Optional, Union
+
+import meshkernel as mk
+import netCDF4 as nc
+import numpy as np
+import numpy.typing as npt
+from meshkernel.py_structures import GeometryList
+from pydantic import Field
+
+from hydrolib.core import __version__
+from hydrolib.core.basemodel import (
+    BaseModel,
+    ModelSaveSettings,
+    ParsableFileModel,
+    file_load_context,
+)
+from hydrolib.core.dflowfm.net.reader import UgridReader
+from hydrolib.core.dflowfm.net.writer import UgridWriter
+
+logger = logging.getLogger(__name__)
+
+
+def split_by(gl: mk.GeometryList, by: float) -> list:
+    """Function to split mk.GeometryList by seperator.
+
+    Args:
+        gl (mk.GeometryList): The geometry list to split.
+        by (float): The value by which to split the gl.
+
+    Returns:
+        list: The split lists.
+    """
+    x, y = gl.x_coordinates.copy(), gl.y_coordinates.copy()
+    idx = np.where(x == by)[0]
+
+    xparts = np.split(x, idx)
+    yparts = np.split(y, idx)
+
+    lists = [
+        mk.GeometryList(xp[min(i, 1) :], yp[min(i, 1) :])
+        for i, (xp, yp) in enumerate(zip(xparts, yparts))
+    ]
+
+    return lists
+
+
+class Mesh2d(BaseModel):
+    """Mesh2d defines a single two dimensional grid.
+
+    Attributes:
+        meshkernel (mk.MeshKernel):
+            The meshkernel used to manimpulate this Mesh2d.
+        mesh2d_node_x (np.ndarray):
+            The node positions on the x-axis. Defaults to np.empty(0, dtype=np.double).
+        mesh2d_node_y (np.ndarray):
+            The node positions on the y-axis. Defaults to np.empty(0, dtype=np.double).
+        mesh2d_node_z (np.ndarray):
+            The node positions on the z-axis. Defaults to np.empty(0, dtype=np.double).
+        mesh2d_edge_x (np.ndarray):
+            The edge positions on the x-axis. Defaults to np.empty(0, dtype=np.double).
+        mesh2d_edge_y (np.ndarray):
+            The edge positions on the y-axis. Defaults to np.empty(0, dtype=np.double).
+        mesh2d_edge_z (np.ndarray):
+            The edge positions on the z-axis. Defaults to np.empty(0, dtype=np.double).
+        mesh2d_edge_nodes (np.ndarray):
+            The mapping of edges to node indices. Defaults to
+            np.empty((0, 2), dtype=np.int32).
+
+
+        mesh2d_face_x (np.ndarray):
+            The face positions on the x-axis. Defaults to np.empty(0, dtype=np.double).
+        mesh2d_face_y (np.ndarray):
+            The face positions on the y-axis. Defaults to np.empty(0, dtype=np.double).
+        mesh2d_face_z (np.ndarray):
+            The face positions on the z-axis. Defaults to np.empty(0, dtype=np.double).
+        mesh2d_face_nodes (np.ndarray):
+            The mapping of faces to node indices. Defaults to
+            np.empty((0, 0), dtype=np.int32)
+    """
+
+    meshkernel: mk.MeshKernel = Field(default_factory=mk.MeshKernel)
+
+    mesh2d_node_x: np.ndarray = Field(
+        default_factory=lambda: np.empty(0, dtype=np.double)
+    )
+    mesh2d_node_y: np.ndarray = Field(
+        default_factory=lambda: np.empty(0, dtype=np.double)
+    )
+    mesh2d_node_z: np.ndarray = Field(
+        default_factory=lambda: np.empty(0, dtype=np.double)
+    )
+
+    mesh2d_edge_x: np.ndarray = Field(
+        default_factory=lambda: np.empty(0, dtype=np.double)
+    )
+    mesh2d_edge_y: np.ndarray = Field(
+        default_factory=lambda: np.empty(0, dtype=np.double)
+    )
+    mesh2d_edge_z: np.ndarray = Field(
+        default_factory=lambda: np.empty(0, dtype=np.double)
+    )
+    mesh2d_edge_nodes: np.ndarray = Field(
+        default_factory=lambda: np.empty((0, 2), dtype=np.int32)
+    )
+
+    mesh2d_face_x: np.ndarray = Field(
+        default_factory=lambda: np.empty(0, dtype=np.double)
+    )
+    mesh2d_face_y: np.ndarray = Field(
+        default_factory=lambda: np.empty(0, dtype=np.double)
+    )
+    mesh2d_face_z: np.ndarray = Field(
+        default_factory=lambda: np.empty(0, dtype=np.double)
+    )
+    mesh2d_face_nodes: np.ndarray = Field(
+        default_factory=lambda: np.empty((0, 0), dtype=np.int32)
+    )
+
+    def is_empty(self) -> bool:
+        """Determine whether this Mesh2d is empty.
+
+        Returns:
+            (bool): Whether this Mesh2d is empty.
+        """
+        return self.mesh2d_node_x.size == 0
+
+    def read_file(self, file_path: Path) -> None:
+        """Read the Mesh2d from the file at file_path.
+
+        Args:
+            file_path (Path): Path to the file to be read.
+        """
+        reader = UgridReader(file_path)
+        reader.read_mesh2d(self)
+
+    def _set_mesh2d(self) -> None:
+        mesh2d = mk.Mesh2d(
+            node_x=self.mesh2d_node_x,
+            node_y=self.mesh2d_node_y,
+            edge_nodes=self.mesh2d_edge_nodes.ravel(),
+        )
+
+        self.meshkernel.mesh2d_set(mesh2d)
+
+    def get_mesh2d(self) -> mk.Mesh2d:
+        """Get the mesh2d as represented in the MeshKernel
+
+        Returns:
+            (mk.Mesh2d): The mesh2d as represented in the MeshKernel
+        """
+        return self.meshkernel.mesh2d_get()
+
+    def create_rectilinear(self, extent: tuple, dx: float, dy: float) -> None:
+        """Create a rectilinear mesh within a polygon. A rectangular grid is generated within the polygon bounds
+
+        Args:
+            extent (tuple): Bounding box of mesh (left, bottom, right, top)
+            dx (float): Horizontal distance
+            dy (float): Vertical distance
+
+        Raises:
+            NotImplementedError: MultiPolygons
+        """
+
+        xmin, ymin, xmax, ymax = extent
+
+        # Generate mesh
+        mesh2d_input = mk.Mesh2dFactory.create(
+            rows=int((ymax - ymin) / dy),
+            columns=int((xmax - xmin) / dx),
+            origin_x=xmin,
+            origin_y=ymin,
+            spacing_x=dx,
+            spacing_y=dy,
+        )
+
+        # Process
+        self._process(mesh2d_input)
+
+    def create_triangular(self, geometry_list: mk.GeometryList) -> None:
+        """Create triangular grid within GeometryList object
+
+        Args:
+            geometry_list (mk.GeometryList): GeometryList represeting a polygon within which the mesh is generated.
+        """
+        # Call meshkernel
+        self.meshkernel.mesh2d_make_mesh_from_polygon(geometry_list)
+
+        # Process new mesh
+        self._process(self.get_mesh2d())
+
+    def _process(self, mesh2d_input) -> None:
+        # Add input
+        self.meshkernel.mesh2d_set(mesh2d_input)
+        # Get output
+        mesh2d_output = self.meshkernel.mesh2d_get()
+        # Add to mesh2d variables
+        self.mesh2d_node_x = mesh2d_output.node_x
+        self.mesh2d_node_y = mesh2d_output.node_y
+
+        self.mesh2d_edge_x = mesh2d_output.edge_x
+        self.mesh2d_edge_y = mesh2d_output.edge_y
+        self.mesh2d_edge_nodes = mesh2d_output.edge_nodes.reshape((-1, 2))
+
+        self.mesh2d_face_x = mesh2d_output.face_x
+        self.mesh2d_face_y = mesh2d_output.face_y
+        npf = mesh2d_output.nodes_per_face
+        self.mesh2d_face_nodes = np.full(
+            (len(self.mesh2d_face_x), max(npf)), np.iinfo(np.int32).min
+        )
+        idx = (
+            np.ones_like(self.mesh2d_face_nodes) * np.arange(max(npf))[None, :]
+        ) < npf[:, None]
+        self.mesh2d_face_nodes[idx] = mesh2d_output.face_nodes
+
+    def clip(
+        self,
+        geometrylist: mk.GeometryList,
+        deletemeshoption: int = 1,
+        inside=False,
+    ) -> None:
+        """Clip the 2D mesh by a polygon. Both outside the exterior and inside the interiors is clipped
+
+        Args:
+            geometrylist (GeometryList): Polygon stored as GeometryList
+            deletemeshoption (int, optional): [description]. Defaults to 1.
+        """
+
+        # Add current mesh to Mesh2d instance
+        self._set_mesh2d()
+
+        deletemeshoption = mk.DeleteMeshOption(deletemeshoption)
+
+        # For clipping outside
+        if not inside:
+            # Check if a multipolygon was provided when clipping outside
+            if geometrylist.geometry_separator in geometrylist.x_coordinates:
+                raise NotImplementedError(
+                    "Deleting outside more than a single exterior (MultiPolygon) is not implemented."
+                )
+
+            # Get exterior and interiors
+            parts = split_by(geometrylist, geometrylist.inner_outer_separator)
+
+            exteriors = [parts[0]]
+            interiors = parts[1:]
+
+        # Inside
+        else:
+            # Check if any polygon contains holes, when clipping inside
+            if geometrylist.inner_outer_separator in geometrylist.x_coordinates:
+                raise NotImplementedError(
+                    "Deleting inside a (Multi)Polygon with holes is not implemented."
+                )
+
+            # Get exterior and interiors
+            parts = split_by(geometrylist, geometrylist.geometry_separator)
+
+            exteriors = parts[:]
+            interiors = []
+
+        # Check if parts are closed
+        for part in exteriors + interiors:
+            if (part.x_coordinates[0], part.y_coordinates[0]) != (
+                part.x_coordinates[-1],
+                part.y_coordinates[-1],
+            ):
+                raise ValueError(
+                    "First and last coordinate of each GeometryList part should match."
+                )
+
+        # Delete everything outside the (Multi)Polygon
+        for exterior in exteriors:
+            self.meshkernel.mesh2d_delete(
+                geometry_list=exterior,
+                delete_option=deletemeshoption,
+                invert_deletion=not inside,
+            )
+
+        # Delete all holes.
+        for interior in interiors:
+            self.meshkernel.mesh2d_delete(
+                geometry_list=interior,
+                delete_option=deletemeshoption,
+                invert_deletion=inside,
+            )
+
+        # Process
+        self._process(self.meshkernel.mesh2d_get())
+
+    def refine(self, polygon: mk.GeometryList, level: int):
+        """Refine the mesh within a polygon, by a number of steps (level)
+
+        Args:
+            polygon (GeometryList): Polygon in which to refine
+            level (int): Number of refinement steps
+        """
+        # Add current mesh to Mesh2d instance
+        mesh2d_input = mk.Mesh2d(
+            node_x=self.mesh2d_node_x,
+            node_y=self.mesh2d_node_y,
+            edge_nodes=self.mesh2d_edge_nodes.ravel(),
+        )
+        self.meshkernel.mesh2d_set(mesh2d_input)
+
+        # Check if parts are closed
+        # if not (polygon.x_coordinates[0], polygon.y_coordinates[0]) == (
+        #     polygon.x_coordinates[-1],
+        #     polygon.y_coordinates[-1],
+        # ):
+        #     raise ValueError("First and last coordinate of each GeometryList part should match.")
+
+        parameters = mk.MeshRefinementParameters(
+            refine_intersected=True,
+            use_mass_center_when_refining=False,
+            min_face_size=10.0,  # Does nothing?
+            refinement_type=1,  # No effect?
+            connect_hanging_nodes=True,
+            account_for_samples_outside_face=False,
+            max_refinement_iterations=level,
+        )
+        self.meshkernel.mesh2d_refine_based_on_polygon(polygon, parameters)
+
+        # Process
+        self._process(self.meshkernel.mesh2d_get())
+
+
+class Branch:
+    def __init__(
+        self,
+        geometry: np.ndarray,
+        branch_offsets: np.ndarray = None,
+        mask: np.ndarray = None,
+    ) -> None:
+        # Check that the array has two collumns (x and y)
+        assert geometry.shape[1] == 2
+
+        # Split in x and y
+        self.geometry = geometry
+        self._x_coordinates = geometry[:, 0]
+        self._y_coordinates = geometry[:, 1]
+
+        # Calculate distance of coordinates along line
+        segment_distances = np.hypot(
+            np.diff(self._x_coordinates), np.diff(self._y_coordinates)
+        )
+        self._distance = np.concatenate([[0], np.cumsum(segment_distances)])
+        self.length = segment_distances.sum()
+
+        # Check if mask and branch offsets (if both given) have same shape
+        if (
+            mask is not None
+            and branch_offsets is not None
+            and branch_offsets.shape != mask.shape
+        ):
+            raise ValueError("Mask and branch offset have different shape.")
+
+        # Set branch offsets
+        self.branch_offsets = branch_offsets
+        # Calculate node positions
+        if branch_offsets is not None:
+            self.node_xy = self.interpolate(branch_offsets)
+
+        # Set which of the nodes are present
+        if (mask is None) and (branch_offsets is not None):
+            self.mask = np.full(branch_offsets.shape, False)
+        else:
+            self.mask = mask
+
+    def generate_nodes(
+        self,
+        mesh1d_edge_length: float,
+        structure_chainage: Optional[List[float]] = None,
+        max_dist_to_struc: Optional[float] = None,
+    ):
+        """Generate the branch offsets and the nodes.
+
+        Args:
+            mesh1d_edge_length (float): The edge length of the 1d mesh.
+            structure_chainage (Optional[List[float]], optional): A list with the structure chainages. If not specified, calculation will not take it into account. Defaults to None.
+            max_dist_to_struc (Optional[float], optional): The maximum distance from a node to a structure. If not specified, calculation will not take it into account. Defaults to None.
+
+        Raises:
+            ValueError: Raised when any of the structure offsets, if specified, is smaller than zero.
+            ValueError: Raised when any of the structure offsets, if specified, is greater than the branch length.
+        """
+        # Generate offsets
+        self.branch_offsets = self._generate_offsets(
+            mesh1d_edge_length, structure_chainage, max_dist_to_struc
+        )
+        # Calculate node positions
+        self.node_xy = self.interpolate(self.branch_offsets)
+        # Add mask (all False)
+        self.mask = np.full(self.branch_offsets.shape, False)
+
+    def _generate_offsets(
+        self,
+        mesh1d_edge_length: float,
+        structure_offsets: Optional[List[float]] = None,
+        max_dist_to_struc: Optional[float] = None,
+    ) -> np.ndarray:
+        """Generate the branch offsets.
+
+        Args:
+            mesh1d_edge_length (float): The edge length of the 1d mesh.
+            structure_chainage (Optional[List[float]], optional): A list with the structure chainages. If not specified, calculation will not take it into account. Defaults to None.
+            max_dist_to_struc (Optional[float], optional): The maximum distance from a node to a structure. If not specified, calculation will not take it into account. Defaults to None.
+
+        Raises:
+            ValueError: Raised when any of the structure offsets, if specified, is smaller than zero.
+            ValueError: Raised when any of the structure offsets, if specified, is greater than the branch length.
+
+        Returns:
+            np.ndarray: The generated branch offsets.
+        """
+        # Generate initial offsets
+        anchor_pts = [0.0, self.length]
+        offsets = self._generate_1d_spacing(anchor_pts, mesh1d_edge_length)
+
+        if structure_offsets is None:
+            return offsets
+
+        # Check the limits
+        if (excess := min(structure_offsets)) < 0.0 or (
+            excess := max(structure_offsets)
+        ) > self.length:
+            raise ValueError(
+                f"Distance {excess} is outside the branch range (0.0 - {self.length})."
+            )
+
+        # Merge limits with start and end of branch
+        limits = [-1e-3] + list(sorted(structure_offsets)) + [self.length + 1e-3]
+
+        # if requested, check if the calculation point are close enough to the structures
+        if max_dist_to_struc is not None:
+            limits = self._generate_extended_limits(max_dist_to_struc, limits)
+
+        offsets = self._add_nodes_to_segments(
+            offsets, anchor_pts, limits, mesh1d_edge_length
+        )
+
+        return offsets
+
+    def _generate_extended_limits(
+        self, max_dist_to_struc: float, limits: List[float]
+    ) -> List[float]:
+        """Generate extended limits by taking into account the maximum distance to a structure.
+
+        Args:
+            max_dist_to_struc (float): The maximum distance from a node to a structure.
+            limits (List[float]): The limits.
+
+        Returns:
+            List[float]: A list with the updated limits.
+        """
+
+        additional = []
+
+        # Skip the first and the last, these are no structures
+        for i in range(1, len(limits) - 1):
+            # if the distance between two limits is large than twice the max distance to structure,
+            # the mesh point will be too far away. Add a limit on the minimum of half the length and
+            # two times the max distance
+            dist_to_prev_limit = limits[i] - (
+                max(additional[-1], limits[i - 1]) if any(additional) else limits[i - 1]
+            )
+            if dist_to_prev_limit > 2 * max_dist_to_struc:
+                additional.append(
+                    limits[i] - min(2 * max_dist_to_struc, dist_to_prev_limit / 2)
+                )
+
+            dist_to_next_limit = limits[i + 1] - limits[i]
+            if dist_to_next_limit > 2 * max_dist_to_struc:
+                additional.append(
+                    limits[i] + min(2 * max_dist_to_struc, dist_to_next_limit / 2)
+                )
+
+        # Join the limits
+        return sorted(limits + additional)
+
+    def _add_nodes_to_segments(
+        self,
+        offsets: np.ndarray,
+        anchor_pts: List[float],
+        limits: List[float],
+        mesh1d_edge_length: float,
+    ) -> np.ndarray:
+        """Add nodes to segments that are missing a mesh node.
+
+        Args:
+            offsets (np.ndarray): The branch offsets.
+            anchor_pts (List[float]): The anchor points.
+            limits (List[float]): The limits.
+            mesh1d_edge_length (float): The edge length of the 1d mesh.
+
+        Returns:
+            np.ndarray: The array with branch offsets.
+        """
+        # Get upper and lower limits
+        upper_limits = limits[1:]
+        lower_limits = limits[:-1]
+
+        def in_range():
+            return [
+                ((offsets > lower) & (offsets < upper)).any()
+                for lower, upper in zip(lower_limits, upper_limits)
+            ]
+
+        # Determine the segments that are missing a mesh node
+        # Anchor points are added on these segments, such that they will get a mesh node
+        nodes_in_range = in_range()
+
+        while not all(nodes_in_range):
+            # Get the index of the first segment without grid point
+            i = nodes_in_range.index(False)
+
+            # Add it to the anchor pts
+            anchor_pts.append((lower_limits[i] + upper_limits[i]) / 2.0)
+            anchor_pts = sorted(anchor_pts)
+
+            # Generate new offsets
+            offsets = self._generate_1d_spacing(anchor_pts, mesh1d_edge_length)
+
+            # Determine the segments that are missing a grid point
+            nodes_in_range = in_range()
+
+        if len(anchor_pts) > 2:
+            logger.info(
+                f"Added 1d mesh nodes on branch at: {anchor_pts}, due to the structures at {limits}."
+            )
+
+        return offsets
+
+    @staticmethod
+    def _generate_1d_spacing(
+        anchor_pts: List[float], mesh1d_edge_length: float
+    ) -> np.ndarray:
+        """
+        Generates 1d distances, called by function generate offsets
+        """
+        offsets = []
+        # Loop through anchor point pairs
+        for i in range(len(anchor_pts) - 1):
+            # Determine section length between anchor point
+            section_length = anchor_pts[i + 1] - anchor_pts[i]
+            if section_length <= 0.0:
+                raise ValueError("Section length must be larger than 0.0")
+            # Determine number of nodes
+            nnodes = max(2, int(round(section_length / mesh1d_edge_length) + 1)) - 1
+            # Add nodes
+            offsets.extend(
+                np.linspace(
+                    anchor_pts[i], anchor_pts[i + 1], nnodes, endpoint=False
+                ).tolist()
+            )
+        # Add last node
+        offsets.append(anchor_pts[-1])
+
+        return np.asarray(offsets)
+
+    def interpolate(self, distance: npt.ArrayLike) -> np.ndarray:
+        """Interpolate coordinates along branch by length
+
+        Args:
+            distance (npt.ArrayLike): Length
+        """
+        intpcoords = np.stack(
+            [
+                np.interp(distance, self._distance, self._x_coordinates),
+                np.interp(distance, self._distance, self._y_coordinates),
+            ],
+            axis=1,
+        )
+
+        return intpcoords
+
+
+class Link1d2d(BaseModel):
+    """Link1d2d defines the 1D2D Links of a model network.
+
+    Attributes:
+        meshkernel (Optional[mk.MeshKernel]):
+            The MeshKernel used to interact with this Link1d2d
+        link1d2d_id (np.ndarray):
+            The id of this Link1d2d
+        link1d2d_long_name (np.ndarray):
+            The long name of this Link1d2d
+        link1d2d_contact_type (np.ndarray):
+            The contact type of this Link1d2d
+        link1d2d (np.ndarray):
+            The underlying data object of this Link1d2d
+    """
+
+    meshkernel: mk.MeshKernel = Field(default_factory=mk.MeshKernel)
+
+    link1d2d_id: np.ndarray = Field(default_factory=lambda: np.empty(0, object))
+    link1d2d_long_name: np.ndarray = Field(default_factory=lambda: np.empty(0, object))
+    link1d2d_contact_type: np.ndarray = Field(
+        default_factory=lambda: np.empty(0, np.int32)
+    )
+    link1d2d: np.ndarray = Field(default_factory=lambda: np.empty((0, 2), np.int32))
+
+    def is_empty(self) -> bool:
+        """Whether this Link1d2d is currently empty.
+
+        Returns:
+            bool: True if the Link1d2d is currently empty; False otherwise.
+        """
+        return self.link1d2d.size == 0
+
+    def read_file(self, file_path: Path) -> None:
+        """Read the Link1d2d data from the specified netCDF file at file_path into this
+
+        Args:
+            file_path (Path): Path to the netCDF file.
+        """
+
+        reader = UgridReader(file_path)
+        reader.read_link1d2d(self)
+
+    def clear(self) -> None:
+        """Remove all saved links from the links administration"""
+        self.link1d2d_id = np.empty(0, object)
+        self.link1d2d_long_name = np.empty(0, object)
+        self.link1d2d_contact_type = np.empty(0, np.int32)
+        self.link1d2d = np.empty((0, 2), np.int32)
+        # The meshkernel object needs to be resetted
+        self.meshkernel._deallocate_state()
+        self.meshkernel._allocate_state(self.meshkernel.is_geographic)
+        self.meshkernel.contacts_get()
+
+    def _process(self) -> None:
+        """
+        Get links from meshkernel and add to the array with link administration
+        """
+        contacts = self.meshkernel.contacts_get()
+
+        self.link1d2d = np.append(
+            self.link1d2d,
+            np.stack([contacts.mesh1d_indices, contacts.mesh2d_indices], axis=1),
+            axis=0,
+        )
+        self.link1d2d_contact_type = np.append(
+            self.link1d2d_contact_type, np.full(contacts.mesh1d_indices.size, 3)
+        )
+        self.link1d2d_id = np.append(
+            self.link1d2d_id,
+            np.array([f"{n1d:d}_{f2d:d}" for n1d, f2d in self.link1d2d]),
+        )
+        self.link1d2d_long_name = np.append(
+            self.link1d2d_long_name,
+            np.array([f"{n1d:d}_{f2d:d}" for n1d, f2d in self.link1d2d]),
+        )
+
+    def _link_from_1d_to_2d(
+        self, node_mask: np.ndarray, polygon: mk.GeometryList = None
+    ):
+        """Connect 1d nodes to 2d face circumcenters. A list of branchid's can be given
+        to indicate where the 1d-side of the connections should be made. A polygon can
+        be given to indicate where the 2d-side of the connections should be made.
+
+        Note that the links are added to the already existing links. To remove these, use the method "clear".
+
+        Args:
+            node_mask (np.ndarray): Array indicating what 1d nodes should be connected. Defaults to None.
+            polygon (mk.GeometryList): Coordinates of the area within which the 2d side of the links are connected.
+        """
+
+        # Computes Mesh1d-Mesh2d contacts, where each single Mesh1d node is connected to one Mesh2d face circumcenter.
+        # The boundary nodes of Mesh1d (those sharing only one Mesh1d edge) are not connected to any Mesh2d face.
+        self.meshkernel.contacts_compute_single(node_mask=node_mask, polygons=polygon)
+        self._process()
+
+        # Note that the function "contacts_compute_multiple" also computes the connections, but does not take into account
+        # a bounding polygon or the end points of the 1d mesh.
+
+    def _link_from_2d_to_1d_embedded(
+        self, node_mask: np.ndarray, points: mk.GeometryList
+    ):
+        """"""
+        self.meshkernel.contacts_compute_with_points(node_mask=node_mask, points=points)
+        self._process()
+
+    def _link_from_2d_to_1d_lateral(
+        self,
+        node_mask: np.ndarray,
+        # boundary_face_xy: np.ndarray,
+        polygon: mk.GeometryList = None,
+        search_radius: float = None,
+    ):
+        # TODO: Missing value double for search radius?
+
+        # Computes Mesh1d-Mesh2d contacts, where Mesh1d nodes are connected to the closest Mesh2d faces at the boundary
+        self.meshkernel.contacts_compute_boundary(
+            node_mask=node_mask, polygons=polygon, search_radius=search_radius
+        )
+        self._process()
+
+
+class Mesh1d(BaseModel):
+    """"""
+
+    meshkernel: mk.MeshKernel = Field(default_factory=mk.MeshKernel)
+
+    branches: Dict[str, Branch] = {}
+
+    network1d_node_id: np.ndarray = Field(default_factory=lambda: np.empty(0, object))
+    network1d_node_long_name: np.ndarray = Field(
+        default_factory=lambda: np.empty(0, object)
+    )
+    network1d_node_x: np.ndarray = Field(default_factory=lambda: np.empty(0, np.double))
+    network1d_node_y: np.ndarray = Field(default_factory=lambda: np.empty(0, np.double))
+    network1d_branch_id: np.ndarray = Field(default_factory=lambda: np.empty(0, object))
+    network1d_branch_long_name: np.ndarray = Field(
+        default_factory=lambda: np.empty(0, object)
+    )
+    network1d_branch_length: np.ndarray = Field(
+        default_factory=lambda: np.empty(0, np.double)
+    )
+    network1d_branch_order: np.ndarray = Field(
+        default_factory=lambda: np.empty(0, np.int32)
+    )
+    network1d_edge_nodes: np.ndarray = Field(
+        default_factory=lambda: np.empty((0, 2), np.int32)
+    )
+    network1d_geom_x: np.ndarray = Field(default_factory=lambda: np.empty(0, np.double))
+    network1d_geom_y: np.ndarray = Field(default_factory=lambda: np.empty(0, np.double))
+    network1d_part_node_count: np.ndarray = Field(
+        default_factory=lambda: np.empty(0, np.int32)
+    )
+
+    mesh1d_node_x: np.ndarray = Field(default_factory=lambda: np.empty(0, np.double))
+    mesh1d_node_y: np.ndarray = Field(default_factory=lambda: np.empty(0, np.double))
+    mesh1d_node_id: np.ndarray = Field(default_factory=lambda: np.empty(0, object))
+    mesh1d_node_long_name: np.ndarray = Field(
+        default_factory=lambda: np.empty(0, object)
+    )
+    mesh1d_node_branch_id: np.ndarray = Field(
+        default_factory=lambda: np.empty(0, np.int32)
+    )
+    mesh1d_node_branch_offset: np.ndarray = Field(
+        default_factory=lambda: np.empty(0, np.double)
+    )
+
+    mesh1d_edge_nodes: np.ndarray = Field(
+        default_factory=lambda: np.empty((0, 2), np.int32)
+    )
+    mesh1d_edge_x: np.ndarray = Field(default_factory=lambda: np.empty(0, np.double))
+    mesh1d_edge_y: np.ndarray = Field(default_factory=lambda: np.empty(0, np.double))
+    mesh1d_edge_branch_id: np.ndarray = Field(
+        default_factory=lambda: np.empty(0, np.int32)
+    )
+    mesh1d_edge_branch_offset: np.ndarray = Field(
+        default_factory=lambda: np.empty(0, np.double)
+    )
+
+    def is_empty(self) -> bool:
+        return self.mesh1d_node_x.size == 0
+
+    def _get_mesh1d(self) -> mk.Mesh1d:
+        """Return mesh1d from meshkernel. Note that the meshkernel.Mesh1d instance
+        does not contain all mesh attributes that are contained in this class"""
+        return self.meshkernel.mesh1d_get()
+
+    def _set_mesh1d(self) -> None:
+        self.meshkernel.mesh1d_set(
+            mk.Mesh1d(
+                node_x=self.mesh1d_node_x.astype(np.float64),
+                node_y=self.mesh1d_node_y.astype(np.float64),
+                edge_nodes=self.mesh1d_edge_nodes.ravel().astype(np.int32),
+            )
+        )
+
+    def _process_network1d(self) -> None:
+        """
+        Determine x, y locations of mesh1d nodes based on the network1d
+        """
+        # Create a list of coordinates to create the branches from
+        ngeom = list(zip(self.network1d_geom_x, self.network1d_geom_y))
+
+        self.branches.clear()
+
+        for i, (name, nnodes) in enumerate(
+            zip(self.network1d_branch_id, self.network1d_part_node_count)
+        ):
+
+            # Create network branch
+            # Get geometry of branch from network geometry
+            geometry = np.array([ngeom.pop(0) for _ in range(nnodes)])
+            # Get branch offsets
+            idx = self.mesh1d_node_branch_id == i
+            branch_offsets = self.mesh1d_node_branch_offset[idx]
+            mask = np.full(branch_offsets.shape, False)
+
+            # Determine if a start or end coordinate needs to be added for constructing a complete branch
+            # As nodes are re-used, the last and first branch_offsets are often missing. However, they are still used
+            # for determining the length along the discretized branch.
+            if branch_offsets.size == 0 or not np.isclose(branch_offsets[0], 0.0):
+                branch_offsets = np.concatenate([[0], branch_offsets])
+                mask = np.concatenate([[True], mask])
+            length = np.hypot(*np.diff(geometry, axis=0).T).sum()
+            if not np.isclose(branch_offsets[-1], length):
+                branch_offsets = np.concatenate([branch_offsets, [length]])
+                mask = np.concatenate([mask, [True]])
+
+            # Create instance of branch object and add to dictionary
+            geo_branch = Branch(geometry, branch_offsets=branch_offsets, mask=mask)
+            self.branches[name.strip()] = geo_branch
+
+        # Convert list with all coordinates (except the appended ones for the schematized branches) to arrays
+        node_x, node_y = np.vstack(
+            [branch.node_xy[~branch.mask] for branch in self.branches.values()]
+        ).T
+
+        # Add to variables
+        self.mesh1d_node_x = node_x
+        self.mesh1d_node_y = node_y
+
+        # Calculate edge coordinates
+        edge_x, edge_y = np.vstack(
+            [
+                branch.interpolate(
+                    self.mesh1d_edge_branch_offset[self.mesh1d_edge_branch_id == i]
+                )
+                for i, branch in enumerate(self.branches.values())
+            ]
+        ).T
+
+        # Add to variables
+        self.mesh1d_edge_x = edge_x
+        self.mesh1d_edge_y = edge_y
+
+    def _network1d_node_position(self, x: float, y: float) -> Union[np.int32, None]:
+        """Determine the position (index) of a x, y coordinate in the network nodes
+
+        Args:
+            x (float): x-coordinate
+            y (float): y-coordinate
+
+        Returns:
+            Union[np.int32, None]: The index of the coordinate. None if not found
+        """
+        return self._node_position(self.network1d_node_x, self.network1d_node_y, x, y)
+
+    def _mesh1d_node_position(self, x: float, y: float) -> Union[np.int32, None]:
+        """Determine the position (index) of a x, y coordinate in the mesh nodes
+
+        Args:
+            x (float): x-coordinate
+            y (float): y-coordinate
+
+        Returns:
+            Union[np.int32, None]: The index of the coordinate. None if not found
+        """
+        return self._node_position(self.mesh1d_node_x, self.mesh1d_node_y, x, y)
+
+    def _node_position(
+        self, arrx: np.ndarray, arry: np.ndarray, x: float, y: float
+    ) -> Union[np.int32, None]:
+        """Determine the position (index) of a x, y coordinate in a given x and y array
+
+        Args:
+            arrx (np.ndarray): x-coordinates in which the position is sought
+            arry (np.ndarray): y-coordiantes in which the position is sought
+            x (float): x-coordinate to be sought
+            y (float): y-coordinate to be sought
+
+        Raises:
+            ValueError: If multiple positions are found for the coordinate
+
+        Returns:
+            Union[np.int32, None]: The index of the coordinate. None if not found
+        """
+        pos = np.where(np.isclose(arrx, x, rtol=0.0) & np.isclose(arry, y, rtol=0.0))[0]
+        if pos.size == 0:
+            return None
+        elif pos.size == 1:
+            return np.int32(pos[0])
+        else:
+            # Find the nearest
+            distance = np.hypot(arrx[pos] - x, arry[pos] - y)
+            if np.unique(distance).size == 1:
+                raise ValueError("Multiple nodes were found at the same position.")
+            else:
+                return np.int32(pos[np.argmin(distance)])
+
+    def _add_branch(
+        self,
+        branch: Branch,
+        name: str = None,
+        branch_order: int = -1,
+        long_name: str = None,
+        force_midpoint: bool = True,
+    ):
+        """Add the branch to mesh1d
+
+        Args:
+            branch (Branch): branch to add to the mesh1d
+            name (str): id of the branch
+            branch_order (int): interpolation order of the branch
+            long_name (str): long name of the branch
+            force_midpoint(bool): argument to control if a midpoint will be forced on the branch, use False for pipes
+
+        Returns:
+            Str: name of the branch.
+        """
+
+        # Check if branch had coordinate discretization
+        if branch.branch_offsets.size == 0:
+            raise ValueError(
+                'Branch has no mesh discretization. Use the function "generate_nodes" solve generate a 1d mesh on the branch.'
+            )
+
+        if name in self.network1d_branch_id:
+            raise KeyError(f'The branch name "{name}" is already used.')
+        if long_name in self.network1d_branch_long_name:
+            raise KeyError(f'The branch long name "{long_name}" is already used.')
+
+        branch_nr = len(self.network1d_branch_id)
+        if name is None:
+            name = f"br{branch_nr:05d}"
+        if long_name is None:
+            long_name = name
+
+        self.branches[name] = branch
+
+        # Add branch administration
+        self.network1d_branch_order = np.append(
+            self.network1d_branch_order, branch_order
+        )
+        self.network1d_branch_length = np.append(
+            self.network1d_branch_length, branch.length
+        )
+        self.network1d_branch_id = np.append(self.network1d_branch_id, name)
+        self.network1d_branch_long_name = np.append(
+            self.network1d_branch_long_name, long_name
+        )
+
+        # Add branch geometry coordinates
+        self.network1d_part_node_count = np.append(
+            self.network1d_part_node_count, len(branch.geometry)
+        )
+        self.network1d_geom_x = np.append(self.network1d_geom_x, branch._x_coordinates)
+        self.network1d_geom_y = np.append(self.network1d_geom_y, branch._y_coordinates)
+
+        # Network edge node administration
+        # -------------------------------
+
+        first_point = branch.geometry[0]
+        last_point = branch.geometry[-1]
+
+        # Get offsets from dictionary
+        offsets = branch.branch_offsets[:]
+        # The number of links on the branch
+        nlinks = len(offsets) - 1
+
+        # Check if the first and last point of the branch are already in the set
+        first_present = self._network1d_node_position(*first_point) is not None
+        if first_present:
+            # If present, remove from branch offsets
+            offsets = offsets[1:]
+            branch.mask[0] = True
+        else:
+            # If not present, add to network nodes
+            self.network1d_node_x = np.append(self.network1d_node_x, first_point[0])
+            self.network1d_node_y = np.append(self.network1d_node_y, first_point[1])
+
+            self.network1d_node_id = np.append(
+                self.network1d_node_id, "{:.6f}_{:.6f}".format(*first_point)
+            )
+            self.network1d_node_long_name = np.append(
+                self.network1d_node_long_name, "x={:.6f}_y={:.6f}".format(*first_point)
+            )
+
+        last_present = self._network1d_node_position(*last_point) is not None
+        if last_present:
+            # If present, remove from branch offsets
+            offsets = offsets[:-1]
+            branch.mask[-1] = True
+        else:
+            # If not present, add to network nodes
+            self.network1d_node_x = np.append(self.network1d_node_x, last_point[0])
+            self.network1d_node_y = np.append(self.network1d_node_y, last_point[1])
+
+            self.network1d_node_id = np.append(
+                self.network1d_node_id, "{:.6f}_{:.6f}".format(*last_point)
+            )
+            self.network1d_node_long_name = np.append(
+                self.network1d_node_long_name, "x={:.6f}_y={:.6f}".format(*last_point)
+            )
+
+        # If no points remain, add an extra halfway: each branch should have at least 1 node
+        # Adjust the branch object as well, by adding the extra point
+        if len(offsets) == 0 and force_midpoint:
+            # Add extra offset
+            extra_offset = branch.length / 2.0
+            offsets = np.array([extra_offset])
+            nlinks += 1
+            # Adjust branch object
+            branch.branch_offsets = np.insert(branch.branch_offsets, 1, extra_offset)
+            branch.node_xy = np.insert(
+                branch.node_xy, 1, branch.interpolate(offsets), axis=0
+            )
+            branch.mask = np.insert(branch.mask, 1, False)
+
+        # Get the index of the first and last node, add as edge_nodes
+        i_from = self._network1d_node_position(first_point[0], first_point[1])
+        i_to = self._network1d_node_position(last_point[0], last_point[1])
+        if i_from == i_to:
+            raise ValueError(
+                "Start and end node are the same. Ring geometries are not accepted."
+            )
+
+        self.network1d_edge_nodes = np.append(
+            self.network1d_edge_nodes,
+            np.array([[i_from, i_to]], dtype=np.int32),
+            axis=0,
+        )
+
+        # Mesh1d edge node administration
+
+        # -------------------------------
+        # First determine the start index. This is equal to the number of already present points
+        start_index = len(self.mesh1d_node_branch_id)
+        # For each link, create a new edge node connection
+        # If the first node is already present, subtract 1, since the first number will be substitud with the present node
+        if first_present:
+            start_index -= 1
+        new_edge_nodes = (
+            np.stack([np.arange(nlinks), np.arange(nlinks) + 1], axis=1) + start_index
+        ).astype(np.int32)
+
+        # If the first node is present, change the first point of the first edge to the existing point
+        if first_present:
+            new_edge_nodes[0, 0] = self._mesh1d_node_position(*first_point)
+        # If the last node is present, change the last point of the last edge too
+        if last_present:
+            new_edge_nodes[-1, 1] = self._mesh1d_node_position(*last_point)
+
+        # Add to variables
+        self.mesh1d_node_x = np.append(
+            self.mesh1d_node_x, branch.node_xy[~branch.mask, 0]
+        )
+        self.mesh1d_node_y = np.append(
+            self.mesh1d_node_y, branch.node_xy[~branch.mask, 1]
+        )
+
+        # Add to edge_nodes
+        self.mesh1d_edge_nodes = np.append(
+            self.mesh1d_edge_nodes, new_edge_nodes, axis=0
+        )
+        edge_coords = np.stack([self.mesh1d_node_x, self.mesh1d_node_y], axis=1)[
+            new_edge_nodes
+        ].mean(1)
+        edge_offsets = (branch.branch_offsets[:-1] + branch.branch_offsets[1:]) / 2
+
+        self.mesh1d_edge_branch_id = np.append(
+            self.mesh1d_edge_branch_id, np.full(len(edge_coords), branch_nr)
+        )
+        self.mesh1d_edge_branch_offset = np.append(
+            self.mesh1d_edge_branch_offset, edge_offsets
+        )
+
+        self.mesh1d_edge_x = np.append(self.mesh1d_edge_x, edge_coords[:, 0])
+        self.mesh1d_edge_y = np.append(self.mesh1d_edge_y, edge_coords[:, 1])
+
+        # Update names of nodes
+        mesh_point_names = np.array(
+            [f"{name}_{offset:.2f}" for offset in offsets], dtype=object
+        )
+        self.mesh1d_node_id = np.append(self.mesh1d_node_id, mesh_point_names)
+        self.mesh1d_node_long_name = np.append(
+            self.mesh1d_node_long_name, mesh_point_names
+        )
+
+        # Add mesh1d nodes
+        self.mesh1d_node_branch_id = np.append(
+            self.mesh1d_node_branch_id, np.full(len(offsets), branch_nr)
+        )
+        self.mesh1d_node_branch_offset = np.append(
+            self.mesh1d_node_branch_offset, offsets
+        )
+        return name
+
+    def get_node_mask(self, branchids: List[str] = None):
+        """Get node mask, give a mask with True for each node that is in the given branchid list"""
+
+        mask = np.full(self.mesh1d_node_id.shape, False, dtype=bool)
+        if branchids is None:
+            mask[:] = True
+            return mask
+
+        # Get number (index) of given branches
+        idx = np.where(np.isin(self.network1d_branch_id, branchids))[0]
+        if idx.size == 0:
+            raise KeyError("No branches corresponding to the given keys were found.")
+
+        mask[np.isin(self.mesh1d_node_branch_id, idx)] = True
+
+        return mask
+
+
+class Network:
+    def __init__(self, is_geographic: bool = False) -> None:
+        self.meshkernel = mk.MeshKernel(is_geographic=is_geographic)
+        # Monkeypatch the meshkernel object, because the "is_geographic" is not saved
+        # otherwise, and needed for reinitializing the meshkernel
+        self.meshkernel.is_geographic = is_geographic
+
+        self._mesh1d = Mesh1d(meshkernel=self.meshkernel)
+        self._mesh2d = Mesh2d(meshkernel=self.meshkernel)
+        self._link1d2d = Link1d2d(meshkernel=self.meshkernel)
+
+        # Spatial index (rtree)
+        # self._idx = index.Index()
+
+    @classmethod
+    def from_file(cls, file_path: Path) -> Network:
+        """Read network from file. This classmethod checks what mesh components (mesh1d & network1d, mesh2d, link1d2d) are
+        present, and loads them one by one.
+
+        Args:
+            file_path (Path): path to netcdf file with network data
+
+        Returns:
+            Network: The instance of the class itself that is returned
+        """
+
+        network = cls()
+        ds = nc.Dataset(file_path)  # type: ignore[import]
+
+        reader = UgridReader(file_path)
+
+        reader.read_mesh1d_network1d(network._mesh1d)
+        reader.read_mesh2d(network._mesh2d)
+        reader.read_link1d2d(network._link1d2d)
+
+        ds.close()
+
+        return network
+
+    def to_file(self, file: Path) -> None:
+        """Write network to file
+
+        Args:
+            file (Path): File where _net.nc is written to.
+        """
+
+        writer = UgridWriter()
+        writer.write(self, file)
+
+    def link1d2d_from_1d_to_2d(
+        self, branchids: List[str] = None, polygon: GeometryList = None
+    ) -> None:
+        self._mesh1d._set_mesh1d()
+        self._mesh2d._set_mesh2d()
+
+        node_mask = self._mesh1d.get_node_mask(branchids)
+        if polygon is None:
+            polygon = self.meshkernel.mesh2d_get_mesh_boundaries_as_polygons()
+
+        self._link1d2d._link_from_1d_to_2d(node_mask, polygon=polygon)
+
+    def mesh2d_create_rectilinear_within_extent(
+        self, extent: tuple, dx: float, dy: float
+    ) -> None:
+        self._mesh2d.create_rectilinear(extent=extent, dx=dx, dy=dy)
+
+    def mesh2d_create_triangular_within_polygon(self, polygon: mk.GeometryList) -> None:
+        """Create triangular grid within GeometryList object. Calls _mesh2d.create_triangular
+        directly, but is easier accessible for users.
+
+        Args:
+            polygon (mk.GeometryList): GeometryList representing a polygon within which the mesh is generated.
+        """
+        self._mesh2d.create_triangular(geometry_list=polygon)
+
+    def mesh2d_clip_mesh(
+        self,
+        geometrylist: mk.GeometryList,
+        deletemeshoption: mk.DeleteMeshOption = mk.DeleteMeshOption.ALL_FACE_CIRCUMCENTERS,
+        inside=True,
+    ) -> None:
+        self._mesh2d.clip(
+            geometrylist=geometrylist,
+            deletemeshoption=deletemeshoption,
+            inside=inside,
+        )
+
+    def mesh2d_refine_mesh(self, polygon: mk.GeometryList, level: int = 1) -> None:
+        self._mesh2d.refine(polygon=polygon, level=level)
+
+    def mesh1d_add_branch(
+        self,
+        branch: Branch,
+        name: str = None,
+        branch_order: int = -1,
+        long_name: str = None,
+        force_midpoint: bool = True,
+    ) -> None:
+        name = self._mesh1d._add_branch(
+            branch=branch,
+            name=name,
+            branch_order=branch_order,
+            long_name=long_name,
+            force_midpoint=force_midpoint,
+        )
+        return name
+
+
+class NetworkModel(ParsableFileModel):
+    """Network model representation."""
+
+    network: Network = Field(default_factory=Network)
+
+    def _post_init_load(self) -> None:
+        """
+        Load the network file if the filepath exists relative to the
+        current FileLoadContext.
+        """
+        super()._post_init_load()
+
+        if self.filepath is None:
+            return
+
+        with file_load_context() as context:
+            network_path = context.resolve(self.filepath)
+
+            if network_path.is_file():
+                self.network = Network.from_file(network_path)
+
+    @property
+    def _mesh1d(self):
+        return self.network._mesh1d
+
+    @property
+    def _mesh2d(self):
+        return self.network._mesh2d
+
+    @property
+    def _link1d2d(self):
+        return self.network._link1d2d
+
+    @classmethod
+    def _ext(cls) -> str:
+        return ".nc"
+
+    @classmethod
+    def _filename(cls) -> str:
+        return "network"
+
+    def _save(self, save_settings: ModelSaveSettings):
+        with file_load_context() as context:
+            write_path = context.resolve(self.filepath)  # type: ignore[arg-type]
+
+            write_path.parent.mkdir(parents=True, exist_ok=True)
+            self.network.to_file(write_path)
+
+    def _export(self, folder: Path) -> None:
+        filename = Path(self.filepath.name) if self.filepath else self._generate_name()
+        self.filepath = folder / filename
+        folder.mkdir(parents=True, exist_ok=True)
+        self.network.to_file(self.filepath)
+
+    def _parse(self, _):
+        return {}
+
+    @classmethod
+    def _get_serializer(cls):
+        # Unused, but requires abstract implementation
+        pass
+
+    @classmethod
+    def _get_parser(cls):
+        # Unused, but requires abstract implementation
+        pass
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/net/reader.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/net/reader.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,300 +1,300 @@
-from __future__ import annotations
-
-import json
-import logging
-from collections import namedtuple
-from pathlib import Path
-from typing import TYPE_CHECKING, Any, Dict, Optional, Tuple
-
-import netCDF4 as nc
-import numpy as np
-
-from hydrolib.core.basemodel import BaseModel
-
-if TYPE_CHECKING:
-    from ..dflowfm.net.models import Link1d2d, Mesh1d, Mesh2d
-
-
-class UgridReader:
-    """UgridReader provides the logic to read a specified UGRID file."""
-
-    def __init__(self, file_path: Path) -> None:
-        """Creates a new UgridReader, reading the specified path.
-
-        Args:
-            file_path (Path): The path to read from.
-
-        Raises:
-            OSError: Thrown when file_path does not exist.
-        """
-        self._ncfile_path = file_path
-
-        if not self._ncfile_path.exists():
-            raise OSError(f'File "{self._ncfile_path}" not found.')
-
-        self._explorer = NCExplorer.from_file_path(self._ncfile_path)
-
-    def read_mesh1d_network1d(self, mesh1d: Mesh1d) -> None:
-        """
-        Read the Ugrid from the netcdf and add the dflowfm cstructure with grid to the
-        specified mesh1d.
-
-        Args:
-            mesh1d (Mesh1d): The object to which the read network1d is added.
-        """
-        if (
-            self._explorer.mesh1d_var_name_mapping is None
-            or self._explorer.network1d_var_name_mapping is None
-        ):
-            logging.debug(
-                "Mesh1d and Network are not found in the dataset, reading is skipped."
-            )
-            return
-
-        # If the mesh is not given (default), use the networks one
-        ds = nc.Dataset(self._ncfile_path)  # type: ignore[import]
-
-        # Read mesh1d
-        for meshkey, nckey in self._explorer.mesh1d_var_name_mapping.items():
-            setattr(mesh1d, meshkey, self._read_nc_attribute(ds[nckey]))
-
-        # Read network variables
-        for meshkey, nckey in self._explorer.network1d_var_name_mapping.items():
-            setattr(mesh1d, meshkey, self._read_nc_attribute(ds[nckey]))
-
-        # Process network
-        mesh1d._process_network1d()
-
-        ds.close()
-
-    def read_mesh2d(self, mesh2d: Mesh2d) -> None:
-        """
-        Read the Ugrid from the netcdf and add the dflowfm cstructure with grid to the
-        specified mesh2d.
-
-        Args:
-            mesh2d (Mesh2d): The object to which the read network1d is added.
-        """
-        if self._explorer.mesh2d_var_name_mapping is None:
-            logging.debug("Mesh2d is not found in the dataset, reading is skipped.")
-            return
-
-        ds = nc.Dataset(self._ncfile_path)  # type: ignore[import]
-
-        # Read mesh1d
-        for meshkey, nckey in self._explorer.mesh2d_var_name_mapping.items():
-            setattr(mesh2d, meshkey, self._read_nc_attribute(ds[nckey]))
-
-        ds.close()
-
-    def read_link1d2d(self, link1d2d: Link1d2d) -> None:
-        """Read the Link1d2d from the wrapped netCDF file of this UgridReader.
-
-        Args:
-            link1d2d (Link1d2d): The Link1d2d to which the data is added.
-        """
-        if self._explorer.link1d2d_var_name_mapping is None:
-            logging.debug("Link1d2d is not found in the dataset, reading is skipped.")
-            return
-
-        ds = nc.Dataset(self._ncfile_path)  # type: ignore[import]
-
-        # Read mesh1d
-        for meshkey, nckey in self._explorer.link1d2d_var_name_mapping.items():
-            setattr(link1d2d, meshkey, self._read_nc_attribute(ds[nckey]))
-
-        ds.close()
-
-    def _read_nc_attribute(self, attr: nc._netCDF4.Variable) -> np.ndarray:
-        """Read values from netcdf attribute.
-        - Character arrays are converted to strings
-        - Masked arrays are converted to regular arrays where the masked values are filled with the fillvalue
-        - The numerical arrays are subtracted by the start value. This is often 1, while python is 0-based.
-
-        Args:
-            attr (netCDF4._netCDF4.Variable): Attribute in the file
-
-        Returns:
-            np.ndarray: returned array
-        """
-        values = attr[:]
-        if values.dtype == "S1":
-            # Convert to strings
-            arr = np.array(list(map(str.strip, nc.chartostring(values.data))))  # type: ignore[import]
-
-        else:
-            # Get data from masked array
-            arr = values[:].data
-            # Fill masked values with fillvalue
-            if values.mask.any():
-                arr[values.mask] = attr._FillValue
-            # Substract start index
-            if hasattr(attr, "start_index"):
-                arr -= attr.start_index
-
-        return arr
-
-
-class NCExplorer(BaseModel):
-    """NCExplorer provides the mapping of the UGRID variable names as used within
-    HYDROLIB models to the actual values used within the netCDF file.
-
-    If a component is not present, the corresponding mapping is set to None.
-    A NCExplorer can be constructed from a file by using the `from_file_path`
-    class method.
-
-    Attributes:
-        network1d_var_name_mapping (Optional[Dict[str, str]]):
-            The mapping of Network variable names.
-        mesh1d_var_name_mapping (Optional[Dict[str, str]]):
-            The mapping of Mesh1d variable names.
-        mesh2d_var_name_mapping (Optional[Dict[str, str]]):
-            The mapping of Mesh2d variable names.
-        link1d2d_var_name_mapping (Optional[Dict[str, str]]):
-            The mapping of Link1d2d variable names.
-    """
-
-    Keys = namedtuple("Keys", ["network1d", "mesh1d", "mesh2d", "link1d2d"])
-
-    network1d_var_name_mapping: Optional[Dict[str, str]]
-    mesh1d_var_name_mapping: Optional[Dict[str, str]]
-    mesh2d_var_name_mapping: Optional[Dict[str, str]]
-    link1d2d_var_name_mapping: Optional[Dict[str, str]]
-
-    @classmethod
-    def from_file_path(cls, file_path: Path) -> "NCExplorer":
-        """Create a new NCExplorer from the specified file_path.
-
-        Args:
-            file_path (Path): The path to the net.nc file.
-
-        Returns:
-            NCExplorer: A newly initialized NCExplorer.
-        """
-        conventions = NCExplorer._read_ugrid_conventions()
-        dataset = nc.Dataset(file_path)  # type: ignore[import]
-
-        keys = NCExplorer._determine_keys(dataset)
-        network1d_mapping = NCExplorer._retrieve_variable_names_mapping(
-            keys.network1d, dataset, conventions["network1d"]
-        )
-        mesh1d_mapping = NCExplorer._retrieve_variable_names_mapping(
-            keys.mesh1d, dataset, conventions["mesh1d"]
-        )
-        mesh2d_mapping = NCExplorer._retrieve_variable_names_mapping(
-            keys.mesh2d, dataset, conventions["mesh2d"]
-        )
-        link1d2d_mapping = NCExplorer._retrieve_variable_names_mapping(
-            keys.link1d2d, dataset, conventions["link1d2d"]
-        )
-
-        dataset.close()
-
-        return cls(
-            network1d_var_name_mapping=network1d_mapping,
-            mesh1d_var_name_mapping=mesh1d_mapping,
-            mesh2d_var_name_mapping=mesh2d_mapping,
-            link1d2d_var_name_mapping=link1d2d_mapping,
-        )
-
-    @staticmethod
-    def _read_ugrid_conventions() -> Dict:
-        with open(Path(__file__).parent.joinpath("ugrid_conventions.json"), "r") as f:
-            return json.load(f)
-
-    @staticmethod
-    def _determine_keys(dataset: nc.Dataset) -> NCExplorer.Keys:  # type: ignore[import]
-        keys1d = NCExplorer._retrieve_1d_keys(dataset)
-
-        mesh1d = keys1d[0] if keys1d is not None else None
-        network1d = keys1d[1] if keys1d is not None else None
-
-        mesh2d = NCExplorer._retrieve_2d_key(dataset)
-        link1d2d = NCExplorer._retrieve_1d2d_key(dataset)
-
-        return NCExplorer.Keys(network1d, mesh1d, mesh2d, link1d2d)
-
-    @staticmethod
-    def _retrieve_1d_keys(dataset) -> Optional[Tuple[str, str]]:
-        def is_mesh1d(ncdata) -> bool:
-            return (
-                NCExplorer._has_cf_role(ncdata, "mesh_topology")
-                and NCExplorer._has_n_topology_dimension(ncdata, 1)
-                and NCExplorer._has_attribute(ncdata, "coordinate_space")
-            )
-
-        for var_key, ncdata in dataset.variables.items():
-            if is_mesh1d(ncdata):
-                return var_key, ncdata.getncattr("coordinate_space")
-        return None
-
-    @staticmethod
-    def _retrieve_2d_key(dataset) -> Optional[str]:
-        def is_mesh2d(ncdata) -> bool:
-            return NCExplorer._has_cf_role(
-                ncdata, "mesh_topology"
-            ) and NCExplorer._has_n_topology_dimension(ncdata, 2)
-
-        for var_key, ncdata in dataset.variables.items():
-            if is_mesh2d(ncdata):
-                return var_key
-        return None
-
-    @staticmethod
-    def _retrieve_1d2d_key(dataset) -> Optional[str]:
-        def is_link1d2d(ncdata) -> bool:
-            return NCExplorer._has_cf_role(ncdata, "mesh_topology_contact")
-
-        for var_key, ncdata in dataset.variables.items():
-            if is_link1d2d(ncdata):
-                return var_key
-        return None
-
-    @staticmethod
-    def _has_cf_role(ncdata, cf_role_value: str) -> bool:
-        return NCExplorer._has_attribute(ncdata, "cf_role", cf_role_value)
-
-    @staticmethod
-    def _has_n_topology_dimension(ncdata, n: int) -> bool:
-        return NCExplorer._has_attribute(ncdata, "topology_dimension", n)
-
-    @staticmethod
-    def _has_attribute(ncdata, name: str, value: Any = None):
-        attributes = ncdata.ncattrs()
-        return name in attributes and (value is None or ncdata.getncattr(name) == value)
-
-    @staticmethod
-    def _retrieve_variable_names_mapping(
-        variable_key: Optional[str], dataset, conventions: Dict
-    ) -> Optional[Dict]:
-        if variable_key is None:
-            # particular key does not exist, as such there are no variables either.
-            return None
-
-        nc_variables_set = set(dataset.variables.keys())
-
-        result = {}
-
-        for key, value in conventions.items():
-            nc_vars = set(variable_key + suffix for suffix in value["suffices"])
-            in_dataset = nc_vars & nc_variables_set
-
-            len_in_data_set = len(in_dataset)
-
-            if len_in_data_set == 1:
-                result[key] = in_dataset.pop()
-            elif len_in_data_set > 1:
-                raise KeyError(
-                    f'Multiple attributes for "{key}" were found in nc file. Got "{" and ".join(in_dataset)}"'
-                )
-            elif "is_optional" in value and value["is_optional"]:
-                logging.info(
-                    "Optional variable '%s' is not present in the nc_file and is skipped",
-                    key,
-                )
-            else:
-                raise KeyError(
-                    f'An attribute for "{key}" was not found in nc file. Expected "{"/".join(nc_vars)}"'
-                )
-
-        return result
+from __future__ import annotations
+
+import json
+import logging
+from collections import namedtuple
+from pathlib import Path
+from typing import TYPE_CHECKING, Any, Dict, Optional, Tuple
+
+import netCDF4 as nc
+import numpy as np
+
+from hydrolib.core.basemodel import BaseModel
+
+if TYPE_CHECKING:
+    from ..dflowfm.net.models import Link1d2d, Mesh1d, Mesh2d
+
+
+class UgridReader:
+    """UgridReader provides the logic to read a specified UGRID file."""
+
+    def __init__(self, file_path: Path) -> None:
+        """Creates a new UgridReader, reading the specified path.
+
+        Args:
+            file_path (Path): The path to read from.
+
+        Raises:
+            OSError: Thrown when file_path does not exist.
+        """
+        self._ncfile_path = file_path
+
+        if not self._ncfile_path.exists():
+            raise OSError(f'File "{self._ncfile_path}" not found.')
+
+        self._explorer = NCExplorer.from_file_path(self._ncfile_path)
+
+    def read_mesh1d_network1d(self, mesh1d: Mesh1d) -> None:
+        """
+        Read the Ugrid from the netcdf and add the dflowfm cstructure with grid to the
+        specified mesh1d.
+
+        Args:
+            mesh1d (Mesh1d): The object to which the read network1d is added.
+        """
+        if (
+            self._explorer.mesh1d_var_name_mapping is None
+            or self._explorer.network1d_var_name_mapping is None
+        ):
+            logging.debug(
+                "Mesh1d and Network are not found in the dataset, reading is skipped."
+            )
+            return
+
+        # If the mesh is not given (default), use the networks one
+        ds = nc.Dataset(self._ncfile_path)  # type: ignore[import]
+
+        # Read mesh1d
+        for meshkey, nckey in self._explorer.mesh1d_var_name_mapping.items():
+            setattr(mesh1d, meshkey, self._read_nc_attribute(ds[nckey]))
+
+        # Read network variables
+        for meshkey, nckey in self._explorer.network1d_var_name_mapping.items():
+            setattr(mesh1d, meshkey, self._read_nc_attribute(ds[nckey]))
+
+        # Process network
+        mesh1d._process_network1d()
+
+        ds.close()
+
+    def read_mesh2d(self, mesh2d: Mesh2d) -> None:
+        """
+        Read the Ugrid from the netcdf and add the dflowfm cstructure with grid to the
+        specified mesh2d.
+
+        Args:
+            mesh2d (Mesh2d): The object to which the read network1d is added.
+        """
+        if self._explorer.mesh2d_var_name_mapping is None:
+            logging.debug("Mesh2d is not found in the dataset, reading is skipped.")
+            return
+
+        ds = nc.Dataset(self._ncfile_path)  # type: ignore[import]
+
+        # Read mesh1d
+        for meshkey, nckey in self._explorer.mesh2d_var_name_mapping.items():
+            setattr(mesh2d, meshkey, self._read_nc_attribute(ds[nckey]))
+
+        ds.close()
+
+    def read_link1d2d(self, link1d2d: Link1d2d) -> None:
+        """Read the Link1d2d from the wrapped netCDF file of this UgridReader.
+
+        Args:
+            link1d2d (Link1d2d): The Link1d2d to which the data is added.
+        """
+        if self._explorer.link1d2d_var_name_mapping is None:
+            logging.debug("Link1d2d is not found in the dataset, reading is skipped.")
+            return
+
+        ds = nc.Dataset(self._ncfile_path)  # type: ignore[import]
+
+        # Read mesh1d
+        for meshkey, nckey in self._explorer.link1d2d_var_name_mapping.items():
+            setattr(link1d2d, meshkey, self._read_nc_attribute(ds[nckey]))
+
+        ds.close()
+
+    def _read_nc_attribute(self, attr: nc._netCDF4.Variable) -> np.ndarray:
+        """Read values from netcdf attribute.
+        - Character arrays are converted to strings
+        - Masked arrays are converted to regular arrays where the masked values are filled with the fillvalue
+        - The numerical arrays are subtracted by the start value. This is often 1, while python is 0-based.
+
+        Args:
+            attr (netCDF4._netCDF4.Variable): Attribute in the file
+
+        Returns:
+            np.ndarray: returned array
+        """
+        values = attr[:]
+        if values.dtype == "S1":
+            # Convert to strings
+            arr = np.array(list(map(str.strip, nc.chartostring(values.data))))  # type: ignore[import]
+
+        else:
+            # Get data from masked array
+            arr = values[:].data
+            # Fill masked values with fillvalue
+            if values.mask.any():
+                arr[values.mask] = attr._FillValue
+            # Substract start index
+            if hasattr(attr, "start_index"):
+                arr -= attr.start_index
+
+        return arr
+
+
+class NCExplorer(BaseModel):
+    """NCExplorer provides the mapping of the UGRID variable names as used within
+    HYDROLIB models to the actual values used within the netCDF file.
+
+    If a component is not present, the corresponding mapping is set to None.
+    A NCExplorer can be constructed from a file by using the `from_file_path`
+    class method.
+
+    Attributes:
+        network1d_var_name_mapping (Optional[Dict[str, str]]):
+            The mapping of Network variable names.
+        mesh1d_var_name_mapping (Optional[Dict[str, str]]):
+            The mapping of Mesh1d variable names.
+        mesh2d_var_name_mapping (Optional[Dict[str, str]]):
+            The mapping of Mesh2d variable names.
+        link1d2d_var_name_mapping (Optional[Dict[str, str]]):
+            The mapping of Link1d2d variable names.
+    """
+
+    Keys = namedtuple("Keys", ["network1d", "mesh1d", "mesh2d", "link1d2d"])
+
+    network1d_var_name_mapping: Optional[Dict[str, str]]
+    mesh1d_var_name_mapping: Optional[Dict[str, str]]
+    mesh2d_var_name_mapping: Optional[Dict[str, str]]
+    link1d2d_var_name_mapping: Optional[Dict[str, str]]
+
+    @classmethod
+    def from_file_path(cls, file_path: Path) -> "NCExplorer":
+        """Create a new NCExplorer from the specified file_path.
+
+        Args:
+            file_path (Path): The path to the net.nc file.
+
+        Returns:
+            NCExplorer: A newly initialized NCExplorer.
+        """
+        conventions = NCExplorer._read_ugrid_conventions()
+        dataset = nc.Dataset(file_path)  # type: ignore[import]
+
+        keys = NCExplorer._determine_keys(dataset)
+        network1d_mapping = NCExplorer._retrieve_variable_names_mapping(
+            keys.network1d, dataset, conventions["network1d"]
+        )
+        mesh1d_mapping = NCExplorer._retrieve_variable_names_mapping(
+            keys.mesh1d, dataset, conventions["mesh1d"]
+        )
+        mesh2d_mapping = NCExplorer._retrieve_variable_names_mapping(
+            keys.mesh2d, dataset, conventions["mesh2d"]
+        )
+        link1d2d_mapping = NCExplorer._retrieve_variable_names_mapping(
+            keys.link1d2d, dataset, conventions["link1d2d"]
+        )
+
+        dataset.close()
+
+        return cls(
+            network1d_var_name_mapping=network1d_mapping,
+            mesh1d_var_name_mapping=mesh1d_mapping,
+            mesh2d_var_name_mapping=mesh2d_mapping,
+            link1d2d_var_name_mapping=link1d2d_mapping,
+        )
+
+    @staticmethod
+    def _read_ugrid_conventions() -> Dict:
+        with open(Path(__file__).parent.joinpath("ugrid_conventions.json"), "r") as f:
+            return json.load(f)
+
+    @staticmethod
+    def _determine_keys(dataset: nc.Dataset) -> NCExplorer.Keys:  # type: ignore[import]
+        keys1d = NCExplorer._retrieve_1d_keys(dataset)
+
+        mesh1d = keys1d[0] if keys1d is not None else None
+        network1d = keys1d[1] if keys1d is not None else None
+
+        mesh2d = NCExplorer._retrieve_2d_key(dataset)
+        link1d2d = NCExplorer._retrieve_1d2d_key(dataset)
+
+        return NCExplorer.Keys(network1d, mesh1d, mesh2d, link1d2d)
+
+    @staticmethod
+    def _retrieve_1d_keys(dataset) -> Optional[Tuple[str, str]]:
+        def is_mesh1d(ncdata) -> bool:
+            return (
+                NCExplorer._has_cf_role(ncdata, "mesh_topology")
+                and NCExplorer._has_n_topology_dimension(ncdata, 1)
+                and NCExplorer._has_attribute(ncdata, "coordinate_space")
+            )
+
+        for var_key, ncdata in dataset.variables.items():
+            if is_mesh1d(ncdata):
+                return var_key, ncdata.getncattr("coordinate_space")
+        return None
+
+    @staticmethod
+    def _retrieve_2d_key(dataset) -> Optional[str]:
+        def is_mesh2d(ncdata) -> bool:
+            return NCExplorer._has_cf_role(
+                ncdata, "mesh_topology"
+            ) and NCExplorer._has_n_topology_dimension(ncdata, 2)
+
+        for var_key, ncdata in dataset.variables.items():
+            if is_mesh2d(ncdata):
+                return var_key
+        return None
+
+    @staticmethod
+    def _retrieve_1d2d_key(dataset) -> Optional[str]:
+        def is_link1d2d(ncdata) -> bool:
+            return NCExplorer._has_cf_role(ncdata, "mesh_topology_contact")
+
+        for var_key, ncdata in dataset.variables.items():
+            if is_link1d2d(ncdata):
+                return var_key
+        return None
+
+    @staticmethod
+    def _has_cf_role(ncdata, cf_role_value: str) -> bool:
+        return NCExplorer._has_attribute(ncdata, "cf_role", cf_role_value)
+
+    @staticmethod
+    def _has_n_topology_dimension(ncdata, n: int) -> bool:
+        return NCExplorer._has_attribute(ncdata, "topology_dimension", n)
+
+    @staticmethod
+    def _has_attribute(ncdata, name: str, value: Any = None):
+        attributes = ncdata.ncattrs()
+        return name in attributes and (value is None or ncdata.getncattr(name) == value)
+
+    @staticmethod
+    def _retrieve_variable_names_mapping(
+        variable_key: Optional[str], dataset, conventions: Dict
+    ) -> Optional[Dict]:
+        if variable_key is None:
+            # particular key does not exist, as such there are no variables either.
+            return None
+
+        nc_variables_set = set(dataset.variables.keys())
+
+        result = {}
+
+        for key, value in conventions.items():
+            nc_vars = set(variable_key + suffix for suffix in value["suffices"])
+            in_dataset = nc_vars & nc_variables_set
+
+            len_in_data_set = len(in_dataset)
+
+            if len_in_data_set == 1:
+                result[key] = in_dataset.pop()
+            elif len_in_data_set > 1:
+                raise KeyError(
+                    f'Multiple attributes for "{key}" were found in nc file. Got "{" and ".join(in_dataset)}"'
+                )
+            elif "is_optional" in value and value["is_optional"]:
+                logging.info(
+                    "Optional variable '%s' is not present in the nc_file and is skipped",
+                    key,
+                )
+            else:
+                raise KeyError(
+                    f'An attribute for "{key}" was not found in nc file. Expected "{"/".join(nc_vars)}"'
+                )
+
+        return result
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/obs/models.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/obs/models.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,110 +1,110 @@
-from typing import Dict, List, Literal, Optional
-
-from pydantic.class_validators import root_validator
-from pydantic.fields import Field
-
-from hydrolib.core.dflowfm.common.models import LocationType
-from hydrolib.core.dflowfm.ini.models import INIBasedModel, INIGeneral, INIModel
-from hydrolib.core.dflowfm.ini.util import (
-    LocationValidationConfiguration,
-    LocationValidationFieldNames,
-    get_enum_validator,
-    make_list_validator,
-    validate_location_specification,
-)
-
-
-class ObservationPointGeneral(INIGeneral):
-    """The observation point file's `[General]` section with file meta data."""
-
-    class Comments(INIBasedModel.Comments):
-        fileversion: Optional[str] = Field(
-            "File version. Do not edit this.", alias="fileVersion"
-        )
-        filetype: Optional[str] = Field(
-            "File type. Should be 'obsPoints'. Do not edit this.",
-            alias="fileType",
-        )
-
-    comments: Comments = Comments()
-    _header: Literal["General"] = "General"
-    fileversion: str = Field("2.00", alias="fileVersion")
-    filetype: Literal["obsPoints"] = Field("obsPoints", alias="fileType")
-
-
-class ObservationPoint(INIBasedModel):
-    """
-    An observation point that is included in the observation point file.
-
-    All lowercased attributes match with the observation point input as described in
-    [UM Sec.F.2.2.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsubsection.F.2.2.1).
-    """
-
-    class Comments(INIBasedModel.Comments):
-        name: Optional[str] = "Name of the observation point (max. 255 characters)."
-        locationtype: Optional[
-            str
-        ] = "Only when x and y are also specified. 1d: snap to closest 1D grid point, 2d: snap to closest 2D grid cell centre, all: snap to closest 1D or 2D point."
-        branchid: Optional[str] = Field(
-            "Branch on which the observation point is located.", alias="branchId"
-        )
-        chainage: Optional[str] = "Chainage on the branch (m)."
-
-        x: Optional[str] = Field(
-            "x-coordinate of the location of the observation point.",
-            alias="x",
-        )
-        y: Optional[str] = Field(
-            "y-coordinate of the location of the observation point.",
-            alias="y",
-        )
-
-    comments: Comments = Comments()
-
-    _header: Literal["ObservationPoint"] = "ObservationPoint"
-
-    name: str = Field("id", max_length=255, alias="name")
-    locationtype: Optional[LocationType] = Field(None, alias="locationType")
-
-    branchid: Optional[str] = Field(None, alias="branchId")
-    chainage: Optional[float] = Field(None, alias="chainage")
-
-    x: Optional[float] = Field(None, alias="x")
-    y: Optional[float] = Field(None, alias="y")
-
-    _type_validator = get_enum_validator("locationtype", enum=LocationType)
-
-    @root_validator(allow_reuse=True)
-    def validate_that_location_specification_is_correct(cls, values: Dict) -> Dict:
-        """Validates that the correct location specification is given."""
-        return validate_location_specification(
-            values,
-            config=LocationValidationConfiguration(
-                validate_node=False, validate_num_coordinates=False
-            ),
-            fields=LocationValidationFieldNames(x_coordinates="x", y_coordinates="y"),
-        )
-
-    def _get_identifier(self, data: dict) -> Optional[str]:
-        return data.get("name")
-
-
-class ObservationPointModel(INIModel):
-    """
-    The overall observation point model that contains the contents of one observation point file.
-
-    This model is typically referenced under a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.output.obsfile[..]`.
-
-    Attributes:
-        general (ObservationPointGeneral): `[General]` block with file metadata.
-        observationpoint (List[ObservationPoint]): List of `[ObservationPoint]` blocks for all observation points.
-    """
-
-    general: ObservationPointGeneral = ObservationPointGeneral()
-    observationpoint: List[ObservationPoint] = []
-
-    _make_list = make_list_validator("observationpoint")
-
-    @classmethod
-    def _filename(cls) -> str:
-        return "obsFile"
+from typing import Dict, List, Literal, Optional
+
+from pydantic.class_validators import root_validator
+from pydantic.fields import Field
+
+from hydrolib.core.dflowfm.common.models import LocationType
+from hydrolib.core.dflowfm.ini.models import INIBasedModel, INIGeneral, INIModel
+from hydrolib.core.dflowfm.ini.util import (
+    LocationValidationConfiguration,
+    LocationValidationFieldNames,
+    get_enum_validator,
+    make_list_validator,
+    validate_location_specification,
+)
+
+
+class ObservationPointGeneral(INIGeneral):
+    """The observation point file's `[General]` section with file meta data."""
+
+    class Comments(INIBasedModel.Comments):
+        fileversion: Optional[str] = Field(
+            "File version. Do not edit this.", alias="fileVersion"
+        )
+        filetype: Optional[str] = Field(
+            "File type. Should be 'obsPoints'. Do not edit this.",
+            alias="fileType",
+        )
+
+    comments: Comments = Comments()
+    _header: Literal["General"] = "General"
+    fileversion: str = Field("2.00", alias="fileVersion")
+    filetype: Literal["obsPoints"] = Field("obsPoints", alias="fileType")
+
+
+class ObservationPoint(INIBasedModel):
+    """
+    An observation point that is included in the observation point file.
+
+    All lowercased attributes match with the observation point input as described in
+    [UM Sec.F.2.2.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsubsection.F.2.2.1).
+    """
+
+    class Comments(INIBasedModel.Comments):
+        name: Optional[str] = "Name of the observation point (max. 255 characters)."
+        locationtype: Optional[
+            str
+        ] = "Only when x and y are also specified. 1d: snap to closest 1D grid point, 2d: snap to closest 2D grid cell centre, all: snap to closest 1D or 2D point."
+        branchid: Optional[str] = Field(
+            "Branch on which the observation point is located.", alias="branchId"
+        )
+        chainage: Optional[str] = "Chainage on the branch (m)."
+
+        x: Optional[str] = Field(
+            "x-coordinate of the location of the observation point.",
+            alias="x",
+        )
+        y: Optional[str] = Field(
+            "y-coordinate of the location of the observation point.",
+            alias="y",
+        )
+
+    comments: Comments = Comments()
+
+    _header: Literal["ObservationPoint"] = "ObservationPoint"
+
+    name: str = Field("id", max_length=255, alias="name")
+    locationtype: Optional[LocationType] = Field(None, alias="locationType")
+
+    branchid: Optional[str] = Field(None, alias="branchId")
+    chainage: Optional[float] = Field(None, alias="chainage")
+
+    x: Optional[float] = Field(None, alias="x")
+    y: Optional[float] = Field(None, alias="y")
+
+    _type_validator = get_enum_validator("locationtype", enum=LocationType)
+
+    @root_validator(allow_reuse=True)
+    def validate_that_location_specification_is_correct(cls, values: Dict) -> Dict:
+        """Validates that the correct location specification is given."""
+        return validate_location_specification(
+            values,
+            config=LocationValidationConfiguration(
+                validate_node=False, validate_num_coordinates=False
+            ),
+            fields=LocationValidationFieldNames(x_coordinates="x", y_coordinates="y"),
+        )
+
+    def _get_identifier(self, data: dict) -> Optional[str]:
+        return data.get("name")
+
+
+class ObservationPointModel(INIModel):
+    """
+    The overall observation point model that contains the contents of one observation point file.
+
+    This model is typically referenced under a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.output.obsfile[..]`.
+
+    Attributes:
+        general (ObservationPointGeneral): `[General]` block with file metadata.
+        observationpoint (List[ObservationPoint]): List of `[ObservationPoint]` blocks for all observation points.
+    """
+
+    general: ObservationPointGeneral = ObservationPointGeneral()
+    observationpoint: List[ObservationPoint] = []
+
+    _make_list = make_list_validator("observationpoint")
+
+    @classmethod
+    def _filename(cls) -> str:
+        return "obsFile"
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/obscrosssection/models.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/obscrosssection/models.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,96 +1,96 @@
-from typing import Dict, List, Literal, Optional
-
-from pydantic.class_validators import root_validator
-from pydantic.fields import Field
-
-from hydrolib.core.dflowfm.ini.models import INIBasedModel, INIGeneral, INIModel
-from hydrolib.core.dflowfm.ini.util import (
-    LocationValidationConfiguration,
-    get_split_string_on_delimiter_validator,
-    validate_location_specification,
-)
-
-
-class ObservationCrossSectionGeneral(INIGeneral):
-    """The observation cross section file's `[General]` section with file meta data."""
-
-    class Comments(INIBasedModel.Comments):
-        fileversion: Optional[str] = Field(
-            "File version. Do not edit this.", alias="fileVersion"
-        )
-        filetype: Optional[str] = Field(
-            "File type. Should be 'obsCross'. Do not edit this.", alias="fileType"
-        )
-
-    comments: Comments = Comments()
-    fileversion: str = Field("2.00", alias="fileVersion")
-    filetype: Literal["obsCross"] = Field("obsCross", alias="fileType")
-
-
-class ObservationCrossSection(INIBasedModel):
-    """
-    The observation cross section that is included in the
-    observation cross section file.
-
-    All lowercased attributes match with the observation cross
-    section output as described in [UM Sec.F2.4.1]
-    (https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsubsection.F.2.4.1)
-    """
-
-    class Comments(INIBasedModel.Comments):
-        name: Optional[str] = "Name of the cross section (max. 255 characters)."
-        branchid: Optional[str] = Field(
-            "(optional) Branch on which the cross section is located.", alias="branchId"
-        )
-        chainage: Optional[str] = "(optional) Location on the branch (m)."
-        numcoordinates: Optional[str] = Field(
-            "(optional) Number of values in xCoordinates and yCoordinates. "
-            "This value should be greater than or equal to 2.",
-            alias="numCoordinates",
-        )
-        xcoordinates: Optional[str] = Field(
-            "(optional) x-coordinates of the cross section line. "
-            "(number of values = numCoordinates)",
-            alias="xCoordinates",
-        )
-        ycoordinates: Optional[str] = Field(
-            "(optional) y-coordinates of the cross section line. "
-            "(number of values = numCoordinates)",
-            alias="yCoordinates",
-        )
-
-    comments: Comments = Comments()
-    _header: Literal["ObservationCrossSection"] = "ObservationCrossSection"
-    name: str = Field(max_length=255, alias="name")
-    branchid: Optional[str] = Field(alias="branchId")
-    chainage: Optional[float] = Field(alias="chainage")
-    numcoordinates: Optional[int] = Field(alias="numCoordinates")
-    xcoordinates: Optional[List[float]] = Field(alias="xCoordinates")
-    ycoordinates: Optional[List[float]] = Field(alias="yCoordinates")
-
-    _split_to_list = get_split_string_on_delimiter_validator(
-        "xcoordinates", "ycoordinates"
-    )
-
-    @root_validator(allow_reuse=True)
-    def validate_that_location_specification_is_correct(cls, values: Dict) -> Dict:
-        """Validates that the correct location specification is given."""
-        return validate_location_specification(
-            values,
-            config=LocationValidationConfiguration(
-                validate_node=False, minimum_num_coordinates=2
-            ),
-        )
-
-    def _get_identifier(self, data: dict) -> Optional[str]:
-        return data.get("name")
-
-
-class ObservationCrossSectionModel(INIModel):
-    """
-    The overall observation cross section model that contains the contents
-    of one observation cross section file.
-    """
-
-    general: ObservationCrossSectionGeneral = ObservationCrossSectionGeneral()
-    observationcrosssection: List[ObservationCrossSection] = []
+from typing import Dict, List, Literal, Optional
+
+from pydantic.class_validators import root_validator
+from pydantic.fields import Field
+
+from hydrolib.core.dflowfm.ini.models import INIBasedModel, INIGeneral, INIModel
+from hydrolib.core.dflowfm.ini.util import (
+    LocationValidationConfiguration,
+    get_split_string_on_delimiter_validator,
+    validate_location_specification,
+)
+
+
+class ObservationCrossSectionGeneral(INIGeneral):
+    """The observation cross section file's `[General]` section with file meta data."""
+
+    class Comments(INIBasedModel.Comments):
+        fileversion: Optional[str] = Field(
+            "File version. Do not edit this.", alias="fileVersion"
+        )
+        filetype: Optional[str] = Field(
+            "File type. Should be 'obsCross'. Do not edit this.", alias="fileType"
+        )
+
+    comments: Comments = Comments()
+    fileversion: str = Field("2.00", alias="fileVersion")
+    filetype: Literal["obsCross"] = Field("obsCross", alias="fileType")
+
+
+class ObservationCrossSection(INIBasedModel):
+    """
+    The observation cross section that is included in the
+    observation cross section file.
+
+    All lowercased attributes match with the observation cross
+    section output as described in [UM Sec.F2.4.1]
+    (https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsubsection.F.2.4.1)
+    """
+
+    class Comments(INIBasedModel.Comments):
+        name: Optional[str] = "Name of the cross section (max. 255 characters)."
+        branchid: Optional[str] = Field(
+            "(optional) Branch on which the cross section is located.", alias="branchId"
+        )
+        chainage: Optional[str] = "(optional) Location on the branch (m)."
+        numcoordinates: Optional[str] = Field(
+            "(optional) Number of values in xCoordinates and yCoordinates. "
+            "This value should be greater than or equal to 2.",
+            alias="numCoordinates",
+        )
+        xcoordinates: Optional[str] = Field(
+            "(optional) x-coordinates of the cross section line. "
+            "(number of values = numCoordinates)",
+            alias="xCoordinates",
+        )
+        ycoordinates: Optional[str] = Field(
+            "(optional) y-coordinates of the cross section line. "
+            "(number of values = numCoordinates)",
+            alias="yCoordinates",
+        )
+
+    comments: Comments = Comments()
+    _header: Literal["ObservationCrossSection"] = "ObservationCrossSection"
+    name: str = Field(max_length=255, alias="name")
+    branchid: Optional[str] = Field(alias="branchId")
+    chainage: Optional[float] = Field(alias="chainage")
+    numcoordinates: Optional[int] = Field(alias="numCoordinates")
+    xcoordinates: Optional[List[float]] = Field(alias="xCoordinates")
+    ycoordinates: Optional[List[float]] = Field(alias="yCoordinates")
+
+    _split_to_list = get_split_string_on_delimiter_validator(
+        "xcoordinates", "ycoordinates"
+    )
+
+    @root_validator(allow_reuse=True)
+    def validate_that_location_specification_is_correct(cls, values: Dict) -> Dict:
+        """Validates that the correct location specification is given."""
+        return validate_location_specification(
+            values,
+            config=LocationValidationConfiguration(
+                validate_node=False, minimum_num_coordinates=2
+            ),
+        )
+
+    def _get_identifier(self, data: dict) -> Optional[str]:
+        return data.get("name")
+
+
+class ObservationCrossSectionModel(INIModel):
+    """
+    The overall observation cross section model that contains the contents
+    of one observation cross section file.
+    """
+
+    general: ObservationCrossSectionGeneral = ObservationCrossSectionGeneral()
+    observationcrosssection: List[ObservationCrossSection] = []
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/onedfield/models.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/onedfield/models.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,143 +1,143 @@
-import logging
-from typing import List, Literal, Optional
-
-from pydantic import Field
-from pydantic.class_validators import root_validator
-from pydantic.types import NonNegativeInt
-
-from hydrolib.core.dflowfm.ini.models import INIBasedModel, INIGeneral, INIModel
-from hydrolib.core.dflowfm.ini.util import (
-    get_split_string_on_delimiter_validator,
-    make_list_validator,
-    validate_correct_length,
-)
-
-logger = logging.getLogger(__name__)
-
-
-class OneDFieldGeneral(INIGeneral):
-    """The 1D field file's `[General]` section with file meta data."""
-
-    class Comments(INIBasedModel.Comments):
-        fileversion: Optional[str] = Field(
-            "File version. Do not edit this.", alias="fileVersion"
-        )
-        filetype: Optional[str] = Field(
-            "File type. Should be '1dField'. Do not edit this.",
-            alias="fileType",
-        )
-
-    comments: Comments = Comments()
-    _header: Literal["General"] = "General"
-
-    fileversion: str = Field("2.00", alias="fileVersion")
-    filetype: Literal["1dField"] = Field("1dField", alias="fileType")
-
-
-class OneDFieldGlobal(INIBasedModel):
-    """The `[Global]` block with a uniform value for use inside a 1D field file."""
-
-    class Comments(INIBasedModel.Comments):
-        quantity: Optional[str] = Field("The name of the quantity", alias="quantity")
-        unit: Optional[str] = Field("The unit of the quantity", alias="unit")
-        value: Optional[str] = Field(
-            "The global default value for this quantity", alias="value"
-        )
-
-    comments: Comments = Comments()
-    _header: Literal["Global"] = "Global"
-
-    quantity: str = Field(alias="quantity")
-    unit: str = Field(alias="unit")
-    value: float = Field(alias="value")
-
-
-class OneDFieldBranch(INIBasedModel):
-    """
-    A `[Branch]` block for use inside a 1D field file.
-
-    Each block can define value(s) on a particular branch.
-    """
-
-    class Comments(INIBasedModel.Comments):
-        branchid: Optional[str] = Field("The name of the branch", alias="branchId")
-        numlocations: Optional[str] = Field(
-            "Number of locations on branch. The default 0 value implies branch uniform values.",
-            alias="numLocations",
-        )
-        chainage: Optional[str] = Field(
-            "Space separated list of locations on the branch (m). Locations sorted by increasing chainage. The keyword must be specified if numLocations >0.",
-            alias="chainage",
-        )
-        values: Optional[str] = Field(
-            "Space separated list of numLocations values; one for each chainage specified. One value required if numLocations =0",
-            alias="values",
-        )
-
-    comments: Comments = Comments()
-    _header: Literal["Branch"] = "Branch"
-
-    branchid: str = Field(alias="branchId")
-    numlocations: Optional[NonNegativeInt] = Field(0, alias="numLocations")
-    chainage: Optional[List[float]] = Field(alias="chainage")
-    values: List[float] = Field(alias="values")
-
-    _split_to_list = get_split_string_on_delimiter_validator(
-        "chainage",
-        "values",
-    )
-
-    @root_validator(allow_reuse=True)
-    def check_list_length_values(cls, values):
-        """Validates that the length of the values field is as expected."""
-        return validate_correct_length(
-            values,
-            "values",
-            length_name="numlocations",
-            list_required_with_length=True,
-            min_length=1,
-        )
-
-    @root_validator(allow_reuse=True)
-    def check_list_length_chainage(cls, values):
-        """Validates that the length of the chainage field is as expected."""
-        return validate_correct_length(
-            values,
-            "chainage",
-            length_name="numlocations",
-            list_required_with_length=True,
-        )
-
-    def _get_identifier(self, data: dict) -> Optional[str]:
-        return data.get("branchid")
-
-
-class OneDFieldModel(INIModel):
-    """
-    The overall 1D field model that contains the contents of a 1D field file.
-
-    This model is typically used when a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.inifieldfile[..].initial[..].datafiletype==DataFileType.onedfield`.
-
-    Attributes:
-        general (OneDFieldGeneral): `[General]` block with file metadata.
-        global_ (Optional[OneDFieldGlobal]): Optional `[Global]` block with uniform value.
-        branch (List[OneDFieldBranch]): Definitions of `[Branch]` field values.
-    """
-
-    general: OneDFieldGeneral = OneDFieldGeneral()
-    global_: Optional[OneDFieldGlobal] = Field(
-        alias="global"
-    )  # to circumvent built-in kw
-    branch: List[OneDFieldBranch] = []
-
-    _split_to_list = make_list_validator(
-        "branch",
-    )
-
-    @classmethod
-    def _ext(cls) -> str:
-        return ".ini"
-
-    @classmethod
-    def _filename(cls) -> str:
-        return "1dfield"
+import logging
+from typing import List, Literal, Optional
+
+from pydantic import Field
+from pydantic.class_validators import root_validator
+from pydantic.types import NonNegativeInt
+
+from hydrolib.core.dflowfm.ini.models import INIBasedModel, INIGeneral, INIModel
+from hydrolib.core.dflowfm.ini.util import (
+    get_split_string_on_delimiter_validator,
+    make_list_validator,
+    validate_correct_length,
+)
+
+logger = logging.getLogger(__name__)
+
+
+class OneDFieldGeneral(INIGeneral):
+    """The 1D field file's `[General]` section with file meta data."""
+
+    class Comments(INIBasedModel.Comments):
+        fileversion: Optional[str] = Field(
+            "File version. Do not edit this.", alias="fileVersion"
+        )
+        filetype: Optional[str] = Field(
+            "File type. Should be '1dField'. Do not edit this.",
+            alias="fileType",
+        )
+
+    comments: Comments = Comments()
+    _header: Literal["General"] = "General"
+
+    fileversion: str = Field("2.00", alias="fileVersion")
+    filetype: Literal["1dField"] = Field("1dField", alias="fileType")
+
+
+class OneDFieldGlobal(INIBasedModel):
+    """The `[Global]` block with a uniform value for use inside a 1D field file."""
+
+    class Comments(INIBasedModel.Comments):
+        quantity: Optional[str] = Field("The name of the quantity", alias="quantity")
+        unit: Optional[str] = Field("The unit of the quantity", alias="unit")
+        value: Optional[str] = Field(
+            "The global default value for this quantity", alias="value"
+        )
+
+    comments: Comments = Comments()
+    _header: Literal["Global"] = "Global"
+
+    quantity: str = Field(alias="quantity")
+    unit: str = Field(alias="unit")
+    value: float = Field(alias="value")
+
+
+class OneDFieldBranch(INIBasedModel):
+    """
+    A `[Branch]` block for use inside a 1D field file.
+
+    Each block can define value(s) on a particular branch.
+    """
+
+    class Comments(INIBasedModel.Comments):
+        branchid: Optional[str] = Field("The name of the branch", alias="branchId")
+        numlocations: Optional[str] = Field(
+            "Number of locations on branch. The default 0 value implies branch uniform values.",
+            alias="numLocations",
+        )
+        chainage: Optional[str] = Field(
+            "Space separated list of locations on the branch (m). Locations sorted by increasing chainage. The keyword must be specified if numLocations >0.",
+            alias="chainage",
+        )
+        values: Optional[str] = Field(
+            "Space separated list of numLocations values; one for each chainage specified. One value required if numLocations =0",
+            alias="values",
+        )
+
+    comments: Comments = Comments()
+    _header: Literal["Branch"] = "Branch"
+
+    branchid: str = Field(alias="branchId")
+    numlocations: Optional[NonNegativeInt] = Field(0, alias="numLocations")
+    chainage: Optional[List[float]] = Field(alias="chainage")
+    values: List[float] = Field(alias="values")
+
+    _split_to_list = get_split_string_on_delimiter_validator(
+        "chainage",
+        "values",
+    )
+
+    @root_validator(allow_reuse=True)
+    def check_list_length_values(cls, values):
+        """Validates that the length of the values field is as expected."""
+        return validate_correct_length(
+            values,
+            "values",
+            length_name="numlocations",
+            list_required_with_length=True,
+            min_length=1,
+        )
+
+    @root_validator(allow_reuse=True)
+    def check_list_length_chainage(cls, values):
+        """Validates that the length of the chainage field is as expected."""
+        return validate_correct_length(
+            values,
+            "chainage",
+            length_name="numlocations",
+            list_required_with_length=True,
+        )
+
+    def _get_identifier(self, data: dict) -> Optional[str]:
+        return data.get("branchid")
+
+
+class OneDFieldModel(INIModel):
+    """
+    The overall 1D field model that contains the contents of a 1D field file.
+
+    This model is typically used when a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.inifieldfile[..].initial[..].datafiletype==DataFileType.onedfield`.
+
+    Attributes:
+        general (OneDFieldGeneral): `[General]` block with file metadata.
+        global_ (Optional[OneDFieldGlobal]): Optional `[Global]` block with uniform value.
+        branch (List[OneDFieldBranch]): Definitions of `[Branch]` field values.
+    """
+
+    general: OneDFieldGeneral = OneDFieldGeneral()
+    global_: Optional[OneDFieldGlobal] = Field(
+        alias="global"
+    )  # to circumvent built-in kw
+    branch: List[OneDFieldBranch] = []
+
+    _split_to_list = make_list_validator(
+        "branch",
+    )
+
+    @classmethod
+    def _ext(cls) -> str:
+        return ".ini"
+
+    @classmethod
+    def _filename(cls) -> str:
+        return "1dfield"
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/polyfile/models.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/polyfile/models.py`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,110 +1,110 @@
-"""models.py defines all classes and functions related to representing pol/pli(z) files.
-"""
-
-from typing import Callable, List, Optional, Sequence
-
-from hydrolib.core.basemodel import BaseModel, ModelSaveSettings, ParsableFileModel
-
-
-class Description(BaseModel):
-    """Description of a single PolyObject.
-
-    The Description will be prepended to a block. Each line will
-    start with a '*'.
-
-    Attributes:
-        content (str): The content of this Description.
-    """
-
-    content: str
-
-
-class Metadata(BaseModel):
-    """Metadata of a single PolyObject.
-
-    Attributes:
-        name (str): The name of the PolyObject
-        n_rows (int): The number of rows (i.e. Point instances) of the PolyObject
-        n_columns (int): The total number of values in a Point, including x, y, and z.
-    """
-
-    name: str
-    n_rows: int
-    n_columns: int
-
-
-class Point(BaseModel):
-    """Point consisting of a x and y coordinate, an optional z coordinate and data.
-
-    Attributes:
-        x (float): The x-coordinate of this Point
-        y (float): The y-coordinate of this Point
-        z (Optional[float]): An optional z-coordinate of this Point.
-        data (Sequence[float]): The additional data variables of this Point.
-    """
-
-    x: float
-    y: float
-    z: Optional[float]
-    data: Sequence[float]
-
-    def _get_identifier(self, data: dict) -> Optional[str]:
-        x = data.get("x")
-        y = data.get("y")
-        z = data.get("z")
-        return f"x:{x} y:{y} z:{z}"
-
-
-class PolyObject(BaseModel):
-    """PolyObject describing a single block in a poly file.
-
-    The metadata should be consistent with the points:
-    - The number of points should be equal to number of rows defined in the metadata
-    - The data of each point should be equal to the number of columns defined in the
-      metadata.
-
-    Attributes:
-        description (Optional[Description]):
-            An optional description of this PolyObject
-        metadata (Metadata):
-            The Metadata of this PolObject, describing the structure
-        points (List[Point]):
-            The points describing this PolyObject, structured according to the Metadata
-    """
-
-    description: Optional[Description]
-    metadata: Metadata
-    points: List[Point]
-
-
-class PolyFile(ParsableFileModel):
-    """Poly-file (.pol/.pli/.pliz) representation."""
-
-    has_z_values: bool = False
-    objects: Sequence[PolyObject] = []
-
-    def _serialize(self, _: dict, save_settings: ModelSaveSettings) -> None:
-        from .serializer import write_polyfile
-
-        # We skip the passed dict for a better one.
-        write_polyfile(self._resolved_filepath, self.objects, self.serializer_config)
-
-    @classmethod
-    def _ext(cls) -> str:
-        return ".pli"
-
-    @classmethod
-    def _filename(cls) -> str:
-        return "objects"
-
-    @classmethod
-    def _get_serializer(cls) -> Callable:
-        # Unused, but requires abstract implementation
-        pass
-
-    @classmethod
-    def _get_parser(cls) -> Callable:
-        # TODO Prevent circular dependency in Parser
-        from .parser import read_polyfile
-
-        return read_polyfile
+"""models.py defines all classes and functions related to representing pol/pli(z) files.
+"""
+
+from typing import Callable, List, Optional, Sequence
+
+from hydrolib.core.basemodel import BaseModel, ModelSaveSettings, ParsableFileModel
+
+
+class Description(BaseModel):
+    """Description of a single PolyObject.
+
+    The Description will be prepended to a block. Each line will
+    start with a '*'.
+
+    Attributes:
+        content (str): The content of this Description.
+    """
+
+    content: str
+
+
+class Metadata(BaseModel):
+    """Metadata of a single PolyObject.
+
+    Attributes:
+        name (str): The name of the PolyObject
+        n_rows (int): The number of rows (i.e. Point instances) of the PolyObject
+        n_columns (int): The total number of values in a Point, including x, y, and z.
+    """
+
+    name: str
+    n_rows: int
+    n_columns: int
+
+
+class Point(BaseModel):
+    """Point consisting of a x and y coordinate, an optional z coordinate and data.
+
+    Attributes:
+        x (float): The x-coordinate of this Point
+        y (float): The y-coordinate of this Point
+        z (Optional[float]): An optional z-coordinate of this Point.
+        data (Sequence[float]): The additional data variables of this Point.
+    """
+
+    x: float
+    y: float
+    z: Optional[float]
+    data: Sequence[float]
+
+    def _get_identifier(self, data: dict) -> Optional[str]:
+        x = data.get("x")
+        y = data.get("y")
+        z = data.get("z")
+        return f"x:{x} y:{y} z:{z}"
+
+
+class PolyObject(BaseModel):
+    """PolyObject describing a single block in a poly file.
+
+    The metadata should be consistent with the points:
+    - The number of points should be equal to number of rows defined in the metadata
+    - The data of each point should be equal to the number of columns defined in the
+      metadata.
+
+    Attributes:
+        description (Optional[Description]):
+            An optional description of this PolyObject
+        metadata (Metadata):
+            The Metadata of this PolObject, describing the structure
+        points (List[Point]):
+            The points describing this PolyObject, structured according to the Metadata
+    """
+
+    description: Optional[Description]
+    metadata: Metadata
+    points: List[Point]
+
+
+class PolyFile(ParsableFileModel):
+    """Poly-file (.pol/.pli/.pliz) representation."""
+
+    has_z_values: bool = False
+    objects: Sequence[PolyObject] = []
+
+    def _serialize(self, _: dict, save_settings: ModelSaveSettings) -> None:
+        from .serializer import write_polyfile
+
+        # We skip the passed dict for a better one.
+        write_polyfile(self._resolved_filepath, self.objects, self.serializer_config)
+
+    @classmethod
+    def _ext(cls) -> str:
+        return ".pli"
+
+    @classmethod
+    def _filename(cls) -> str:
+        return "objects"
+
+    @classmethod
+    def _get_serializer(cls) -> Callable:
+        # Unused, but requires abstract implementation
+        pass
+
+    @classmethod
+    def _get_parser(cls) -> Callable:
+        # TODO Prevent circular dependency in Parser
+        from .parser import read_polyfile
+
+        return read_polyfile
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/polyfile/parser.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/polyfile/parser.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,590 +1,590 @@
-"""parser.py defines all classes and functions related to parsing pol/pli(z) files.
-"""
-
-import warnings
-from enum import IntEnum
-from pathlib import Path
-from typing import Callable, Dict, Iterator, List, Optional, Sequence, Tuple, Union
-
-from hydrolib.core.basemodel import BaseModel
-from hydrolib.core.dflowfm.polyfile.models import (
-    Description,
-    Metadata,
-    Point,
-    PolyObject,
-)
-
-
-class ParseMsg(BaseModel):
-    """ParseMsg defines a single message indicating a significant parse event.
-
-    Attributes:
-        line_start (int): The start line of the block to which this ParseMsg refers.
-        line_end (int): The end line of the block to which this ParseMsg refers.
-        column (Optional[Tuple[int, int]]):
-            An optional begin and end column to which this ParseMsg refers.
-        reason (str): A human-readable string detailing the reason of the ParseMsg.
-    """
-
-    line_start: int
-    line_end: int
-
-    column: Optional[Tuple[int, int]]
-    reason: str
-
-    def format_parsemsg_to_string(self, file_path: Optional[Path] = None) -> str:
-        """Format string describing this ParseMsg
-
-        Args:
-            file_path (Optional[Path], optional):
-                The file path mentioned in the message if specified. Defaults to None.
-        """
-        if self.line_start != self.line_end:
-            block_suffix = f"\nInvalid block {self.line_start}:{self.line_end}"
-        else:
-            block_suffix = f"\nInvalid line {self.line_start}"
-
-        col_suffix = (
-            f"\nColumns {self.column[0]}:{self.column[1]}"
-            if self.column is not None
-            else ""
-        )
-        file_suffix = f"\nFile: {file_path}" if file_path is not None else ""
-
-        message = f"{self.reason}{block_suffix}{col_suffix}{file_suffix}"
-
-        return message
-
-
-class Block(BaseModel):
-    """Block is a temporary object which will be converted into a PolyObject.
-
-    The fields are supposed to be set during the lifetime of this object.
-    When all fields are set, finalize can be called.
-
-    Attributes:
-        start_line (int): The starting line of this current block.
-        name (Optional[str]): The name of this block. Defaults to None.
-        dimensions (Optional[Tuple[int, int]]):
-            The dimensions (n_rows, n_columns) of this Block. Defaults to None.
-        points (Optional[List[Point]]):
-            The points of this block. Defaults to None.
-        ws_warnings (List[ParseMsg]):
-            The whitespace warnings associated with this block.
-            Defaults to an empty list.
-        empty_lines (List[int]):
-            The line numbers of the empty lines. Defaults to an empty list.
-    """
-
-    start_line: int
-
-    description: Optional[List[str]] = None
-    name: Optional[str] = None
-    dimensions: Optional[Tuple[int, int]] = None
-    points: Optional[List[Point]] = None
-
-    ws_warnings: List[ParseMsg] = []
-    empty_lines: List[int] = []
-
-    def finalize(self) -> Optional[Tuple[PolyObject, List[ParseMsg]]]:
-        """Finalise this Block and return the constructed PolyObject and warnings
-
-        If the metadata or the points are None, then None is returned.
-
-        Returns:
-            Optional[Tuple[PolyObject, List[ParseMsg]]]:
-                The constructed PolyObject and warnings encountered while parsing it.
-        """
-        metadata = self._get_metadata()
-
-        if metadata is None or self.points is None:
-            return None
-
-        obj = PolyObject(
-            description=self._get_description(), metadata=metadata, points=self.points
-        )
-
-        return obj, self.ws_warnings + self._get_empty_line_warnings()
-
-    def _get_description(self) -> Optional[Description]:
-        if self.description is not None:
-            return Description(content="\n".join(self.description))
-        else:
-            return None
-
-    def _get_metadata(self) -> Optional[Metadata]:
-        if self.name is None or self.dimensions is None:
-            return None
-
-        (n_rows, n_columns) = self.dimensions
-        return Metadata(name=self.name, n_rows=n_rows, n_columns=n_columns)
-
-    def _get_empty_line_warnings(self):
-        if len(self.empty_lines) == 0:
-            return []
-
-        warnings = []
-        empty_line = (self.empty_lines[0], self.empty_lines[0])
-
-        for line in self.empty_lines[1:]:
-            if line == empty_line[1] + 1:
-                empty_line = (empty_line[0], line)
-            else:
-                warnings.append(Block._get_empty_line_msg(empty_line))
-                empty_line = (line, line)
-        warnings.append(Block._get_empty_line_msg(empty_line))
-
-        return warnings
-
-    @staticmethod
-    def _get_empty_line_msg(line_range: Tuple[int, int]) -> ParseMsg:
-        return ParseMsg(
-            line_start=line_range[0],
-            line_end=line_range[1],
-            reason="Empty lines are ignored.",
-        )
-
-
-class InvalidBlock(BaseModel):
-    """InvalidBlock is a temporary object which will be converted into a ParseMsg.
-
-    Attributes:
-        start_line (int): The start line of this InvalidBlock
-        end_line (Optional[int]):
-            The end line of this InvalidBlock if it is set. Defaults to None.
-        invalid_line (int): The line which is causing this block to be invalid.
-        reason (str): A human-readable string detailing the reason of the ParseMsg.
-    """
-
-    start_line: int
-    end_line: Optional[int] = None
-    invalid_line: int
-    reason: str
-
-    def to_msg(self) -> ParseMsg:
-        """Convert this InvalidBlock to the corresponding ParseMsg
-
-        Returns:
-            ParseMsg: The ParseMsg corresponding with this InvalidBlock
-        """
-        return ParseMsg(
-            line_start=self.start_line,
-            line_end=self.end_line,
-            reason=f"{self.reason} at line {self.invalid_line}.",
-        )
-
-
-class ErrorBuilder:
-    """ErrorBuilder provides the functionality to the Parser to keep track of errors."""
-
-    def __init__(self) -> None:
-        """Create a new ErorrorBuilder"""
-        self._current_block: Optional[InvalidBlock] = None
-
-    def start_invalid_block(
-        self, block_start: int, invalid_line: int, reason: str
-    ) -> None:
-        """Start a new invalid block if none exists at the moment
-
-        If we are already in an invalid block, or the previous one
-        was never finalised, we will not log the reason, and assume
-        it is one long invalid block.
-
-        Args:
-            block_start (int): The start of the invalid block.
-            invalid_line (int): The actual offending line number.
-            reason (str): The reason why this block is invalid.
-        """
-        if self._current_block is None:
-            self._current_block = InvalidBlock(
-                start_line=block_start, invalid_line=invalid_line, reason=reason
-            )
-
-    def end_invalid_block(self, line: int) -> None:
-        """Store the end line of the current block
-
-        If no invalid block currently exists, nothing will be done.
-
-        Args:
-            line (int): the final line of this invalid block
-        """
-        if self._current_block is not None:
-            self._current_block.end_line = line
-
-    def finalize_previous_error(self) -> Optional[ParseMsg]:
-        """Finalize the current invalid block if it exists
-
-        If no current invalid block exists, None will be returned, and nothing will
-        change. If a current block exists, it will be converted into a ParseMsg and
-        returned. The current invalid block will be reset.
-
-        Returns:
-            Optional[ParseMsg]: The corresponding ParseMsg if an InvalidBlock exists.
-        """
-        if self._current_block is not None:
-            msg = self._current_block.to_msg()
-            self._current_block = None
-
-            return msg
-        else:
-            return None
-
-
-class StateType(IntEnum):
-    """The types of state of a Parser."""
-
-    NEW_BLOCK = 0
-    PARSED_DESCRIPTION = 1
-    PARSED_NAME = 2
-    PARSING_POINTS = 3
-    INVALID_STATE = 4
-
-
-class Parser:
-    """Parser provides the functionality to parse a polyfile line by line.
-
-    The Parser parses blocks describing PolyObject instances by relying on
-    a rudimentary state machine. The states are encoded with the StateType.
-    New lines are fed through the feed_line method. After each line the
-    internal state will be updated. When a complete block is read, it will
-    be converted into a PolyObject and stored internally.
-    When finalise is called, the constructed objects, as well as any warnings
-    and errors describing invalid blocks, will be returned.
-
-    Each state defines a feed_line method, stored in the _feed_line dict,
-    which consumes a line and potentially transitions the state into the next.
-    Each state further defines a finalise method, stored in the _finalise dict,
-    which is called upon finalising the parser.
-
-    Invalid states are encoded with INVALID_STATE. In this state the Parser
-    attempts to find a new block, and thus looks for a new description or
-    name.
-
-    Unexpected whitespace before comments, names, and dimensions, as well as
-    empty lines will generate a warning, and will be ignored by the parser.
-    """
-
-    def __init__(self, file_path: Path, has_z_value: bool = False) -> None:
-        """Create a new Parser
-
-        Args:
-            file_path (Path):
-                Name of the file being parsed, only used for providing proper warnings.
-            has_z_value (bool, optional):
-                Whether to interpret the third column as z-coordinates.
-                Defaults to False.
-        """
-        self._has_z_value = has_z_value
-        self._file_path = file_path
-
-        self._line = 0
-        self._new_block()
-
-        self._error_builder = ErrorBuilder()
-
-        self._poly_objects: List[PolyObject] = []
-
-        self._current_point: int = 0
-
-        self._feed_line: Dict[StateType, Callable[[str], None]] = {
-            StateType.NEW_BLOCK: self._parse_name_or_new_description,
-            StateType.PARSED_DESCRIPTION: self._parse_name_or_next_description,
-            StateType.PARSED_NAME: self._parse_dimensions,
-            StateType.PARSING_POINTS: self._parse_next_point,
-            StateType.INVALID_STATE: self._parse_name_or_new_description,
-        }
-
-        self._finalise: Dict[StateType, Callable[[], None]] = {
-            StateType.NEW_BLOCK: self._noop,
-            StateType.PARSED_DESCRIPTION: self._add_current_block_as_incomplete_error,
-            StateType.PARSED_NAME: self._add_current_block_as_incomplete_error,
-            StateType.PARSING_POINTS: self._add_current_block_as_incomplete_error,
-            StateType.INVALID_STATE: self._noop,
-        }
-
-    def feed_line(self, line: str) -> None:
-        """Parse the next line with this Parser.
-
-        Args:
-            line (str): The line to parse
-        """
-
-        if not Parser._is_empty_line(line):
-            self._feed_line[self._state](line)
-        else:
-            self._handle_empty_line()
-
-        self._increment_line()
-
-    def finalize(self) -> Sequence[PolyObject]:
-        """Finalize parsing and return the constructed PolyObject.
-
-        Raises:
-            ValueError: When the plifile is invalid.
-
-        Returns:
-            PolyObject:
-                A PolyObject containing the constructed PolyObject instances.
-        """
-        self._error_builder.end_invalid_block(self._line)
-        last_error_msg = self._error_builder.finalize_previous_error()
-        if last_error_msg is not None:
-            self._notify_as_error(last_error_msg)
-
-        self._finalise[self._state]()
-
-        return self._poly_objects
-
-    def _new_block(self, offset: int = 0) -> None:
-        self._state = StateType.NEW_BLOCK
-        self._current_block = Block(start_line=(self._line + offset))
-
-    def _finish_block(self):
-        (obj, warnings) = self._current_block.finalize()  # type: ignore
-        self._poly_objects.append(obj)
-
-        for msg in warnings:
-            self._notify_as_warning(msg)
-
-        last_error = self._error_builder.finalize_previous_error()
-        if last_error is not None:
-            self._notify_as_error(last_error)
-
-    def _increment_line(self) -> None:
-        self._line += 1
-
-    def _noop(self, *_, **__) -> None:
-        # no operation
-        pass
-
-    def _add_current_block_as_incomplete_error(self) -> None:
-        msg = ParseMsg(
-            line_start=self._current_block.start_line,
-            line_end=self._line,
-            reason="EoF encountered before the block is finished.",
-        )
-        self._notify_as_error(msg)
-
-    def _parse_name_or_new_description(self, line: str) -> None:
-        if Parser._is_comment(line):
-            self._handle_new_description(line)
-        elif Parser._is_name(line):
-            self._handle_parse_name(line)
-        elif self._state != StateType.INVALID_STATE:
-            self._handle_new_error(
-                "Settings of block might be incorrect, expected a valid name or description"
-            )
-            return
-
-        # If we come from an invalid state, and we started a correct new block
-        # we will end the previous invalid block, if it exists.
-        self._error_builder.end_invalid_block(self._line)
-
-    def _parse_name_or_next_description(self, line: str) -> None:
-        if Parser._is_comment(line):
-            self._handle_next_description(line)
-        elif Parser._is_name(line):
-            self._handle_parse_name(line)
-        else:
-            self._handle_new_error("Expected a valid name or description")
-
-    def _parse_dimensions(self, line: str) -> None:
-        dimensions = Parser._convert_to_dimensions(line)
-
-        if dimensions is not None:
-            self._current_block.dimensions = dimensions
-            self._current_block.points = []
-            self._current_point = 0
-            self._state = StateType.PARSING_POINTS
-        else:
-            self._handle_new_error("Expected valid dimensions")
-
-    def _parse_next_point(self, line: str) -> None:
-        point = Parser._convert_to_point(
-            line, self._current_block.dimensions[1], self._has_z_value  # type: ignore
-        )
-
-        if point is not None:
-            self._current_block.points.append(point)  # type: ignore
-            self._current_point += 1
-
-            if self._current_block.dimensions[0] == self._current_point:  # type: ignore
-                self._finish_block()
-                self._new_block(offset=1)
-
-        else:
-            self._handle_new_error("Expected a valid next point")
-            # we parse the line again, as it might be the first line of a new valid
-            # block. For example when the invalid block was missing points.
-            self._feed_line[self._state](line)
-
-    def _handle_parse_name(self, line: str) -> None:
-        self._current_block.name = Parser._convert_to_name(line)
-        self._state = StateType.PARSED_NAME
-
-    def _handle_new_description(self, line: str) -> None:
-        comment = Parser._convert_to_comment(line)
-        self._current_block.description = [
-            comment,
-        ]
-        self._state = StateType.PARSED_DESCRIPTION
-
-    def _handle_next_description(self, line: str) -> None:
-        comment = Parser._convert_to_comment(line)
-        self._current_block.description.append(comment)  # type: ignore
-
-    def _handle_empty_line(self) -> None:
-        if self._state != StateType.INVALID_STATE:
-            self._current_block.empty_lines.append(self._line)
-
-    def _handle_new_error(self, reason: str) -> None:
-        self._error_builder.start_invalid_block(
-            self._current_block.start_line, self._line, reason
-        )
-        self._state = StateType.INVALID_STATE
-
-    def _notify_as_warning(self, msg: ParseMsg) -> None:
-        warning_message = msg.format_parsemsg_to_string(self._file_path)
-        warnings.warn(warning_message)
-
-    def _notify_as_error(self, msg: ParseMsg) -> None:
-        error_message = msg.format_parsemsg_to_string(self._file_path)
-        raise ValueError(f"Invalid formatted plifile, {error_message}")
-
-    @staticmethod
-    def _is_empty_line(line: str) -> bool:
-        return len(line.strip()) == 0
-
-    @staticmethod
-    def _is_name(line: str) -> bool:
-        stripped = line.strip()
-        return len(stripped) >= 1 and line[0] != "*" and " " not in stripped
-
-    @staticmethod
-    def _convert_to_name(line: str) -> str:
-        return line.strip()
-
-    @staticmethod
-    def _is_comment(line: str) -> bool:
-        return line.strip().startswith("*")
-
-    @staticmethod
-    def _convert_to_comment(line: str) -> str:
-        return line.strip()[1:]
-
-    @staticmethod
-    def _convert_to_dimensions(line: str) -> Optional[Tuple[int, int]]:
-        stripped = line.strip()
-        elems = stripped.split()
-
-        if len(elems) != 2:
-            return None
-
-        try:
-            n_rows = int(elems[0])
-            n_cols = int(elems[1])
-
-            if n_rows <= 0 or n_cols <= 0:
-                return None
-
-            return (n_rows, n_cols)
-        except ValueError:
-            return None
-
-    @staticmethod
-    def _convert_to_point(
-        line: str, expected_n_points: int, has_z: bool
-    ) -> Optional[Point]:
-        stripped = line.strip()
-        elems = stripped.split()
-
-        if len(elems) < expected_n_points:
-            return None
-
-        try:
-            values = list(float(x) for x in elems[:expected_n_points])
-
-            if has_z:
-                x, y, z, *data = values
-            else:
-                x, y, *data = values
-                z = None  # type: ignore
-
-            return Point(x=x, y=y, z=z, data=data)
-
-        except ValueError:
-            return None
-
-
-def _determine_has_z_value(input_val: Union[Path, Iterator[str]]) -> bool:
-    return isinstance(input_val, Path) and input_val.suffix == ".pliz"
-
-
-def read_polyfile(filepath: Path, has_z_values: Optional[bool] = None) -> Dict:
-    """Read the specified file and return the corresponding data.
-
-    The file is expected to follow the .pli(z) / .pol convention. A .pli(z) or .pol
-    file is defined as consisting of a number of blocks of lines adhering to the
-    following format:
-
-    - Optional description record consisting of one or more lines starting with '*'.
-        These will be ignored.
-    - Name consisting of a non-blank character string
-    - Two integers, Nr and Nc, representing the numbers of rows and columns respectively
-    - Nr number of data points, consisting of Nc floats separated by whitespace
-
-    For example:
-    ```
-    ...
-    *
-    * Polyline L008
-    *
-    L008
-    4 2
-        131595.0 549685.0
-        131750.0 549865.0
-        131595.0 550025.0
-        131415.0 550175.0
-    ...
-    ```
-
-    Note that the points can be arbitrarily indented, and the comments are optional.
-
-    if no has_z_value has been defined, it will be based on the file path
-    extensions of the filepath:
-    - .pliz will default to True
-    - .pli and .pol will default to False
-
-    Empty lines and unexpected whitespace will be flagged as warnings, and ignored.
-
-    If invalid syntax is detected within a block, an error will be created. This block
-    will be ignored for the purpose of creating PolyObject instances.
-    Once an error is encountered, any following lines will be marked as part of the
-    invalid block, until a new valid block is found. Note that this means that sequential
-    invalid blocks will be reported as a single invalid block. Such invalid blocks will
-    be reported as warnings.
-
-    Args:
-        filepath:
-            Path to the pli(z)/pol convention structured file.
-        has_z_values:
-            Whether to create points containing a z-value. Defaults to None.
-
-    Raises:
-        ValueError: When the plifile is invalid.
-
-    Returns:
-        Dict: The dictionary describing the data of a PolyObject.
-    """
-    if has_z_values is None:
-        has_z_values = _determine_has_z_value(filepath)
-
-    parser = Parser(filepath, has_z_value=has_z_values)
-
-    with filepath.open("r", encoding="utf8") as f:
-        for line in f:
-            parser.feed_line(line)
-
-    objs = parser.finalize()
-
-    return {"has_z_values": has_z_values, "objects": objs}
+"""parser.py defines all classes and functions related to parsing pol/pli(z) files.
+"""
+
+import warnings
+from enum import IntEnum
+from pathlib import Path
+from typing import Callable, Dict, Iterator, List, Optional, Sequence, Tuple, Union
+
+from hydrolib.core.basemodel import BaseModel
+from hydrolib.core.dflowfm.polyfile.models import (
+    Description,
+    Metadata,
+    Point,
+    PolyObject,
+)
+
+
+class ParseMsg(BaseModel):
+    """ParseMsg defines a single message indicating a significant parse event.
+
+    Attributes:
+        line_start (int): The start line of the block to which this ParseMsg refers.
+        line_end (int): The end line of the block to which this ParseMsg refers.
+        column (Optional[Tuple[int, int]]):
+            An optional begin and end column to which this ParseMsg refers.
+        reason (str): A human-readable string detailing the reason of the ParseMsg.
+    """
+
+    line_start: int
+    line_end: int
+
+    column: Optional[Tuple[int, int]]
+    reason: str
+
+    def format_parsemsg_to_string(self, file_path: Optional[Path] = None) -> str:
+        """Format string describing this ParseMsg
+
+        Args:
+            file_path (Optional[Path], optional):
+                The file path mentioned in the message if specified. Defaults to None.
+        """
+        if self.line_start != self.line_end:
+            block_suffix = f"\nInvalid block {self.line_start}:{self.line_end}"
+        else:
+            block_suffix = f"\nInvalid line {self.line_start}"
+
+        col_suffix = (
+            f"\nColumns {self.column[0]}:{self.column[1]}"
+            if self.column is not None
+            else ""
+        )
+        file_suffix = f"\nFile: {file_path}" if file_path is not None else ""
+
+        message = f"{self.reason}{block_suffix}{col_suffix}{file_suffix}"
+
+        return message
+
+
+class Block(BaseModel):
+    """Block is a temporary object which will be converted into a PolyObject.
+
+    The fields are supposed to be set during the lifetime of this object.
+    When all fields are set, finalize can be called.
+
+    Attributes:
+        start_line (int): The starting line of this current block.
+        name (Optional[str]): The name of this block. Defaults to None.
+        dimensions (Optional[Tuple[int, int]]):
+            The dimensions (n_rows, n_columns) of this Block. Defaults to None.
+        points (Optional[List[Point]]):
+            The points of this block. Defaults to None.
+        ws_warnings (List[ParseMsg]):
+            The whitespace warnings associated with this block.
+            Defaults to an empty list.
+        empty_lines (List[int]):
+            The line numbers of the empty lines. Defaults to an empty list.
+    """
+
+    start_line: int
+
+    description: Optional[List[str]] = None
+    name: Optional[str] = None
+    dimensions: Optional[Tuple[int, int]] = None
+    points: Optional[List[Point]] = None
+
+    ws_warnings: List[ParseMsg] = []
+    empty_lines: List[int] = []
+
+    def finalize(self) -> Optional[Tuple[PolyObject, List[ParseMsg]]]:
+        """Finalise this Block and return the constructed PolyObject and warnings
+
+        If the metadata or the points are None, then None is returned.
+
+        Returns:
+            Optional[Tuple[PolyObject, List[ParseMsg]]]:
+                The constructed PolyObject and warnings encountered while parsing it.
+        """
+        metadata = self._get_metadata()
+
+        if metadata is None or self.points is None:
+            return None
+
+        obj = PolyObject(
+            description=self._get_description(), metadata=metadata, points=self.points
+        )
+
+        return obj, self.ws_warnings + self._get_empty_line_warnings()
+
+    def _get_description(self) -> Optional[Description]:
+        if self.description is not None:
+            return Description(content="\n".join(self.description))
+        else:
+            return None
+
+    def _get_metadata(self) -> Optional[Metadata]:
+        if self.name is None or self.dimensions is None:
+            return None
+
+        (n_rows, n_columns) = self.dimensions
+        return Metadata(name=self.name, n_rows=n_rows, n_columns=n_columns)
+
+    def _get_empty_line_warnings(self):
+        if len(self.empty_lines) == 0:
+            return []
+
+        warnings = []
+        empty_line = (self.empty_lines[0], self.empty_lines[0])
+
+        for line in self.empty_lines[1:]:
+            if line == empty_line[1] + 1:
+                empty_line = (empty_line[0], line)
+            else:
+                warnings.append(Block._get_empty_line_msg(empty_line))
+                empty_line = (line, line)
+        warnings.append(Block._get_empty_line_msg(empty_line))
+
+        return warnings
+
+    @staticmethod
+    def _get_empty_line_msg(line_range: Tuple[int, int]) -> ParseMsg:
+        return ParseMsg(
+            line_start=line_range[0],
+            line_end=line_range[1],
+            reason="Empty lines are ignored.",
+        )
+
+
+class InvalidBlock(BaseModel):
+    """InvalidBlock is a temporary object which will be converted into a ParseMsg.
+
+    Attributes:
+        start_line (int): The start line of this InvalidBlock
+        end_line (Optional[int]):
+            The end line of this InvalidBlock if it is set. Defaults to None.
+        invalid_line (int): The line which is causing this block to be invalid.
+        reason (str): A human-readable string detailing the reason of the ParseMsg.
+    """
+
+    start_line: int
+    end_line: Optional[int] = None
+    invalid_line: int
+    reason: str
+
+    def to_msg(self) -> ParseMsg:
+        """Convert this InvalidBlock to the corresponding ParseMsg
+
+        Returns:
+            ParseMsg: The ParseMsg corresponding with this InvalidBlock
+        """
+        return ParseMsg(
+            line_start=self.start_line,
+            line_end=self.end_line,
+            reason=f"{self.reason} at line {self.invalid_line}.",
+        )
+
+
+class ErrorBuilder:
+    """ErrorBuilder provides the functionality to the Parser to keep track of errors."""
+
+    def __init__(self) -> None:
+        """Create a new ErorrorBuilder"""
+        self._current_block: Optional[InvalidBlock] = None
+
+    def start_invalid_block(
+        self, block_start: int, invalid_line: int, reason: str
+    ) -> None:
+        """Start a new invalid block if none exists at the moment
+
+        If we are already in an invalid block, or the previous one
+        was never finalised, we will not log the reason, and assume
+        it is one long invalid block.
+
+        Args:
+            block_start (int): The start of the invalid block.
+            invalid_line (int): The actual offending line number.
+            reason (str): The reason why this block is invalid.
+        """
+        if self._current_block is None:
+            self._current_block = InvalidBlock(
+                start_line=block_start, invalid_line=invalid_line, reason=reason
+            )
+
+    def end_invalid_block(self, line: int) -> None:
+        """Store the end line of the current block
+
+        If no invalid block currently exists, nothing will be done.
+
+        Args:
+            line (int): the final line of this invalid block
+        """
+        if self._current_block is not None:
+            self._current_block.end_line = line
+
+    def finalize_previous_error(self) -> Optional[ParseMsg]:
+        """Finalize the current invalid block if it exists
+
+        If no current invalid block exists, None will be returned, and nothing will
+        change. If a current block exists, it will be converted into a ParseMsg and
+        returned. The current invalid block will be reset.
+
+        Returns:
+            Optional[ParseMsg]: The corresponding ParseMsg if an InvalidBlock exists.
+        """
+        if self._current_block is not None:
+            msg = self._current_block.to_msg()
+            self._current_block = None
+
+            return msg
+        else:
+            return None
+
+
+class StateType(IntEnum):
+    """The types of state of a Parser."""
+
+    NEW_BLOCK = 0
+    PARSED_DESCRIPTION = 1
+    PARSED_NAME = 2
+    PARSING_POINTS = 3
+    INVALID_STATE = 4
+
+
+class Parser:
+    """Parser provides the functionality to parse a polyfile line by line.
+
+    The Parser parses blocks describing PolyObject instances by relying on
+    a rudimentary state machine. The states are encoded with the StateType.
+    New lines are fed through the feed_line method. After each line the
+    internal state will be updated. When a complete block is read, it will
+    be converted into a PolyObject and stored internally.
+    When finalise is called, the constructed objects, as well as any warnings
+    and errors describing invalid blocks, will be returned.
+
+    Each state defines a feed_line method, stored in the _feed_line dict,
+    which consumes a line and potentially transitions the state into the next.
+    Each state further defines a finalise method, stored in the _finalise dict,
+    which is called upon finalising the parser.
+
+    Invalid states are encoded with INVALID_STATE. In this state the Parser
+    attempts to find a new block, and thus looks for a new description or
+    name.
+
+    Unexpected whitespace before comments, names, and dimensions, as well as
+    empty lines will generate a warning, and will be ignored by the parser.
+    """
+
+    def __init__(self, file_path: Path, has_z_value: bool = False) -> None:
+        """Create a new Parser
+
+        Args:
+            file_path (Path):
+                Name of the file being parsed, only used for providing proper warnings.
+            has_z_value (bool, optional):
+                Whether to interpret the third column as z-coordinates.
+                Defaults to False.
+        """
+        self._has_z_value = has_z_value
+        self._file_path = file_path
+
+        self._line = 0
+        self._new_block()
+
+        self._error_builder = ErrorBuilder()
+
+        self._poly_objects: List[PolyObject] = []
+
+        self._current_point: int = 0
+
+        self._feed_line: Dict[StateType, Callable[[str], None]] = {
+            StateType.NEW_BLOCK: self._parse_name_or_new_description,
+            StateType.PARSED_DESCRIPTION: self._parse_name_or_next_description,
+            StateType.PARSED_NAME: self._parse_dimensions,
+            StateType.PARSING_POINTS: self._parse_next_point,
+            StateType.INVALID_STATE: self._parse_name_or_new_description,
+        }
+
+        self._finalise: Dict[StateType, Callable[[], None]] = {
+            StateType.NEW_BLOCK: self._noop,
+            StateType.PARSED_DESCRIPTION: self._add_current_block_as_incomplete_error,
+            StateType.PARSED_NAME: self._add_current_block_as_incomplete_error,
+            StateType.PARSING_POINTS: self._add_current_block_as_incomplete_error,
+            StateType.INVALID_STATE: self._noop,
+        }
+
+    def feed_line(self, line: str) -> None:
+        """Parse the next line with this Parser.
+
+        Args:
+            line (str): The line to parse
+        """
+
+        if not Parser._is_empty_line(line):
+            self._feed_line[self._state](line)
+        else:
+            self._handle_empty_line()
+
+        self._increment_line()
+
+    def finalize(self) -> Sequence[PolyObject]:
+        """Finalize parsing and return the constructed PolyObject.
+
+        Raises:
+            ValueError: When the plifile is invalid.
+
+        Returns:
+            PolyObject:
+                A PolyObject containing the constructed PolyObject instances.
+        """
+        self._error_builder.end_invalid_block(self._line)
+        last_error_msg = self._error_builder.finalize_previous_error()
+        if last_error_msg is not None:
+            self._notify_as_error(last_error_msg)
+
+        self._finalise[self._state]()
+
+        return self._poly_objects
+
+    def _new_block(self, offset: int = 0) -> None:
+        self._state = StateType.NEW_BLOCK
+        self._current_block = Block(start_line=(self._line + offset))
+
+    def _finish_block(self):
+        (obj, warnings) = self._current_block.finalize()  # type: ignore
+        self._poly_objects.append(obj)
+
+        for msg in warnings:
+            self._notify_as_warning(msg)
+
+        last_error = self._error_builder.finalize_previous_error()
+        if last_error is not None:
+            self._notify_as_error(last_error)
+
+    def _increment_line(self) -> None:
+        self._line += 1
+
+    def _noop(self, *_, **__) -> None:
+        # no operation
+        pass
+
+    def _add_current_block_as_incomplete_error(self) -> None:
+        msg = ParseMsg(
+            line_start=self._current_block.start_line,
+            line_end=self._line,
+            reason="EoF encountered before the block is finished.",
+        )
+        self._notify_as_error(msg)
+
+    def _parse_name_or_new_description(self, line: str) -> None:
+        if Parser._is_comment(line):
+            self._handle_new_description(line)
+        elif Parser._is_name(line):
+            self._handle_parse_name(line)
+        elif self._state != StateType.INVALID_STATE:
+            self._handle_new_error(
+                "Settings of block might be incorrect, expected a valid name or description"
+            )
+            return
+
+        # If we come from an invalid state, and we started a correct new block
+        # we will end the previous invalid block, if it exists.
+        self._error_builder.end_invalid_block(self._line)
+
+    def _parse_name_or_next_description(self, line: str) -> None:
+        if Parser._is_comment(line):
+            self._handle_next_description(line)
+        elif Parser._is_name(line):
+            self._handle_parse_name(line)
+        else:
+            self._handle_new_error("Expected a valid name or description")
+
+    def _parse_dimensions(self, line: str) -> None:
+        dimensions = Parser._convert_to_dimensions(line)
+
+        if dimensions is not None:
+            self._current_block.dimensions = dimensions
+            self._current_block.points = []
+            self._current_point = 0
+            self._state = StateType.PARSING_POINTS
+        else:
+            self._handle_new_error("Expected valid dimensions")
+
+    def _parse_next_point(self, line: str) -> None:
+        point = Parser._convert_to_point(
+            line, self._current_block.dimensions[1], self._has_z_value  # type: ignore
+        )
+
+        if point is not None:
+            self._current_block.points.append(point)  # type: ignore
+            self._current_point += 1
+
+            if self._current_block.dimensions[0] == self._current_point:  # type: ignore
+                self._finish_block()
+                self._new_block(offset=1)
+
+        else:
+            self._handle_new_error("Expected a valid next point")
+            # we parse the line again, as it might be the first line of a new valid
+            # block. For example when the invalid block was missing points.
+            self._feed_line[self._state](line)
+
+    def _handle_parse_name(self, line: str) -> None:
+        self._current_block.name = Parser._convert_to_name(line)
+        self._state = StateType.PARSED_NAME
+
+    def _handle_new_description(self, line: str) -> None:
+        comment = Parser._convert_to_comment(line)
+        self._current_block.description = [
+            comment,
+        ]
+        self._state = StateType.PARSED_DESCRIPTION
+
+    def _handle_next_description(self, line: str) -> None:
+        comment = Parser._convert_to_comment(line)
+        self._current_block.description.append(comment)  # type: ignore
+
+    def _handle_empty_line(self) -> None:
+        if self._state != StateType.INVALID_STATE:
+            self._current_block.empty_lines.append(self._line)
+
+    def _handle_new_error(self, reason: str) -> None:
+        self._error_builder.start_invalid_block(
+            self._current_block.start_line, self._line, reason
+        )
+        self._state = StateType.INVALID_STATE
+
+    def _notify_as_warning(self, msg: ParseMsg) -> None:
+        warning_message = msg.format_parsemsg_to_string(self._file_path)
+        warnings.warn(warning_message)
+
+    def _notify_as_error(self, msg: ParseMsg) -> None:
+        error_message = msg.format_parsemsg_to_string(self._file_path)
+        raise ValueError(f"Invalid formatted plifile, {error_message}")
+
+    @staticmethod
+    def _is_empty_line(line: str) -> bool:
+        return len(line.strip()) == 0
+
+    @staticmethod
+    def _is_name(line: str) -> bool:
+        stripped = line.strip()
+        return len(stripped) >= 1 and line[0] != "*" and " " not in stripped
+
+    @staticmethod
+    def _convert_to_name(line: str) -> str:
+        return line.strip()
+
+    @staticmethod
+    def _is_comment(line: str) -> bool:
+        return line.strip().startswith("*")
+
+    @staticmethod
+    def _convert_to_comment(line: str) -> str:
+        return line.strip()[1:]
+
+    @staticmethod
+    def _convert_to_dimensions(line: str) -> Optional[Tuple[int, int]]:
+        stripped = line.strip()
+        elems = stripped.split()
+
+        if len(elems) != 2:
+            return None
+
+        try:
+            n_rows = int(elems[0])
+            n_cols = int(elems[1])
+
+            if n_rows <= 0 or n_cols <= 0:
+                return None
+
+            return (n_rows, n_cols)
+        except ValueError:
+            return None
+
+    @staticmethod
+    def _convert_to_point(
+        line: str, expected_n_points: int, has_z: bool
+    ) -> Optional[Point]:
+        stripped = line.strip()
+        elems = stripped.split()
+
+        if len(elems) < expected_n_points:
+            return None
+
+        try:
+            values = list(float(x) for x in elems[:expected_n_points])
+
+            if has_z:
+                x, y, z, *data = values
+            else:
+                x, y, *data = values
+                z = None  # type: ignore
+
+            return Point(x=x, y=y, z=z, data=data)
+
+        except ValueError:
+            return None
+
+
+def _determine_has_z_value(input_val: Union[Path, Iterator[str]]) -> bool:
+    return isinstance(input_val, Path) and input_val.suffix == ".pliz"
+
+
+def read_polyfile(filepath: Path, has_z_values: Optional[bool] = None) -> Dict:
+    """Read the specified file and return the corresponding data.
+
+    The file is expected to follow the .pli(z) / .pol convention. A .pli(z) or .pol
+    file is defined as consisting of a number of blocks of lines adhering to the
+    following format:
+
+    - Optional description record consisting of one or more lines starting with '*'.
+        These will be ignored.
+    - Name consisting of a non-blank character string
+    - Two integers, Nr and Nc, representing the numbers of rows and columns respectively
+    - Nr number of data points, consisting of Nc floats separated by whitespace
+
+    For example:
+    ```
+    ...
+    *
+    * Polyline L008
+    *
+    L008
+    4 2
+        131595.0 549685.0
+        131750.0 549865.0
+        131595.0 550025.0
+        131415.0 550175.0
+    ...
+    ```
+
+    Note that the points can be arbitrarily indented, and the comments are optional.
+
+    if no has_z_value has been defined, it will be based on the file path
+    extensions of the filepath:
+    - .pliz will default to True
+    - .pli and .pol will default to False
+
+    Empty lines and unexpected whitespace will be flagged as warnings, and ignored.
+
+    If invalid syntax is detected within a block, an error will be created. This block
+    will be ignored for the purpose of creating PolyObject instances.
+    Once an error is encountered, any following lines will be marked as part of the
+    invalid block, until a new valid block is found. Note that this means that sequential
+    invalid blocks will be reported as a single invalid block. Such invalid blocks will
+    be reported as warnings.
+
+    Args:
+        filepath:
+            Path to the pli(z)/pol convention structured file.
+        has_z_values:
+            Whether to create points containing a z-value. Defaults to None.
+
+    Raises:
+        ValueError: When the plifile is invalid.
+
+    Returns:
+        Dict: The dictionary describing the data of a PolyObject.
+    """
+    if has_z_values is None:
+        has_z_values = _determine_has_z_value(filepath)
+
+    parser = Parser(filepath, has_z_value=has_z_values)
+
+    with filepath.open("r", encoding="utf8") as f:
+        for line in f:
+            parser.feed_line(line)
+
+    objs = parser.finalize()
+
+    return {"has_z_values": has_z_values, "objects": objs}
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/structure/models.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/structure/models.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,1114 +1,1114 @@
-"""
-structure namespace for storing the contents of an [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]'s structure file.
-"""
-# TODO Implement the following structures
-# - Gate
-
-import logging
-from enum import Enum
-from operator import gt, ne
-from typing import Dict, List, Literal, Optional, Set, Union
-
-from pydantic import Field
-from pydantic.class_validators import root_validator, validator
-
-from hydrolib.core.basemodel import DiskOnlyFileModel
-from hydrolib.core.dflowfm.bc.models import ForcingModel
-from hydrolib.core.dflowfm.friction.models import FrictionType
-from hydrolib.core.dflowfm.ini.models import INIBasedModel, INIGeneral, INIModel
-from hydrolib.core.dflowfm.ini.util import (
-    get_enum_validator,
-    get_from_subclass_defaults,
-    get_split_string_on_delimiter_validator,
-    make_list_validator,
-    validate_conditionally,
-    validate_correct_length,
-    validate_forbidden_fields,
-    validate_required_fields,
-)
-from hydrolib.core.dflowfm.tim.models import TimModel
-from hydrolib.core.utils import str_is_empty_or_none
-
-logger = logging.getLogger(__name__)
-
-ForcingData = Union[float, TimModel, ForcingModel]
-
-# TODO: handle comment blocks
-# TODO: handle duplicate keys
-class Structure(INIBasedModel):
-    # TODO: would we want to load this from something externally and generate these automatically
-    class Comments(INIBasedModel.Comments):
-        id: Optional[str] = "Unique structure id (max. 256 characters)."
-        name: Optional[str] = "Given name in the user interface."
-        polylinefile: Optional[str] = Field(
-            "*.pli; Polyline geometry definition for 2D structure.",
-            alias="polylinefile",
-        )
-        branchid: Optional[str] = Field(
-            "Branch on which the structure is located.", alias="branchId"
-        )
-        chainage: Optional[str] = "Chainage on the branch (m)."
-
-        numcoordinates: Optional[str] = Field(
-            "Number of values in xCoordinates and yCoordinates", alias="numCoordinates"
-        )
-        xcoordinates: Optional[str] = Field(
-            "x-coordinates of the location of the structure. (number of values = numCoordinates)",
-            alias="xCoordinates",
-        )
-        ycoordinates: Optional[str] = Field(
-            "y-coordinates of the location of the structure. (number of values = numCoordinates)",
-            alias="yCoordinates",
-        )
-
-    comments: Comments = Comments()
-
-    _header: Literal["Structure"] = "Structure"
-
-    id: str = Field("id", max_length=256, alias="id")
-    name: str = Field("id", alias="name")
-    type: str = Field(alias="type")
-
-    polylinefile: Optional[DiskOnlyFileModel] = Field(None, alias="polylinefile")
-
-    branchid: Optional[str] = Field(None, alias="branchId")
-    chainage: Optional[float] = Field(None, alias="chainage")
-
-    numcoordinates: Optional[int] = Field(None, alias="numCoordinates")
-    xcoordinates: Optional[List[float]] = Field(None, alias="xCoordinates")
-    ycoordinates: Optional[List[float]] = Field(None, alias="yCoordinates")
-
-    _loc_coord_fields = {"numcoordinates", "xcoordinates", "ycoordinates"}
-    _loc_branch_fields = {"branchid", "chainage"}
-    _loc_all_fields = _loc_coord_fields | _loc_branch_fields
-
-    _split_to_list = get_split_string_on_delimiter_validator(
-        "xcoordinates", "ycoordinates"
-    )
-
-    @validator("type", pre=True)
-    def _validate_type(cls, value):
-        return get_from_subclass_defaults(Structure, "type", value)
-
-    @root_validator
-    @classmethod
-    def check_location(cls, values: dict) -> dict:
-        """
-        Validates the location of the structure based on the given parameters.
-        For instance, if a branchid is given, then it is expected also the chainage,
-        otherwise numcoordinates xcoordinates and ycoordinates shall be expected.
-
-        Args:
-            values (dict): Dictionary of values validated for the new structure.
-
-        Raises:
-            ValueError: When branchid or chainage values are not valid (empty strings).
-            ValueError: When the number of xcoordinates and ycoordinates do not match numcoordinates.
-
-        Returns:
-            dict: Dictionary of values validated for the new structure.
-        """
-        filtered_values = {k: v for k, v in values.items() if v is not None}
-        structype = filtered_values.get("type", "").lower()
-
-        if structype == "compound" or issubclass(cls, (Compound)):
-            # Compound structure does not require a location specification.
-            return values
-
-        # Backwards compatibility for old-style polylinefile input field (instead of num/x/yCoordinates):
-        polyline_compatible_structures = dict(
-            pump="Pump",
-            dambreak="Dambreak",
-            gate="Gate",
-            weir="Weir",
-            generalstructure="GeneralStructure",
-        )
-        polylinefile_in_model = (
-            structype in polyline_compatible_structures.keys()
-            and filtered_values.get("polylinefile") is not None
-        )
-
-        # No branchId+chainage for some structures:
-        only_coordinates_structures = dict(
-            longculvert="LongCulvert", dambreak="Dambreak"
-        )
-        coordinates_in_model = Structure.validate_coordinates_in_model(filtered_values)
-
-        # Error: do not allow both x/y and polyline file:
-        assert not (
-            polylinefile_in_model and coordinates_in_model
-        ), f"`Specify location either by `num/x/yCoordinates` or `polylinefile`, but not both."
-
-        # Error: require x/y or polyline file:
-        if (
-            structype in polyline_compatible_structures.keys()
-            and structype in only_coordinates_structures.keys()
-        ):
-            assert (
-                coordinates_in_model or polylinefile_in_model
-            ), f"Specify location either by setting `num/x/yCoordinates` or `polylinefile` fields for a {polyline_compatible_structures[structype]} structure."
-
-        # Error: Some structures require coordinates_in_model, but not branchId and chainage.
-        if (
-            not polylinefile_in_model
-            and structype in only_coordinates_structures.keys()
-        ):
-            assert (
-                coordinates_in_model
-            ), f"Specify location by setting `num/x/yCoordinates` for a {only_coordinates_structures[structype]} structure."
-
-        # Error: final check: at least one of x/y, branchId+chainage or polyline file must be given
-        branch_and_chainage_in_model = Structure.validate_branch_and_chainage_in_model(
-            filtered_values
-        )
-        assert (
-            branch_and_chainage_in_model
-            or coordinates_in_model
-            or polylinefile_in_model
-        ), "Specify location either by setting `branchId` and `chainage` or `num/x/yCoordinates` or `polylinefile` fields."
-
-        return values
-
-    @staticmethod
-    def validate_branch_and_chainage_in_model(values: dict) -> bool:
-        """
-        Static method to validate whether the given branchid and chainage values
-        match the expectation of a new structure.
-
-        Args:
-            values (dict): Dictionary of values to be used to generate a structure.
-
-        Raises:
-            ValueError: When the value for branchid or chainage are not valid.
-
-        Returns:
-            bool: Result of valid branchid / chainage in dictionary.
-        """
-        branchid = values.get("branchid", None)
-        if branchid is None:
-            return False
-
-        chainage = values.get("chainage", None)
-        if str_is_empty_or_none(branchid) or chainage is None:
-            raise ValueError(
-                "A valid value for branchId and chainage is required when branchId key is specified."
-            )
-        return True
-
-    @staticmethod
-    def validate_coordinates_in_model(values: dict) -> bool:
-        """
-        Static method to validate whether the given values match the expectations
-        of a structure to define its coordinates.
-
-        Args:
-            values (dict): Dictionary of values to be used to generate a structure.
-
-        Raises:
-            ValueError: When the given coordinates is less than 2.
-            ValueError: When the given coordinates do not match in expected size.
-
-        Returns:
-            bool: Result of valid coordinates in dictionary.
-        """
-        searched_keys = ["numcoordinates", "xcoordinates", "ycoordinates"]
-        if any(values.get(k, None) is None for k in searched_keys):
-            return False
-
-        n_coords = values["numcoordinates"]
-        if n_coords < 2:
-            raise ValueError(
-                f"Expected at least 2 coordinates, but only {n_coords} declared."
-            )
-
-        def get_coord_len(coord: str) -> int:
-            if values[coord] is None:
-                return 0
-            return len(values[coord])
-
-        len_x_coords = get_coord_len("xcoordinates")
-        len_y_coords = get_coord_len("ycoordinates")
-        if n_coords == len_x_coords == len_y_coords:
-            return True
-        raise ValueError(
-            f"Expected {n_coords} coordinates, given {len_x_coords} for xCoordinates and {len_y_coords} for yCoordinates."
-        )
-
-    @classmethod
-    def validate(cls, v):
-        """Try to initialize subclass based on the `type` field.
-        This field is compared to each `type` field of the derived models of `Structure`.
-        The derived model with an equal structure type will be initialized.
-
-        Raises:
-            ValueError: When the given type is not a known structure type.
-        """
-
-        # should be replaced by discriminated unions once merged
-        # https://github.com/samuelcolvin/pydantic/pull/2336
-        if isinstance(v, dict):
-            for c in cls.__subclasses__():
-                if (
-                    c.__fields__.get("type").default.lower()
-                    == v.get("type", "").lower()
-                ):
-                    v = c(**v)
-                    break
-            else:
-                raise ValueError(
-                    f"Type of {cls.__name__} with id={v.get('id', '')} and type={v.get('type', '')} is not recognized."
-                )
-        return super().validate(v)
-
-    def _exclude_fields(self) -> Set:
-        # exclude the non-applicable, or unset props like coordinates or branches
-        if self.type == "compound":
-            exclude_set = self._loc_all_fields
-        elif self.branchid is not None:
-            exclude_set = self._loc_coord_fields
-        else:
-            exclude_set = self._loc_branch_fields
-        exclude_set = super()._exclude_fields().union(exclude_set)
-        return exclude_set
-
-    def _get_identifier(self, data: dict) -> Optional[str]:
-        return data.get("id") or data.get("name")
-
-
-class FlowDirection(str, Enum):
-    """
-    Enum class containing the valid values for the allowedFlowDirection
-    attribute in several subclasses of Structure.
-    """
-
-    none = "none"
-    positive = "positive"
-    negative = "negative"
-    both = "both"
-    allowedvaluestext = "Possible values: both, positive, negative, none."
-
-
-class Orientation(str, Enum):
-    """
-    Enum class containing the valid values for the orientation
-    attribute in several subclasses of Structure.
-    """
-
-    positive = "positive"
-    negative = "negative"
-    allowedvaluestext = "Possible values: positive, negative."
-
-
-class Weir(Structure):
-    """
-    Hydraulic structure with `type=weir`, to be included in a structure file.
-    Typically inside the structure list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.structurefile[0].structure[..]`
-
-    All lowercased attributes match with the weir input as described in
-    [UM Sec.C.12.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.12.1).
-    """
-
-    class Comments(Structure.Comments):
-        type: Optional[str] = Field("Structure type; must read weir", alias="type")
-        allowedflowdir: Optional[str] = Field(
-            FlowDirection.allowedvaluestext, alias="allowedFlowdir"
-        )
-
-        crestlevel: Optional[str] = Field(
-            "Crest level of weir (m AD).", alias="crestLevel"
-        )
-        crestwidth: Optional[str] = Field("Width of the weir (m).", alias="crestWidth")
-        corrcoeff: Optional[str] = Field(
-            "Correction coefficient (-).", alias="corrCoeff"
-        )
-        usevelocityheight: Optional[str] = Field(
-            "Flag indicating whether the velocity height is to be calculated or not.",
-            alias="useVelocityHeight",
-        )
-
-    comments: Comments = Comments()
-
-    type: Literal["weir"] = Field("weir", alias="type")
-    allowedflowdir: Optional[FlowDirection] = Field(
-        FlowDirection.both.value, alias="allowedFlowDir"
-    )
-
-    crestlevel: ForcingData = Field(alias="crestLevel")
-    crestwidth: Optional[float] = Field(None, alias="crestWidth")
-    corrcoeff: float = Field(1.0, alias="corrCoeff")
-    usevelocityheight: bool = Field(True, alias="useVelocityHeight")
-
-    _flowdirection_validator = get_enum_validator("allowedflowdir", enum=FlowDirection)
-
-
-class UniversalWeir(Structure):
-    """
-    Hydraulic structure with `type=universalWeir`, to be included in a structure file.
-    Typically inside the structure list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.structurefile[0].structure[..]`
-
-    All lowercased attributes match with the universal weir input as described in
-    [UM Sec.C.12.2](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.12.2).
-    """
-
-    class Comments(Structure.Comments):
-        type: Optional[str] = Field(
-            "Structure type; must read universalWeir", alias="type"
-        )
-        allowedflowdir: Optional[str] = Field(
-            FlowDirection.allowedvaluestext, alias="allowedFlowdir"
-        )
-
-        numlevels: Optional[str] = Field("Number of yz-Values.", alias="numLevels")
-        yvalues: Optional[str] = Field(
-            "y-values of the cross section (m). (number of values = numLevels)",
-            alias="yValues",
-        )
-        zvalues: Optional[str] = Field(
-            "z-values of the cross section (m). (number of values = numLevels)",
-            alias="zValues",
-        )
-        crestlevel: Optional[str] = Field(
-            "Crest level of weir (m AD).", alias="crestLevel"
-        )
-        dischargecoeff: Optional[str] = Field(
-            "Discharge coefficient c_e (-).", alias="dischargeCoeff"
-        )
-
-    comments: Comments = Comments()
-
-    type: Literal["universalWeir"] = Field("universalWeir", alias="type")
-    allowedflowdir: FlowDirection = Field(alias="allowedFlowDir")
-
-    numlevels: int = Field(alias="numLevels")
-    yvalues: List[float] = Field(alias="yValues")
-    zvalues: List[float] = Field(alias="zValues")
-    crestlevel: float = Field(alias="crestLevel")
-    dischargecoeff: float = Field(alias="dischargeCoeff")
-
-    _split_to_list = get_split_string_on_delimiter_validator("yvalues", "zvalues")
-    _flowdirection_validator = get_enum_validator("allowedflowdir", enum=FlowDirection)
-
-
-class CulvertSubType(str, Enum):
-    """Enum class to store a [Culvert][hydrolib.core.dflowfm.structure.models.Culvert]'s subType."""
-
-    culvert = "culvert"
-    invertedSiphon = "invertedSiphon"
-
-
-class Culvert(Structure):
-    """
-    Hydraulic structure with `type=culvert`, to be included in a structure file.
-    Typically inside the structure list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.structurefile[0].structure[..]`
-
-    All lowercased attributes match with the culvert input as described in
-    [UM Sec.C.12.3](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.12.3).
-    """
-
-    type: Literal["culvert"] = Field("culvert", alias="type")
-    allowedflowdir: FlowDirection = Field(alias="allowedFlowDir")
-
-    leftlevel: float = Field(alias="leftLevel")
-    rightlevel: float = Field(alias="rightLevel")
-    csdefid: str = Field(alias="csDefId")
-    length: float = Field(alias="length")
-    inletlosscoeff: float = Field(alias="inletLossCoeff")
-    outletlosscoeff: float = Field(alias="outletLossCoeff")
-    valveonoff: bool = Field(alias="valveOnOff")
-    valveopeningheight: Optional[ForcingData] = Field(alias="valveOpeningHeight")
-    numlosscoeff: Optional[int] = Field(alias="numLossCoeff")
-    relopening: Optional[List[float]] = Field(alias="relOpening")
-    losscoeff: Optional[List[float]] = Field(alias="lossCoeff")
-    bedfrictiontype: Optional[FrictionType] = Field(alias="bedFrictionType")
-    bedfriction: Optional[float] = Field(alias="bedFriction")
-    subtype: Optional[CulvertSubType] = Field(
-        CulvertSubType.culvert.value, alias="subType"
-    )
-    bendlosscoeff: Optional[float] = Field(alias="bendLossCoeff")
-
-    _split_to_list = get_split_string_on_delimiter_validator("relopening", "losscoeff")
-    _flowdirection_validator = get_enum_validator("allowedflowdir", enum=FlowDirection)
-    _subtype_validator = get_enum_validator("subtype", enum=CulvertSubType)
-    _frictiontype_validator = get_enum_validator("bedfrictiontype", enum=FrictionType)
-
-    @root_validator(allow_reuse=True)
-    def validate_that_valve_related_fields_are_present_for_culverts_with_valves(
-        cls, values: Dict
-    ) -> Dict:
-        """Validates that valve-related fields are present when there is a valve present."""
-        return validate_required_fields(
-            values,
-            "valveopeningheight",
-            "numlosscoeff",
-            "relopening",
-            "losscoeff",
-            conditional_field_name="valveonoff",
-            conditional_value=True,
-        )
-
-    @root_validator(allow_reuse=True)
-    def validate_that_bendlosscoeff_field_is_present_for_invertedsyphons(
-        cls, values: Dict
-    ) -> Dict:
-        """Validates that the bendlosscoeff value is present when dealing with inverted syphons."""
-        return validate_required_fields(
-            values,
-            "bendlosscoeff",
-            conditional_field_name="subtype",
-            conditional_value=CulvertSubType.invertedSiphon,
-        )
-
-    @root_validator(allow_reuse=True)
-    def check_list_lengths(cls, values):
-        """Validates that the length of the relopening and losscoeff fields are as expected."""
-        return validate_correct_length(
-            values,
-            "relopening",
-            "losscoeff",
-            length_name="numlosscoeff",
-            list_required_with_length=True,
-        )
-
-    @root_validator(allow_reuse=True)
-    def validate_that_bendlosscoeff_is_not_provided_for_culverts(
-        cls, values: Dict
-    ) -> Dict:
-        """Validates that the bendlosscoeff field is not provided when the subtype is a culvert."""
-        return validate_forbidden_fields(
-            values,
-            "bendlosscoeff",
-            conditional_field_name="subtype",
-            conditional_value=CulvertSubType.culvert,
-        )
-
-
-class Pump(Structure):
-    """
-    Hydraulic structure with `type=pump`, to be included in a structure file.
-    Typically inside the structure list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.structurefile[0].structure[..]`
-
-    All lowercased attributes match with the pump input as described in
-    [UM Sec.C.12.6](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.12.6).
-    """
-
-    type: Literal["pump"] = Field("pump", alias="type")
-
-    orientation: Optional[Orientation] = Field(alias="orientation")
-    controlside: Optional[str] = Field(alias="controlSide")  # TODO Enum
-    numstages: Optional[int] = Field(alias="numStages")
-    capacity: ForcingData = Field(alias="capacity")
-
-    startlevelsuctionside: Optional[List[float]] = Field(alias="startLevelSuctionSide")
-    stoplevelsuctionside: Optional[List[float]] = Field(alias="stopLevelSuctionSide")
-    startleveldeliveryside: Optional[List[float]] = Field(
-        alias="startLevelDeliverySide"
-    )
-    stopleveldeliveryside: Optional[List[float]] = Field(alias="stopLevelDeliverySide")
-    numreductionlevels: Optional[int] = Field(alias="numReductionLevels")
-    head: Optional[List[float]] = Field(alias="head")
-    reductionfactor: Optional[List[float]] = Field(alias="reductionFactor")
-
-    _split_to_list = get_split_string_on_delimiter_validator(
-        "startlevelsuctionside",
-        "stoplevelsuctionside",
-        "startleveldeliveryside",
-        "stopleveldeliveryside",
-        "head",
-        "reductionfactor",
-    )
-
-    _orientation_validator = get_enum_validator("orientation", enum=Orientation)
-
-    @root_validator(allow_reuse=True)
-    def validate_that_controlside_is_provided_when_numstages_is_provided(
-        cls, values: Dict
-    ) -> Dict:
-        return validate_required_fields(
-            values,
-            "controlside",
-            conditional_field_name="numstages",
-            conditional_value=0,
-            comparison_func=gt,
-        )
-
-    @classmethod
-    def _check_list_lengths_suctionside(cls, values: Dict) -> Dict:
-        """Validates that the length of the startlevelsuctionside and stoplevelsuctionside fields are as expected."""
-        return validate_correct_length(
-            values,
-            "startlevelsuctionside",
-            "stoplevelsuctionside",
-            length_name="numstages",
-            list_required_with_length=True,
-        )
-
-    @root_validator(allow_reuse=True)
-    def conditionally_check_list_lengths_suctionside(cls, values: Dict) -> Dict:
-        """
-        Validates the length of the suction side fields, but only if there is a controlside value
-        present in the values and the controlside is not equal to the deliverySide.
-        """
-        return validate_conditionally(
-            cls,
-            values,
-            Pump._check_list_lengths_suctionside,
-            "controlside",
-            "deliverySide",
-            ne,
-        )
-
-    @classmethod
-    def _check_list_lengths_deliveryside(cls, values: Dict) -> Dict:
-        """Validates that the length of the startleveldeliveryside and stopleveldeliveryside fields are as expected."""
-        return validate_correct_length(
-            values,
-            "startleveldeliveryside",
-            "stopleveldeliveryside",
-            length_name="numstages",
-            list_required_with_length=True,
-        )
-
-    @root_validator(allow_reuse=True)
-    def conditionally_check_list_lengths_deliveryside(cls, values: Dict) -> Dict:
-        """
-        Validates the length of the delivery side fields, but only if there is a controlside value
-        present in the values and the controlside is not equal to the suctionSide.
-        """
-        return validate_conditionally(
-            cls,
-            values,
-            Pump._check_list_lengths_deliveryside,
-            "controlside",
-            "suctionSide",
-            ne,
-        )
-
-    @root_validator(allow_reuse=True)
-    def check_list_lengths_head_and_reductionfactor(cls, values):
-        """Validates that the lengths of the head and reductionfactor fields are as expected."""
-        return validate_correct_length(
-            values,
-            "head",
-            "reductionfactor",
-            length_name="numreductionlevels",
-            list_required_with_length=True,
-        )
-
-
-class Compound(Structure):
-    """
-    Hydraulic structure with `type=compound`, to be included in a structure file.
-    Typically inside the structure list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.structurefile[0].structure[..]`
-
-    All lowercased attributes match with the compound input as described in
-    [UM Sec.C.12.11](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.12.11).
-    """
-
-    type: Literal["compound"] = Field("compound", alias="type")
-    numstructures: int = Field(alias="numStructures")
-    structureids: List[str] = Field(alias="structureIds", delimiter=";")
-
-    _split_to_list = get_split_string_on_delimiter_validator(
-        "structureids",
-    )
-
-
-class Orifice(Structure):
-    """
-    Hydraulic structure with `type=orifice`, to be included in a structure file.
-    Typically inside the structure list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.structurefile[0].structure[..]`
-
-    All lowercased attributes match with the orifice input as described in
-    [UM Sec.C.12.7](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.12.7).
-    """
-
-    type: Literal["orifice"] = Field("orifice", alias="type")
-    allowedflowdir: Optional[FlowDirection] = Field(
-        FlowDirection.both.value, alias="allowedFlowDir"
-    )
-
-    crestlevel: ForcingData = Field(alias="crestLevel")
-    crestwidth: Optional[float] = Field(None, alias="crestWidth")
-    gateloweredgelevel: ForcingData = Field(alias="gateLowerEdgeLevel")
-    corrcoeff: float = Field(1.0, alias="corrCoeff")
-    usevelocityheight: bool = Field(True, alias="useVelocityHeight")
-
-    # TODO Use a validator here to check the optionals related to the bool field
-    uselimitflowpos: Optional[bool] = Field(False, alias="useLimitFlowPos")
-    limitflowpos: Optional[float] = Field(alias="limitFlowPos")
-
-    uselimitflowneg: Optional[bool] = Field(False, alias="useLimitFlowNeg")
-    limitflowneg: Optional[float] = Field(alias="limitFlowNeg")
-
-    _flowdirection_validator = get_enum_validator("allowedflowdir", enum=FlowDirection)
-
-    @validator("limitflowpos", always=True)
-    @classmethod
-    def _validate_limitflowpos(cls, v, values):
-        return cls._validate_limitflow(v, values, "limitFlowPos", "useLimitFlowPos")
-
-    @validator("limitflowneg", always=True)
-    @classmethod
-    def _validate_limitflowneg(cls, v, values):
-        return cls._validate_limitflow(v, values, "limitFlowNeg", "useLimitFlowNeg")
-
-    @classmethod
-    def _validate_limitflow(cls, v, values, limitflow: str, uselimitflow: str):
-        if v is None and values[uselimitflow.lower()] == True:
-            raise ValueError(
-                f"{limitflow} should be defined when {uselimitflow} is true"
-            )
-
-        return v
-
-
-class GateOpeningHorizontalDirection(str, Enum):
-    """Horizontal opening direction of gate door[s]."""
-
-    symmetric = "symmetric"
-    from_left = "fromLeft"
-    from_right = "fromRight"
-    allowedvaluestext = "Possible values: symmetric, fromLeft, fromRight."
-
-
-class GeneralStructure(Structure):
-    """
-    Hydraulic structure with `type=generalStructure`, to be included in a structure file.
-    Typically inside the structure list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.structurefile[0].structure[..]`
-
-    All lowercased attributes match with the orifice input as described in
-    [UM Sec.C.12.9](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.12.9).
-    """
-
-    class Comments(Structure.Comments):
-        type: Optional[str] = Field(
-            "Structure type; must read generalStructure", alias="type"
-        )
-        allowedflowdir: Optional[str] = Field(
-            FlowDirection.allowedvaluestext, alias="allowedFlowDir"
-        )
-
-        upstream1width: Optional[str] = Field("w_u1 [m]", alias="upstream1Width")
-        upstream1level: Optional[str] = Field("z_u1 [m AD]", alias="upstream1Level")
-        upstream2width: Optional[str] = Field("w_u2 [m]", alias="upstream2Width")
-        upstream2level: Optional[str] = Field("z_u2 [m D]", alias="upstream2Level")
-
-        crestwidth: Optional[str] = Field("w_s [m]", alias="crestWidth")
-        crestlevel: Optional[str] = Field("z_s [m AD]", alias="crestLevel")
-        crestlength: Optional[str] = Field(
-            "The crest length across the general structure [m]. When the crest length > 0, the extra resistance for this structure will be ls * g/(C2 * waterdepth)",
-            alias="crestLength",
-        )
-
-        downstream1width: Optional[str] = Field("w_d1 [m]", alias="downstream1Width")
-        downstream1level: Optional[str] = Field("z_d1 [m AD]", alias="downstream1Level")
-        downstream2width: Optional[str] = Field("w_d2 [m]", alias="downstream2Width")
-        downstream2level: Optional[str] = Field("z_d2 [m AD]", alias="downstream2Level")
-
-        gateloweredgelevel: Optional[str] = Field(
-            "Position of gate doorâ€™s lower edge [m AD]", alias="gateLowerEdgeLevel"
-        )
-        posfreegateflowcoeff: Optional[str] = Field(
-            "Positive free gate flow corr.coeff. cgf [-]", alias="posFreeGateFlowCoeff"
-        )
-        posdrowngateflowcoeff: Optional[str] = Field(
-            "Positive drowned gate flow corr.coeff. cgd [-]",
-            alias="posDrownGateFlowCoeff",
-        )
-        posfreeweirflowcoeff: Optional[str] = Field(
-            "Positive free weir flow corr.coeff. cwf [-]", alias="posFreeWeirFlowCoeff"
-        )
-        posdrownweirflowcoeff: Optional[str] = Field(
-            "Positive drowned weir flow corr.coeff. cwd [-]",
-            alias="posDrownWeirFlowCoeff",
-        )
-        poscontrcoeffreegate: Optional[str] = Field(
-            "Positive gate flow contraction coefficient Âµgf [-]",
-            alias="posContrCoefFreeGate",
-        )
-        negfreegateflowcoeff: Optional[str] = Field(
-            "Negative free gate flow corr.coeff. cgf [-]", alias="negFreeGateFlowCoeff"
-        )
-        negdrowngateflowcoeff: Optional[str] = Field(
-            "Negative drowned gate flow corr.coeff. cgd [-]",
-            alias="negDrownGateFlowCoeff",
-        )
-        negfreeweirflowcoeff: Optional[str] = Field(
-            "Negative free weir flow corr.coeff. cwf [-]", alias="negFreeWeirFlowCoeff"
-        )
-        negdrownweirflowcoeff: Optional[str] = Field(
-            "Negative drowned weir flow corr.coeff. cwd [-]",
-            alias="negDrownWeirFlowCoeff",
-        )
-        negcontrcoeffreegate: Optional[str] = Field(
-            "Negative gate flow contraction coefficient mu gf [-]",
-            alias="negContrCoefFreeGate",
-        )
-        extraresistance: Optional[str] = Field(
-            "Extra resistance [-]", alias="extraResistance"
-        )
-        gateheight: Optional[str] = Field(None, alias="gateHeight")
-        gateopeningwidth: Optional[str] = Field(
-            "Opening width between gate doors [m], should be smaller than (or equal to) crestWidth",
-            alias="gateOpeningWidth",
-        )
-        gateopeninghorizontaldirection: Optional[str] = Field(
-            "Horizontal opening direction of gate door[s]. Possible values are: symmetric, fromLeft, fromRight",
-            alias="gateOpeningHorizontalDirection",
-        )
-        usevelocityheight: Optional[str] = Field(
-            "Flag indicates whether the velocity height is to be calculated or not",
-            alias="useVelocityHeight",
-        )
-
-    comments: Optional[Comments] = Comments()
-
-    type: Literal["generalStructure"] = Field("generalStructure", alias="type")
-    allowedflowdir: Optional[FlowDirection] = Field(
-        FlowDirection.both.value, alias="allowedFlowDir"
-    )
-
-    upstream1width: Optional[float] = Field(10.0, alias="upstream1Width")
-    upstream1level: Optional[float] = Field(0.0, alias="upstream1Level")
-    upstream2width: Optional[float] = Field(10.0, alias="upstream2Width")
-    upstream2level: Optional[float] = Field(0.0, alias="upstream2Level")
-
-    crestwidth: Optional[float] = Field(10.0, alias="crestWidth")
-    crestlevel: Optional[ForcingData] = Field(0.0, alias="crestLevel")
-    crestlength: Optional[float] = Field(0.0, alias="crestLength")
-
-    downstream1width: Optional[float] = Field(10.0, alias="downstream1Width")
-    downstream1level: Optional[float] = Field(0.0, alias="downstream1Level")
-    downstream2width: Optional[float] = Field(10.0, alias="downstream2Width")
-    downstream2level: Optional[float] = Field(0.0, alias="downstream2Level")
-
-    gateloweredgelevel: Optional[ForcingData] = Field(11.0, alias="gateLowerEdgeLevel")
-    posfreegateflowcoeff: Optional[float] = Field(1.0, alias="posFreeGateFlowCoeff")
-    posdrowngateflowcoeff: Optional[float] = Field(1.0, alias="posDrownGateFlowCoeff")
-    posfreeweirflowcoeff: Optional[float] = Field(1.0, alias="posFreeWeirFlowCoeff")
-    posdrownweirflowcoeff: Optional[float] = Field(1.0, alias="posDrownWeirFlowCoeff")
-    poscontrcoeffreegate: Optional[float] = Field(1.0, alias="posContrCoefFreeGate")
-    negfreegateflowcoeff: Optional[float] = Field(1.0, alias="negFreeGateFlowCoeff")
-    negdrowngateflowcoeff: Optional[float] = Field(1.0, alias="negDrownGateFlowCoeff")
-    negfreeweirflowcoeff: Optional[float] = Field(1.0, alias="negFreeWeirFlowCoeff")
-    negdrownweirflowcoeff: Optional[float] = Field(1.0, alias="negDrownWeirFlowCoeff")
-    negcontrcoeffreegate: Optional[float] = Field(1.0, alias="negContrCoefFreeGate")
-    extraresistance: Optional[float] = Field(0.0, alias="extraResistance")
-    gateheight: Optional[float] = Field(1e10, alias="gateHeight")
-    gateopeningwidth: Optional[ForcingData] = Field(0.0, alias="gateOpeningWidth")
-    gateopeninghorizontaldirection: Optional[GateOpeningHorizontalDirection] = Field(
-        GateOpeningHorizontalDirection.symmetric.value,
-        alias="gateOpeningHorizontalDirection",
-    )
-    usevelocityheight: Optional[bool] = Field(True, alias="useVelocityHeight")
-
-
-class DambreakAlgorithm(int, Enum):
-    van_der_knaap = 1  # "van der Knaap, 2000"
-    verheij_van_der_knaap = 2  # "Verheij-van der Knaap, 2002"
-    timeseries = 3  # "Predefined time series, dambreakLevelsAndWidths."
-
-    @property
-    def description(self) -> str:
-        """
-        Property to return the description of the enums defined above.
-        Useful for comments in output files.
-
-        Returns:
-            str: Description for the current enum.
-        """
-        description_dict = dict(
-            van_der_knaap="van der Knaap, 2000",
-            verheij_van_der_knaap="Verheij-van der Knaap, 2002",
-            timeseries="Predefined time series, dambreakLevelsAndWidths",
-        )
-        return description_dict[self.name]
-
-
-class Dambreak(Structure):
-    """
-    Hydraulic structure with `type=dambreak`, to be included in a structure file.
-    Typically inside the structure list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.structurefile[0].structure[..]`
-
-    All lowercased attributes match with the dambreak input as described in
-    [UM Sec.C.12.10](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.12.10).
-    """
-
-    class Comments(Structure.Comments):
-        type: Optional[str] = Field("Structure type; must read dambreak", alias="type")
-        startlocationx: Optional[str] = Field(
-            "x-coordinate of breach starting point.", alias="startLocationX"
-        )
-        startlocationy: Optional[str] = Field(
-            "y-coordinate of breach starting point.", alias="startLocationY"
-        )
-        algorithm: Optional[str] = Field(
-            "Breach growth algorithm. Possible values are: 1 (van der Knaap (2000)), 2 (Verheijâ€“van der Knaap (2002)), 3: Predefined time series, see dambreakLevelsAndWidths",
-            alias="algorithm",
-        )
-        crestlevelini: Optional[str] = Field(
-            "Initial breach level zcrest level [m AD].", alias="crestLevelIni"
-        )
-        breachwidthini: Optional[str] = Field(
-            "Initial breach width B0 [m].", alias="breachWidthIni"
-        )
-        crestlevelmin: Optional[str] = Field(
-            "Minimal breach level zmin [m AD].", alias="crestLevelMin"
-        )
-        t0: Optional[str] = Field("Breach start time Tstart [s].", alias="t0")
-        timetobreachtomaximumdepth: Optional[str] = Field(
-            "tPhase 1 [s].", alias="timeToBreachToMaximumDepth"
-        )
-        f1: Optional[str] = Field("Factor f1 [-]", alias="f1")
-        f2: Optional[str] = Field("Factor f2 [-]", alias="f2")
-        ucrit: Optional[str] = Field(
-            "Critical flow velocity uc for erosion [m/s].", alias="uCrit"
-        )
-        waterlevelupstreamlocationx: Optional[str] = Field(
-            "(optional) x-coordinate of custom upstream water level point.",
-            alias="waterLevelUpstreamLocationX",
-        )
-        waterlevelupstreamlocationy: Optional[str] = Field(
-            "(optional) y-coordinate of custom upstream water level point.",
-            alias="waterLevelUpstreamLocationY",
-        )
-        waterleveldownstreamlocationx: Optional[str] = Field(
-            "(optional) x-coordinate of custom downstream water level point.",
-            alias="waterLevelDownstreamLocationX",
-        )
-        waterleveldownstreamlocationy: Optional[str] = Field(
-            "(optional) y-coordinate of custom downstream water level point.",
-            alias="waterLevelDownstreamLocationY",
-        )
-        waterlevelupstreamnodeid: Optional[str] = Field(
-            "(optional) Node Id of custom upstream water level point.",
-            alias="waterLevelUpstreamNodeId",
-        )
-        waterleveldownstreamnodeid: Optional[str] = Field(
-            "(optional) Node Id of custom downstream water level point.",
-            alias="waterLevelDownstreamNodeId",
-        )
-        dambreaklevelsandwidths: Optional[str] = Field(
-            "(only when algorithm=3) Filename of <*.tim> file (Section C.4) containing the breach levels and widths.",
-            alias="dambreakLevelsAndWidths",
-        )
-
-    comments: Comments = Comments()
-    type: Literal["dambreak"] = Field("dambreak", alias="type")
-    startlocationx: float = Field(alias="startLocationX")
-    startlocationy: float = Field(alias="startLocationY")
-    algorithm: DambreakAlgorithm = Field(alias="algorithm")
-
-    crestlevelini: float = Field(alias="crestLevelIni")
-    breachwidthini: float = Field(alias="breachWidthIni")
-    crestlevelmin: float = Field(alias="crestLevelMin")
-    t0: float = Field(alias="t0")
-    timetobreachtomaximumdepth: float = Field(alias="timeToBreachToMaximumDepth")
-    f1: float = Field(alias="f1")
-    f2: float = Field(alias="f2")
-    ucrit: float = Field(alias="uCrit")
-    waterlevelupstreamlocationx: Optional[float] = Field(
-        alias="waterLevelUpstreamLocationX"
-    )
-    waterlevelupstreamlocationy: Optional[float] = Field(
-        alias="waterLevelUpstreamLocationY"
-    )
-    waterleveldownstreamlocationx: Optional[float] = Field(
-        alias="waterLevelDownstreamLocationX"
-    )
-    waterleveldownstreamlocationy: Optional[float] = Field(
-        alias="waterLevelDownstreamLocationY"
-    )
-    waterlevelupstreamnodeid: Optional[str] = Field(alias="waterLevelUpstreamNodeId")
-    waterleveldownstreamnodeid: Optional[str] = Field(
-        alias="waterLevelDownstreamNodeId"
-    )
-    dambreaklevelsandwidths: Optional[Union[TimModel, ForcingModel]] = Field(
-        alias="dambreakLevelsAndWidths"
-    )
-
-    @validator("algorithm", pre=True)
-    @classmethod
-    def validate_algorithm(cls, value: str) -> DambreakAlgorithm:
-        """
-        Validates the algorithm parameter for the dambreak structure.
-
-        Args:
-            value (int): algorithm value read from the user's input.
-
-        Raises:
-            ValueError: When the value given is not of type int.
-            ValueError: When the value given is not in the range [1,3]
-
-        Returns:
-            int: Validated value.
-        """
-        int_value = -1
-        try:
-            int_value = int(value)
-        except Exception:
-            raise ValueError("Dambreak algorithm value should be of type int.")
-        if 0 < int_value <= 3:
-            return DambreakAlgorithm(int_value)
-        raise ValueError("Dambreak algorithm value should be 1, 2 or 3.")
-
-    @validator("dambreaklevelsandwidths")
-    @classmethod
-    def validate_dambreak_levels_and_widths(
-        cls, field_value: Optional[Union[TimModel, ForcingModel]], values: dict
-    ) -> Optional[Union[TimModel, ForcingModel]]:
-        """
-        Validates whether a dambreak can be created with the given dambreakLevelsAndWidths
-        property. This property should be given when the algorithm value is 3.
-
-        Args:
-            field_value (Optional[Union[TimModel, ForcingModel]]): Value given for dambreakLevelsAndWidths.
-            values (dict): Dictionary of values already validated (assuming algorithm is in it).
-
-        Raises:
-            ValueError: When algorithm value is not 3 and field_value has a value.
-
-        Returns:
-            Optional[Union[TimModel, ForcingModel]]: The value given for dambreakLevelsAndwidths.
-        """
-        # Retrieve the algorithm value (if not found use 0).
-        algorithm_value = values.get("algorithm", 0)
-        if field_value is not None and algorithm_value != 3:
-            # dambreakLevelsAndWidths can only be set when algorithm = 3
-            raise ValueError(
-                f"Dambreak field dambreakLevelsAndWidths can only be set when algorithm = 3, current value: {algorithm_value}."
-            )
-        return field_value
-
-    @root_validator
-    @classmethod
-    def check_location_dambreak(cls, values: dict) -> dict:
-        """
-        Verifies whether the location for this structure contains valid values for
-        numCoordinates, xCoordinates and yCoordinates or instead is using a polyline file.
-        Verifies whether de water level location specifications are valid.
-
-        Args:
-            values (dict): Dictionary of validated values to create a Dambreak.
-
-        Raises:
-            ValueError: When the values dictionary does not contain valid coordinates or polyline file or when the water level location specifications are not valid.
-
-        Returns:
-            dict: Dictionary of validated values.
-        """
-
-        def _validate_waterlevel_location(x_key: str, y_key: str, node_key: str):
-            x_is_given = values.get(x_key.lower()) is not None
-            y_is_given = values.get(y_key.lower()) is not None
-            node_is_given = values.get(node_key.lower()) is not None
-
-            if (x_is_given and y_is_given and not node_is_given) or (
-                node_is_given and not x_is_given and not y_is_given
-            ):
-                return
-
-            raise ValueError(
-                f"Either `{node_key}` should be specified or `{x_key}` and `{y_key}`."
-            )
-
-        _validate_waterlevel_location(
-            "waterLevelUpstreamLocationX",
-            "waterLevelUpstreamLocationY",
-            "waterLevelUpstreamNodeId",
-        )
-        _validate_waterlevel_location(
-            "waterLevelDownstreamLocationX",
-            "waterLevelDownstreamLocationY",
-            "waterLevelDownstreamNodeId",
-        )
-
-        return values
-
-
-class Bridge(Structure):
-    """
-    Hydraulic structure with `type=bridge`, to be included in a structure file.
-    Typically inside the structure list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.structurefile[0].structure[..]`
-
-    All lowercased attributes match with the bridge input as described in
-    [UM Sec.C.12.5](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.12.5).
-    """
-
-    class Comments(Structure.Comments):
-        type: Optional[str] = Field("Structure type; must read bridge", alias="type")
-        allowedflowdir: Optional[str] = Field(
-            FlowDirection.allowedvaluestext, alias="allowedFlowdir"
-        )
-
-        csdefid: Optional[str] = Field(
-            "Id of Cross-Section Definition.", alias="csDefId"
-        )
-        shift: Optional[str] = Field(
-            "Vertical shift of the cross section definition [m]. Defined positive upwards."
-        )
-        inletlosscoeff: Optional[str] = Field(
-            "Inlet loss coefficient [-], Î¾_i.",
-            alias="inletLossCoeff",
-        )
-        outletlosscoeff: Optional[str] = Field(
-            "Outlet loss coefficient [-], k.",
-            alias="outletLossCoeff",
-        )
-        frictiontype: Optional[str] = Field(
-            "Friction type, possible values are: Chezy, Manning, wallLawNikuradse, WhiteColebrook, StricklerNikuradse, Strickler, deBosBijkerk.",
-            alias="frictionType",
-        )
-        friction: Optional[str] = Field(
-            "Friction value, used in friction loss.",
-            alias="friction",
-        )
-        length: Optional[str] = Field("Length [m], L.")
-
-    comments: Comments = Comments()
-
-    type: Literal["bridge"] = Field("bridge", alias="type")
-    allowedflowdir: FlowDirection = Field(alias="allowedFlowdir")
-
-    csdefid: str = Field(alias="csDefId")
-    shift: float
-    inletlosscoeff: float = Field(alias="inletLossCoeff")
-    outletlosscoeff: float = Field(alias="outletLossCoeff")
-    frictiontype: FrictionType = Field(alias="frictionType")
-    friction: float
-    length: float
-
-    _frictiontype_validator = get_enum_validator("frictiontype", enum=FrictionType)
-
-
-class StructureGeneral(INIGeneral):
-    """`[General]` section with structure file metadata."""
-
-    _header: Literal["General"] = "General"
-    fileversion: str = Field("3.00", alias="fileVersion")
-    filetype: Literal["structure"] = Field("structure", alias="fileType")
-
-
-class StructureModel(INIModel):
-    """
-    The overall structure model that contains the contents of one structure file.
-
-    This model is typically referenced under a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.structurefile[..]`.
-
-    Attributes:
-        general (StructureGeneral): `[General]` block with file metadata.
-        branch (List[Structure]): List of `[Structure]` blocks for all hydraulic structures.
-    """
-
-    general: StructureGeneral = StructureGeneral()
-    structure: List[Structure] = []
-
-    @classmethod
-    def _ext(cls) -> str:
-        return ".ini"
-
-    @classmethod
-    def _filename(cls) -> str:
-        return "structures"
-
-    _split_to_list = make_list_validator("structure")
+"""
+structure namespace for storing the contents of an [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]'s structure file.
+"""
+# TODO Implement the following structures
+# - Gate
+
+import logging
+from enum import Enum
+from operator import gt, ne
+from typing import Dict, List, Literal, Optional, Set, Union
+
+from pydantic import Field
+from pydantic.class_validators import root_validator, validator
+
+from hydrolib.core.basemodel import DiskOnlyFileModel
+from hydrolib.core.dflowfm.bc.models import ForcingModel
+from hydrolib.core.dflowfm.friction.models import FrictionType
+from hydrolib.core.dflowfm.ini.models import INIBasedModel, INIGeneral, INIModel
+from hydrolib.core.dflowfm.ini.util import (
+    get_enum_validator,
+    get_from_subclass_defaults,
+    get_split_string_on_delimiter_validator,
+    make_list_validator,
+    validate_conditionally,
+    validate_correct_length,
+    validate_forbidden_fields,
+    validate_required_fields,
+)
+from hydrolib.core.dflowfm.tim.models import TimModel
+from hydrolib.core.utils import str_is_empty_or_none
+
+logger = logging.getLogger(__name__)
+
+ForcingData = Union[float, TimModel, ForcingModel]
+
+# TODO: handle comment blocks
+# TODO: handle duplicate keys
+class Structure(INIBasedModel):
+    # TODO: would we want to load this from something externally and generate these automatically
+    class Comments(INIBasedModel.Comments):
+        id: Optional[str] = "Unique structure id (max. 256 characters)."
+        name: Optional[str] = "Given name in the user interface."
+        polylinefile: Optional[str] = Field(
+            "*.pli; Polyline geometry definition for 2D structure.",
+            alias="polylinefile",
+        )
+        branchid: Optional[str] = Field(
+            "Branch on which the structure is located.", alias="branchId"
+        )
+        chainage: Optional[str] = "Chainage on the branch (m)."
+
+        numcoordinates: Optional[str] = Field(
+            "Number of values in xCoordinates and yCoordinates", alias="numCoordinates"
+        )
+        xcoordinates: Optional[str] = Field(
+            "x-coordinates of the location of the structure. (number of values = numCoordinates)",
+            alias="xCoordinates",
+        )
+        ycoordinates: Optional[str] = Field(
+            "y-coordinates of the location of the structure. (number of values = numCoordinates)",
+            alias="yCoordinates",
+        )
+
+    comments: Comments = Comments()
+
+    _header: Literal["Structure"] = "Structure"
+
+    id: str = Field("id", max_length=256, alias="id")
+    name: str = Field("id", alias="name")
+    type: str = Field(alias="type")
+
+    polylinefile: Optional[DiskOnlyFileModel] = Field(None, alias="polylinefile")
+
+    branchid: Optional[str] = Field(None, alias="branchId")
+    chainage: Optional[float] = Field(None, alias="chainage")
+
+    numcoordinates: Optional[int] = Field(None, alias="numCoordinates")
+    xcoordinates: Optional[List[float]] = Field(None, alias="xCoordinates")
+    ycoordinates: Optional[List[float]] = Field(None, alias="yCoordinates")
+
+    _loc_coord_fields = {"numcoordinates", "xcoordinates", "ycoordinates"}
+    _loc_branch_fields = {"branchid", "chainage"}
+    _loc_all_fields = _loc_coord_fields | _loc_branch_fields
+
+    _split_to_list = get_split_string_on_delimiter_validator(
+        "xcoordinates", "ycoordinates"
+    )
+
+    @validator("type", pre=True)
+    def _validate_type(cls, value):
+        return get_from_subclass_defaults(Structure, "type", value)
+
+    @root_validator
+    @classmethod
+    def check_location(cls, values: dict) -> dict:
+        """
+        Validates the location of the structure based on the given parameters.
+        For instance, if a branchid is given, then it is expected also the chainage,
+        otherwise numcoordinates xcoordinates and ycoordinates shall be expected.
+
+        Args:
+            values (dict): Dictionary of values validated for the new structure.
+
+        Raises:
+            ValueError: When branchid or chainage values are not valid (empty strings).
+            ValueError: When the number of xcoordinates and ycoordinates do not match numcoordinates.
+
+        Returns:
+            dict: Dictionary of values validated for the new structure.
+        """
+        filtered_values = {k: v for k, v in values.items() if v is not None}
+        structype = filtered_values.get("type", "").lower()
+
+        if structype == "compound" or issubclass(cls, (Compound)):
+            # Compound structure does not require a location specification.
+            return values
+
+        # Backwards compatibility for old-style polylinefile input field (instead of num/x/yCoordinates):
+        polyline_compatible_structures = dict(
+            pump="Pump",
+            dambreak="Dambreak",
+            gate="Gate",
+            weir="Weir",
+            generalstructure="GeneralStructure",
+        )
+        polylinefile_in_model = (
+            structype in polyline_compatible_structures.keys()
+            and filtered_values.get("polylinefile") is not None
+        )
+
+        # No branchId+chainage for some structures:
+        only_coordinates_structures = dict(
+            longculvert="LongCulvert", dambreak="Dambreak"
+        )
+        coordinates_in_model = Structure.validate_coordinates_in_model(filtered_values)
+
+        # Error: do not allow both x/y and polyline file:
+        assert not (
+            polylinefile_in_model and coordinates_in_model
+        ), f"`Specify location either by `num/x/yCoordinates` or `polylinefile`, but not both."
+
+        # Error: require x/y or polyline file:
+        if (
+            structype in polyline_compatible_structures.keys()
+            and structype in only_coordinates_structures.keys()
+        ):
+            assert (
+                coordinates_in_model or polylinefile_in_model
+            ), f"Specify location either by setting `num/x/yCoordinates` or `polylinefile` fields for a {polyline_compatible_structures[structype]} structure."
+
+        # Error: Some structures require coordinates_in_model, but not branchId and chainage.
+        if (
+            not polylinefile_in_model
+            and structype in only_coordinates_structures.keys()
+        ):
+            assert (
+                coordinates_in_model
+            ), f"Specify location by setting `num/x/yCoordinates` for a {only_coordinates_structures[structype]} structure."
+
+        # Error: final check: at least one of x/y, branchId+chainage or polyline file must be given
+        branch_and_chainage_in_model = Structure.validate_branch_and_chainage_in_model(
+            filtered_values
+        )
+        assert (
+            branch_and_chainage_in_model
+            or coordinates_in_model
+            or polylinefile_in_model
+        ), "Specify location either by setting `branchId` and `chainage` or `num/x/yCoordinates` or `polylinefile` fields."
+
+        return values
+
+    @staticmethod
+    def validate_branch_and_chainage_in_model(values: dict) -> bool:
+        """
+        Static method to validate whether the given branchid and chainage values
+        match the expectation of a new structure.
+
+        Args:
+            values (dict): Dictionary of values to be used to generate a structure.
+
+        Raises:
+            ValueError: When the value for branchid or chainage are not valid.
+
+        Returns:
+            bool: Result of valid branchid / chainage in dictionary.
+        """
+        branchid = values.get("branchid", None)
+        if branchid is None:
+            return False
+
+        chainage = values.get("chainage", None)
+        if str_is_empty_or_none(branchid) or chainage is None:
+            raise ValueError(
+                "A valid value for branchId and chainage is required when branchId key is specified."
+            )
+        return True
+
+    @staticmethod
+    def validate_coordinates_in_model(values: dict) -> bool:
+        """
+        Static method to validate whether the given values match the expectations
+        of a structure to define its coordinates.
+
+        Args:
+            values (dict): Dictionary of values to be used to generate a structure.
+
+        Raises:
+            ValueError: When the given coordinates is less than 2.
+            ValueError: When the given coordinates do not match in expected size.
+
+        Returns:
+            bool: Result of valid coordinates in dictionary.
+        """
+        searched_keys = ["numcoordinates", "xcoordinates", "ycoordinates"]
+        if any(values.get(k, None) is None for k in searched_keys):
+            return False
+
+        n_coords = values["numcoordinates"]
+        if n_coords < 2:
+            raise ValueError(
+                f"Expected at least 2 coordinates, but only {n_coords} declared."
+            )
+
+        def get_coord_len(coord: str) -> int:
+            if values[coord] is None:
+                return 0
+            return len(values[coord])
+
+        len_x_coords = get_coord_len("xcoordinates")
+        len_y_coords = get_coord_len("ycoordinates")
+        if n_coords == len_x_coords == len_y_coords:
+            return True
+        raise ValueError(
+            f"Expected {n_coords} coordinates, given {len_x_coords} for xCoordinates and {len_y_coords} for yCoordinates."
+        )
+
+    @classmethod
+    def validate(cls, v):
+        """Try to initialize subclass based on the `type` field.
+        This field is compared to each `type` field of the derived models of `Structure`.
+        The derived model with an equal structure type will be initialized.
+
+        Raises:
+            ValueError: When the given type is not a known structure type.
+        """
+
+        # should be replaced by discriminated unions once merged
+        # https://github.com/samuelcolvin/pydantic/pull/2336
+        if isinstance(v, dict):
+            for c in cls.__subclasses__():
+                if (
+                    c.__fields__.get("type").default.lower()
+                    == v.get("type", "").lower()
+                ):
+                    v = c(**v)
+                    break
+            else:
+                raise ValueError(
+                    f"Type of {cls.__name__} with id={v.get('id', '')} and type={v.get('type', '')} is not recognized."
+                )
+        return super().validate(v)
+
+    def _exclude_fields(self) -> Set:
+        # exclude the non-applicable, or unset props like coordinates or branches
+        if self.type == "compound":
+            exclude_set = self._loc_all_fields
+        elif self.branchid is not None:
+            exclude_set = self._loc_coord_fields
+        else:
+            exclude_set = self._loc_branch_fields
+        exclude_set = super()._exclude_fields().union(exclude_set)
+        return exclude_set
+
+    def _get_identifier(self, data: dict) -> Optional[str]:
+        return data.get("id") or data.get("name")
+
+
+class FlowDirection(str, Enum):
+    """
+    Enum class containing the valid values for the allowedFlowDirection
+    attribute in several subclasses of Structure.
+    """
+
+    none = "none"
+    positive = "positive"
+    negative = "negative"
+    both = "both"
+    allowedvaluestext = "Possible values: both, positive, negative, none."
+
+
+class Orientation(str, Enum):
+    """
+    Enum class containing the valid values for the orientation
+    attribute in several subclasses of Structure.
+    """
+
+    positive = "positive"
+    negative = "negative"
+    allowedvaluestext = "Possible values: positive, negative."
+
+
+class Weir(Structure):
+    """
+    Hydraulic structure with `type=weir`, to be included in a structure file.
+    Typically inside the structure list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.structurefile[0].structure[..]`
+
+    All lowercased attributes match with the weir input as described in
+    [UM Sec.C.12.1](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.12.1).
+    """
+
+    class Comments(Structure.Comments):
+        type: Optional[str] = Field("Structure type; must read weir", alias="type")
+        allowedflowdir: Optional[str] = Field(
+            FlowDirection.allowedvaluestext, alias="allowedFlowdir"
+        )
+
+        crestlevel: Optional[str] = Field(
+            "Crest level of weir (m AD).", alias="crestLevel"
+        )
+        crestwidth: Optional[str] = Field("Width of the weir (m).", alias="crestWidth")
+        corrcoeff: Optional[str] = Field(
+            "Correction coefficient (-).", alias="corrCoeff"
+        )
+        usevelocityheight: Optional[str] = Field(
+            "Flag indicating whether the velocity height is to be calculated or not.",
+            alias="useVelocityHeight",
+        )
+
+    comments: Comments = Comments()
+
+    type: Literal["weir"] = Field("weir", alias="type")
+    allowedflowdir: Optional[FlowDirection] = Field(
+        FlowDirection.both.value, alias="allowedFlowDir"
+    )
+
+    crestlevel: ForcingData = Field(alias="crestLevel")
+    crestwidth: Optional[float] = Field(None, alias="crestWidth")
+    corrcoeff: float = Field(1.0, alias="corrCoeff")
+    usevelocityheight: bool = Field(True, alias="useVelocityHeight")
+
+    _flowdirection_validator = get_enum_validator("allowedflowdir", enum=FlowDirection)
+
+
+class UniversalWeir(Structure):
+    """
+    Hydraulic structure with `type=universalWeir`, to be included in a structure file.
+    Typically inside the structure list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.structurefile[0].structure[..]`
+
+    All lowercased attributes match with the universal weir input as described in
+    [UM Sec.C.12.2](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.12.2).
+    """
+
+    class Comments(Structure.Comments):
+        type: Optional[str] = Field(
+            "Structure type; must read universalWeir", alias="type"
+        )
+        allowedflowdir: Optional[str] = Field(
+            FlowDirection.allowedvaluestext, alias="allowedFlowdir"
+        )
+
+        numlevels: Optional[str] = Field("Number of yz-Values.", alias="numLevels")
+        yvalues: Optional[str] = Field(
+            "y-values of the cross section (m). (number of values = numLevels)",
+            alias="yValues",
+        )
+        zvalues: Optional[str] = Field(
+            "z-values of the cross section (m). (number of values = numLevels)",
+            alias="zValues",
+        )
+        crestlevel: Optional[str] = Field(
+            "Crest level of weir (m AD).", alias="crestLevel"
+        )
+        dischargecoeff: Optional[str] = Field(
+            "Discharge coefficient c_e (-).", alias="dischargeCoeff"
+        )
+
+    comments: Comments = Comments()
+
+    type: Literal["universalWeir"] = Field("universalWeir", alias="type")
+    allowedflowdir: FlowDirection = Field(alias="allowedFlowDir")
+
+    numlevels: int = Field(alias="numLevels")
+    yvalues: List[float] = Field(alias="yValues")
+    zvalues: List[float] = Field(alias="zValues")
+    crestlevel: float = Field(alias="crestLevel")
+    dischargecoeff: float = Field(alias="dischargeCoeff")
+
+    _split_to_list = get_split_string_on_delimiter_validator("yvalues", "zvalues")
+    _flowdirection_validator = get_enum_validator("allowedflowdir", enum=FlowDirection)
+
+
+class CulvertSubType(str, Enum):
+    """Enum class to store a [Culvert][hydrolib.core.dflowfm.structure.models.Culvert]'s subType."""
+
+    culvert = "culvert"
+    invertedSiphon = "invertedSiphon"
+
+
+class Culvert(Structure):
+    """
+    Hydraulic structure with `type=culvert`, to be included in a structure file.
+    Typically inside the structure list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.structurefile[0].structure[..]`
+
+    All lowercased attributes match with the culvert input as described in
+    [UM Sec.C.12.3](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.12.3).
+    """
+
+    type: Literal["culvert"] = Field("culvert", alias="type")
+    allowedflowdir: FlowDirection = Field(alias="allowedFlowDir")
+
+    leftlevel: float = Field(alias="leftLevel")
+    rightlevel: float = Field(alias="rightLevel")
+    csdefid: str = Field(alias="csDefId")
+    length: float = Field(alias="length")
+    inletlosscoeff: float = Field(alias="inletLossCoeff")
+    outletlosscoeff: float = Field(alias="outletLossCoeff")
+    valveonoff: bool = Field(alias="valveOnOff")
+    valveopeningheight: Optional[ForcingData] = Field(alias="valveOpeningHeight")
+    numlosscoeff: Optional[int] = Field(alias="numLossCoeff")
+    relopening: Optional[List[float]] = Field(alias="relOpening")
+    losscoeff: Optional[List[float]] = Field(alias="lossCoeff")
+    bedfrictiontype: Optional[FrictionType] = Field(alias="bedFrictionType")
+    bedfriction: Optional[float] = Field(alias="bedFriction")
+    subtype: Optional[CulvertSubType] = Field(
+        CulvertSubType.culvert.value, alias="subType"
+    )
+    bendlosscoeff: Optional[float] = Field(alias="bendLossCoeff")
+
+    _split_to_list = get_split_string_on_delimiter_validator("relopening", "losscoeff")
+    _flowdirection_validator = get_enum_validator("allowedflowdir", enum=FlowDirection)
+    _subtype_validator = get_enum_validator("subtype", enum=CulvertSubType)
+    _frictiontype_validator = get_enum_validator("bedfrictiontype", enum=FrictionType)
+
+    @root_validator(allow_reuse=True)
+    def validate_that_valve_related_fields_are_present_for_culverts_with_valves(
+        cls, values: Dict
+    ) -> Dict:
+        """Validates that valve-related fields are present when there is a valve present."""
+        return validate_required_fields(
+            values,
+            "valveopeningheight",
+            "numlosscoeff",
+            "relopening",
+            "losscoeff",
+            conditional_field_name="valveonoff",
+            conditional_value=True,
+        )
+
+    @root_validator(allow_reuse=True)
+    def validate_that_bendlosscoeff_field_is_present_for_invertedsyphons(
+        cls, values: Dict
+    ) -> Dict:
+        """Validates that the bendlosscoeff value is present when dealing with inverted syphons."""
+        return validate_required_fields(
+            values,
+            "bendlosscoeff",
+            conditional_field_name="subtype",
+            conditional_value=CulvertSubType.invertedSiphon,
+        )
+
+    @root_validator(allow_reuse=True)
+    def check_list_lengths(cls, values):
+        """Validates that the length of the relopening and losscoeff fields are as expected."""
+        return validate_correct_length(
+            values,
+            "relopening",
+            "losscoeff",
+            length_name="numlosscoeff",
+            list_required_with_length=True,
+        )
+
+    @root_validator(allow_reuse=True)
+    def validate_that_bendlosscoeff_is_not_provided_for_culverts(
+        cls, values: Dict
+    ) -> Dict:
+        """Validates that the bendlosscoeff field is not provided when the subtype is a culvert."""
+        return validate_forbidden_fields(
+            values,
+            "bendlosscoeff",
+            conditional_field_name="subtype",
+            conditional_value=CulvertSubType.culvert,
+        )
+
+
+class Pump(Structure):
+    """
+    Hydraulic structure with `type=pump`, to be included in a structure file.
+    Typically inside the structure list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.structurefile[0].structure[..]`
+
+    All lowercased attributes match with the pump input as described in
+    [UM Sec.C.12.6](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.12.6).
+    """
+
+    type: Literal["pump"] = Field("pump", alias="type")
+
+    orientation: Optional[Orientation] = Field(alias="orientation")
+    controlside: Optional[str] = Field(alias="controlSide")  # TODO Enum
+    numstages: Optional[int] = Field(alias="numStages")
+    capacity: ForcingData = Field(alias="capacity")
+
+    startlevelsuctionside: Optional[List[float]] = Field(alias="startLevelSuctionSide")
+    stoplevelsuctionside: Optional[List[float]] = Field(alias="stopLevelSuctionSide")
+    startleveldeliveryside: Optional[List[float]] = Field(
+        alias="startLevelDeliverySide"
+    )
+    stopleveldeliveryside: Optional[List[float]] = Field(alias="stopLevelDeliverySide")
+    numreductionlevels: Optional[int] = Field(alias="numReductionLevels")
+    head: Optional[List[float]] = Field(alias="head")
+    reductionfactor: Optional[List[float]] = Field(alias="reductionFactor")
+
+    _split_to_list = get_split_string_on_delimiter_validator(
+        "startlevelsuctionside",
+        "stoplevelsuctionside",
+        "startleveldeliveryside",
+        "stopleveldeliveryside",
+        "head",
+        "reductionfactor",
+    )
+
+    _orientation_validator = get_enum_validator("orientation", enum=Orientation)
+
+    @root_validator(allow_reuse=True)
+    def validate_that_controlside_is_provided_when_numstages_is_provided(
+        cls, values: Dict
+    ) -> Dict:
+        return validate_required_fields(
+            values,
+            "controlside",
+            conditional_field_name="numstages",
+            conditional_value=0,
+            comparison_func=gt,
+        )
+
+    @classmethod
+    def _check_list_lengths_suctionside(cls, values: Dict) -> Dict:
+        """Validates that the length of the startlevelsuctionside and stoplevelsuctionside fields are as expected."""
+        return validate_correct_length(
+            values,
+            "startlevelsuctionside",
+            "stoplevelsuctionside",
+            length_name="numstages",
+            list_required_with_length=True,
+        )
+
+    @root_validator(allow_reuse=True)
+    def conditionally_check_list_lengths_suctionside(cls, values: Dict) -> Dict:
+        """
+        Validates the length of the suction side fields, but only if there is a controlside value
+        present in the values and the controlside is not equal to the deliverySide.
+        """
+        return validate_conditionally(
+            cls,
+            values,
+            Pump._check_list_lengths_suctionside,
+            "controlside",
+            "deliverySide",
+            ne,
+        )
+
+    @classmethod
+    def _check_list_lengths_deliveryside(cls, values: Dict) -> Dict:
+        """Validates that the length of the startleveldeliveryside and stopleveldeliveryside fields are as expected."""
+        return validate_correct_length(
+            values,
+            "startleveldeliveryside",
+            "stopleveldeliveryside",
+            length_name="numstages",
+            list_required_with_length=True,
+        )
+
+    @root_validator(allow_reuse=True)
+    def conditionally_check_list_lengths_deliveryside(cls, values: Dict) -> Dict:
+        """
+        Validates the length of the delivery side fields, but only if there is a controlside value
+        present in the values and the controlside is not equal to the suctionSide.
+        """
+        return validate_conditionally(
+            cls,
+            values,
+            Pump._check_list_lengths_deliveryside,
+            "controlside",
+            "suctionSide",
+            ne,
+        )
+
+    @root_validator(allow_reuse=True)
+    def check_list_lengths_head_and_reductionfactor(cls, values):
+        """Validates that the lengths of the head and reductionfactor fields are as expected."""
+        return validate_correct_length(
+            values,
+            "head",
+            "reductionfactor",
+            length_name="numreductionlevels",
+            list_required_with_length=True,
+        )
+
+
+class Compound(Structure):
+    """
+    Hydraulic structure with `type=compound`, to be included in a structure file.
+    Typically inside the structure list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.structurefile[0].structure[..]`
+
+    All lowercased attributes match with the compound input as described in
+    [UM Sec.C.12.11](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.12.11).
+    """
+
+    type: Literal["compound"] = Field("compound", alias="type")
+    numstructures: int = Field(alias="numStructures")
+    structureids: List[str] = Field(alias="structureIds", delimiter=";")
+
+    _split_to_list = get_split_string_on_delimiter_validator(
+        "structureids",
+    )
+
+
+class Orifice(Structure):
+    """
+    Hydraulic structure with `type=orifice`, to be included in a structure file.
+    Typically inside the structure list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.structurefile[0].structure[..]`
+
+    All lowercased attributes match with the orifice input as described in
+    [UM Sec.C.12.7](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.12.7).
+    """
+
+    type: Literal["orifice"] = Field("orifice", alias="type")
+    allowedflowdir: Optional[FlowDirection] = Field(
+        FlowDirection.both.value, alias="allowedFlowDir"
+    )
+
+    crestlevel: ForcingData = Field(alias="crestLevel")
+    crestwidth: Optional[float] = Field(None, alias="crestWidth")
+    gateloweredgelevel: ForcingData = Field(alias="gateLowerEdgeLevel")
+    corrcoeff: float = Field(1.0, alias="corrCoeff")
+    usevelocityheight: bool = Field(True, alias="useVelocityHeight")
+
+    # TODO Use a validator here to check the optionals related to the bool field
+    uselimitflowpos: Optional[bool] = Field(False, alias="useLimitFlowPos")
+    limitflowpos: Optional[float] = Field(alias="limitFlowPos")
+
+    uselimitflowneg: Optional[bool] = Field(False, alias="useLimitFlowNeg")
+    limitflowneg: Optional[float] = Field(alias="limitFlowNeg")
+
+    _flowdirection_validator = get_enum_validator("allowedflowdir", enum=FlowDirection)
+
+    @validator("limitflowpos", always=True)
+    @classmethod
+    def _validate_limitflowpos(cls, v, values):
+        return cls._validate_limitflow(v, values, "limitFlowPos", "useLimitFlowPos")
+
+    @validator("limitflowneg", always=True)
+    @classmethod
+    def _validate_limitflowneg(cls, v, values):
+        return cls._validate_limitflow(v, values, "limitFlowNeg", "useLimitFlowNeg")
+
+    @classmethod
+    def _validate_limitflow(cls, v, values, limitflow: str, uselimitflow: str):
+        if v is None and values[uselimitflow.lower()] == True:
+            raise ValueError(
+                f"{limitflow} should be defined when {uselimitflow} is true"
+            )
+
+        return v
+
+
+class GateOpeningHorizontalDirection(str, Enum):
+    """Horizontal opening direction of gate door[s]."""
+
+    symmetric = "symmetric"
+    from_left = "fromLeft"
+    from_right = "fromRight"
+    allowedvaluestext = "Possible values: symmetric, fromLeft, fromRight."
+
+
+class GeneralStructure(Structure):
+    """
+    Hydraulic structure with `type=generalStructure`, to be included in a structure file.
+    Typically inside the structure list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.structurefile[0].structure[..]`
+
+    All lowercased attributes match with the orifice input as described in
+    [UM Sec.C.12.9](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.12.9).
+    """
+
+    class Comments(Structure.Comments):
+        type: Optional[str] = Field(
+            "Structure type; must read generalStructure", alias="type"
+        )
+        allowedflowdir: Optional[str] = Field(
+            FlowDirection.allowedvaluestext, alias="allowedFlowDir"
+        )
+
+        upstream1width: Optional[str] = Field("w_u1 [m]", alias="upstream1Width")
+        upstream1level: Optional[str] = Field("z_u1 [m AD]", alias="upstream1Level")
+        upstream2width: Optional[str] = Field("w_u2 [m]", alias="upstream2Width")
+        upstream2level: Optional[str] = Field("z_u2 [m D]", alias="upstream2Level")
+
+        crestwidth: Optional[str] = Field("w_s [m]", alias="crestWidth")
+        crestlevel: Optional[str] = Field("z_s [m AD]", alias="crestLevel")
+        crestlength: Optional[str] = Field(
+            "The crest length across the general structure [m]. When the crest length > 0, the extra resistance for this structure will be ls * g/(C2 * waterdepth)",
+            alias="crestLength",
+        )
+
+        downstream1width: Optional[str] = Field("w_d1 [m]", alias="downstream1Width")
+        downstream1level: Optional[str] = Field("z_d1 [m AD]", alias="downstream1Level")
+        downstream2width: Optional[str] = Field("w_d2 [m]", alias="downstream2Width")
+        downstream2level: Optional[str] = Field("z_d2 [m AD]", alias="downstream2Level")
+
+        gateloweredgelevel: Optional[str] = Field(
+            "Position of gate doorâ€™s lower edge [m AD]", alias="gateLowerEdgeLevel"
+        )
+        posfreegateflowcoeff: Optional[str] = Field(
+            "Positive free gate flow corr.coeff. cgf [-]", alias="posFreeGateFlowCoeff"
+        )
+        posdrowngateflowcoeff: Optional[str] = Field(
+            "Positive drowned gate flow corr.coeff. cgd [-]",
+            alias="posDrownGateFlowCoeff",
+        )
+        posfreeweirflowcoeff: Optional[str] = Field(
+            "Positive free weir flow corr.coeff. cwf [-]", alias="posFreeWeirFlowCoeff"
+        )
+        posdrownweirflowcoeff: Optional[str] = Field(
+            "Positive drowned weir flow corr.coeff. cwd [-]",
+            alias="posDrownWeirFlowCoeff",
+        )
+        poscontrcoeffreegate: Optional[str] = Field(
+            "Positive gate flow contraction coefficient Âµgf [-]",
+            alias="posContrCoefFreeGate",
+        )
+        negfreegateflowcoeff: Optional[str] = Field(
+            "Negative free gate flow corr.coeff. cgf [-]", alias="negFreeGateFlowCoeff"
+        )
+        negdrowngateflowcoeff: Optional[str] = Field(
+            "Negative drowned gate flow corr.coeff. cgd [-]",
+            alias="negDrownGateFlowCoeff",
+        )
+        negfreeweirflowcoeff: Optional[str] = Field(
+            "Negative free weir flow corr.coeff. cwf [-]", alias="negFreeWeirFlowCoeff"
+        )
+        negdrownweirflowcoeff: Optional[str] = Field(
+            "Negative drowned weir flow corr.coeff. cwd [-]",
+            alias="negDrownWeirFlowCoeff",
+        )
+        negcontrcoeffreegate: Optional[str] = Field(
+            "Negative gate flow contraction coefficient mu gf [-]",
+            alias="negContrCoefFreeGate",
+        )
+        extraresistance: Optional[str] = Field(
+            "Extra resistance [-]", alias="extraResistance"
+        )
+        gateheight: Optional[str] = Field(None, alias="gateHeight")
+        gateopeningwidth: Optional[str] = Field(
+            "Opening width between gate doors [m], should be smaller than (or equal to) crestWidth",
+            alias="gateOpeningWidth",
+        )
+        gateopeninghorizontaldirection: Optional[str] = Field(
+            "Horizontal opening direction of gate door[s]. Possible values are: symmetric, fromLeft, fromRight",
+            alias="gateOpeningHorizontalDirection",
+        )
+        usevelocityheight: Optional[str] = Field(
+            "Flag indicates whether the velocity height is to be calculated or not",
+            alias="useVelocityHeight",
+        )
+
+    comments: Optional[Comments] = Comments()
+
+    type: Literal["generalStructure"] = Field("generalStructure", alias="type")
+    allowedflowdir: Optional[FlowDirection] = Field(
+        FlowDirection.both.value, alias="allowedFlowDir"
+    )
+
+    upstream1width: Optional[float] = Field(10.0, alias="upstream1Width")
+    upstream1level: Optional[float] = Field(0.0, alias="upstream1Level")
+    upstream2width: Optional[float] = Field(10.0, alias="upstream2Width")
+    upstream2level: Optional[float] = Field(0.0, alias="upstream2Level")
+
+    crestwidth: Optional[float] = Field(10.0, alias="crestWidth")
+    crestlevel: Optional[ForcingData] = Field(0.0, alias="crestLevel")
+    crestlength: Optional[float] = Field(0.0, alias="crestLength")
+
+    downstream1width: Optional[float] = Field(10.0, alias="downstream1Width")
+    downstream1level: Optional[float] = Field(0.0, alias="downstream1Level")
+    downstream2width: Optional[float] = Field(10.0, alias="downstream2Width")
+    downstream2level: Optional[float] = Field(0.0, alias="downstream2Level")
+
+    gateloweredgelevel: Optional[ForcingData] = Field(11.0, alias="gateLowerEdgeLevel")
+    posfreegateflowcoeff: Optional[float] = Field(1.0, alias="posFreeGateFlowCoeff")
+    posdrowngateflowcoeff: Optional[float] = Field(1.0, alias="posDrownGateFlowCoeff")
+    posfreeweirflowcoeff: Optional[float] = Field(1.0, alias="posFreeWeirFlowCoeff")
+    posdrownweirflowcoeff: Optional[float] = Field(1.0, alias="posDrownWeirFlowCoeff")
+    poscontrcoeffreegate: Optional[float] = Field(1.0, alias="posContrCoefFreeGate")
+    negfreegateflowcoeff: Optional[float] = Field(1.0, alias="negFreeGateFlowCoeff")
+    negdrowngateflowcoeff: Optional[float] = Field(1.0, alias="negDrownGateFlowCoeff")
+    negfreeweirflowcoeff: Optional[float] = Field(1.0, alias="negFreeWeirFlowCoeff")
+    negdrownweirflowcoeff: Optional[float] = Field(1.0, alias="negDrownWeirFlowCoeff")
+    negcontrcoeffreegate: Optional[float] = Field(1.0, alias="negContrCoefFreeGate")
+    extraresistance: Optional[float] = Field(0.0, alias="extraResistance")
+    gateheight: Optional[float] = Field(1e10, alias="gateHeight")
+    gateopeningwidth: Optional[ForcingData] = Field(0.0, alias="gateOpeningWidth")
+    gateopeninghorizontaldirection: Optional[GateOpeningHorizontalDirection] = Field(
+        GateOpeningHorizontalDirection.symmetric.value,
+        alias="gateOpeningHorizontalDirection",
+    )
+    usevelocityheight: Optional[bool] = Field(True, alias="useVelocityHeight")
+
+
+class DambreakAlgorithm(int, Enum):
+    van_der_knaap = 1  # "van der Knaap, 2000"
+    verheij_van_der_knaap = 2  # "Verheij-van der Knaap, 2002"
+    timeseries = 3  # "Predefined time series, dambreakLevelsAndWidths."
+
+    @property
+    def description(self) -> str:
+        """
+        Property to return the description of the enums defined above.
+        Useful for comments in output files.
+
+        Returns:
+            str: Description for the current enum.
+        """
+        description_dict = dict(
+            van_der_knaap="van der Knaap, 2000",
+            verheij_van_der_knaap="Verheij-van der Knaap, 2002",
+            timeseries="Predefined time series, dambreakLevelsAndWidths",
+        )
+        return description_dict[self.name]
+
+
+class Dambreak(Structure):
+    """
+    Hydraulic structure with `type=dambreak`, to be included in a structure file.
+    Typically inside the structure list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.structurefile[0].structure[..]`
+
+    All lowercased attributes match with the dambreak input as described in
+    [UM Sec.C.12.10](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.12.10).
+    """
+
+    class Comments(Structure.Comments):
+        type: Optional[str] = Field("Structure type; must read dambreak", alias="type")
+        startlocationx: Optional[str] = Field(
+            "x-coordinate of breach starting point.", alias="startLocationX"
+        )
+        startlocationy: Optional[str] = Field(
+            "y-coordinate of breach starting point.", alias="startLocationY"
+        )
+        algorithm: Optional[str] = Field(
+            "Breach growth algorithm. Possible values are: 1 (van der Knaap (2000)), 2 (Verheijâ€“van der Knaap (2002)), 3: Predefined time series, see dambreakLevelsAndWidths",
+            alias="algorithm",
+        )
+        crestlevelini: Optional[str] = Field(
+            "Initial breach level zcrest level [m AD].", alias="crestLevelIni"
+        )
+        breachwidthini: Optional[str] = Field(
+            "Initial breach width B0 [m].", alias="breachWidthIni"
+        )
+        crestlevelmin: Optional[str] = Field(
+            "Minimal breach level zmin [m AD].", alias="crestLevelMin"
+        )
+        t0: Optional[str] = Field("Breach start time Tstart [s].", alias="t0")
+        timetobreachtomaximumdepth: Optional[str] = Field(
+            "tPhase 1 [s].", alias="timeToBreachToMaximumDepth"
+        )
+        f1: Optional[str] = Field("Factor f1 [-]", alias="f1")
+        f2: Optional[str] = Field("Factor f2 [-]", alias="f2")
+        ucrit: Optional[str] = Field(
+            "Critical flow velocity uc for erosion [m/s].", alias="uCrit"
+        )
+        waterlevelupstreamlocationx: Optional[str] = Field(
+            "(optional) x-coordinate of custom upstream water level point.",
+            alias="waterLevelUpstreamLocationX",
+        )
+        waterlevelupstreamlocationy: Optional[str] = Field(
+            "(optional) y-coordinate of custom upstream water level point.",
+            alias="waterLevelUpstreamLocationY",
+        )
+        waterleveldownstreamlocationx: Optional[str] = Field(
+            "(optional) x-coordinate of custom downstream water level point.",
+            alias="waterLevelDownstreamLocationX",
+        )
+        waterleveldownstreamlocationy: Optional[str] = Field(
+            "(optional) y-coordinate of custom downstream water level point.",
+            alias="waterLevelDownstreamLocationY",
+        )
+        waterlevelupstreamnodeid: Optional[str] = Field(
+            "(optional) Node Id of custom upstream water level point.",
+            alias="waterLevelUpstreamNodeId",
+        )
+        waterleveldownstreamnodeid: Optional[str] = Field(
+            "(optional) Node Id of custom downstream water level point.",
+            alias="waterLevelDownstreamNodeId",
+        )
+        dambreaklevelsandwidths: Optional[str] = Field(
+            "(only when algorithm=3) Filename of <*.tim> file (Section C.4) containing the breach levels and widths.",
+            alias="dambreakLevelsAndWidths",
+        )
+
+    comments: Comments = Comments()
+    type: Literal["dambreak"] = Field("dambreak", alias="type")
+    startlocationx: float = Field(alias="startLocationX")
+    startlocationy: float = Field(alias="startLocationY")
+    algorithm: DambreakAlgorithm = Field(alias="algorithm")
+
+    crestlevelini: float = Field(alias="crestLevelIni")
+    breachwidthini: float = Field(alias="breachWidthIni")
+    crestlevelmin: float = Field(alias="crestLevelMin")
+    t0: float = Field(alias="t0")
+    timetobreachtomaximumdepth: float = Field(alias="timeToBreachToMaximumDepth")
+    f1: float = Field(alias="f1")
+    f2: float = Field(alias="f2")
+    ucrit: float = Field(alias="uCrit")
+    waterlevelupstreamlocationx: Optional[float] = Field(
+        alias="waterLevelUpstreamLocationX"
+    )
+    waterlevelupstreamlocationy: Optional[float] = Field(
+        alias="waterLevelUpstreamLocationY"
+    )
+    waterleveldownstreamlocationx: Optional[float] = Field(
+        alias="waterLevelDownstreamLocationX"
+    )
+    waterleveldownstreamlocationy: Optional[float] = Field(
+        alias="waterLevelDownstreamLocationY"
+    )
+    waterlevelupstreamnodeid: Optional[str] = Field(alias="waterLevelUpstreamNodeId")
+    waterleveldownstreamnodeid: Optional[str] = Field(
+        alias="waterLevelDownstreamNodeId"
+    )
+    dambreaklevelsandwidths: Optional[Union[TimModel, ForcingModel]] = Field(
+        alias="dambreakLevelsAndWidths"
+    )
+
+    @validator("algorithm", pre=True)
+    @classmethod
+    def validate_algorithm(cls, value: str) -> DambreakAlgorithm:
+        """
+        Validates the algorithm parameter for the dambreak structure.
+
+        Args:
+            value (int): algorithm value read from the user's input.
+
+        Raises:
+            ValueError: When the value given is not of type int.
+            ValueError: When the value given is not in the range [1,3]
+
+        Returns:
+            int: Validated value.
+        """
+        int_value = -1
+        try:
+            int_value = int(value)
+        except Exception:
+            raise ValueError("Dambreak algorithm value should be of type int.")
+        if 0 < int_value <= 3:
+            return DambreakAlgorithm(int_value)
+        raise ValueError("Dambreak algorithm value should be 1, 2 or 3.")
+
+    @validator("dambreaklevelsandwidths")
+    @classmethod
+    def validate_dambreak_levels_and_widths(
+        cls, field_value: Optional[Union[TimModel, ForcingModel]], values: dict
+    ) -> Optional[Union[TimModel, ForcingModel]]:
+        """
+        Validates whether a dambreak can be created with the given dambreakLevelsAndWidths
+        property. This property should be given when the algorithm value is 3.
+
+        Args:
+            field_value (Optional[Union[TimModel, ForcingModel]]): Value given for dambreakLevelsAndWidths.
+            values (dict): Dictionary of values already validated (assuming algorithm is in it).
+
+        Raises:
+            ValueError: When algorithm value is not 3 and field_value has a value.
+
+        Returns:
+            Optional[Union[TimModel, ForcingModel]]: The value given for dambreakLevelsAndwidths.
+        """
+        # Retrieve the algorithm value (if not found use 0).
+        algorithm_value = values.get("algorithm", 0)
+        if field_value is not None and algorithm_value != 3:
+            # dambreakLevelsAndWidths can only be set when algorithm = 3
+            raise ValueError(
+                f"Dambreak field dambreakLevelsAndWidths can only be set when algorithm = 3, current value: {algorithm_value}."
+            )
+        return field_value
+
+    @root_validator
+    @classmethod
+    def check_location_dambreak(cls, values: dict) -> dict:
+        """
+        Verifies whether the location for this structure contains valid values for
+        numCoordinates, xCoordinates and yCoordinates or instead is using a polyline file.
+        Verifies whether de water level location specifications are valid.
+
+        Args:
+            values (dict): Dictionary of validated values to create a Dambreak.
+
+        Raises:
+            ValueError: When the values dictionary does not contain valid coordinates or polyline file or when the water level location specifications are not valid.
+
+        Returns:
+            dict: Dictionary of validated values.
+        """
+
+        def _validate_waterlevel_location(x_key: str, y_key: str, node_key: str):
+            x_is_given = values.get(x_key.lower()) is not None
+            y_is_given = values.get(y_key.lower()) is not None
+            node_is_given = values.get(node_key.lower()) is not None
+
+            if (x_is_given and y_is_given and not node_is_given) or (
+                node_is_given and not x_is_given and not y_is_given
+            ):
+                return
+
+            raise ValueError(
+                f"Either `{node_key}` should be specified or `{x_key}` and `{y_key}`."
+            )
+
+        _validate_waterlevel_location(
+            "waterLevelUpstreamLocationX",
+            "waterLevelUpstreamLocationY",
+            "waterLevelUpstreamNodeId",
+        )
+        _validate_waterlevel_location(
+            "waterLevelDownstreamLocationX",
+            "waterLevelDownstreamLocationY",
+            "waterLevelDownstreamNodeId",
+        )
+
+        return values
+
+
+class Bridge(Structure):
+    """
+    Hydraulic structure with `type=bridge`, to be included in a structure file.
+    Typically inside the structure list of a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.structurefile[0].structure[..]`
+
+    All lowercased attributes match with the bridge input as described in
+    [UM Sec.C.12.5](https://content.oss.deltares.nl/delft3dfm1d2d/D-Flow_FM_User_Manual_1D2D.pdf#subsection.C.12.5).
+    """
+
+    class Comments(Structure.Comments):
+        type: Optional[str] = Field("Structure type; must read bridge", alias="type")
+        allowedflowdir: Optional[str] = Field(
+            FlowDirection.allowedvaluestext, alias="allowedFlowdir"
+        )
+
+        csdefid: Optional[str] = Field(
+            "Id of Cross-Section Definition.", alias="csDefId"
+        )
+        shift: Optional[str] = Field(
+            "Vertical shift of the cross section definition [m]. Defined positive upwards."
+        )
+        inletlosscoeff: Optional[str] = Field(
+            "Inlet loss coefficient [-], Î¾_i.",
+            alias="inletLossCoeff",
+        )
+        outletlosscoeff: Optional[str] = Field(
+            "Outlet loss coefficient [-], k.",
+            alias="outletLossCoeff",
+        )
+        frictiontype: Optional[str] = Field(
+            "Friction type, possible values are: Chezy, Manning, wallLawNikuradse, WhiteColebrook, StricklerNikuradse, Strickler, deBosBijkerk.",
+            alias="frictionType",
+        )
+        friction: Optional[str] = Field(
+            "Friction value, used in friction loss.",
+            alias="friction",
+        )
+        length: Optional[str] = Field("Length [m], L.")
+
+    comments: Comments = Comments()
+
+    type: Literal["bridge"] = Field("bridge", alias="type")
+    allowedflowdir: FlowDirection = Field(alias="allowedFlowdir")
+
+    csdefid: str = Field(alias="csDefId")
+    shift: float
+    inletlosscoeff: float = Field(alias="inletLossCoeff")
+    outletlosscoeff: float = Field(alias="outletLossCoeff")
+    frictiontype: FrictionType = Field(alias="frictionType")
+    friction: float
+    length: float
+
+    _frictiontype_validator = get_enum_validator("frictiontype", enum=FrictionType)
+
+
+class StructureGeneral(INIGeneral):
+    """`[General]` section with structure file metadata."""
+
+    _header: Literal["General"] = "General"
+    fileversion: str = Field("3.00", alias="fileVersion")
+    filetype: Literal["structure"] = Field("structure", alias="fileType")
+
+
+class StructureModel(INIModel):
+    """
+    The overall structure model that contains the contents of one structure file.
+
+    This model is typically referenced under a [FMModel][hydrolib.core.dflowfm.mdu.models.FMModel]`.geometry.structurefile[..]`.
+
+    Attributes:
+        general (StructureGeneral): `[General]` block with file metadata.
+        branch (List[Structure]): List of `[Structure]` blocks for all hydraulic structures.
+    """
+
+    general: StructureGeneral = StructureGeneral()
+    structure: List[Structure] = []
+
+    @classmethod
+    def _ext(cls) -> str:
+        return ".ini"
+
+    @classmethod
+    def _filename(cls) -> str:
+        return "structures"
+
+    _split_to_list = make_list_validator("structure")
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/tim/models.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/tim/models.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,97 +1,97 @@
-from pathlib import Path
-from typing import Callable, Dict, List
-
-from pydantic.class_validators import validator
-
-from hydrolib.core.basemodel import BaseModel, ModelSaveSettings, ParsableFileModel
-
-from .parser import TimParser
-from .serializer import TimSerializer, TimSerializerConfig
-
-
-class TimRecord(BaseModel):
-    """Single tim record, representing a time and a list of data."""
-
-    time: float
-    """float: Time of the time record."""
-
-    data: List[float] = []
-    """List[float]: Record of the time record."""
-
-
-class TimModel(ParsableFileModel):
-    """Class representing a tim (*.tim) file."""
-
-    serializer_config = TimSerializerConfig()
-    """TimSerializerConfig: The serialization configuration for the tim file."""
-
-    comments: List[str] = []
-    """List[str]: A list with the header comment of the tim file."""
-
-    timeseries: List[TimRecord] = []
-    """List[TimRecord]: A list containing the timeseries."""
-
-    @classmethod
-    def _ext(cls) -> str:
-        return ".tim"
-
-    @classmethod
-    def _filename(cls) -> str:
-        return "timeseries"
-
-    @classmethod
-    def _get_serializer(
-        cls,
-    ) -> Callable[[Path, Dict, TimSerializerConfig, ModelSaveSettings], None]:
-        return TimSerializer.serialize
-
-    @classmethod
-    def _get_parser(cls) -> Callable[[Path], Dict]:
-        return TimParser.parse
-
-    @validator("timeseries")
-    @classmethod
-    def _validate_timeseries_values(cls, v: List[TimRecord]) -> List[TimRecord]:
-        """Validate if the amount of columns per timeseries match and if the timeseries have no duplicate times.
-
-        Args:
-            v (List[TimRecord]): Timeseries to validate.
-
-        Raises:
-            ValueError: When the amount of columns for timeseries is zero.
-            ValueError: When the amount of columns differs per timeseries.
-            ValueError: When the timeseries has a duplicate time.
-
-        Returns:
-            List[TimRecord]: Validated timeseries.
-        """
-        if len(v) == 0:
-            return v
-
-        cls._raise_error_if_amount_of_columns_differ(v)
-        cls._raise_error_if_duplicate_time(v)
-
-        return v
-
-    @staticmethod
-    def _raise_error_if_amount_of_columns_differ(timeseries: List[TimRecord]) -> None:
-        n_columns = len(timeseries[0].data)
-
-        if n_columns == 0:
-            raise ValueError("Time series cannot be empty.")
-
-        for timrecord in timeseries:
-            if len(timrecord.data) != n_columns:
-                raise ValueError(
-                    f"Time {timrecord.time}: Expected {n_columns} columns, but was {len(timrecord.data)}"
-                )
-
-    @staticmethod
-    def _raise_error_if_duplicate_time(timeseries: List[TimRecord]) -> None:
-        seen_times = set()
-        for timrecord in timeseries:
-            if timrecord.time in seen_times:
-                raise ValueError(
-                    f"Timeseries cannot contain duplicate times. Time: {timrecord.time} is duplicate."
-                )
-            seen_times.add(timrecord.time)
+from pathlib import Path
+from typing import Callable, Dict, List
+
+from pydantic.class_validators import validator
+
+from hydrolib.core.basemodel import BaseModel, ModelSaveSettings, ParsableFileModel
+
+from .parser import TimParser
+from .serializer import TimSerializer, TimSerializerConfig
+
+
+class TimRecord(BaseModel):
+    """Single tim record, representing a time and a list of data."""
+
+    time: float
+    """float: Time of the time record."""
+
+    data: List[float] = []
+    """List[float]: Record of the time record."""
+
+
+class TimModel(ParsableFileModel):
+    """Class representing a tim (*.tim) file."""
+
+    serializer_config = TimSerializerConfig()
+    """TimSerializerConfig: The serialization configuration for the tim file."""
+
+    comments: List[str] = []
+    """List[str]: A list with the header comment of the tim file."""
+
+    timeseries: List[TimRecord] = []
+    """List[TimRecord]: A list containing the timeseries."""
+
+    @classmethod
+    def _ext(cls) -> str:
+        return ".tim"
+
+    @classmethod
+    def _filename(cls) -> str:
+        return "timeseries"
+
+    @classmethod
+    def _get_serializer(
+        cls,
+    ) -> Callable[[Path, Dict, TimSerializerConfig, ModelSaveSettings], None]:
+        return TimSerializer.serialize
+
+    @classmethod
+    def _get_parser(cls) -> Callable[[Path], Dict]:
+        return TimParser.parse
+
+    @validator("timeseries")
+    @classmethod
+    def _validate_timeseries_values(cls, v: List[TimRecord]) -> List[TimRecord]:
+        """Validate if the amount of columns per timeseries match and if the timeseries have no duplicate times.
+
+        Args:
+            v (List[TimRecord]): Timeseries to validate.
+
+        Raises:
+            ValueError: When the amount of columns for timeseries is zero.
+            ValueError: When the amount of columns differs per timeseries.
+            ValueError: When the timeseries has a duplicate time.
+
+        Returns:
+            List[TimRecord]: Validated timeseries.
+        """
+        if len(v) == 0:
+            return v
+
+        cls._raise_error_if_amount_of_columns_differ(v)
+        cls._raise_error_if_duplicate_time(v)
+
+        return v
+
+    @staticmethod
+    def _raise_error_if_amount_of_columns_differ(timeseries: List[TimRecord]) -> None:
+        n_columns = len(timeseries[0].data)
+
+        if n_columns == 0:
+            raise ValueError("Time series cannot be empty.")
+
+        for timrecord in timeseries:
+            if len(timrecord.data) != n_columns:
+                raise ValueError(
+                    f"Time {timrecord.time}: Expected {n_columns} columns, but was {len(timrecord.data)}"
+                )
+
+    @staticmethod
+    def _raise_error_if_duplicate_time(timeseries: List[TimRecord]) -> None:
+        seen_times = set()
+        for timrecord in timeseries:
+            if timrecord.time in seen_times:
+                raise ValueError(
+                    f"Timeseries cannot contain duplicate times. Time: {timrecord.time} is duplicate."
+                )
+            seen_times.add(timrecord.time)
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/tim/parser.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/tim/parser.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,106 +1,106 @@
-from pathlib import Path
-from typing import Any, Dict, List, Tuple
-
-TimData = Dict[str, List[str]]
-
-
-class TimParser:
-    """
-    A parser for .tim files.
-    Full line comments at the start of the file are supported. Comment lines start with either a `*` or a `#`.
-    No other comments are supported.
-    """
-
-    @staticmethod
-    def parse(filepath: Path) -> Dict[str, List[Any]]:
-        """Parse a .tim file into a dictionary with comments and time series data.
-
-        Args:
-            filepath (Path): Path to the .tim file to be parsed.
-
-        Returns:
-            Dict[str, List[Any]]: A dictionary with keys "comments" and "timeseries".
-            - "comments" represents comments found at the start of the file.
-            - "timeseries" is a list of dictionaries with the key as "time" and values as "data".
-                - "time" is a time as a string.
-                - "data" is data as a list of strings.
-
-        Raises:
-            ValueError: If the file contains a comment that is not at the start of the file.
-            ValueError: If the data of the timeseries is empty.
-        """
-
-        comments: List[str] = []
-        timeseries: List[TimData] = []
-
-        with filepath.open(encoding="utf8") as file:
-            lines = file.readlines()
-            comments, start_timeseries_index = TimParser._read_header_comments(lines)
-            timeseries = TimParser._read_time_series_data(lines, start_timeseries_index)
-
-        return {"comments": comments, "timeseries": timeseries}
-
-    @staticmethod
-    def _read_header_comments(lines: List[str]) -> Tuple[List[str], int]:
-        """Read the header comments of the lines from the .tim file.
-        The comments are only expected at the start of the .tim file.
-        When a non comment line is encountered, all comments from the header will be retuned together with the start index of the timeseries data.
-
-        Args:
-            lines (List[str]): Lines from the the .tim file which is read.
-
-        Returns:
-            Tuple of List[str] and int, the List[str] contains the commenst from the header, the int is the start index of the timeseries.
-        """
-        comments: List[str] = []
-        start_timeseries_index = 0
-        for line_index in range(len(lines)):
-
-            line = lines[line_index].strip()
-
-            if len(line) == 0:
-                comments.append(line)
-                continue
-
-            if line.startswith("#") or line.startswith("*"):
-                comments.append(line[1:])
-                continue
-
-            start_timeseries_index = line_index
-            break
-
-        return comments, start_timeseries_index
-
-    @staticmethod
-    def _read_time_series_data(
-        lines: List[str], start_timeseries_index: int
-    ) -> List[TimData]:
-        timeseries: List[TimData] = []
-        for line_index in range(start_timeseries_index, len(lines)):
-            line = lines[line_index].strip()
-
-            if len(line) == 0:
-                continue
-
-            TimParser._raise_error_if_contains_comment(line, line_index + 1)
-
-            time, *values = line.split()
-
-            TimParser._raise_error_if_values_empty(values, line_index)
-
-            timrecord = {"time": time, "data": values}
-            timeseries.append(timrecord)
-
-        return timeseries
-
-    @staticmethod
-    def _raise_error_if_contains_comment(line: str, line_index: int) -> None:
-        if "#" in line or "*" in line:
-            raise ValueError(
-                f"Line {line_index}: comments are only supported at the start of the file, before the time series data."
-            )
-
-    @staticmethod
-    def _raise_error_if_values_empty(values: List[str], line_index: int) -> None:
-        if len(values) == 0:
-            raise ValueError(f"Line {line_index}: Time series cannot be empty.")
+from pathlib import Path
+from typing import Any, Dict, List, Tuple
+
+TimData = Dict[str, List[str]]
+
+
+class TimParser:
+    """
+    A parser for .tim files.
+    Full line comments at the start of the file are supported. Comment lines start with either a `*` or a `#`.
+    No other comments are supported.
+    """
+
+    @staticmethod
+    def parse(filepath: Path) -> Dict[str, List[Any]]:
+        """Parse a .tim file into a dictionary with comments and time series data.
+
+        Args:
+            filepath (Path): Path to the .tim file to be parsed.
+
+        Returns:
+            Dict[str, List[Any]]: A dictionary with keys "comments" and "timeseries".
+            - "comments" represents comments found at the start of the file.
+            - "timeseries" is a list of dictionaries with the key as "time" and values as "data".
+                - "time" is a time as a string.
+                - "data" is data as a list of strings.
+
+        Raises:
+            ValueError: If the file contains a comment that is not at the start of the file.
+            ValueError: If the data of the timeseries is empty.
+        """
+
+        comments: List[str] = []
+        timeseries: List[TimData] = []
+
+        with filepath.open(encoding="utf8") as file:
+            lines = file.readlines()
+            comments, start_timeseries_index = TimParser._read_header_comments(lines)
+            timeseries = TimParser._read_time_series_data(lines, start_timeseries_index)
+
+        return {"comments": comments, "timeseries": timeseries}
+
+    @staticmethod
+    def _read_header_comments(lines: List[str]) -> Tuple[List[str], int]:
+        """Read the header comments of the lines from the .tim file.
+        The comments are only expected at the start of the .tim file.
+        When a non comment line is encountered, all comments from the header will be retuned together with the start index of the timeseries data.
+
+        Args:
+            lines (List[str]): Lines from the the .tim file which is read.
+
+        Returns:
+            Tuple of List[str] and int, the List[str] contains the commenst from the header, the int is the start index of the timeseries.
+        """
+        comments: List[str] = []
+        start_timeseries_index = 0
+        for line_index in range(len(lines)):
+
+            line = lines[line_index].strip()
+
+            if len(line) == 0:
+                comments.append(line)
+                continue
+
+            if line.startswith("#") or line.startswith("*"):
+                comments.append(line[1:])
+                continue
+
+            start_timeseries_index = line_index
+            break
+
+        return comments, start_timeseries_index
+
+    @staticmethod
+    def _read_time_series_data(
+        lines: List[str], start_timeseries_index: int
+    ) -> List[TimData]:
+        timeseries: List[TimData] = []
+        for line_index in range(start_timeseries_index, len(lines)):
+            line = lines[line_index].strip()
+
+            if len(line) == 0:
+                continue
+
+            TimParser._raise_error_if_contains_comment(line, line_index + 1)
+
+            time, *values = line.split()
+
+            TimParser._raise_error_if_values_empty(values, line_index)
+
+            timrecord = {"time": time, "data": values}
+            timeseries.append(timrecord)
+
+        return timeseries
+
+    @staticmethod
+    def _raise_error_if_contains_comment(line: str, line_index: int) -> None:
+        if "#" in line or "*" in line:
+            raise ValueError(
+                f"Line {line_index}: comments are only supported at the start of the file, before the time series data."
+            )
+
+    @staticmethod
+    def _raise_error_if_values_empty(values: List[str], line_index: int) -> None:
+        if len(values) == 0:
+            raise ValueError(f"Line {line_index}: Time series cannot be empty.")
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/tim/serializer.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/tim/serializer.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,131 +1,131 @@
-from pathlib import Path
-from typing import Any, Callable, Dict, List, Optional
-
-from hydrolib.core.basemodel import ModelSaveSettings, SerializerConfig
-
-TimeSeriesRow = List[str]
-TimeSeriesBlock = List[TimeSeriesRow]
-
-
-class TimSerializerConfig(SerializerConfig):
-    """Configuration settings for the TimSerializer."""
-
-    column_spacing: int = 1
-    """(int): The number of spaces to include between columns in the serialized .tim file."""
-
-
-class TimSerializer:
-    @staticmethod
-    def serialize(
-        path: Path,
-        data: Dict[str, List[Any]],
-        config: TimSerializerConfig,
-        save_settings: ModelSaveSettings,
-    ) -> None:
-        """
-        Serialize timeseries data to a file in .tim format.
-
-        Args:
-            path (Path): The path to the destination .tim file.
-            data (Dict[str, List[Any]]): A dictionary with keys "comments" and "timeseries".
-            - "comments" represents comments found at the start of the file.
-            - "timeseries" is a list of dictionaries with the key as "time" and values as "data".
-                - "time" is a time as a string.
-                - "data" is data as a list of strings.
-
-            config (TimSerializerConfig): The serialization configuration settings.
-            save_settings (ModelSaveSettings): The save settings to be used.
-        """
-        path.parent.mkdir(parents=True, exist_ok=True)
-
-        commentlines = TimSerializer._serialize_comment_lines(data)
-        timeserieslines = TimSerializer._serialize_timeseries_lines(data, config)
-
-        file_content = TimSerializer._serialize_file_content(
-            timeserieslines, commentlines
-        )
-        with path.open("w", encoding="utf8") as file:
-            file.write(file_content)
-
-    @staticmethod
-    def _serialize_comment_lines(data: Dict[str, List[Any]]) -> List[str]:
-        commentlines = []
-        for comment in data["comments"]:
-            commentlines.append(f"#{comment}")
-        return commentlines
-
-    @staticmethod
-    def _serialize_timeseries_lines(
-        data: Dict[str, List[Any]], config: TimSerializerConfig
-    ) -> List[str]:
-        format_float = lambda v: f"{v:{config.float_format}}"
-        timeseriesblock = TimSerializer._serialize_to_timeseries_block(
-            data, format_float
-        )
-        timeserieslines = TimSerializer._serialize_timeseries_to_lines(
-            timeseriesblock, config
-        )
-        return timeserieslines
-
-    @staticmethod
-    def _serialize_to_timeseries_block(
-        data: Dict[str, List[Any]], format_float: Callable[[float], str]
-    ) -> TimeSeriesBlock:
-        timeseries_block: TimeSeriesBlock = []
-        for timeseries in data["timeseries"]:
-            time = timeseries["time"]
-            row_elements = timeseries["data"]
-            timeseries_row = [format_float(time)] + [
-                format_float(value) for value in row_elements
-            ]
-            timeseries_block.append(timeseries_row)
-        return timeseries_block
-
-    @staticmethod
-    def _serialize_timeseries_to_lines(
-        timeseries_block: TimeSeriesBlock, config: TimSerializerConfig
-    ) -> List[str]:
-        # Make sure the columns are aligned and have the proper spacing
-        column_space = " " * config.column_spacing
-        column_lengths = TimSerializer._get_column_lengths(timeseries_block)
-
-        timeserieslines = []
-        for timeseries_row in timeseries_block:
-            row_elements: List[str] = []
-            for index, value in enumerate(timeseries_row):
-                whitespace_offset = TimSerializer._get_offset_whitespace(
-                    value, column_lengths[index]
-                )
-                row_elements.append(value + whitespace_offset)
-
-            line = column_space.join(row_elements)
-            timeserieslines.append(line)
-        return timeserieslines
-
-    @staticmethod
-    def _get_offset_whitespace(value: Optional[str], max_length: int) -> str:
-        value_length = len(value) if value is not None else 0
-        return " " * max(max_length - value_length, 0)
-
-    @staticmethod
-    def _serialize_file_content(timeserieslines: List[str], commentlines: List[str]):
-        lines = []
-        lines.extend(commentlines)
-        lines.extend(timeserieslines)
-        file_content = "\n".join(lines)
-        return file_content
-
-    @staticmethod
-    def _get_column_lengths(timeseries_block: TimeSeriesBlock) -> List[int]:
-        if len(timeseries_block) == 0:
-            return []
-
-        n_columns = len(timeseries_block[0])
-        column_lengths = [0] * n_columns
-
-        for timeseries_row in timeseries_block:
-            for index, row_element in enumerate(timeseries_row):
-                if len(row_element) > column_lengths[index]:
-                    column_lengths[index] = len(row_element)
-
-        return column_lengths
+from pathlib import Path
+from typing import Any, Callable, Dict, List, Optional
+
+from hydrolib.core.basemodel import ModelSaveSettings, SerializerConfig
+
+TimeSeriesRow = List[str]
+TimeSeriesBlock = List[TimeSeriesRow]
+
+
+class TimSerializerConfig(SerializerConfig):
+    """Configuration settings for the TimSerializer."""
+
+    column_spacing: int = 1
+    """(int): The number of spaces to include between columns in the serialized .tim file."""
+
+
+class TimSerializer:
+    @staticmethod
+    def serialize(
+        path: Path,
+        data: Dict[str, List[Any]],
+        config: TimSerializerConfig,
+        save_settings: ModelSaveSettings,
+    ) -> None:
+        """
+        Serialize timeseries data to a file in .tim format.
+
+        Args:
+            path (Path): The path to the destination .tim file.
+            data (Dict[str, List[Any]]): A dictionary with keys "comments" and "timeseries".
+            - "comments" represents comments found at the start of the file.
+            - "timeseries" is a list of dictionaries with the key as "time" and values as "data".
+                - "time" is a time as a string.
+                - "data" is data as a list of strings.
+
+            config (TimSerializerConfig): The serialization configuration settings.
+            save_settings (ModelSaveSettings): The save settings to be used.
+        """
+        path.parent.mkdir(parents=True, exist_ok=True)
+
+        commentlines = TimSerializer._serialize_comment_lines(data)
+        timeserieslines = TimSerializer._serialize_timeseries_lines(data, config)
+
+        file_content = TimSerializer._serialize_file_content(
+            timeserieslines, commentlines
+        )
+        with path.open("w", encoding="utf8") as file:
+            file.write(file_content)
+
+    @staticmethod
+    def _serialize_comment_lines(data: Dict[str, List[Any]]) -> List[str]:
+        commentlines = []
+        for comment in data["comments"]:
+            commentlines.append(f"#{comment}")
+        return commentlines
+
+    @staticmethod
+    def _serialize_timeseries_lines(
+        data: Dict[str, List[Any]], config: TimSerializerConfig
+    ) -> List[str]:
+        format_float = lambda v: f"{v:{config.float_format}}"
+        timeseriesblock = TimSerializer._serialize_to_timeseries_block(
+            data, format_float
+        )
+        timeserieslines = TimSerializer._serialize_timeseries_to_lines(
+            timeseriesblock, config
+        )
+        return timeserieslines
+
+    @staticmethod
+    def _serialize_to_timeseries_block(
+        data: Dict[str, List[Any]], format_float: Callable[[float], str]
+    ) -> TimeSeriesBlock:
+        timeseries_block: TimeSeriesBlock = []
+        for timeseries in data["timeseries"]:
+            time = timeseries["time"]
+            row_elements = timeseries["data"]
+            timeseries_row = [format_float(time)] + [
+                format_float(value) for value in row_elements
+            ]
+            timeseries_block.append(timeseries_row)
+        return timeseries_block
+
+    @staticmethod
+    def _serialize_timeseries_to_lines(
+        timeseries_block: TimeSeriesBlock, config: TimSerializerConfig
+    ) -> List[str]:
+        # Make sure the columns are aligned and have the proper spacing
+        column_space = " " * config.column_spacing
+        column_lengths = TimSerializer._get_column_lengths(timeseries_block)
+
+        timeserieslines = []
+        for timeseries_row in timeseries_block:
+            row_elements: List[str] = []
+            for index, value in enumerate(timeseries_row):
+                whitespace_offset = TimSerializer._get_offset_whitespace(
+                    value, column_lengths[index]
+                )
+                row_elements.append(value + whitespace_offset)
+
+            line = column_space.join(row_elements)
+            timeserieslines.append(line)
+        return timeserieslines
+
+    @staticmethod
+    def _get_offset_whitespace(value: Optional[str], max_length: int) -> str:
+        value_length = len(value) if value is not None else 0
+        return " " * max(max_length - value_length, 0)
+
+    @staticmethod
+    def _serialize_file_content(timeserieslines: List[str], commentlines: List[str]):
+        lines = []
+        lines.extend(commentlines)
+        lines.extend(timeserieslines)
+        file_content = "\n".join(lines)
+        return file_content
+
+    @staticmethod
+    def _get_column_lengths(timeseries_block: TimeSeriesBlock) -> List[int]:
+        if len(timeseries_block) == 0:
+            return []
+
+        n_columns = len(timeseries_block[0])
+        column_lengths = [0] * n_columns
+
+        for timeseries_row in timeseries_block:
+            for index, row_element in enumerate(timeseries_row):
+                if len(row_element) > column_lengths[index]:
+                    column_lengths[index] = len(row_element)
+
+        return column_lengths
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/xyn/models.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/xyn/models.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,75 +1,75 @@
-from pathlib import Path
-from typing import Callable, Dict, List, Optional
-
-from pydantic import validator
-
-from hydrolib.core.basemodel import (
-    BaseModel,
-    ModelSaveSettings,
-    ParsableFileModel,
-    SerializerConfig,
-)
-from hydrolib.core.utils import str_is_empty_or_none
-
-from .parser import XYNParser
-from .serializer import XYNSerializer
-
-
-class XYNPoint(BaseModel):
-    """Single XYN point, representing a named station location."""
-
-    x: float
-    """float: The x or Î» coordinate."""
-
-    y: float
-    """float: The y or Ï† coordinate."""
-
-    n: str
-    """float: The name of the point."""
-
-    def _get_identifier(self, data: dict) -> Optional[str]:
-        x = data.get("x")
-        y = data.get("y")
-        n = data.get("n")
-        return f"x:{x} y:{y} n:{n}"
-
-    @validator("n", pre=True)
-    def _validate_name(cls, value):
-        if str_is_empty_or_none(value):
-            raise ValueError("Name cannot be empty.")
-
-        if "'" in value or '"' in value:
-            raise ValueError(
-                "Name cannot contain single or double quotes except at the start and end."
-            )
-
-        return value
-
-
-class XYNModel(ParsableFileModel):
-    """Observation station (.xyn) file."""
-
-    points: List[XYNPoint] = []
-    """List[`XYNPoint`]: List of XYN points."""
-
-    def dict(self, *args, **kwargs):
-        # speed up serializing by not converting these lowest models to dict
-        return dict(points=self.points)
-
-    @classmethod
-    def _filename(cls) -> str:
-        return "stations_obs"
-
-    @classmethod
-    def _ext(cls) -> str:
-        return ".xyn"
-
-    @classmethod
-    def _get_serializer(
-        cls,
-    ) -> Callable[[Path, Dict, SerializerConfig, ModelSaveSettings], None]:
-        return XYNSerializer.serialize
-
-    @classmethod
-    def _get_parser(cls) -> Callable[[Path], Dict]:
-        return XYNParser.parse
+from pathlib import Path
+from typing import Callable, Dict, List, Optional
+
+from pydantic import validator
+
+from hydrolib.core.basemodel import (
+    BaseModel,
+    ModelSaveSettings,
+    ParsableFileModel,
+    SerializerConfig,
+)
+from hydrolib.core.utils import str_is_empty_or_none
+
+from .parser import XYNParser
+from .serializer import XYNSerializer
+
+
+class XYNPoint(BaseModel):
+    """Single XYN point, representing a named station location."""
+
+    x: float
+    """float: The x or Î» coordinate."""
+
+    y: float
+    """float: The y or Ï† coordinate."""
+
+    n: str
+    """float: The name of the point."""
+
+    def _get_identifier(self, data: dict) -> Optional[str]:
+        x = data.get("x")
+        y = data.get("y")
+        n = data.get("n")
+        return f"x:{x} y:{y} n:{n}"
+
+    @validator("n", pre=True)
+    def _validate_name(cls, value):
+        if str_is_empty_or_none(value):
+            raise ValueError("Name cannot be empty.")
+
+        if "'" in value or '"' in value:
+            raise ValueError(
+                "Name cannot contain single or double quotes except at the start and end."
+            )
+
+        return value
+
+
+class XYNModel(ParsableFileModel):
+    """Observation station (.xyn) file."""
+
+    points: List[XYNPoint] = []
+    """List[`XYNPoint`]: List of XYN points."""
+
+    def dict(self, *args, **kwargs):
+        # speed up serializing by not converting these lowest models to dict
+        return dict(points=self.points)
+
+    @classmethod
+    def _filename(cls) -> str:
+        return "stations_obs"
+
+    @classmethod
+    def _ext(cls) -> str:
+        return ".xyn"
+
+    @classmethod
+    def _get_serializer(
+        cls,
+    ) -> Callable[[Path, Dict, SerializerConfig, ModelSaveSettings], None]:
+        return XYNSerializer.serialize
+
+    @classmethod
+    def _get_parser(cls) -> Callable[[Path], Dict]:
+        return XYNParser.parse
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/xyn/parser.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/xyn/parser.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,77 +1,77 @@
-from pathlib import Path
-from typing import Dict
-
-
-class XYNParser:
-    """
-    A parser for .xyn files with contents like this:
-
-    number number id
-
-    Note that the amount of whitespace can vary.
-    """
-
-    @staticmethod
-    def parse(filepath: Path) -> Dict:
-        """Parse an .xyn file into a Dict with the list of points read.
-
-        Args:
-            filepath (Path): .xyn file to be read.
-
-        Returns:
-            Dict[str, List[XYNPoint]]: dictionary with "points" value set to a list of points
-                each of which is a dict itself, with keys 'x', 'y', and 'n'.
-
-        Raises:
-            ValueError: if a line in the file cannot be parsed
-                or if the name contains whitespace while not surrounded with
-                single or double quotes.
-        """
-
-        def is_surrounded_by_single_quotes(name: str) -> bool:
-            return name.startswith("'") and name.endswith("'")
-
-        def is_surrounded_by_double_quotes(name: str) -> bool:
-            return name.startswith('"') and name.endswith('"')
-
-        def is_surrounded_by_quotes(name: str) -> bool:
-            return is_surrounded_by_single_quotes(
-                name
-            ) or is_surrounded_by_double_quotes(name)
-
-        def may_contain_whitespace(name: str) -> bool:
-            return is_surrounded_by_quotes(name)
-
-        def contains_whitespace(name: str) -> bool:
-            return " " in name
-
-        def contains_whitespace_while_not_allowed(name: str) -> bool:
-            return not may_contain_whitespace(name) and contains_whitespace(name)
-
-        points = []
-
-        with filepath.open(encoding="utf8") as f:
-            for linenr, line in enumerate(f.readlines()):
-
-                line = line.strip()
-                if line.startswith("*") or len(line) == 0:
-                    continue
-
-                try:
-                    x, y, n = line.split(maxsplit=2)
-                except ValueError:
-                    raise ValueError(
-                        f"Error parsing XYN file '{filepath}', line {linenr+1}."
-                    )
-
-                if contains_whitespace_while_not_allowed(n):
-                    raise ValueError(
-                        f"Error parsing XYN file '{filepath}', line {linenr+1}. Name `{n}` contains whitespace, so should be enclosed in quotes."
-                    )
-
-                if is_surrounded_by_quotes(n):
-                    n = n[1:-1]
-
-                points.append(dict(x=x, y=y, n=n))
-
-        return dict(points=points)
+from pathlib import Path
+from typing import Dict
+
+
+class XYNParser:
+    """
+    A parser for .xyn files with contents like this:
+
+    number number id
+
+    Note that the amount of whitespace can vary.
+    """
+
+    @staticmethod
+    def parse(filepath: Path) -> Dict:
+        """Parse an .xyn file into a Dict with the list of points read.
+
+        Args:
+            filepath (Path): .xyn file to be read.
+
+        Returns:
+            Dict[str, List[XYNPoint]]: dictionary with "points" value set to a list of points
+                each of which is a dict itself, with keys 'x', 'y', and 'n'.
+
+        Raises:
+            ValueError: if a line in the file cannot be parsed
+                or if the name contains whitespace while not surrounded with
+                single or double quotes.
+        """
+
+        def is_surrounded_by_single_quotes(name: str) -> bool:
+            return name.startswith("'") and name.endswith("'")
+
+        def is_surrounded_by_double_quotes(name: str) -> bool:
+            return name.startswith('"') and name.endswith('"')
+
+        def is_surrounded_by_quotes(name: str) -> bool:
+            return is_surrounded_by_single_quotes(
+                name
+            ) or is_surrounded_by_double_quotes(name)
+
+        def may_contain_whitespace(name: str) -> bool:
+            return is_surrounded_by_quotes(name)
+
+        def contains_whitespace(name: str) -> bool:
+            return " " in name
+
+        def contains_whitespace_while_not_allowed(name: str) -> bool:
+            return not may_contain_whitespace(name) and contains_whitespace(name)
+
+        points = []
+
+        with filepath.open(encoding="utf8") as f:
+            for linenr, line in enumerate(f.readlines()):
+
+                line = line.strip()
+                if line.startswith("*") or len(line) == 0:
+                    continue
+
+                try:
+                    x, y, n = line.split(maxsplit=2)
+                except ValueError:
+                    raise ValueError(
+                        f"Error parsing XYN file '{filepath}', line {linenr+1}."
+                    )
+
+                if contains_whitespace_while_not_allowed(n):
+                    raise ValueError(
+                        f"Error parsing XYN file '{filepath}', line {linenr+1}. Name `{n}` contains whitespace, so should be enclosed in quotes."
+                    )
+
+                if is_surrounded_by_quotes(n):
+                    n = n[1:-1]
+
+                points.append(dict(x=x, y=y, n=n))
+
+        return dict(points=points)
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/xyn/serializer.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/xyn/serializer.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,48 +1,48 @@
-from pathlib import Path
-from typing import Dict
-
-from hydrolib.core.basemodel import ModelSaveSettings, SerializerConfig
-
-
-class XYNSerializer:
-    @staticmethod
-    def serialize(
-        path: Path,
-        data: Dict,
-        config: SerializerConfig,
-        save_settings: ModelSaveSettings,
-    ) -> None:
-        """
-        Serializes the observation point data to an .xyn file at the specified path.
-
-        If the name contains spaces, it will be surrounded with single quotes.
-
-        Args:
-            path (Path): The path to the destination file.
-            data (Dict[str, List[XYNPoint]]): The data to be serialized.
-                The dictionary should contain a single key 'points' that holds a list of XYNPoints.
-            config (SerializerConfig): The serialization configuration.
-            save_settings (ModelSaveSettings): The model save settings.
-        """
-        path.parent.mkdir(parents=True, exist_ok=True)
-
-        space = 1 * " "
-        format_float = lambda x: f"{x:{config.float_format}}"
-        format_name = lambda n: f"'{n}'" if " " in n else n
-
-        serialized_points = []
-
-        for point in data["points"]:
-            serialized_point: str = space.join(
-                [
-                    format_float(point.x),
-                    format_float(point.y),
-                    format_name(point.n),
-                ]
-            )
-            serialized_points.append(serialized_point)
-
-        file_content: str = "\n".join(serialized_points)
-
-        with path.open("w", encoding="utf8") as f:
-            f.write(file_content)
+from pathlib import Path
+from typing import Dict
+
+from hydrolib.core.basemodel import ModelSaveSettings, SerializerConfig
+
+
+class XYNSerializer:
+    @staticmethod
+    def serialize(
+        path: Path,
+        data: Dict,
+        config: SerializerConfig,
+        save_settings: ModelSaveSettings,
+    ) -> None:
+        """
+        Serializes the observation point data to an .xyn file at the specified path.
+
+        If the name contains spaces, it will be surrounded with single quotes.
+
+        Args:
+            path (Path): The path to the destination file.
+            data (Dict[str, List[XYNPoint]]): The data to be serialized.
+                The dictionary should contain a single key 'points' that holds a list of XYNPoints.
+            config (SerializerConfig): The serialization configuration.
+            save_settings (ModelSaveSettings): The model save settings.
+        """
+        path.parent.mkdir(parents=True, exist_ok=True)
+
+        space = 1 * " "
+        format_float = lambda x: f"{x:{config.float_format}}"
+        format_name = lambda n: f"'{n}'" if " " in n else n
+
+        serialized_points = []
+
+        for point in data["points"]:
+            serialized_point: str = space.join(
+                [
+                    format_float(point.x),
+                    format_float(point.y),
+                    format_name(point.n),
+                ]
+            )
+            serialized_points.append(serialized_point)
+
+        file_content: str = "\n".join(serialized_points)
+
+        with path.open("w", encoding="utf8") as f:
+            f.write(file_content)
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/xyz/parser.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/xyz/parser.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,59 +1,59 @@
-import re
-from pathlib import Path
-from typing import Dict
-
-xyzpattern = re.compile(r"\s+")
-
-
-class XYZParser:
-    """
-    A parser for .xyz files which are like this:
-
-    number number    number
-    number number number # comment
-
-    Note that the whitespace can vary and the comment
-    left out.
-    """
-
-    @staticmethod
-    def parse(filepath: Path) -> Dict:
-        """Parse an .xyz file into a Dict with the list of points read.
-
-        Args:
-            filepath (Path): .xyz file to be read.
-
-        Returns:
-            Dict: dictionary with "points" value set to a list of points
-                each of which is a dict itself, with keys 'x', 'y', 'z'
-                and 'c' for an optional comment.
-
-        Raises:
-            ValueError: if a line in the file contains no values that
-                could be parsed.
-        """
-
-        data: Dict = dict(points=[])
-
-        with filepath.open(encoding="utf8") as f:
-            for linenr, line in enumerate(f.readlines()):
-
-                line = line.strip()
-                if line.startswith("*") or len(line) == 0:
-                    continue
-
-                try:
-                    x, y, z, *c = re.split(xyzpattern, line, maxsplit=3)
-                except ValueError:
-                    raise ValueError(
-                        f"Error parsing XYZ file '{filepath}', line {linenr+1}."
-                    )
-
-                c = c[0] if len(c) > 0 else ""
-                c = c.strip("#").strip()
-                if len(c) == 0:
-                    c = None
-
-                data["points"].append(dict(x=x, y=y, z=z, comment=c))
-
-        return data
+import re
+from pathlib import Path
+from typing import Dict
+
+xyzpattern = re.compile(r"\s+")
+
+
+class XYZParser:
+    """
+    A parser for .xyz files which are like this:
+
+    number number    number
+    number number number # comment
+
+    Note that the whitespace can vary and the comment
+    left out.
+    """
+
+    @staticmethod
+    def parse(filepath: Path) -> Dict:
+        """Parse an .xyz file into a Dict with the list of points read.
+
+        Args:
+            filepath (Path): .xyz file to be read.
+
+        Returns:
+            Dict: dictionary with "points" value set to a list of points
+                each of which is a dict itself, with keys 'x', 'y', 'z'
+                and 'c' for an optional comment.
+
+        Raises:
+            ValueError: if a line in the file contains no values that
+                could be parsed.
+        """
+
+        data: Dict = dict(points=[])
+
+        with filepath.open(encoding="utf8") as f:
+            for linenr, line in enumerate(f.readlines()):
+
+                line = line.strip()
+                if line.startswith("*") or len(line) == 0:
+                    continue
+
+                try:
+                    x, y, z, *c = re.split(xyzpattern, line, maxsplit=3)
+                except ValueError:
+                    raise ValueError(
+                        f"Error parsing XYZ file '{filepath}', line {linenr+1}."
+                    )
+
+                c = c[0] if len(c) > 0 else ""
+                c = c.strip("#").strip()
+                if len(c) == 0:
+                    c = None
+
+                data["points"].append(dict(x=x, y=y, z=z, comment=c))
+
+        return data
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dflowfm/xyz/serializer.py` & `hydrolib_core-0.5.1/hydrolib/core/dflowfm/xyz/serializer.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,43 +1,43 @@
-from pathlib import Path
-from typing import Dict, Generator
-
-from hydrolib.core.basemodel import ModelSaveSettings, SerializerConfig
-
-
-class XYZSerializer:
-    @staticmethod
-    def serialize(
-        path: Path,
-        data: Dict,
-        config: SerializerConfig,
-        save_settings: ModelSaveSettings,
-    ) -> None:
-        """
-        Serializes the XYZ data to the file at the specified path.
-
-        Attributes:
-            path (Path): The path to the destination file.
-            data (Dict): The data to be serialized.
-            config (SerializerConfig): The serialization configuration.
-            save_settings (ModelSaveSettings): The model save settings.
-        """
-        path.parent.mkdir(parents=True, exist_ok=True)
-
-        space = 1 * " "
-        format_float = lambda x: f"{x:{config.float_format}}"
-
-        with path.open("w", encoding="utf8") as f:
-            for point in data["points"]:
-                geometry: str = space.join(
-                    [format_float(p) for p in XYZSerializer._get_point_values(point)]
-                )
-                if point.comment:
-                    f.write(f"{geometry} # {point.comment}\n")
-                else:
-                    f.write(f"{geometry}\n")
-
-    @staticmethod
-    def _get_point_values(point) -> Generator[float, None, None]:
-        yield point.x
-        yield point.y
-        yield point.z
+from pathlib import Path
+from typing import Dict, Generator
+
+from hydrolib.core.basemodel import ModelSaveSettings, SerializerConfig
+
+
+class XYZSerializer:
+    @staticmethod
+    def serialize(
+        path: Path,
+        data: Dict,
+        config: SerializerConfig,
+        save_settings: ModelSaveSettings,
+    ) -> None:
+        """
+        Serializes the XYZ data to the file at the specified path.
+
+        Attributes:
+            path (Path): The path to the destination file.
+            data (Dict): The data to be serialized.
+            config (SerializerConfig): The serialization configuration.
+            save_settings (ModelSaveSettings): The model save settings.
+        """
+        path.parent.mkdir(parents=True, exist_ok=True)
+
+        space = 1 * " "
+        format_float = lambda x: f"{x:{config.float_format}}"
+
+        with path.open("w", encoding="utf8") as f:
+            for point in data["points"]:
+                geometry: str = space.join(
+                    [format_float(p) for p in XYZSerializer._get_point_values(point)]
+                )
+                if point.comment:
+                    f.write(f"{geometry} # {point.comment}\n")
+                else:
+                    f.write(f"{geometry}\n")
+
+    @staticmethod
+    def _get_point_values(point) -> Generator[float, None, None]:
+        yield point.x
+        yield point.y
+        yield point.z
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dimr/__init__.py` & `hydrolib_core-0.5.1/hydrolib/core/dimr/__init__.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,35 +1,35 @@
-from .models import (
-    DIMR,
-    Component,
-    ComponentOrCouplerRef,
-    ControlModel,
-    CoupledItem,
-    Coupler,
-    Documentation,
-    FMComponent,
-    GlobalSettings,
-    KeyValuePair,
-    Logger,
-    Parallel,
-    RRComponent,
-    Start,
-    StartGroup,
-)
-
-__all__ = [
-    "KeyValuePair",
-    "Component",
-    "FMComponent",
-    "RRComponent",
-    "Documentation",
-    "GlobalSettings",
-    "ComponentOrCouplerRef",
-    "CoupledItem",
-    "Logger",
-    "Coupler",
-    "StartGroup",
-    "ControlModel",
-    "Parallel",
-    "Start",
-    "DIMR",
-]
+from .models import (
+    DIMR,
+    Component,
+    ComponentOrCouplerRef,
+    ControlModel,
+    CoupledItem,
+    Coupler,
+    Documentation,
+    FMComponent,
+    GlobalSettings,
+    KeyValuePair,
+    Logger,
+    Parallel,
+    RRComponent,
+    Start,
+    StartGroup,
+)
+
+__all__ = [
+    "KeyValuePair",
+    "Component",
+    "FMComponent",
+    "RRComponent",
+    "Documentation",
+    "GlobalSettings",
+    "ComponentOrCouplerRef",
+    "CoupledItem",
+    "Logger",
+    "Coupler",
+    "StartGroup",
+    "ControlModel",
+    "Parallel",
+    "Start",
+    "DIMR",
+]
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dimr/models.py` & `hydrolib_core-0.5.1/hydrolib/core/dimr/models.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,344 +1,344 @@
-from abc import ABC, abstractclassmethod
-from datetime import datetime
-from pathlib import Path
-from typing import Callable, Dict, List, Literal, Optional, Type, Union
-
-from pydantic import Field, validator
-
-from hydrolib.core import __version__
-from hydrolib.core.basemodel import (
-    BaseModel,
-    FileModel,
-    ModelSaveSettings,
-    ParsableFileModel,
-    SerializerConfig,
-)
-from hydrolib.core.dflowfm.mdu.models import FMModel
-from hydrolib.core.dimr.parser import DIMRParser
-from hydrolib.core.dimr.serializer import DIMRSerializer
-from hydrolib.core.rr.models import RainfallRunoffModel
-from hydrolib.core.utils import to_list
-
-
-class KeyValuePair(BaseModel):
-    """Key value pair to specify settings and parameters.
-
-    Attributes:
-        key: The key.
-        value: The value.
-    """
-
-    key: str
-    value: str
-
-
-class Component(BaseModel, ABC):
-    """
-    Specification of a BMI-compliant model component instance that will be executed by DIMR.
-
-    Attributes:
-        library: The library name of the compoment.
-        name: The component name.
-        workingDir: The working directory.
-        inputFile: The name of the input file.
-        process: Number of subprocesses in the component.
-        setting: A list of variables that are provided to the BMI model before initialization.
-        parameter: A list of variables that are provided to the BMI model after initialization.
-        mpiCommunicator: The MPI communicator value.
-        model: The model represented by this component.
-    """
-
-    library: str
-    name: str
-    workingDir: Path
-    inputFile: Path
-    process: Optional[int]
-    setting: Optional[List[KeyValuePair]] = []
-    parameter: Optional[List[KeyValuePair]] = []
-    mpiCommunicator: Optional[str]
-
-    model: Optional[FileModel]
-
-    @property
-    def filepath(self):
-        return self.workingDir / self.inputFile
-
-    @abstractclassmethod
-    def get_model(cls) -> Type[FileModel]:
-        raise NotImplementedError("Model not implemented yet.")
-
-    @validator("setting", "parameter", pre=True, allow_reuse=True)
-    def validate_setting(cls, v):
-        return to_list(v)
-
-    def is_intermediate_link(self) -> bool:
-        return True
-
-    def _get_identifier(self, data: dict) -> Optional[str]:
-        return data.get("name")
-
-    def dict(self, *args, **kwargs):
-        # Exclude the FileModel from any DIMR serialization.
-        kwargs["exclude"] = {"model"}
-        return super().dict(*args, **kwargs)
-
-
-class FMComponent(Component):
-    """Component to include the D-Flow FM program in a DIMR control flow."""
-
-    library: Literal["dflowfm"] = "dflowfm"
-
-    @classmethod
-    def get_model(cls):
-        return FMModel
-
-
-class RRComponent(Component):
-    """Component to include the RainfallRunoff program in a DIMR control flow."""
-
-    library: Literal["rr_dll"] = "rr_dll"
-
-    @classmethod
-    def get_model(cls):
-        return RainfallRunoffModel
-
-
-class Documentation(BaseModel):
-    """
-    Information on the present DIMR configuration file.
-
-    Attributes:
-        fileVersion: The DIMR file version.
-        createdBy: Creators of the DIMR file.
-        creationDate: The creation date of the DIMR file.
-    """
-
-    fileVersion: str = "1.3"
-    createdBy: str = f"hydrolib-core {__version__}"
-    creationDate: datetime = Field(default_factory=datetime.utcnow)
-
-
-class GlobalSettings(BaseModel):
-    """
-    Global settings for the DIMR configuration.
-
-    Attributes:
-        logger_ncFormat: NetCDF format type for logging.
-    """
-
-    logger_ncFormat: int
-
-
-class ComponentOrCouplerRef(BaseModel):
-    """
-    Reference to a BMI-compliant model component instance.
-
-    Attributes:
-        name: Name of the reference to a BMI-compliant model component instance.
-    """
-
-    name: str
-
-    def _get_identifier(self, data: dict) -> Optional[str]:
-        return data.get("name")
-
-
-class CoupledItem(BaseModel):
-    """
-    Specification of an item that has to be exchanged.
-
-    Attributes:
-        sourceName: Name of the item at the source component.
-        targetName: Name of the item at the target component.
-    """
-
-    sourceName: str
-    targetName: str
-
-    def is_intermediate_link(self) -> bool:
-        # TODO set to True once we replace Paths with FileModels
-        return False
-
-
-class Logger(BaseModel):
-    """
-    Used to log values to the specified file in workingdir for each timestep
-
-    Attributes:
-        workingDir: Directory where the log file is written.
-        outputFile: Name of the log file.
-    """
-
-    workingDir: Path
-    outputFile: Path
-
-
-class Coupler(BaseModel):
-    """
-    Specification of the coupling actions to be performed between two BMI-compliant model components.
-
-    Attributes:
-        name: The name of the coupler.
-        sourceComponent: The component that provides the data to has to be exchanged.
-        targetComponent: The component that consumes the data to has to be exchanged.
-        item: A list of items that have to be exchanged.
-        logger: Logger for logging the values that get exchanged.
-    """
-
-    name: str
-    sourceComponent: str
-    targetComponent: str
-    item: List[CoupledItem] = []
-    logger: Optional[Logger]
-
-    @validator("item", pre=True)
-    def validate_item(cls, v):
-        return to_list(v)
-
-    def is_intermediate_link(self) -> bool:
-        # TODO set to True once we replace Paths with FileModels
-        return False
-
-    def _get_identifier(self, data: dict) -> Optional[str]:
-        return data.get("name")
-
-
-class StartGroup(BaseModel):
-    """
-    Specification of model components and couplers to be executed with a certain frequency.
-
-    Attributes:
-        time: Time frame specification for the present group: start time, stop time and frequency.
-              Expressed in terms of the time frame of the main component.
-        start: Ordered list of components to be executed.
-        coupler: Oredered list of couplers to be executed.
-    """
-
-    time: str
-    start: List[ComponentOrCouplerRef] = []
-    coupler: List[ComponentOrCouplerRef] = []
-
-    @validator("start", "coupler", pre=True)
-    def validate_start(cls, v):
-        return to_list(v)
-
-
-class ControlModel(BaseModel):
-    """
-    Overrides to make sure that the control elements in the DIMR
-    are parsed and serialized correctly.
-    """
-
-    _type: str
-
-    def dict(self, *args, **kwargs):
-        """Add control element prefixes for serialized data."""
-        return {
-            str(self._type): super().dict(*args, **kwargs),
-        }
-
-    @classmethod
-    def validate(cls, v):
-        """Remove control element prefixes from parsed data."""
-
-        # should be replaced by discriminated unions once merged
-        # https://github.com/samuelcolvin/pydantic/pull/2336
-        if isinstance(v, dict) and len(v.keys()) == 1:
-            key = list(v.keys())[0]
-            v = v[key]
-        return super().validate(v)
-
-
-class Parallel(ControlModel):
-    """
-    Specification of a parallel control flow: one main component and a group of related components and couplers.
-    Step wise execution order according to order in parallel control flow.
-
-    Attributes:
-        startGroup: Group of components and couplers to be executed.
-        start: Main component to be executed step wise (provides start time, end time and time step).
-    """
-
-    _type: Literal["parallel"] = "parallel"
-    startGroup: StartGroup
-    start: ComponentOrCouplerRef
-
-
-class Start(ControlModel):
-    """
-    Specification of a serial control flow: one main component.
-
-    Attributes:
-        name: Name of the reference to a BMI-compliant model component instance
-    """
-
-    _type: Literal["start"] = "start"
-    name: str
-
-
-class DIMR(ParsableFileModel):
-    """DIMR model representation.
-
-    Attributes:
-        documentation (Documentation): File metadata.
-        control (List[Union[Start, Parallel]]): The `<control>` element with a list
-            of [Start][hydrolib.core.dimr.models.Start]
-            and [Parallel][hydrolib.core.dimr.models.Parallel] sub-elements,
-            which defines the (sequence of) program(s) to be run.
-            May be empty while constructing, but must be non-empty when saving!
-            Also, all referenced components must be present in `component` when
-            saving. Similarly, all referenced couplers must be present in `coupler`.
-        component (List[Union[RRComponent, FMComponent, Component]]): List of
-            `<component>` elements that defines which programs can be used inside
-            the `<control>` subelements. Must be non-empty when saving!
-        coupler (Optional[List[Coupler]]): optional list of `<coupler>` elements
-            that defines which couplers can be used inside the `<parallel>`
-            elements under `<control>`.
-        waitFile (Optional[str]): Optional waitfile name for debugging.
-        global_settings (Optional[GlobalSettings]): Optional global DIMR settings.
-    """
-
-    documentation: Documentation = Documentation()
-    control: List[Union[Start, Parallel]] = Field([])
-    component: List[Union[RRComponent, FMComponent, Component]] = []
-    coupler: Optional[List[Coupler]] = []
-    waitFile: Optional[str]
-    global_settings: Optional[GlobalSettings]
-
-    @validator("component", "coupler", "control", pre=True)
-    def validate_component(cls, v):
-        return to_list(v)
-
-    def dict(self, *args, **kwargs):
-        kwargs["exclude_none"] = True
-        return super().dict(*args, **kwargs)
-
-    def _post_init_load(self) -> None:
-        """
-        Load the component models of this DIMR model.
-        """
-        super()._post_init_load()
-
-        for comp in self.component:
-            try:
-                comp.model = comp.get_model()(filepath=comp.filepath)
-            except NotImplementedError:
-                pass
-
-    @classmethod
-    def _ext(cls) -> str:
-        return ".xml"
-
-    @classmethod
-    def _filename(cls) -> str:
-        return "dimr_config"
-
-    @classmethod
-    def _get_serializer(
-        cls,
-    ) -> Callable[[Path, Dict, SerializerConfig, ModelSaveSettings], None]:
-        return DIMRSerializer.serialize
-
-    @classmethod
-    def _get_parser(cls) -> Callable:
-        return DIMRParser.parse
+from abc import ABC, abstractclassmethod
+from datetime import datetime
+from pathlib import Path
+from typing import Callable, Dict, List, Literal, Optional, Type, Union
+
+from pydantic import Field, validator
+
+from hydrolib.core import __version__
+from hydrolib.core.basemodel import (
+    BaseModel,
+    FileModel,
+    ModelSaveSettings,
+    ParsableFileModel,
+    SerializerConfig,
+)
+from hydrolib.core.dflowfm.mdu.models import FMModel
+from hydrolib.core.dimr.parser import DIMRParser
+from hydrolib.core.dimr.serializer import DIMRSerializer
+from hydrolib.core.rr.models import RainfallRunoffModel
+from hydrolib.core.utils import to_list
+
+
+class KeyValuePair(BaseModel):
+    """Key value pair to specify settings and parameters.
+
+    Attributes:
+        key: The key.
+        value: The value.
+    """
+
+    key: str
+    value: str
+
+
+class Component(BaseModel, ABC):
+    """
+    Specification of a BMI-compliant model component instance that will be executed by DIMR.
+
+    Attributes:
+        library: The library name of the compoment.
+        name: The component name.
+        workingDir: The working directory.
+        inputFile: The name of the input file.
+        process: Number of subprocesses in the component.
+        setting: A list of variables that are provided to the BMI model before initialization.
+        parameter: A list of variables that are provided to the BMI model after initialization.
+        mpiCommunicator: The MPI communicator value.
+        model: The model represented by this component.
+    """
+
+    library: str
+    name: str
+    workingDir: Path
+    inputFile: Path
+    process: Optional[int]
+    setting: Optional[List[KeyValuePair]] = []
+    parameter: Optional[List[KeyValuePair]] = []
+    mpiCommunicator: Optional[str]
+
+    model: Optional[FileModel]
+
+    @property
+    def filepath(self):
+        return self.workingDir / self.inputFile
+
+    @abstractclassmethod
+    def get_model(cls) -> Type[FileModel]:
+        raise NotImplementedError("Model not implemented yet.")
+
+    @validator("setting", "parameter", pre=True, allow_reuse=True)
+    def validate_setting(cls, v):
+        return to_list(v)
+
+    def is_intermediate_link(self) -> bool:
+        return True
+
+    def _get_identifier(self, data: dict) -> Optional[str]:
+        return data.get("name")
+
+    def dict(self, *args, **kwargs):
+        # Exclude the FileModel from any DIMR serialization.
+        kwargs["exclude"] = {"model"}
+        return super().dict(*args, **kwargs)
+
+
+class FMComponent(Component):
+    """Component to include the D-Flow FM program in a DIMR control flow."""
+
+    library: Literal["dflowfm"] = "dflowfm"
+
+    @classmethod
+    def get_model(cls):
+        return FMModel
+
+
+class RRComponent(Component):
+    """Component to include the RainfallRunoff program in a DIMR control flow."""
+
+    library: Literal["rr_dll"] = "rr_dll"
+
+    @classmethod
+    def get_model(cls):
+        return RainfallRunoffModel
+
+
+class Documentation(BaseModel):
+    """
+    Information on the present DIMR configuration file.
+
+    Attributes:
+        fileVersion: The DIMR file version.
+        createdBy: Creators of the DIMR file.
+        creationDate: The creation date of the DIMR file.
+    """
+
+    fileVersion: str = "1.3"
+    createdBy: str = f"hydrolib-core {__version__}"
+    creationDate: datetime = Field(default_factory=datetime.utcnow)
+
+
+class GlobalSettings(BaseModel):
+    """
+    Global settings for the DIMR configuration.
+
+    Attributes:
+        logger_ncFormat: NetCDF format type for logging.
+    """
+
+    logger_ncFormat: int
+
+
+class ComponentOrCouplerRef(BaseModel):
+    """
+    Reference to a BMI-compliant model component instance.
+
+    Attributes:
+        name: Name of the reference to a BMI-compliant model component instance.
+    """
+
+    name: str
+
+    def _get_identifier(self, data: dict) -> Optional[str]:
+        return data.get("name")
+
+
+class CoupledItem(BaseModel):
+    """
+    Specification of an item that has to be exchanged.
+
+    Attributes:
+        sourceName: Name of the item at the source component.
+        targetName: Name of the item at the target component.
+    """
+
+    sourceName: str
+    targetName: str
+
+    def is_intermediate_link(self) -> bool:
+        # TODO set to True once we replace Paths with FileModels
+        return False
+
+
+class Logger(BaseModel):
+    """
+    Used to log values to the specified file in workingdir for each timestep
+
+    Attributes:
+        workingDir: Directory where the log file is written.
+        outputFile: Name of the log file.
+    """
+
+    workingDir: Path
+    outputFile: Path
+
+
+class Coupler(BaseModel):
+    """
+    Specification of the coupling actions to be performed between two BMI-compliant model components.
+
+    Attributes:
+        name: The name of the coupler.
+        sourceComponent: The component that provides the data to has to be exchanged.
+        targetComponent: The component that consumes the data to has to be exchanged.
+        item: A list of items that have to be exchanged.
+        logger: Logger for logging the values that get exchanged.
+    """
+
+    name: str
+    sourceComponent: str
+    targetComponent: str
+    item: List[CoupledItem] = []
+    logger: Optional[Logger]
+
+    @validator("item", pre=True)
+    def validate_item(cls, v):
+        return to_list(v)
+
+    def is_intermediate_link(self) -> bool:
+        # TODO set to True once we replace Paths with FileModels
+        return False
+
+    def _get_identifier(self, data: dict) -> Optional[str]:
+        return data.get("name")
+
+
+class StartGroup(BaseModel):
+    """
+    Specification of model components and couplers to be executed with a certain frequency.
+
+    Attributes:
+        time: Time frame specification for the present group: start time, stop time and frequency.
+              Expressed in terms of the time frame of the main component.
+        start: Ordered list of components to be executed.
+        coupler: Oredered list of couplers to be executed.
+    """
+
+    time: str
+    start: List[ComponentOrCouplerRef] = []
+    coupler: List[ComponentOrCouplerRef] = []
+
+    @validator("start", "coupler", pre=True)
+    def validate_start(cls, v):
+        return to_list(v)
+
+
+class ControlModel(BaseModel):
+    """
+    Overrides to make sure that the control elements in the DIMR
+    are parsed and serialized correctly.
+    """
+
+    _type: str
+
+    def dict(self, *args, **kwargs):
+        """Add control element prefixes for serialized data."""
+        return {
+            str(self._type): super().dict(*args, **kwargs),
+        }
+
+    @classmethod
+    def validate(cls, v):
+        """Remove control element prefixes from parsed data."""
+
+        # should be replaced by discriminated unions once merged
+        # https://github.com/samuelcolvin/pydantic/pull/2336
+        if isinstance(v, dict) and len(v.keys()) == 1:
+            key = list(v.keys())[0]
+            v = v[key]
+        return super().validate(v)
+
+
+class Parallel(ControlModel):
+    """
+    Specification of a parallel control flow: one main component and a group of related components and couplers.
+    Step wise execution order according to order in parallel control flow.
+
+    Attributes:
+        startGroup: Group of components and couplers to be executed.
+        start: Main component to be executed step wise (provides start time, end time and time step).
+    """
+
+    _type: Literal["parallel"] = "parallel"
+    startGroup: StartGroup
+    start: ComponentOrCouplerRef
+
+
+class Start(ControlModel):
+    """
+    Specification of a serial control flow: one main component.
+
+    Attributes:
+        name: Name of the reference to a BMI-compliant model component instance
+    """
+
+    _type: Literal["start"] = "start"
+    name: str
+
+
+class DIMR(ParsableFileModel):
+    """DIMR model representation.
+
+    Attributes:
+        documentation (Documentation): File metadata.
+        control (List[Union[Start, Parallel]]): The `<control>` element with a list
+            of [Start][hydrolib.core.dimr.models.Start]
+            and [Parallel][hydrolib.core.dimr.models.Parallel] sub-elements,
+            which defines the (sequence of) program(s) to be run.
+            May be empty while constructing, but must be non-empty when saving!
+            Also, all referenced components must be present in `component` when
+            saving. Similarly, all referenced couplers must be present in `coupler`.
+        component (List[Union[RRComponent, FMComponent, Component]]): List of
+            `<component>` elements that defines which programs can be used inside
+            the `<control>` subelements. Must be non-empty when saving!
+        coupler (Optional[List[Coupler]]): optional list of `<coupler>` elements
+            that defines which couplers can be used inside the `<parallel>`
+            elements under `<control>`.
+        waitFile (Optional[str]): Optional waitfile name for debugging.
+        global_settings (Optional[GlobalSettings]): Optional global DIMR settings.
+    """
+
+    documentation: Documentation = Documentation()
+    control: List[Union[Start, Parallel]] = Field([])
+    component: List[Union[RRComponent, FMComponent, Component]] = []
+    coupler: Optional[List[Coupler]] = []
+    waitFile: Optional[str]
+    global_settings: Optional[GlobalSettings]
+
+    @validator("component", "coupler", "control", pre=True)
+    def validate_component(cls, v):
+        return to_list(v)
+
+    def dict(self, *args, **kwargs):
+        kwargs["exclude_none"] = True
+        return super().dict(*args, **kwargs)
+
+    def _post_init_load(self) -> None:
+        """
+        Load the component models of this DIMR model.
+        """
+        super()._post_init_load()
+
+        for comp in self.component:
+            try:
+                comp.model = comp.get_model()(filepath=comp.filepath)
+            except NotImplementedError:
+                pass
+
+    @classmethod
+    def _ext(cls) -> str:
+        return ".xml"
+
+    @classmethod
+    def _filename(cls) -> str:
+        return "dimr_config"
+
+    @classmethod
+    def _get_serializer(
+        cls,
+    ) -> Callable[[Path, Dict, SerializerConfig, ModelSaveSettings], None]:
+        return DIMRSerializer.serialize
+
+    @classmethod
+    def _get_parser(cls) -> Callable:
+        return DIMRParser.parse
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dimr/parser.py` & `hydrolib_core-0.5.1/hydrolib/core/dimr/parser.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,61 +1,61 @@
-from pathlib import Path
-from warnings import warn
-
-from lxml import etree
-
-
-class DIMRParser:
-    """A parser for DIMR xml files."""
-
-    @staticmethod
-    def parse(path: Path) -> dict:
-        """Parses a DIMR file to a dictionary.
-
-        Args:
-            path (Path): Path to the DIMR configuration file.
-        """
-        if not path.is_file():
-            warn(f"File: `{path}` not found, skipped parsing.")
-            return {}
-
-        parser = etree.XMLParser(
-            remove_comments=True, resolve_entities=False, no_network=True
-        )
-        root = etree.parse(str(path), parser=parser).getroot()
-
-        return DIMRParser._node_to_dictionary(root, True)
-
-    @staticmethod
-    def _node_to_dictionary(node: etree, ignore_attributes: bool = False):
-        """
-        Convert an lxml.etree node tree recursively into a nested dictionary.
-        The node's attributes and child items will be added to it's dictionary.
-
-        Args:
-            node (etree): The etree node
-            ignore_attributes (bool): Optional parameter; whether or not to
-                                      skip the node's attributes. Default is False.
-        """
-
-        result = {} if ignore_attributes else dict(node.attrib)
-
-        for child_node in node.iterchildren():
-
-            key = child_node.tag.split("}")[1]
-
-            if child_node.text and child_node.text.strip():
-                value = child_node.text
-            else:
-                value = DIMRParser._node_to_dictionary(child_node)
-
-            if key in result:
-
-                if type(result[key]) is list:
-                    result[key].append(value)
-                else:
-                    first_value = result[key].copy()
-                    result[key] = [first_value, value]
-            else:
-                result[key] = value
-
-        return result
+from pathlib import Path
+from warnings import warn
+
+from lxml import etree
+
+
+class DIMRParser:
+    """A parser for DIMR xml files."""
+
+    @staticmethod
+    def parse(path: Path) -> dict:
+        """Parses a DIMR file to a dictionary.
+
+        Args:
+            path (Path): Path to the DIMR configuration file.
+        """
+        if not path.is_file():
+            warn(f"File: `{path}` not found, skipped parsing.")
+            return {}
+
+        parser = etree.XMLParser(
+            remove_comments=True, resolve_entities=False, no_network=True
+        )
+        root = etree.parse(str(path), parser=parser).getroot()
+
+        return DIMRParser._node_to_dictionary(root, True)
+
+    @staticmethod
+    def _node_to_dictionary(node: etree, ignore_attributes: bool = False):
+        """
+        Convert an lxml.etree node tree recursively into a nested dictionary.
+        The node's attributes and child items will be added to it's dictionary.
+
+        Args:
+            node (etree): The etree node
+            ignore_attributes (bool): Optional parameter; whether or not to
+                                      skip the node's attributes. Default is False.
+        """
+
+        result = {} if ignore_attributes else dict(node.attrib)
+
+        for child_node in node.iterchildren():
+
+            key = child_node.tag.split("}")[1]
+
+            if child_node.text and child_node.text.strip():
+                value = child_node.text
+            else:
+                value = DIMRParser._node_to_dictionary(child_node)
+
+            if key in result:
+
+                if type(result[key]) is list:
+                    result[key].append(value)
+                else:
+                    first_value = result[key].copy()
+                    result[key] = [first_value, value]
+            else:
+                result[key] = value
+
+        return result
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/dimr/serializer.py` & `hydrolib_core-0.5.1/hydrolib/core/dimr/serializer.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,96 +1,96 @@
-from datetime import datetime
-from pathlib import Path
-from typing import List
-from xml.dom import minidom
-
-from lxml import etree as e
-
-from hydrolib.core.basemodel import ModelSaveSettings, SerializerConfig
-from hydrolib.core.utils import FilePathStyleConverter
-
-
-class DIMRSerializer:
-    """A serializer for DIMR files."""
-
-    @staticmethod
-    def serialize(
-        path: Path,
-        data: dict,
-        config: SerializerConfig,
-        save_settings: ModelSaveSettings,
-    ):
-        """
-        Serializes the DIMR data to the file at the specified path.
-
-        Attributes:
-            path (Path): The path to the destination file.
-            data (Dict): The data to be serialized.
-            config (SerializerConfig): The serialization configuration.
-            save_settings (ModelSaveSettings): The model save settings.
-        """
-
-        path.parent.mkdir(parents=True, exist_ok=True)
-
-        xmlns = "http://schemas.deltares.nl/dimr"
-        xsi = "http://www.w3.org/2001/XMLSchema-instance"
-        schema_location = "http://content.oss.deltares.nl/schemas/dimr-1.3.xsd"
-
-        attrib = {e.QName(xsi, "schemaLocation"): f"{xmlns} {schema_location}"}
-        namespaces = {None: xmlns, "xsi": xsi}
-
-        root = e.Element(
-            "dimrConfig",
-            attrib=attrib,
-            nsmap=namespaces,
-        )
-
-        path_style_converter = FilePathStyleConverter()
-        DIMRSerializer._build_tree(
-            root, data, config, save_settings, path_style_converter
-        )
-
-        to_string = minidom.parseString(e.tostring(root))
-        xml = to_string.toprettyxml(indent="  ", encoding="utf-8")
-
-        with path.open("wb") as f:
-            f.write(xml)
-
-    @staticmethod
-    def _build_tree(
-        root,
-        data: dict,
-        config: SerializerConfig,
-        save_settings: ModelSaveSettings,
-        path_style_converter: FilePathStyleConverter,
-    ):
-        name = data.pop("name", None)
-        if name:
-            root.set("name", name)
-
-        for key, val in data.items():
-            if isinstance(val, dict):
-                c = e.Element(key)
-                DIMRSerializer._build_tree(
-                    c, val, config, save_settings, path_style_converter
-                )
-                root.append(c)
-            elif isinstance(val, List):
-                for item in val:
-                    c = e.Element(key)
-                    DIMRSerializer._build_tree(
-                        c, item, config, save_settings, path_style_converter
-                    )
-                    root.append(c)
-            else:
-                c = e.Element(key)
-                if isinstance(val, datetime):
-                    c.text = val.isoformat(sep="T", timespec="auto")
-                elif isinstance(val, float):
-                    c.text = f"{val:{config.float_format}}"
-                elif isinstance(val, Path):
-                    c.text = path_style_converter.convert_from_os_style(
-                        val, save_settings.path_style
-                    )
-                else:
-                    c.text = str(val)
-                root.append(c)
+from datetime import datetime
+from pathlib import Path
+from typing import List
+from xml.dom import minidom
+
+from lxml import etree as e
+
+from hydrolib.core.basemodel import ModelSaveSettings, SerializerConfig
+from hydrolib.core.utils import FilePathStyleConverter
+
+
+class DIMRSerializer:
+    """A serializer for DIMR files."""
+
+    @staticmethod
+    def serialize(
+        path: Path,
+        data: dict,
+        config: SerializerConfig,
+        save_settings: ModelSaveSettings,
+    ):
+        """
+        Serializes the DIMR data to the file at the specified path.
+
+        Attributes:
+            path (Path): The path to the destination file.
+            data (Dict): The data to be serialized.
+            config (SerializerConfig): The serialization configuration.
+            save_settings (ModelSaveSettings): The model save settings.
+        """
+
+        path.parent.mkdir(parents=True, exist_ok=True)
+
+        xmlns = "http://schemas.deltares.nl/dimr"
+        xsi = "http://www.w3.org/2001/XMLSchema-instance"
+        schema_location = "http://content.oss.deltares.nl/schemas/dimr-1.3.xsd"
+
+        attrib = {e.QName(xsi, "schemaLocation"): f"{xmlns} {schema_location}"}
+        namespaces = {None: xmlns, "xsi": xsi}
+
+        root = e.Element(
+            "dimrConfig",
+            attrib=attrib,
+            nsmap=namespaces,
+        )
+
+        path_style_converter = FilePathStyleConverter()
+        DIMRSerializer._build_tree(
+            root, data, config, save_settings, path_style_converter
+        )
+
+        to_string = minidom.parseString(e.tostring(root))
+        xml = to_string.toprettyxml(indent="  ", encoding="utf-8")
+
+        with path.open("wb") as f:
+            f.write(xml)
+
+    @staticmethod
+    def _build_tree(
+        root,
+        data: dict,
+        config: SerializerConfig,
+        save_settings: ModelSaveSettings,
+        path_style_converter: FilePathStyleConverter,
+    ):
+        name = data.pop("name", None)
+        if name:
+            root.set("name", name)
+
+        for key, val in data.items():
+            if isinstance(val, dict):
+                c = e.Element(key)
+                DIMRSerializer._build_tree(
+                    c, val, config, save_settings, path_style_converter
+                )
+                root.append(c)
+            elif isinstance(val, List):
+                for item in val:
+                    c = e.Element(key)
+                    DIMRSerializer._build_tree(
+                        c, item, config, save_settings, path_style_converter
+                    )
+                    root.append(c)
+            else:
+                c = e.Element(key)
+                if isinstance(val, datetime):
+                    c.text = val.isoformat(sep="T", timespec="auto")
+                elif isinstance(val, float):
+                    c.text = f"{val:{config.float_format}}"
+                elif isinstance(val, Path):
+                    c.text = path_style_converter.convert_from_os_style(
+                        val, save_settings.path_style
+                    )
+                else:
+                    c.text = str(val)
+                root.append(c)
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/rr/meteo/parser.py` & `hydrolib_core-0.5.1/hydrolib/core/rr/meteo/parser.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,186 +1,186 @@
-from datetime import datetime, timedelta
-from pathlib import Path
-from typing import Dict, Iterator, List, Tuple
-
-
-class BuiEventParser:
-    """
-    A parser for the precipitation event section within a .bui file.
-    It resembles something like this:
-    StartTime (YYYY mm dd HH MM SS) TimeSeriesLength (dd HH MM SS)
-    PrecipitationPerTimestep
-    Example given:
-    2021 12 20 0 0 0 1 0 4 20
-    4.2 2.4
-    4.2 2.4
-    4.2 2.4
-    (it should match the timeseries length based on the seconds per timstep.)
-    Each column of the last three lines represents a station.
-    """
-
-    @staticmethod
-    def parse(raw_text: str) -> Dict:
-        """
-        Given text representing a single BuiPrecipitationEvent parses it into a dictionary.
-
-        Args:
-            raw_text (str): Text containing a single precipitation event.
-
-        Returns:
-            Dict: Mapped contents of the text.
-        """
-
-        def get_precipitations_per_ts(line: str) -> List[str]:
-            return [prec for prec in line.split()]
-
-        event_lines = raw_text.splitlines(keepends=False)
-        time_reference = BuiEventParser.parse_event_time_reference(event_lines[0])
-        return dict(
-            start_time=time_reference["start_time"],
-            timeseries_length=time_reference["timeseries_length"],
-            precipitation_per_timestep=list(
-                map(get_precipitations_per_ts, event_lines[1:])
-            ),
-        )
-
-    @staticmethod
-    def parse_event_time_reference(raw_text: str) -> Dict:
-        """
-        Parses a single event time reference line containing both the start time
-        and the timeseries length into a dictionary.
-
-        Args:
-            raw_text (str): Line representing both start time and timeseries length.
-
-        Returns:
-            Dict: Resulting dictionary with keys start_time and timeseries_length.
-        """
-
-        def get_start_time(line: str) -> datetime:
-            return datetime.strptime(line, "%Y %m %d %H %M %S")
-
-        def get_timeseries_length(line: str) -> timedelta:
-            time_fields = line.split()
-            return timedelta(
-                days=int(time_fields[0]),
-                hours=int(time_fields[1]),
-                minutes=int(time_fields[2]),
-                seconds=int(time_fields[3]),
-            )
-
-        timeref = raw_text.split()
-        return dict(
-            start_time=get_start_time(" ".join(timeref[:6])),
-            timeseries_length=get_timeseries_length(" ".join(timeref[6:])),
-        )
-
-
-class BuiEventListParser:
-    """
-    A parser for .bui events which are like this:
-    StartTime (YYYY mm dd HH MM SS) TimeSeriesLength (dd HH MM SS)
-    PrecipitationPerTimestep
-    StartTime (YYYY mm dd HH MM SS) TimeSeriesLength (dd HH MM SS)
-    PrecipitationPerTimestep
-    Example given:
-    2021 12 20 0 0 0 1 0 4 20
-    4.2
-    4.2
-    4.2
-    2021 12 21 0 0 0 1 0 4 20
-    2.4
-    2.4
-    2.4
-    """
-
-    @staticmethod
-    def parse(raw_text: str, n_events: int, timestep: int) -> List[Dict]:
-        """
-        Parses a given raw text containing 0 to many text blocks representing a precipitation event.
-
-        Args:
-            raw_text (str): Text blocks representing precipitation events.
-            n_events (int): Number of events contained in the text block.
-            timestep (int): Number of seconds conforming a timestep.
-
-        Returns:
-            List[Dict]: List containing all the events represented as dictionaries.
-        """
-
-        def get_event_timestep_length(raw_line: str) -> int:
-            timereference = BuiEventParser.parse_event_time_reference(raw_line)
-            ts_length: timedelta = timereference["timeseries_length"]
-            return ts_length.total_seconds()
-
-        def get_multiple_events(raw_lines: List[str]) -> Iterator[BuiEventParser]:
-            n_line = 0
-            while n_line < len(raw_lines):
-                ts_seconds = get_event_timestep_length(raw_lines[n_line])
-                event_lines = int(ts_seconds / timestep) + 1
-                yield BuiEventParser.parse("\n".join(raw_lines[n_line:][:event_lines]))
-                n_line += event_lines
-
-        event_list = []
-        if n_events == 1:
-            event_list.append(BuiEventParser.parse(raw_text))
-        elif n_events > 1:
-            raw_lines = raw_text.splitlines(keepends=False)
-            event_list = list(get_multiple_events(raw_lines))
-
-        return event_list
-
-
-class BuiParser:
-    """
-    A parser for .bui files which are like this:
-    * comments
-    Dataset type to use (always 1).
-    * comments
-    Number of stations.
-    * comments
-    Name of stations
-    * comments
-    Number of events Number of seconds per timestep.
-    * comments
-    First datetime reference.
-    Precipitation per timestep per station.
-    """
-
-    @staticmethod
-    def parse(filepath: Path) -> Dict:
-        """
-        Parses a given file, in case valid, into a dictionary which can later be mapped
-        to the BuiModel.
-
-        Args:
-            filepath (Path): Path to file containing the data to parse.
-
-        Returns:
-            Dict: Parsed values.
-        """
-
-        def get_station_ids(line: str) -> List[str]:
-            return [s_id for s_id in line.split(",")]
-
-        def parse_events_and_timestep(line: str) -> Tuple[int, int]:
-            n_events_timestep = line.split()
-            return (int(n_events_timestep[0]), int(n_events_timestep[1]))
-
-        bui_lines = [
-            line
-            for line in filepath.read_text(encoding="utf8").splitlines()
-            if not line.startswith("*")
-        ]
-
-        n_events, timestep = parse_events_and_timestep(bui_lines[3])
-
-        return dict(
-            default_dataset=bui_lines[0],
-            number_of_stations=bui_lines[1],
-            name_of_stations=get_station_ids(bui_lines[2]),
-            number_of_events=n_events,
-            seconds_per_timestep=timestep,
-            precipitation_events=BuiEventListParser.parse(
-                "\n".join(bui_lines[4:]), n_events, timestep
-            ),
-        )
+from datetime import datetime, timedelta
+from pathlib import Path
+from typing import Dict, Iterator, List, Tuple
+
+
+class BuiEventParser:
+    """
+    A parser for the precipitation event section within a .bui file.
+    It resembles something like this:
+    StartTime (YYYY mm dd HH MM SS) TimeSeriesLength (dd HH MM SS)
+    PrecipitationPerTimestep
+    Example given:
+    2021 12 20 0 0 0 1 0 4 20
+    4.2 2.4
+    4.2 2.4
+    4.2 2.4
+    (it should match the timeseries length based on the seconds per timstep.)
+    Each column of the last three lines represents a station.
+    """
+
+    @staticmethod
+    def parse(raw_text: str) -> Dict:
+        """
+        Given text representing a single BuiPrecipitationEvent parses it into a dictionary.
+
+        Args:
+            raw_text (str): Text containing a single precipitation event.
+
+        Returns:
+            Dict: Mapped contents of the text.
+        """
+
+        def get_precipitations_per_ts(line: str) -> List[str]:
+            return [prec for prec in line.split()]
+
+        event_lines = raw_text.splitlines(keepends=False)
+        time_reference = BuiEventParser.parse_event_time_reference(event_lines[0])
+        return dict(
+            start_time=time_reference["start_time"],
+            timeseries_length=time_reference["timeseries_length"],
+            precipitation_per_timestep=list(
+                map(get_precipitations_per_ts, event_lines[1:])
+            ),
+        )
+
+    @staticmethod
+    def parse_event_time_reference(raw_text: str) -> Dict:
+        """
+        Parses a single event time reference line containing both the start time
+        and the timeseries length into a dictionary.
+
+        Args:
+            raw_text (str): Line representing both start time and timeseries length.
+
+        Returns:
+            Dict: Resulting dictionary with keys start_time and timeseries_length.
+        """
+
+        def get_start_time(line: str) -> datetime:
+            return datetime.strptime(line, "%Y %m %d %H %M %S")
+
+        def get_timeseries_length(line: str) -> timedelta:
+            time_fields = line.split()
+            return timedelta(
+                days=int(time_fields[0]),
+                hours=int(time_fields[1]),
+                minutes=int(time_fields[2]),
+                seconds=int(time_fields[3]),
+            )
+
+        timeref = raw_text.split()
+        return dict(
+            start_time=get_start_time(" ".join(timeref[:6])),
+            timeseries_length=get_timeseries_length(" ".join(timeref[6:])),
+        )
+
+
+class BuiEventListParser:
+    """
+    A parser for .bui events which are like this:
+    StartTime (YYYY mm dd HH MM SS) TimeSeriesLength (dd HH MM SS)
+    PrecipitationPerTimestep
+    StartTime (YYYY mm dd HH MM SS) TimeSeriesLength (dd HH MM SS)
+    PrecipitationPerTimestep
+    Example given:
+    2021 12 20 0 0 0 1 0 4 20
+    4.2
+    4.2
+    4.2
+    2021 12 21 0 0 0 1 0 4 20
+    2.4
+    2.4
+    2.4
+    """
+
+    @staticmethod
+    def parse(raw_text: str, n_events: int, timestep: int) -> List[Dict]:
+        """
+        Parses a given raw text containing 0 to many text blocks representing a precipitation event.
+
+        Args:
+            raw_text (str): Text blocks representing precipitation events.
+            n_events (int): Number of events contained in the text block.
+            timestep (int): Number of seconds conforming a timestep.
+
+        Returns:
+            List[Dict]: List containing all the events represented as dictionaries.
+        """
+
+        def get_event_timestep_length(raw_line: str) -> int:
+            timereference = BuiEventParser.parse_event_time_reference(raw_line)
+            ts_length: timedelta = timereference["timeseries_length"]
+            return ts_length.total_seconds()
+
+        def get_multiple_events(raw_lines: List[str]) -> Iterator[BuiEventParser]:
+            n_line = 0
+            while n_line < len(raw_lines):
+                ts_seconds = get_event_timestep_length(raw_lines[n_line])
+                event_lines = int(ts_seconds / timestep) + 1
+                yield BuiEventParser.parse("\n".join(raw_lines[n_line:][:event_lines]))
+                n_line += event_lines
+
+        event_list = []
+        if n_events == 1:
+            event_list.append(BuiEventParser.parse(raw_text))
+        elif n_events > 1:
+            raw_lines = raw_text.splitlines(keepends=False)
+            event_list = list(get_multiple_events(raw_lines))
+
+        return event_list
+
+
+class BuiParser:
+    """
+    A parser for .bui files which are like this:
+    * comments
+    Dataset type to use (always 1).
+    * comments
+    Number of stations.
+    * comments
+    Name of stations
+    * comments
+    Number of events Number of seconds per timestep.
+    * comments
+    First datetime reference.
+    Precipitation per timestep per station.
+    """
+
+    @staticmethod
+    def parse(filepath: Path) -> Dict:
+        """
+        Parses a given file, in case valid, into a dictionary which can later be mapped
+        to the BuiModel.
+
+        Args:
+            filepath (Path): Path to file containing the data to parse.
+
+        Returns:
+            Dict: Parsed values.
+        """
+
+        def get_station_ids(line: str) -> List[str]:
+            return [s_id for s_id in line.split(",")]
+
+        def parse_events_and_timestep(line: str) -> Tuple[int, int]:
+            n_events_timestep = line.split()
+            return (int(n_events_timestep[0]), int(n_events_timestep[1]))
+
+        bui_lines = [
+            line
+            for line in filepath.read_text(encoding="utf8").splitlines()
+            if not line.startswith("*")
+        ]
+
+        n_events, timestep = parse_events_and_timestep(bui_lines[3])
+
+        return dict(
+            default_dataset=bui_lines[0],
+            number_of_stations=bui_lines[1],
+            name_of_stations=get_station_ids(bui_lines[2]),
+            number_of_events=n_events,
+            seconds_per_timestep=timestep,
+            precipitation_events=BuiEventListParser.parse(
+                "\n".join(bui_lines[4:]), n_events, timestep
+            ),
+        )
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/rr/meteo/serializer.py` & `hydrolib_core-0.5.1/hydrolib/core/rr/meteo/serializer.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,238 +1,238 @@
-import inspect
-from datetime import datetime, timedelta
-from pathlib import Path
-from typing import Dict, List
-
-from hydrolib.core.basemodel import ModelSaveSettings, SerializerConfig
-
-
-class BuiEventSerializer:
-    """
-    Serializer class to transform a bui event into a text block.
-    """
-
-    bui_event_template = inspect.cleandoc(
-        """
-        * Event {event_idx} duration days:{d_days} hours:{d_hours} minutes:{d_minutes} seconds:{d_seconds}
-        * Start date and time of the event: yyyy mm dd hh mm ss
-        * Duration of the event           : dd hh mm ss
-        * Rainfall value per time step [mm/time step]
-        {start_time} {timeseries_length}
-        {precipitation_per_timestep}
-    """
-    )
-
-    @staticmethod
-    def serialize(event_data: Dict, config: SerializerConfig) -> str:
-        """
-        Serializes a dictionary representing an event into a text block.
-
-        Args:
-            event_data (Dict): Dictionary representing precipitation event.
-            config (SerializerConfig): The serialization configuration.
-
-        Returns:
-            str: Formatted string.
-        """
-        event_data["start_time"] = BuiEventSerializer.serialize_start_time(
-            event_data["start_time"]
-        )
-        ts_duration = event_data["timeseries_length"]
-        event_data = {
-            **event_data,
-            **BuiEventSerializer.get_timedelta_fields(ts_duration),
-        }
-        event_data[
-            "timeseries_length"
-        ] = BuiEventSerializer.serialize_timeseries_length(
-            event_data["timeseries_length"]
-        )
-        event_data[
-            "precipitation_per_timestep"
-        ] = BuiEventSerializer.serialize_precipitation_per_timestep(
-            event_data["precipitation_per_timestep"], config
-        )
-        if "event_idx" not in event_data.keys():
-            event_data["event_idx"] = 1
-        return BuiEventSerializer.bui_event_template.format(**event_data)
-
-    @staticmethod
-    def get_timedelta_fields(duration: timedelta) -> Dict:
-        """
-        Gets a dictionary containing the time delta in days, hours, minutes and seconds.
-        This means that the seconds field does not contain the accumulative value of days
-        hours and minutes.
-
-        Args:
-            duration (timedelta): Timedelta to convert.
-
-        Returns:
-            Dict: Dictionary containing all fields.
-        """
-        total_hours = int(duration.seconds / (60 * 60))
-        total_minutes = int((duration.seconds / 60) - (total_hours * 60))
-        total_seconds = int(
-            duration.seconds - ((total_hours * 60 + total_minutes) * 60)
-        )
-        return dict(
-            d_seconds=total_seconds,
-            d_minutes=total_minutes,
-            d_hours=total_hours,
-            d_days=duration.days,
-        )
-
-    @staticmethod
-    def serialize_start_time(data_to_serialize: datetime) -> str:
-        """
-        Serializes a datetime into the expected .bui format.
-
-        Args:
-            data_to_serialize (datetime): Datetime representing reference time.
-
-        Returns:
-            str: Converted datetime into string.
-        """
-        # Not using the following format because we only want one digit instead of
-        # double (day 1 -> 1, instead of 01).
-        # data_to_serialize.strftime("%Y %m %d %H %M %S")
-        dt = data_to_serialize
-        return f"{dt.year} {dt.month} {dt.day} {dt.hour} {dt.minute} {dt.second}"
-
-    @staticmethod
-    def serialize_timeseries_length(data_to_serialize: timedelta) -> str:
-        """
-        Serializes a given timedelta into the .bui format.
-
-        Args:
-            data_to_serialize (timedelta): Reference timespan to serialize.
-
-        Returns:
-            str: Converted timedelta in string.
-        """
-        fields_dict = BuiEventSerializer.get_timedelta_fields(data_to_serialize)
-        total_hours = fields_dict["d_hours"]
-        total_minutes = fields_dict["d_minutes"]
-        total_seconds = fields_dict["d_seconds"]
-        return f"{data_to_serialize.days} {total_hours} {total_minutes} {total_seconds}"
-
-    @staticmethod
-    def serialize_precipitation_per_timestep(
-        data_to_serialize: List[List[float]], config: SerializerConfig
-    ) -> str:
-        """
-        Serialized the data containing all the precipitations per timestep (and station)
-        into a single string ready to be mapped.
-
-        Args:
-            data_to_serialize (List[List[str]]): Data to be mapped.
-            config (SerializerConfig): The serialization configuration.
-
-        Returns:
-            str: Serialized string in .bui format.
-        """
-        float_format = lambda v: f"{v:{config.float_format}}"
-        serialized_data = str.join(
-            "\n",
-            [
-                str.join(" ", map(float_format, listed_data))
-                for listed_data in data_to_serialize
-            ],
-        )
-        return serialized_data
-
-
-class BuiSerializer:
-    """
-    Serializer class to transform an object into a .bui file text format.
-    """
-
-    bui_template = inspect.cleandoc(
-        """
-        *Name of this file: {filepath}
-        *Date and time of construction: {datetime_now}
-        *Comments are following an * (asterisk) and written above variables
-        {default_dataset}
-        *Number of stations
-        {number_of_stations}
-        *Station Name
-        {name_of_stations}
-        *Number_of_events seconds_per_timestamp
-        {number_of_events} {seconds_per_timestep}
-        {precipitation_events}
-        """
-    )
-
-    @staticmethod
-    def serialize(bui_data: Dict, config: SerializerConfig) -> str:
-        """
-        Formats the bui_template with the content of the given data.
-        NOTE: It requires that caller injects file_path into bui_data prior to this call.
-        Otherwise it will crash.
-
-        Args:
-            bui_data (Dict): Data to serialize.
-            config (SerializerConfig): The serialization configuration.
-
-        Returns:
-            str: The serialized data.
-        """
-        bui_data["datetime_now"] = datetime.now().strftime("%d-%m-%y %H:%M:%S")
-        bui_data["name_of_stations"] = BuiSerializer.serialize_stations_ids(
-            bui_data["name_of_stations"]
-        )
-        bui_data["precipitation_events"] = BuiSerializer.serialize_event_list(
-            bui_data["precipitation_events"], config
-        )
-        return BuiSerializer.bui_template.format(**bui_data)
-
-    @staticmethod
-    def serialize_event_list(
-        data_to_serialize: List[Dict], config: SerializerConfig
-    ) -> str:
-        """
-        Serializes a event list dictionary into a single text block.
-
-        Args:
-            data_to_serialize (Dict): Dictionary containing list of events.
-            config (SerializerConfig): The serialization configuration.
-
-        Returns:
-            str: Text block representing all precipitation events.
-        """
-        serialized_list = []
-        for n_event, event in enumerate(data_to_serialize):
-            event["event_idx"] = n_event + 1
-            serialized_list.append(BuiEventSerializer.serialize(event, config))
-        return "\n".join(serialized_list)
-
-    @staticmethod
-    def serialize_stations_ids(data_to_serialize: List[str]) -> str:
-        """
-        Serializes the stations ids into a single string as expected in a .bui file.
-
-        Args:
-            data_to_serialize (List[str]): List of station ids.
-
-        Returns:
-            str: Serialized string.
-        """
-        return str.join(" ", data_to_serialize)
-
-
-def write_bui_file(
-    path: Path, data: Dict, config: SerializerConfig, save_settings: ModelSaveSettings
-) -> None:
-    """
-    Writes a .bui file in the given path based on the data given in a dictionary.
-
-    Args:
-        path (Path): Path where to output the text.
-        data (Dict): Data to serialize into the file.
-        config (SerializerConfig): The serialization configuration.
-        save_settings (ModelSaveSettings): The model save settings.
-    """
-    data["filepath"] = path  # This is redundant as already exists in the data.
-    serialized_bui_data = BuiSerializer.serialize(data, config)
-
-    path.parent.mkdir(parents=True, exist_ok=True)
-    path.write_text(serialized_bui_data, encoding="utf8")
+import inspect
+from datetime import datetime, timedelta
+from pathlib import Path
+from typing import Dict, List
+
+from hydrolib.core.basemodel import ModelSaveSettings, SerializerConfig
+
+
+class BuiEventSerializer:
+    """
+    Serializer class to transform a bui event into a text block.
+    """
+
+    bui_event_template = inspect.cleandoc(
+        """
+        * Event {event_idx} duration days:{d_days} hours:{d_hours} minutes:{d_minutes} seconds:{d_seconds}
+        * Start date and time of the event: yyyy mm dd hh mm ss
+        * Duration of the event           : dd hh mm ss
+        * Rainfall value per time step [mm/time step]
+        {start_time} {timeseries_length}
+        {precipitation_per_timestep}
+    """
+    )
+
+    @staticmethod
+    def serialize(event_data: Dict, config: SerializerConfig) -> str:
+        """
+        Serializes a dictionary representing an event into a text block.
+
+        Args:
+            event_data (Dict): Dictionary representing precipitation event.
+            config (SerializerConfig): The serialization configuration.
+
+        Returns:
+            str: Formatted string.
+        """
+        event_data["start_time"] = BuiEventSerializer.serialize_start_time(
+            event_data["start_time"]
+        )
+        ts_duration = event_data["timeseries_length"]
+        event_data = {
+            **event_data,
+            **BuiEventSerializer.get_timedelta_fields(ts_duration),
+        }
+        event_data[
+            "timeseries_length"
+        ] = BuiEventSerializer.serialize_timeseries_length(
+            event_data["timeseries_length"]
+        )
+        event_data[
+            "precipitation_per_timestep"
+        ] = BuiEventSerializer.serialize_precipitation_per_timestep(
+            event_data["precipitation_per_timestep"], config
+        )
+        if "event_idx" not in event_data.keys():
+            event_data["event_idx"] = 1
+        return BuiEventSerializer.bui_event_template.format(**event_data)
+
+    @staticmethod
+    def get_timedelta_fields(duration: timedelta) -> Dict:
+        """
+        Gets a dictionary containing the time delta in days, hours, minutes and seconds.
+        This means that the seconds field does not contain the accumulative value of days
+        hours and minutes.
+
+        Args:
+            duration (timedelta): Timedelta to convert.
+
+        Returns:
+            Dict: Dictionary containing all fields.
+        """
+        total_hours = int(duration.seconds / (60 * 60))
+        total_minutes = int((duration.seconds / 60) - (total_hours * 60))
+        total_seconds = int(
+            duration.seconds - ((total_hours * 60 + total_minutes) * 60)
+        )
+        return dict(
+            d_seconds=total_seconds,
+            d_minutes=total_minutes,
+            d_hours=total_hours,
+            d_days=duration.days,
+        )
+
+    @staticmethod
+    def serialize_start_time(data_to_serialize: datetime) -> str:
+        """
+        Serializes a datetime into the expected .bui format.
+
+        Args:
+            data_to_serialize (datetime): Datetime representing reference time.
+
+        Returns:
+            str: Converted datetime into string.
+        """
+        # Not using the following format because we only want one digit instead of
+        # double (day 1 -> 1, instead of 01).
+        # data_to_serialize.strftime("%Y %m %d %H %M %S")
+        dt = data_to_serialize
+        return f"{dt.year} {dt.month} {dt.day} {dt.hour} {dt.minute} {dt.second}"
+
+    @staticmethod
+    def serialize_timeseries_length(data_to_serialize: timedelta) -> str:
+        """
+        Serializes a given timedelta into the .bui format.
+
+        Args:
+            data_to_serialize (timedelta): Reference timespan to serialize.
+
+        Returns:
+            str: Converted timedelta in string.
+        """
+        fields_dict = BuiEventSerializer.get_timedelta_fields(data_to_serialize)
+        total_hours = fields_dict["d_hours"]
+        total_minutes = fields_dict["d_minutes"]
+        total_seconds = fields_dict["d_seconds"]
+        return f"{data_to_serialize.days} {total_hours} {total_minutes} {total_seconds}"
+
+    @staticmethod
+    def serialize_precipitation_per_timestep(
+        data_to_serialize: List[List[float]], config: SerializerConfig
+    ) -> str:
+        """
+        Serialized the data containing all the precipitations per timestep (and station)
+        into a single string ready to be mapped.
+
+        Args:
+            data_to_serialize (List[List[str]]): Data to be mapped.
+            config (SerializerConfig): The serialization configuration.
+
+        Returns:
+            str: Serialized string in .bui format.
+        """
+        float_format = lambda v: f"{v:{config.float_format}}"
+        serialized_data = str.join(
+            "\n",
+            [
+                str.join(" ", map(float_format, listed_data))
+                for listed_data in data_to_serialize
+            ],
+        )
+        return serialized_data
+
+
+class BuiSerializer:
+    """
+    Serializer class to transform an object into a .bui file text format.
+    """
+
+    bui_template = inspect.cleandoc(
+        """
+        *Name of this file: {filepath}
+        *Date and time of construction: {datetime_now}
+        *Comments are following an * (asterisk) and written above variables
+        {default_dataset}
+        *Number of stations
+        {number_of_stations}
+        *Station Name
+        {name_of_stations}
+        *Number_of_events seconds_per_timestamp
+        {number_of_events} {seconds_per_timestep}
+        {precipitation_events}
+        """
+    )
+
+    @staticmethod
+    def serialize(bui_data: Dict, config: SerializerConfig) -> str:
+        """
+        Formats the bui_template with the content of the given data.
+        NOTE: It requires that caller injects file_path into bui_data prior to this call.
+        Otherwise it will crash.
+
+        Args:
+            bui_data (Dict): Data to serialize.
+            config (SerializerConfig): The serialization configuration.
+
+        Returns:
+            str: The serialized data.
+        """
+        bui_data["datetime_now"] = datetime.now().strftime("%d-%m-%y %H:%M:%S")
+        bui_data["name_of_stations"] = BuiSerializer.serialize_stations_ids(
+            bui_data["name_of_stations"]
+        )
+        bui_data["precipitation_events"] = BuiSerializer.serialize_event_list(
+            bui_data["precipitation_events"], config
+        )
+        return BuiSerializer.bui_template.format(**bui_data)
+
+    @staticmethod
+    def serialize_event_list(
+        data_to_serialize: List[Dict], config: SerializerConfig
+    ) -> str:
+        """
+        Serializes a event list dictionary into a single text block.
+
+        Args:
+            data_to_serialize (Dict): Dictionary containing list of events.
+            config (SerializerConfig): The serialization configuration.
+
+        Returns:
+            str: Text block representing all precipitation events.
+        """
+        serialized_list = []
+        for n_event, event in enumerate(data_to_serialize):
+            event["event_idx"] = n_event + 1
+            serialized_list.append(BuiEventSerializer.serialize(event, config))
+        return "\n".join(serialized_list)
+
+    @staticmethod
+    def serialize_stations_ids(data_to_serialize: List[str]) -> str:
+        """
+        Serializes the stations ids into a single string as expected in a .bui file.
+
+        Args:
+            data_to_serialize (List[str]): List of station ids.
+
+        Returns:
+            str: Serialized string.
+        """
+        return str.join(" ", data_to_serialize)
+
+
+def write_bui_file(
+    path: Path, data: Dict, config: SerializerConfig, save_settings: ModelSaveSettings
+) -> None:
+    """
+    Writes a .bui file in the given path based on the data given in a dictionary.
+
+    Args:
+        path (Path): Path where to output the text.
+        data (Dict): Data to serialize into the file.
+        config (SerializerConfig): The serialization configuration.
+        save_settings (ModelSaveSettings): The model save settings.
+    """
+    data["filepath"] = path  # This is redundant as already exists in the data.
+    serialized_bui_data = BuiSerializer.serialize(data, config)
+
+    path.parent.mkdir(parents=True, exist_ok=True)
+    path.write_text(serialized_bui_data, encoding="utf8")
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/rr/parser.py` & `hydrolib_core-0.5.1/hydrolib/core/rr/parser.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,58 +1,58 @@
-"""parser.py defines the read method for the RainfallRunoffModel."""
-
-from typing import Dict, Iterable, Optional
-
-from pydantic.types import FilePath
-
-
-def _strip(lines: Iterable[str]) -> Iterable[str]:
-    return (l.strip() for l in lines)
-
-
-def _is_empty(line: str) -> bool:
-    return len(line) == 0
-
-
-def _is_comment(line: str) -> bool:
-    return line[0] == "*"
-
-
-def _to_path(line: str) -> Optional[str]:
-    value = line.split("*", 1)[0].strip()[1:-1].strip()
-
-    if len(value) == 0 or value == "not used":
-        return None
-
-    return value
-
-
-def _to_values(lines: Iterable[str]) -> Iterable[Optional[str]]:
-    return (_to_path(v) for v in _strip(lines) if not (_is_empty(v) or _is_comment(v)))
-
-
-def parse(keys: Iterable[str], lines: Iterable[str]) -> Dict:
-    """Parse the set of lines to its corresponding RainfallRunoffModel.
-
-    Args:
-        keys (Iterable[str]): The property keys of the RainfallRunoffModel.
-        lines (Iterable[str]): The content of a file in .fnm format.
-
-    Returns:
-        RainfallRunoffModel: The corresponding RainfallRunoffModel.
-    """
-    values = _to_values(lines)
-    return dict(zip(keys, values))
-
-
-def read(keys: Iterable[str], path: FilePath) -> Dict:
-    """Parse the file at the specified path into a RainfallRunoffModel
-
-    Args:
-        keys (Iterable[str]): The property keys of the RainfallRunoffModel.
-        path (FilePath): The path to the Rainfall Runoff definition file
-
-    Returns:
-        RainfallRunoffModel: The RainfallRunoffModel corresponding with the file.
-    """
-    with path.open("r", encoding="utf8") as f:
-        return parse(keys, f)
+"""parser.py defines the read method for the RainfallRunoffModel."""
+
+from typing import Dict, Iterable, Optional
+
+from pydantic.types import FilePath
+
+
+def _strip(lines: Iterable[str]) -> Iterable[str]:
+    return (l.strip() for l in lines)
+
+
+def _is_empty(line: str) -> bool:
+    return len(line) == 0
+
+
+def _is_comment(line: str) -> bool:
+    return line[0] == "*"
+
+
+def _to_path(line: str) -> Optional[str]:
+    value = line.split("*", 1)[0].strip()[1:-1].strip()
+
+    if len(value) == 0 or value == "not used":
+        return None
+
+    return value
+
+
+def _to_values(lines: Iterable[str]) -> Iterable[Optional[str]]:
+    return (_to_path(v) for v in _strip(lines) if not (_is_empty(v) or _is_comment(v)))
+
+
+def parse(keys: Iterable[str], lines: Iterable[str]) -> Dict:
+    """Parse the set of lines to its corresponding RainfallRunoffModel.
+
+    Args:
+        keys (Iterable[str]): The property keys of the RainfallRunoffModel.
+        lines (Iterable[str]): The content of a file in .fnm format.
+
+    Returns:
+        RainfallRunoffModel: The corresponding RainfallRunoffModel.
+    """
+    values = _to_values(lines)
+    return dict(zip(keys, values))
+
+
+def read(keys: Iterable[str], path: FilePath) -> Dict:
+    """Parse the file at the specified path into a RainfallRunoffModel
+
+    Args:
+        keys (Iterable[str]): The property keys of the RainfallRunoffModel.
+        path (FilePath): The path to the Rainfall Runoff definition file
+
+    Returns:
+        RainfallRunoffModel: The RainfallRunoffModel corresponding with the file.
+    """
+    with path.open("r", encoding="utf8") as f:
+        return parse(keys, f)
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/rr/serializer.py` & `hydrolib_core-0.5.1/hydrolib/core/rr/serializer.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,226 +1,226 @@
-"""serializer.py defines the write method for the RainfallRunoffModel."""
-
-import inspect
-from pathlib import Path
-from typing import Dict, Iterable, Optional, Union
-
-from hydrolib.core.basemodel import ModelSaveSettings, SerializerConfig
-from hydrolib.core.utils import FilePathStyleConverter, get_str_len
-
-
-def _calculate_max_value_length(data: Iterable) -> int:
-    return max(map(get_str_len, data))
-
-
-def _get_string_value(
-    path_value: Optional[Union[dict, Path, str]], save_settings: ModelSaveSettings
-) -> str:
-    """Get printable string value of the path value in a typical
-    RainfallRunoffModel.
-
-    str type as input is intentionally accepted to support file names
-    pointing to non-existent files (e.g. in incomplete model input).
-
-    Args:
-        path_value (Union[dict, Path, str]): path-like value to be printed.
-        save_settings (ModelSaveSettings): The model save settings.
-    Returns:
-        str: The str representation of input path_value.
-    """
-
-    file_path_style_converter = FilePathStyleConverter()
-
-    path = None
-    if isinstance(path_value, dict) and (file_path := path_value.get("filepath", None)):
-        path = file_path
-    elif isinstance(path_value, Path):
-        path = path_value
-    elif isinstance(path_value, str):
-        path = Path(path_value)
-
-    if path is None:
-        value = ""
-    else:
-        value = file_path_style_converter.convert_from_os_style(
-            path, save_settings.path_style
-        )
-
-    return f"'{value}'"
-
-
-def serialize(data: Dict, save_settings: ModelSaveSettings) -> str:
-    """Serialize the specified model.
-
-    Args:
-        data (Dict): dict values of the RainfallRunoffModel to serialize.
-        save_settings (ModelSaveSettings): The model save settings.
-
-    Returns:
-        str: The serialized RainfallRunoffModel in .fnm format.
-    """
-
-    values = [_get_string_value(v, save_settings) for v in data.values()]
-    max_len = _calculate_max_value_length(values)
-    padded_values = [s.ljust(max_len) if s else " " * max_len for s in values]
-
-    # fmt: off
-    return inspect.cleandoc("""
-        *
-        * DELFT_3B Version 1.00
-        * -----------------------------------------------------------------
-        *
-        * Last update : March 1995
-        *
-        * All input- and output file names (free format)
-        *
-        *   Namen Mappix files (*.DIR, *.TST, *.his) mogen NIET gewijzigd worden.
-        *   Overige filenamen mogen wel gewijzigd worden.
-        *
-        *
-        {}    *   1. Control file                                                         I
-        {}    *   2. Knoop data                                                           I
-        {}    *   3. Tak data                                                             I
-        {}    *   4. Open water data                                                      I
-        {}    *   5. Verhard gebied algemeen                                              I
-        {}    *   6. Verhard gebied storage                                               I
-        {}    *   7. Verhard gebied DWA                                                   I
-        {}    *   8. Verhard gebied sewer pump capacity                                   I
-        {}    *   9. Boundaries                                                           I
-        {}    *  10. Pluvius                                                              I
-        {}    *  11. Pluvius algemeen                                                     I
-        {}    *  12. Kasklasse                                                            I
-        {}    *  13. buifile                                                              I
-        {}    *  14. verdampingsfile                                                      I
-        {}    *  15. unpaved algemeen                                                     I
-        {}    *  16. unpaved storage                                                      I
-        {}    *  17. kasgebied initialisatie (SC)                                         I
-        {}    *  18. kasgebied verbruiksdata (SC)                                         I
-        {}    *  19. crop factors gewassen                                                I
-        {}    *  20. tabel bergingscoef=f(ontw.diepte,grondsoort)                         I
-        {}    *  21. Unpaved - alfa factor definities                                     I
-        {}    *  22. Run messages                                                         O
-        {}    *  23. Overzicht van schematisatie, algemene gegevens                       O
-        {}    *  24. Output results verhard                                               O
-        {}    *  25. Output results onverhard                                             O
-        {}    *  26. Output results kas                                                   O
-        {}    *  27. Output results open water                                            O
-        {}    *  28. Output results kunstwerk                                             O
-        {}    *  29. Output results boundaries                                            O
-        {}    *  30. Output results Pluvius                                               O
-        {}    *  31. Unpaved infiltratie definities                                       I
-        {}    *  32. Debugfile                                                            O
-        {}    *  33. Unpaved seepage                                                      I
-        {}    *  34. Unpaved tabels initial gwl and Scurve                                I
-        {}    *  35. Kassen general data                                                  I
-        {}    *  36. Kassen roof storage                                                  I
-        {}    *  37. Pluvius rioolinloop ASCII file                                       O
-        {}    *  38. Invoerfile met variabele peilen op randknopen                        I
-        {}    *  39. Invoerfile met zoutgegevens                                          I
-        {}    *  40. Invoerfile met cropfactors open water                                I
-        {}    *  41. Restart file input                                                   I
-        {}    *  42. Restart file output                                                  O
-        {}    *  43. Binary file input                                                    I
-        {}    *  44. Sacramento input I        
-        {}    *  45. Uitvoer ASCII file met debieten van/naar randknopen                  O
-        {}    *  46. Uitvoer ASCII file met zoutconcentratie op rand                      O
-        {}    *  47. Zout uitvoer in ASCII file                                           O
-        {}    *  48. Greenhouse silo definitions                                          I
-        {}    *  49. Open water general data                                              I
-        {}    *  50. Open water seepage definitions                                       I
-        {}    *  51. Open water tables target levels                                      I
-        {}    *  52. General structure data                                               I
-        {}    *  53. Structure definitions                                                I
-        {}    *  54. Controller definitions                                               I
-        {}    *  55. Tabellen structures                                                  I
-        {}    *  56. Boundary data                                                        I
-        {}    *  57. Boundary tables                                                      I
-        {}    *  58.                                                                      I
-        {}    *  59. Wwtp data                                                            I
-        {}    *  60. Wwtp tabellen                                                        I
-        {}    *  61. Industry general data                                                I
-        {}    *  62. Mappix output file detail berging riool verhard gebied per tijdstap  O
-        {}    *  63. Mappix output file detail debiet verhard gebied        per tijdstap  O
-        {}    *  64. Mappix output file detail debiet onverhard gebied      per tijdstap  O
-        {}    *  65. Mappix output file detail grondwaterstand              per tijdstap  O
-        {}    *  66. Mappix output file detail bergingsgraad kasbassins     per tijdstap  O
-        {}    *  67. Mappix output file detail uitslag kasbassins           per tijdstap  O
-        {}    *  68. Mappix output file detail open water peil              per tijdstap  O
-        {}    *  69  Mappix output file detail overschrijdingsduur ref.peil per tijdstap  O
-        {}    *  70. Mappix output file detail debiet over kunstwerk        per tijdstap  O
-        {}    *  71. Mappix output file detail debiet naar rand             per tijdstap  O
-        {}    *  72. Mappix output file max.berging riool Pluvius           per tijdstap  O
-        {}    *  73. Mappix output file max.debiet Pluvius                  per tijdstap  O
-        {}    *  74. Mappix output file detail balans                       per tijdstap  O
-        {}    *  75. Mappix output file detail balans cumulatief            per tijdstap  O
-        {}    *  76. Mappix output file detail zoutconcentraties            per tijdstap  O
-        {}    *  77. Industry tabellen                                                    I
-        {}    *  78. Maalstop                                                             I
-        {}    *  79. Temperature time series                                              I
-        {}    *  80. Runoff time series              
-        {}    *  81. Totalen/lozingen op randknopen                                       O
-        {}    *  82. Language file                                                        I
-        {}    *  83. OW-volume                                                            O
-        {}    *  84. OW_peilen                                                            O
-        {}    *  85. Balans file                                                          O
-        {}    *  86. 3B-arealen in HIS file                                               O
-        {}    *  87. 3B-structure data in HIS file                                        O
-        {}    *  88. RR Runoff his file              
-        {}    *  89. Sacramento HIS file              
-        {}    *  90. rwzi HIS file                                                        O
-        {}    *  91. Industry HIS file                                                    O
-        {}    *  92. CTRL.INI                                                             I
-        {}    *  93. CAPSIM input file                                                    I
-        {}    *  94. CAPSIM input file                                                    I
-        {}    *  95. CAPSIM message file                                                  O
-        {}    *  96. CAPSIM debug file                                                    O
-        {}    *  97. Restart file na 1 uur                                                O
-        {}    *  98. Restart file na 12 uur                                               O
-        {}    *  99. Ready                                                                O
-        {}    * 100. NWRW detailed areas                                                  O
-        {}    * 101. Link flows                                                           O
-        {}    * 102. Modflow-RR                                                           O
-        {}    * 103. RR-Modflow                                                           O
-        {}    * 104. RR-balance for WLM              
-        {}    * 105. Sacramento ASCII output              
-        {}    * 106. Additional NWRW input file with DWA table                            I
-        {}    * 107. RR balans
-        {}    * 108. Kasklasse, new format                                                I
-        {}    * 109. KasInit, new format                                                  I
-        {}    * 110. KasGebr, new format                                                  I
-        {}    * 111. CropFact, new format                                                 I
-        {}    * 112. CropOW, new format                                                   I
-        {}    * 113. Soildata, new format                                                 I
-        {}    * 114. DioConfig Ini file
-        {}    * 115. Buifile voor continue berekening Reeksen
-        {}    * 116. NWRW output
-        {}    * 117. RR Routing link definitions                                          I
-        {}    * 118. Cel input file
-        {}    * 119. Cel output file
-        {}    * 120. RR Log file for Simulate
-        {}    * 121. coupling WQ salt RTC
-        {}    * 122. RR Boundary conditions file for SOBEK3
-        {}    * 123. Optional RR ASCII restart (test) for OpenDA
-        {}    * 124. Optional LGSI cachefile
-        {}    * 125. Optional meteo NetCdf timeseries inputfile rainfall
-        {}    * 126. Optional meteo NetCdf timeseries inputfile evaporation
-        {}    * 127. Optional meteo NetCdf timeseries inputfile temperature (only for RR-HBV)
-        """.format(*padded_values))
-    # fmt: on
-
-
-def write(
-    path: Path, data: Dict, config: SerializerConfig, save_settings: ModelSaveSettings
-) -> None:
-    """Write the specified model to the specified path.
-
-    If the parent of the path does not exist, it will be created.
-
-    Args:
-        model (RainfallRunoffModel): The model to write to file
-        path (Path): The file path to write to.
-        config (SerializerConfig): The serialization configuration.
-        save_settings (ModelSaveSettings): The model save settings.
-    """
-    path.parent.mkdir(parents=True, exist_ok=True)
-    with path.open("w", encoding="utf8") as f:
-        f.write(serialize(data, save_settings))
+"""serializer.py defines the write method for the RainfallRunoffModel."""
+
+import inspect
+from pathlib import Path
+from typing import Dict, Iterable, Optional, Union
+
+from hydrolib.core.basemodel import ModelSaveSettings, SerializerConfig
+from hydrolib.core.utils import FilePathStyleConverter, get_str_len
+
+
+def _calculate_max_value_length(data: Iterable) -> int:
+    return max(map(get_str_len, data))
+
+
+def _get_string_value(
+    path_value: Optional[Union[dict, Path, str]], save_settings: ModelSaveSettings
+) -> str:
+    """Get printable string value of the path value in a typical
+    RainfallRunoffModel.
+
+    str type as input is intentionally accepted to support file names
+    pointing to non-existent files (e.g. in incomplete model input).
+
+    Args:
+        path_value (Union[dict, Path, str]): path-like value to be printed.
+        save_settings (ModelSaveSettings): The model save settings.
+    Returns:
+        str: The str representation of input path_value.
+    """
+
+    file_path_style_converter = FilePathStyleConverter()
+
+    path = None
+    if isinstance(path_value, dict) and (file_path := path_value.get("filepath", None)):
+        path = file_path
+    elif isinstance(path_value, Path):
+        path = path_value
+    elif isinstance(path_value, str):
+        path = Path(path_value)
+
+    if path is None:
+        value = ""
+    else:
+        value = file_path_style_converter.convert_from_os_style(
+            path, save_settings.path_style
+        )
+
+    return f"'{value}'"
+
+
+def serialize(data: Dict, save_settings: ModelSaveSettings) -> str:
+    """Serialize the specified model.
+
+    Args:
+        data (Dict): dict values of the RainfallRunoffModel to serialize.
+        save_settings (ModelSaveSettings): The model save settings.
+
+    Returns:
+        str: The serialized RainfallRunoffModel in .fnm format.
+    """
+
+    values = [_get_string_value(v, save_settings) for v in data.values()]
+    max_len = _calculate_max_value_length(values)
+    padded_values = [s.ljust(max_len) if s else " " * max_len for s in values]
+
+    # fmt: off
+    return inspect.cleandoc("""
+        *
+        * DELFT_3B Version 1.00
+        * -----------------------------------------------------------------
+        *
+        * Last update : March 1995
+        *
+        * All input- and output file names (free format)
+        *
+        *   Namen Mappix files (*.DIR, *.TST, *.his) mogen NIET gewijzigd worden.
+        *   Overige filenamen mogen wel gewijzigd worden.
+        *
+        *
+        {}    *   1. Control file                                                         I
+        {}    *   2. Knoop data                                                           I
+        {}    *   3. Tak data                                                             I
+        {}    *   4. Open water data                                                      I
+        {}    *   5. Verhard gebied algemeen                                              I
+        {}    *   6. Verhard gebied storage                                               I
+        {}    *   7. Verhard gebied DWA                                                   I
+        {}    *   8. Verhard gebied sewer pump capacity                                   I
+        {}    *   9. Boundaries                                                           I
+        {}    *  10. Pluvius                                                              I
+        {}    *  11. Pluvius algemeen                                                     I
+        {}    *  12. Kasklasse                                                            I
+        {}    *  13. buifile                                                              I
+        {}    *  14. verdampingsfile                                                      I
+        {}    *  15. unpaved algemeen                                                     I
+        {}    *  16. unpaved storage                                                      I
+        {}    *  17. kasgebied initialisatie (SC)                                         I
+        {}    *  18. kasgebied verbruiksdata (SC)                                         I
+        {}    *  19. crop factors gewassen                                                I
+        {}    *  20. tabel bergingscoef=f(ontw.diepte,grondsoort)                         I
+        {}    *  21. Unpaved - alfa factor definities                                     I
+        {}    *  22. Run messages                                                         O
+        {}    *  23. Overzicht van schematisatie, algemene gegevens                       O
+        {}    *  24. Output results verhard                                               O
+        {}    *  25. Output results onverhard                                             O
+        {}    *  26. Output results kas                                                   O
+        {}    *  27. Output results open water                                            O
+        {}    *  28. Output results kunstwerk                                             O
+        {}    *  29. Output results boundaries                                            O
+        {}    *  30. Output results Pluvius                                               O
+        {}    *  31. Unpaved infiltratie definities                                       I
+        {}    *  32. Debugfile                                                            O
+        {}    *  33. Unpaved seepage                                                      I
+        {}    *  34. Unpaved tabels initial gwl and Scurve                                I
+        {}    *  35. Kassen general data                                                  I
+        {}    *  36. Kassen roof storage                                                  I
+        {}    *  37. Pluvius rioolinloop ASCII file                                       O
+        {}    *  38. Invoerfile met variabele peilen op randknopen                        I
+        {}    *  39. Invoerfile met zoutgegevens                                          I
+        {}    *  40. Invoerfile met cropfactors open water                                I
+        {}    *  41. Restart file input                                                   I
+        {}    *  42. Restart file output                                                  O
+        {}    *  43. Binary file input                                                    I
+        {}    *  44. Sacramento input I        
+        {}    *  45. Uitvoer ASCII file met debieten van/naar randknopen                  O
+        {}    *  46. Uitvoer ASCII file met zoutconcentratie op rand                      O
+        {}    *  47. Zout uitvoer in ASCII file                                           O
+        {}    *  48. Greenhouse silo definitions                                          I
+        {}    *  49. Open water general data                                              I
+        {}    *  50. Open water seepage definitions                                       I
+        {}    *  51. Open water tables target levels                                      I
+        {}    *  52. General structure data                                               I
+        {}    *  53. Structure definitions                                                I
+        {}    *  54. Controller definitions                                               I
+        {}    *  55. Tabellen structures                                                  I
+        {}    *  56. Boundary data                                                        I
+        {}    *  57. Boundary tables                                                      I
+        {}    *  58.                                                                      I
+        {}    *  59. Wwtp data                                                            I
+        {}    *  60. Wwtp tabellen                                                        I
+        {}    *  61. Industry general data                                                I
+        {}    *  62. Mappix output file detail berging riool verhard gebied per tijdstap  O
+        {}    *  63. Mappix output file detail debiet verhard gebied        per tijdstap  O
+        {}    *  64. Mappix output file detail debiet onverhard gebied      per tijdstap  O
+        {}    *  65. Mappix output file detail grondwaterstand              per tijdstap  O
+        {}    *  66. Mappix output file detail bergingsgraad kasbassins     per tijdstap  O
+        {}    *  67. Mappix output file detail uitslag kasbassins           per tijdstap  O
+        {}    *  68. Mappix output file detail open water peil              per tijdstap  O
+        {}    *  69  Mappix output file detail overschrijdingsduur ref.peil per tijdstap  O
+        {}    *  70. Mappix output file detail debiet over kunstwerk        per tijdstap  O
+        {}    *  71. Mappix output file detail debiet naar rand             per tijdstap  O
+        {}    *  72. Mappix output file max.berging riool Pluvius           per tijdstap  O
+        {}    *  73. Mappix output file max.debiet Pluvius                  per tijdstap  O
+        {}    *  74. Mappix output file detail balans                       per tijdstap  O
+        {}    *  75. Mappix output file detail balans cumulatief            per tijdstap  O
+        {}    *  76. Mappix output file detail zoutconcentraties            per tijdstap  O
+        {}    *  77. Industry tabellen                                                    I
+        {}    *  78. Maalstop                                                             I
+        {}    *  79. Temperature time series                                              I
+        {}    *  80. Runoff time series              
+        {}    *  81. Totalen/lozingen op randknopen                                       O
+        {}    *  82. Language file                                                        I
+        {}    *  83. OW-volume                                                            O
+        {}    *  84. OW_peilen                                                            O
+        {}    *  85. Balans file                                                          O
+        {}    *  86. 3B-arealen in HIS file                                               O
+        {}    *  87. 3B-structure data in HIS file                                        O
+        {}    *  88. RR Runoff his file              
+        {}    *  89. Sacramento HIS file              
+        {}    *  90. rwzi HIS file                                                        O
+        {}    *  91. Industry HIS file                                                    O
+        {}    *  92. CTRL.INI                                                             I
+        {}    *  93. CAPSIM input file                                                    I
+        {}    *  94. CAPSIM input file                                                    I
+        {}    *  95. CAPSIM message file                                                  O
+        {}    *  96. CAPSIM debug file                                                    O
+        {}    *  97. Restart file na 1 uur                                                O
+        {}    *  98. Restart file na 12 uur                                               O
+        {}    *  99. Ready                                                                O
+        {}    * 100. NWRW detailed areas                                                  O
+        {}    * 101. Link flows                                                           O
+        {}    * 102. Modflow-RR                                                           O
+        {}    * 103. RR-Modflow                                                           O
+        {}    * 104. RR-balance for WLM              
+        {}    * 105. Sacramento ASCII output              
+        {}    * 106. Additional NWRW input file with DWA table                            I
+        {}    * 107. RR balans
+        {}    * 108. Kasklasse, new format                                                I
+        {}    * 109. KasInit, new format                                                  I
+        {}    * 110. KasGebr, new format                                                  I
+        {}    * 111. CropFact, new format                                                 I
+        {}    * 112. CropOW, new format                                                   I
+        {}    * 113. Soildata, new format                                                 I
+        {}    * 114. DioConfig Ini file
+        {}    * 115. Buifile voor continue berekening Reeksen
+        {}    * 116. NWRW output
+        {}    * 117. RR Routing link definitions                                          I
+        {}    * 118. Cel input file
+        {}    * 119. Cel output file
+        {}    * 120. RR Log file for Simulate
+        {}    * 121. coupling WQ salt RTC
+        {}    * 122. RR Boundary conditions file for SOBEK3
+        {}    * 123. Optional RR ASCII restart (test) for OpenDA
+        {}    * 124. Optional LGSI cachefile
+        {}    * 125. Optional meteo NetCdf timeseries inputfile rainfall
+        {}    * 126. Optional meteo NetCdf timeseries inputfile evaporation
+        {}    * 127. Optional meteo NetCdf timeseries inputfile temperature (only for RR-HBV)
+        """.format(*padded_values))
+    # fmt: on
+
+
+def write(
+    path: Path, data: Dict, config: SerializerConfig, save_settings: ModelSaveSettings
+) -> None:
+    """Write the specified model to the specified path.
+
+    If the parent of the path does not exist, it will be created.
+
+    Args:
+        model (RainfallRunoffModel): The model to write to file
+        path (Path): The file path to write to.
+        config (SerializerConfig): The serialization configuration.
+        save_settings (ModelSaveSettings): The model save settings.
+    """
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf8") as f:
+        f.write(serialize(data, save_settings))
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/rr/topology/models.py` & `hydrolib_core-0.5.1/hydrolib/core/rr/topology/models.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,190 +1,190 @@
-from pathlib import Path
-from typing import Callable, Dict, List, Optional
-
-from pydantic.class_validators import root_validator
-from pydantic.fields import Field
-
-from hydrolib.core.basemodel import (
-    BaseModel,
-    ModelSaveSettings,
-    ParsableFileModel,
-    SerializerConfig,
-)
-from hydrolib.core.rr.topology.parser import NetworkTopologyFileParser
-from hydrolib.core.rr.topology.serializer import LinkFileSerializer, NodeFileSerializer
-
-nodetypes_netter_to_rr = {
-    43: 1,  # 1: Paved area
-    44: 2,  # 2: Unpaved area
-    45: 3,  # 3: Greenhouse
-    46: 4,  # 4: Open water
-    -5: 5,  # 5: Internally reserved for all structures
-    34: 6,  # 6: Boundary
-    35: 6,  # 6: Boundary
-    47: 6,  # 6: Boundary
-    48: 8,  # 8: pump
-    49: 9,  # 9: weir
-    50: 10,  # 10: orifice
-    51: 11,  # 11: Manning resistance
-    52: 12,  # 12: Q-h relation
-    56: 14,  # 14: WWTP (RWZI)
-    55: 15,  # 15: Industry
-    54: 16,  # 16: Sacramento (ObId â€™3B_SACRAMENTOâ€™)
-    -21: 21,  # 21: Open water with only precipitation and evaporation (SOBEK3)
-    69: 23,  # 23: Wagmod/Walrus
-}
-""" Dictionary with `nt` mapped against the expected `mt`.
-
-Some model types `mt` do not have a related netter type; in that case the
-dict key is a dummy value of -<mt>."""
-
-
-class Node(BaseModel):
-    """Represents a node from the topology node file."""
-
-    id: str = Field(alias="id")
-    name: Optional[str] = Field(alias="nm")
-    branchid: int = Field(alias="ri")
-    modelnodetype: int = Field(alias="mt")
-    netternodetype: int = Field(alias="nt")
-    objectid: str = Field(alias="ObID")
-    xposition: float = Field(alias="px")
-    yposition: float = Field(alias="py")
-
-    def _get_identifier(self, data: dict) -> Optional[str]:
-        return data.get("id") or data.get("nm")
-
-    def dict(self, *args, **kwargs):
-        kwargs["by_alias"] = True
-        return super().dict(*args, **kwargs)
-
-    @root_validator()
-    @classmethod
-    def _validate_node_type(cls, values):
-
-        cls._raise_if_invalid_type(
-            values,
-            "modelnodetype",
-            set(nodetypes_netter_to_rr.values()),
-            "model node type (mt)",
-        )
-
-        modelnodetype = values.get("modelnodetype")
-
-        # modelnodetype=6 ("boundary node") is a special case that allows various netter nodetypes,
-        # so therefore it always validates well.
-        if modelnodetype == 6:
-            return values
-
-        cls._raise_if_invalid_type(
-            values,
-            "netternodetype",
-            set(nodetypes_netter_to_rr.keys()),
-            "netter node type (nt)",
-        )
-
-        netternodetype = values.get("netternodetype")
-        modelnodetype_expected = nodetypes_netter_to_rr[netternodetype]
-
-        if modelnodetype != modelnodetype_expected:
-            raise ValueError(
-                f"{modelnodetype} is not a supported model node type (mt) when netter node type (nt) is {netternodetype}. Supported value: {modelnodetype_expected}."
-            )
-
-        return values
-
-    @classmethod
-    def _raise_if_invalid_type(
-        cls, values, field_name: str, supported_values: set, description: str
-    ):
-        """Validates the node type for the provided `field_name`.
-        The specified node type should contain a supported value,
-        otherwise a `ValueError` is raised.
-
-
-        Args:
-            values ([type]): Dictionary with values that are used to create this `Node`.
-            field_name (str): Field name of the node type to validate.
-            supported_values (set): Set of all the supported values for this node type.
-            description (str): Description of this node type that will be readable for the user.
-
-        Raises:
-            ValueError: Thrown when `supported_values` does node contain the node type.
-        """
-        field_value = values.get(field_name)
-
-        if field_value not in supported_values:
-            str_supported_values = ", ".join([str(t) for t in supported_values])
-            raise ValueError(
-                f"{field_value} is not a supported {description}. Supported values: {str_supported_values}."
-            )
-
-
-class NodeFile(ParsableFileModel):
-    """Represents the file with the RR node topology data."""
-
-    _parser = NetworkTopologyFileParser(enclosing_tag="node")
-    node: List[Node] = Field([], alias="node")
-
-    @classmethod
-    def _ext(cls) -> str:
-        return ".tp"
-
-    @classmethod
-    def _filename(cls) -> str:
-        return "3b_nod"
-
-    @classmethod
-    def _get_serializer(
-        cls,
-    ) -> Callable[[Path, Dict, SerializerConfig, ModelSaveSettings], None]:
-        return NodeFileSerializer.serialize
-
-    @classmethod
-    def _get_parser(cls) -> Callable:
-        return cls._parser.parse
-
-
-class Link(BaseModel):
-    """Represents a link from the topology link file."""
-
-    id: str = Field(alias="id")
-    name: Optional[str] = Field(alias="nm")
-    branchid: int = Field(alias="ri")
-    modellinktype: int = Field(alias="mt")
-    branchtype: int = Field(alias="bt")
-    objectid: str = Field(alias="ObID")
-    beginnode: str = Field(alias="bn")
-    endnode: str = Field(alias="en")
-
-    def _get_identifier(self, data: dict) -> Optional[str]:
-        return data.get("id") or data.get("nm")
-
-    def dict(self, *args, **kwargs):
-        kwargs["by_alias"] = True
-        return super().dict(*args, **kwargs)
-
-
-class LinkFile(ParsableFileModel):
-    """Represents the file with the RR link topology data."""
-
-    _parser = NetworkTopologyFileParser(enclosing_tag="brch")
-    link: List[Link] = Field([], alias="brch")
-
-    @classmethod
-    def _ext(cls) -> str:
-        return ".tp"
-
-    @classmethod
-    def _filename(cls) -> str:
-        return "3b_link"
-
-    @classmethod
-    def _get_serializer(
-        cls,
-    ) -> Callable[[Path, Dict, SerializerConfig, ModelSaveSettings], None]:
-        return LinkFileSerializer.serialize
-
-    @classmethod
-    def _get_parser(cls) -> Callable:
-        return cls._parser.parse
+from pathlib import Path
+from typing import Callable, Dict, List, Optional
+
+from pydantic.class_validators import root_validator
+from pydantic.fields import Field
+
+from hydrolib.core.basemodel import (
+    BaseModel,
+    ModelSaveSettings,
+    ParsableFileModel,
+    SerializerConfig,
+)
+from hydrolib.core.rr.topology.parser import NetworkTopologyFileParser
+from hydrolib.core.rr.topology.serializer import LinkFileSerializer, NodeFileSerializer
+
+nodetypes_netter_to_rr = {
+    43: 1,  # 1: Paved area
+    44: 2,  # 2: Unpaved area
+    45: 3,  # 3: Greenhouse
+    46: 4,  # 4: Open water
+    -5: 5,  # 5: Internally reserved for all structures
+    34: 6,  # 6: Boundary
+    35: 6,  # 6: Boundary
+    47: 6,  # 6: Boundary
+    48: 8,  # 8: pump
+    49: 9,  # 9: weir
+    50: 10,  # 10: orifice
+    51: 11,  # 11: Manning resistance
+    52: 12,  # 12: Q-h relation
+    56: 14,  # 14: WWTP (RWZI)
+    55: 15,  # 15: Industry
+    54: 16,  # 16: Sacramento (ObId â€™3B_SACRAMENTOâ€™)
+    -21: 21,  # 21: Open water with only precipitation and evaporation (SOBEK3)
+    69: 23,  # 23: Wagmod/Walrus
+}
+""" Dictionary with `nt` mapped against the expected `mt`.
+
+Some model types `mt` do not have a related netter type; in that case the
+dict key is a dummy value of -<mt>."""
+
+
+class Node(BaseModel):
+    """Represents a node from the topology node file."""
+
+    id: str = Field(alias="id")
+    name: Optional[str] = Field(alias="nm")
+    branchid: int = Field(alias="ri")
+    modelnodetype: int = Field(alias="mt")
+    netternodetype: int = Field(alias="nt")
+    objectid: str = Field(alias="ObID")
+    xposition: float = Field(alias="px")
+    yposition: float = Field(alias="py")
+
+    def _get_identifier(self, data: dict) -> Optional[str]:
+        return data.get("id") or data.get("nm")
+
+    def dict(self, *args, **kwargs):
+        kwargs["by_alias"] = True
+        return super().dict(*args, **kwargs)
+
+    @root_validator()
+    @classmethod
+    def _validate_node_type(cls, values):
+
+        cls._raise_if_invalid_type(
+            values,
+            "modelnodetype",
+            set(nodetypes_netter_to_rr.values()),
+            "model node type (mt)",
+        )
+
+        modelnodetype = values.get("modelnodetype")
+
+        # modelnodetype=6 ("boundary node") is a special case that allows various netter nodetypes,
+        # so therefore it always validates well.
+        if modelnodetype == 6:
+            return values
+
+        cls._raise_if_invalid_type(
+            values,
+            "netternodetype",
+            set(nodetypes_netter_to_rr.keys()),
+            "netter node type (nt)",
+        )
+
+        netternodetype = values.get("netternodetype")
+        modelnodetype_expected = nodetypes_netter_to_rr[netternodetype]
+
+        if modelnodetype != modelnodetype_expected:
+            raise ValueError(
+                f"{modelnodetype} is not a supported model node type (mt) when netter node type (nt) is {netternodetype}. Supported value: {modelnodetype_expected}."
+            )
+
+        return values
+
+    @classmethod
+    def _raise_if_invalid_type(
+        cls, values, field_name: str, supported_values: set, description: str
+    ):
+        """Validates the node type for the provided `field_name`.
+        The specified node type should contain a supported value,
+        otherwise a `ValueError` is raised.
+
+
+        Args:
+            values ([type]): Dictionary with values that are used to create this `Node`.
+            field_name (str): Field name of the node type to validate.
+            supported_values (set): Set of all the supported values for this node type.
+            description (str): Description of this node type that will be readable for the user.
+
+        Raises:
+            ValueError: Thrown when `supported_values` does node contain the node type.
+        """
+        field_value = values.get(field_name)
+
+        if field_value not in supported_values:
+            str_supported_values = ", ".join([str(t) for t in supported_values])
+            raise ValueError(
+                f"{field_value} is not a supported {description}. Supported values: {str_supported_values}."
+            )
+
+
+class NodeFile(ParsableFileModel):
+    """Represents the file with the RR node topology data."""
+
+    _parser = NetworkTopologyFileParser(enclosing_tag="node")
+    node: List[Node] = Field([], alias="node")
+
+    @classmethod
+    def _ext(cls) -> str:
+        return ".tp"
+
+    @classmethod
+    def _filename(cls) -> str:
+        return "3b_nod"
+
+    @classmethod
+    def _get_serializer(
+        cls,
+    ) -> Callable[[Path, Dict, SerializerConfig, ModelSaveSettings], None]:
+        return NodeFileSerializer.serialize
+
+    @classmethod
+    def _get_parser(cls) -> Callable:
+        return cls._parser.parse
+
+
+class Link(BaseModel):
+    """Represents a link from the topology link file."""
+
+    id: str = Field(alias="id")
+    name: Optional[str] = Field(alias="nm")
+    branchid: int = Field(alias="ri")
+    modellinktype: int = Field(alias="mt")
+    branchtype: int = Field(alias="bt")
+    objectid: str = Field(alias="ObID")
+    beginnode: str = Field(alias="bn")
+    endnode: str = Field(alias="en")
+
+    def _get_identifier(self, data: dict) -> Optional[str]:
+        return data.get("id") or data.get("nm")
+
+    def dict(self, *args, **kwargs):
+        kwargs["by_alias"] = True
+        return super().dict(*args, **kwargs)
+
+
+class LinkFile(ParsableFileModel):
+    """Represents the file with the RR link topology data."""
+
+    _parser = NetworkTopologyFileParser(enclosing_tag="brch")
+    link: List[Link] = Field([], alias="brch")
+
+    @classmethod
+    def _ext(cls) -> str:
+        return ".tp"
+
+    @classmethod
+    def _filename(cls) -> str:
+        return "3b_link"
+
+    @classmethod
+    def _get_serializer(
+        cls,
+    ) -> Callable[[Path, Dict, SerializerConfig, ModelSaveSettings], None]:
+        return LinkFileSerializer.serialize
+
+    @classmethod
+    def _get_parser(cls) -> Callable:
+        return cls._parser.parse
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/rr/topology/parser.py` & `hydrolib_core-0.5.1/hydrolib/core/rr/topology/parser.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,74 +1,74 @@
-from pathlib import Path
-from typing import Iterable
-from warnings import warn
-
-from hydrolib.core.utils import get_substring_between
-
-
-class NetworkTopologyFileParser:
-    """A parser for RR topology files such as node and link files."""
-
-    def __init__(self, enclosing_tag: str):
-        """Initializes a new instance of the `NetworkTopologyFileParser` class.
-
-        Args:
-            enclosing_tag (str): The enclosing tag for the enclosed topology data per record.
-        """
-
-        self._enclosing_tag = enclosing_tag
-
-    def parse(self, path: Path) -> dict:
-        """Parses a network topology file to a dictionary.
-
-        Args:
-            path (Path): Path to the network topology file.
-        """
-
-        file_content = self._read_file(path)
-        return self._parse_lines(file_content)
-
-    def _read_file(self, path: Path) -> Iterable[str]:
-        if not path.is_file():
-            warn(f"File: `{path}` not found, skipped parsing.")
-            return []
-
-        with open(path, encoding="utf8") as file:
-            lines = file.readlines()
-        return lines
-
-    def _parse_lines(self, lines: Iterable[str]) -> dict:
-        records = []
-
-        key_start = self._enclosing_tag.upper()
-        key_end = self._enclosing_tag.lower()
-
-        for line in lines:
-
-            substring = get_substring_between(line, key_start, key_end)
-            if substring == None:
-                continue
-
-            record = self._parse_line(substring)
-            records.append(record)
-
-        return {key_end: records}
-
-    def _parse_line(self, line: str) -> dict:
-        parts = line.split()
-
-        record = {}
-
-        index = 0
-        while index < len(parts) - 1:
-            key = parts[index]
-            if key == "mt" and parts[index + 1] == "1":
-                # `mt 1` is one keyword, but was parsed as two separate parts.
-                index += 1
-
-            index += 1
-            value = parts[index].strip("'")
-            index += 1
-
-            record[key] = value
-
-        return record
+from pathlib import Path
+from typing import Iterable
+from warnings import warn
+
+from hydrolib.core.utils import get_substring_between
+
+
+class NetworkTopologyFileParser:
+    """A parser for RR topology files such as node and link files."""
+
+    def __init__(self, enclosing_tag: str):
+        """Initializes a new instance of the `NetworkTopologyFileParser` class.
+
+        Args:
+            enclosing_tag (str): The enclosing tag for the enclosed topology data per record.
+        """
+
+        self._enclosing_tag = enclosing_tag
+
+    def parse(self, path: Path) -> dict:
+        """Parses a network topology file to a dictionary.
+
+        Args:
+            path (Path): Path to the network topology file.
+        """
+
+        file_content = self._read_file(path)
+        return self._parse_lines(file_content)
+
+    def _read_file(self, path: Path) -> Iterable[str]:
+        if not path.is_file():
+            warn(f"File: `{path}` not found, skipped parsing.")
+            return []
+
+        with open(path, encoding="utf8") as file:
+            lines = file.readlines()
+        return lines
+
+    def _parse_lines(self, lines: Iterable[str]) -> dict:
+        records = []
+
+        key_start = self._enclosing_tag.upper()
+        key_end = self._enclosing_tag.lower()
+
+        for line in lines:
+
+            substring = get_substring_between(line, key_start, key_end)
+            if substring == None:
+                continue
+
+            record = self._parse_line(substring)
+            records.append(record)
+
+        return {key_end: records}
+
+    def _parse_line(self, line: str) -> dict:
+        parts = line.split()
+
+        record = {}
+
+        index = 0
+        while index < len(parts) - 1:
+            key = parts[index]
+            if key == "mt" and parts[index + 1] == "1":
+                # `mt 1` is one keyword, but was parsed as two separate parts.
+                index += 1
+
+            index += 1
+            value = parts[index].strip("'")
+            index += 1
+
+            record[key] = value
+
+        return record
```

### Comparing `hydrolib_core-0.5.0/hydrolib/core/rr/topology/serializer.py` & `hydrolib_core-0.5.1/hydrolib/core/rr/topology/serializer.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,89 +1,89 @@
-import os
-from pathlib import Path
-from typing import Any, Dict
-
-from hydrolib.core.basemodel import ModelSaveSettings, SerializerConfig
-
-
-class NodeFileSerializer:
-    """Serializer for the RR node topology data."""
-
-    @staticmethod
-    def serialize(
-        path: Path,
-        data: dict,
-        config: SerializerConfig,
-        save_settings: ModelSaveSettings,
-    ):
-        """
-        Serializes the RR node topology data to the file at the specified path.
-
-        Args:
-            path (Path): The path to the destination file.
-            data (Dict): The data to be serialized.
-            config (SerializerConfig): The serialization configuration.
-            save_settings (ModelSaveSettings): The model save settings.
-        """
-
-        path.parent.mkdir(parents=True, exist_ok=True)
-
-        with path.open("w", encoding="utf8") as f:
-            for node in data["node"]:
-                line = f"NODE {NodeFileSerializer._to_line(node, config)} node\n"
-                f.write(line)
-
-    @staticmethod
-    def _to_line(node: Dict[str, Any], config: SerializerConfig) -> str:
-
-        identifier = node["id"]
-        nm = node["nm"]
-        ri = node["ri"]
-        mt = node["mt"]
-        nt = node["nt"]
-        obid = node["ObID"]
-        px = node["px"]
-        py = node["py"]
-
-        float_format = lambda v: f"{v:{config.float_format}}"
-        return f"id '{identifier}' nm '{nm}' ri '{ri}' mt 1 '{mt}' nt {nt} ObID '{obid}' px {float_format(px)} py {float_format(py)}"
-
-
-class LinkFileSerializer:
-    """Serializer for the RR link topology data."""
-
-    @staticmethod
-    def serialize(
-        path: Path,
-        data: dict,
-        config: SerializerConfig,
-        save_settings: ModelSaveSettings,
-    ):
-        """
-        Serializes the RR link topology data to the file at the specified path.
-
-        Args:
-            path (Path): The path to the destination file.
-            data (Dict): The data to be serialized.
-            config (SerializerConfig): The serialization configuration.
-            save_settings (ModelSaveSettings): The model save settings.
-        """
-
-        path.parent.mkdir(parents=True, exist_ok=True)
-
-        with path.open("w", encoding="utf8") as f:
-            for link in data["link"]:
-                line = f"BRCH {LinkFileSerializer._to_line(link)} brch\n"
-                f.write(line)
-
-    @staticmethod
-    def _to_line(link: Dict[str, Any]) -> str:
-        identifier = link["id"]
-        nm = link["nm"]
-        ri = link["ri"]
-        mt = link["mt"]
-        bt = link["bt"]
-        obid = link["ObID"]
-        bn = link["bn"]
-        en = link["en"]
-
-        return f"id '{identifier}' nm '{nm}' ri '{ri}' mt 1 '{mt}' bt {bt} ObID '{obid}' bn '{bn}' en '{en}'"
+import os
+from pathlib import Path
+from typing import Any, Dict
+
+from hydrolib.core.basemodel import ModelSaveSettings, SerializerConfig
+
+
+class NodeFileSerializer:
+    """Serializer for the RR node topology data."""
+
+    @staticmethod
+    def serialize(
+        path: Path,
+        data: dict,
+        config: SerializerConfig,
+        save_settings: ModelSaveSettings,
+    ):
+        """
+        Serializes the RR node topology data to the file at the specified path.
+
+        Args:
+            path (Path): The path to the destination file.
+            data (Dict): The data to be serialized.
+            config (SerializerConfig): The serialization configuration.
+            save_settings (ModelSaveSettings): The model save settings.
+        """
+
+        path.parent.mkdir(parents=True, exist_ok=True)
+
+        with path.open("w", encoding="utf8") as f:
+            for node in data["node"]:
+                line = f"NODE {NodeFileSerializer._to_line(node, config)} node\n"
+                f.write(line)
+
+    @staticmethod
+    def _to_line(node: Dict[str, Any], config: SerializerConfig) -> str:
+
+        identifier = node["id"]
+        nm = node["nm"]
+        ri = node["ri"]
+        mt = node["mt"]
+        nt = node["nt"]
+        obid = node["ObID"]
+        px = node["px"]
+        py = node["py"]
+
+        float_format = lambda v: f"{v:{config.float_format}}"
+        return f"id '{identifier}' nm '{nm}' ri '{ri}' mt 1 '{mt}' nt {nt} ObID '{obid}' px {float_format(px)} py {float_format(py)}"
+
+
+class LinkFileSerializer:
+    """Serializer for the RR link topology data."""
+
+    @staticmethod
+    def serialize(
+        path: Path,
+        data: dict,
+        config: SerializerConfig,
+        save_settings: ModelSaveSettings,
+    ):
+        """
+        Serializes the RR link topology data to the file at the specified path.
+
+        Args:
+            path (Path): The path to the destination file.
+            data (Dict): The data to be serialized.
+            config (SerializerConfig): The serialization configuration.
+            save_settings (ModelSaveSettings): The model save settings.
+        """
+
+        path.parent.mkdir(parents=True, exist_ok=True)
+
+        with path.open("w", encoding="utf8") as f:
+            for link in data["link"]:
+                line = f"BRCH {LinkFileSerializer._to_line(link)} brch\n"
+                f.write(line)
+
+    @staticmethod
+    def _to_line(link: Dict[str, Any]) -> str:
+        identifier = link["id"]
+        nm = link["nm"]
+        ri = link["ri"]
+        mt = link["mt"]
+        bt = link["bt"]
+        obid = link["ObID"]
+        bn = link["bn"]
+        en = link["en"]
+
+        return f"id '{identifier}' nm '{nm}' ri '{ri}' mt 1 '{mt}' bt {bt} ObID '{obid}' bn '{bn}' en '{en}'"
```

### Comparing `hydrolib_core-0.5.0/pyproject.toml` & `hydrolib_core-0.5.1/pyproject.toml`

 * *Files 24% similar despite different names*

```diff
@@ -1,99 +1,99 @@
-[tool.poetry]
-name = "hydrolib-core"
-version = "0.5.0"
-description = "Python wrappers around D-HYDRO Suite."
-authors = ["Deltares"]
-license = "MIT"
-packages = [
-    { include = "hydrolib"},
-]
-readme = "README.md"
-repository = "https://github.com/deltares/hydrolib-core"
-documentation = "https://deltares.github.io/HYDROLIB-core"
-homepage = "https://deltares.github.io/HYDROLIB-core"
-
-[tool.poetry.urls]
-"issue tracker" = "https://github.com/Deltares/HYDROLIB-core/issues"
-
-[tool.poetry.dependencies]
-python = "^3.8"
-netCDF4 = "^1.5"
-numpy = "^1.21"
-pydantic = "~1.10"
-lxml = "^4.6"
-meshkernel = "^2.0.2"
-
-[tool.poetry.dev-dependencies]
-pytest = "^6.2"
-black = "^22.1"
-isort = "^5.8"
-mkdocs = "^1.2"
-mkdocs-material = "^8.0"
-mkdocstrings = "^0.16"
-mkdocs-autorefs = "^0.3, !=0.3.1"
-mkdocs-macros-plugin = "^0.6.3"
-pytest-cov = "^2.11"
-pymdown-extensions = "^9.1"
-commitizen = "^2.17"
-flake8 = "^3.9.2"
-mypy = "^0.910"
-devtools = "^0.6.1"
-matplotlib = "^3.4"
-mkdocs-table-reader-plugin = "^0.6.1"
-openpyxl = "^3.0.9"
-mike = "^1.1.2"
-jinja2 = "<3"
-markupsafe = "<2.1"
-mkdocs-jupyter = "^0.21.0"
-jupyter = "^1.0.0"
-ipykernel = "^6.15.0"
-
-[tool.commitizen]
-name = "cz_conventional_commits"
-version = "0.5.0"
-tag_format = "$version"
-version_files = [
-    "hydrolib/core/__init__.py",
-    "pyproject.toml:version",
-    "tests/data/reference/dimr/test_serialize.xml:createdBy",
-    "tests/data/reference/model/test_dimr_model_save.xml:createdBy",
-    "tests/data/reference/crosssection/crsloc.ini:HYDROLIB-core"
-]
-changelog_file = "docs/changelog.md"
-
-[tool.black]
-line-length = 88
-target-version = ['py38', 'py39']
-exclude = '''
-(
-  /(
-      \.eggs         # exclude a few common directories in the
-    | \.git          # root of the project
-    | \.hg
-    | \.mypy_cache
-    | \.tox
-    | \.venv
-    | _build
-    | buck-out
-    | build
-    | dist
-    | \.virtualenvs
-  )/
-)
-'''
-
-[tool.isort]
-profile = "black"
-multi_line_output = 3
-line_length = 88
-
-[tool.pytest.ini_options]
-addopts = "-m \"not plots\""
-markers = [
-    "plots",
-    "docker"
-]
-[build-system]
-requires = ["poetry-core>=1.0.0"]
-build-backend = "poetry.core.masonry.api"
-
+[tool.poetry]
+name = "hydrolib-core"
+version = "0.5.1"
+description = "Python wrappers around D-HYDRO Suite."
+authors = ["Deltares"]
+license = "MIT"
+packages = [
+    { include = "hydrolib"},
+]
+readme = "README.md"
+repository = "https://github.com/deltares/hydrolib-core"
+documentation = "https://deltares.github.io/HYDROLIB-core"
+homepage = "https://deltares.github.io/HYDROLIB-core"
+
+[tool.poetry.urls]
+"issue tracker" = "https://github.com/Deltares/HYDROLIB-core/issues"
+
+[tool.poetry.dependencies]
+python = "^3.8"
+netCDF4 = "^1.5"
+numpy = "^1.21"
+pydantic = "~1.10"
+lxml = "^4.6"
+meshkernel = "^2.0.2"
+
+[tool.poetry.dev-dependencies]
+pytest = "^6.2"
+black = "^22.1"
+isort = "^5.8"
+mkdocs = "^1.2"
+mkdocs-material = "^8.0"
+mkdocstrings = "^0.16"
+mkdocs-autorefs = "^0.3, !=0.3.1"
+mkdocs-macros-plugin = "^0.6.3"
+pytest-cov = "^2.11"
+pymdown-extensions = "^9.1"
+commitizen = "^2.17"
+flake8 = "^3.9.2"
+mypy = "^0.910"
+devtools = "^0.6.1"
+matplotlib = "^3.4"
+mkdocs-table-reader-plugin = "^0.6.1"
+openpyxl = "^3.0.9"
+mike = "^1.1.2"
+jinja2 = "<3"
+markupsafe = "<2.1"
+mkdocs-jupyter = "^0.21.0"
+jupyter = "^1.0.0"
+ipykernel = "^6.15.0"
+
+[tool.commitizen]
+name = "cz_conventional_commits"
+version = "0.5.1"
+tag_format = "$version"
+version_files = [
+    "hydrolib/core/__init__.py",
+    "pyproject.toml:version",
+    "tests/data/reference/dimr/test_serialize.xml:createdBy",
+    "tests/data/reference/model/test_dimr_model_save.xml:createdBy",
+    "tests/data/reference/crosssection/crsloc.ini:HYDROLIB-core"
+]
+changelog_file = "docs/changelog.md"
+
+[tool.black]
+line-length = 88
+target-version = ['py38', 'py39']
+exclude = '''
+(
+  /(
+      \.eggs         # exclude a few common directories in the
+    | \.git          # root of the project
+    | \.hg
+    | \.mypy_cache
+    | \.tox
+    | \.venv
+    | _build
+    | buck-out
+    | build
+    | dist
+    | \.virtualenvs
+  )/
+)
+'''
+
+[tool.isort]
+profile = "black"
+multi_line_output = 3
+line_length = 88
+
+[tool.pytest.ini_options]
+addopts = "-m \"not plots\""
+markers = [
+    "plots",
+    "docker"
+]
+[build-system]
+requires = ["poetry-core>=1.0.0"]
+build-backend = "poetry.core.masonry.api"
+
```

### Comparing `hydrolib_core-0.5.0/PKG-INFO` & `hydrolib_core-0.5.1/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: hydrolib-core
-Version: 0.5.0
+Version: 0.5.1
 Summary: Python wrappers around D-HYDRO Suite.
 Home-page: https://deltares.github.io/HYDROLIB-core
 License: MIT
 Author: Deltares
 Requires-Python: >=3.8,<4.0
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Programming Language :: Python :: 3
```

