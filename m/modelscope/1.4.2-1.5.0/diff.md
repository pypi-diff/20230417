# Comparing `tmp/modelscope-1.4.2.tar.gz` & `tmp/modelscope-1.5.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/modelscope-1.4.2.tar", last modified: Tue Mar 21 07:14:57 2023, max compression
+gzip compressed data, was "dist/modelscope-1.5.0.tar", last modified: Mon Apr 17 02:47:51 2023, max compression
```

## Comparing `modelscope-1.4.2.tar` & `modelscope-1.5.0.tar`

### file list

```diff
@@ -1,2714 +1,2786 @@
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/
--rw-r--r--   0 runner    (1001) docker     (122)       57 2023-03-21 07:14:56.000000 modelscope-1.4.2/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (122)    18732 2023-03-21 07:14:57.000000 modelscope-1.4.2/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (122)    15442 2023-03-21 07:14:56.000000 modelscope-1.4.2/README.md
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/
--rw-r--r--   0 runner    (1001) docker     (122)      156 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/cli/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/cli/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      404 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/cli/base.py
--rw-r--r--   0 runner    (1001) docker     (122)      829 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/cli/cli.py
--rw-r--r--   0 runner    (1001) docker     (122)     1229 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/cli/download.py
--rw-r--r--   0 runner    (1001) docker     (122)     6239 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/cli/modelcard.py
--rw-r--r--   0 runner    (1001) docker     (122)     4368 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/cli/pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3303 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/cli/plugins.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/configs/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/configs/examples/
--rw-r--r--   0 runner    (1001) docker     (122)       36 2023-03-21 07:14:05.000000 modelscope-1.4.2/modelscope/configs/examples/configuration.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/exporters/
--rw-r--r--   0 runner    (1001) docker     (122)     1255 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/exporters/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2667 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/exporters/base.py
--rw-r--r--   0 runner    (1001) docker     (122)      732 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/exporters/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/exporters/cv/
--rw-r--r--   0 runner    (1001) docker     (122)      869 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/exporters/cv/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2593 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/exporters/cv/cartoon_translation_exporter.py
--rw-r--r--   0 runner    (1001) docker     (122)     3619 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/exporters/cv/face_detection_scrfd_exporter.py
--rw-r--r--   0 runner    (1001) docker     (122)     1245 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/exporters/cv/object_detection_damoyolo_exporter.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/exporters/nlp/
--rw-r--r--   0 runner    (1001) docker     (122)     1082 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/exporters/nlp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7341 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/exporters/nlp/csanmt_for_translation_exporter.py
--rw-r--r--   0 runner    (1001) docker     (122)     4579 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/exporters/nlp/model_for_token_classification_exporter.py
--rw-r--r--   0 runner    (1001) docker     (122)     3409 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/exporters/nlp/sbert_for_sequence_classification_exporter.py
--rw-r--r--   0 runner    (1001) docker     (122)     2316 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/exporters/nlp/sbert_for_zero_shot_classification_exporter.py
--rw-r--r--   0 runner    (1001) docker     (122)     4809 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/exporters/tf_model_exporter.py
--rw-r--r--   0 runner    (1001) docker     (122)    15145 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/exporters/torch_model_exporter.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/fileio/
--rw-r--r--   0 runner    (1001) docker     (122)      122 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/fileio/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10241 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/fileio/file.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/fileio/format/
--rw-r--r--   0 runner    (1001) docker     (122)      143 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/fileio/format/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      454 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/fileio/format/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     1125 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/fileio/format/json.py
--rw-r--r--   0 runner    (1001) docker     (122)      669 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/fileio/format/yaml.py
--rw-r--r--   0 runner    (1001) docker     (122)     4254 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/fileio/io.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/hub/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/hub/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    38418 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/hub/api.py
--rw-r--r--   0 runner    (1001) docker     (122)     3216 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/hub/check_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     1395 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/hub/constants.py
--rw-r--r--   0 runner    (1001) docker     (122)    11358 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/hub/deploy.py
--rw-r--r--   0 runner    (1001) docker     (122)     3728 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/hub/errors.py
--rw-r--r--   0 runner    (1001) docker     (122)    10284 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/hub/file_download.py
--rw-r--r--   0 runner    (1001) docker     (122)     9101 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/hub/git.py
--rw-r--r--   0 runner    (1001) docker     (122)    11794 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/hub/repository.py
--rw-r--r--   0 runner    (1001) docker     (122)     6535 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/hub/snapshot_download.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/hub/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/hub/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9916 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/hub/utils/caching.py
--rw-r--r--   0 runner    (1001) docker     (122)     2978 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/hub/utils/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    53187 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metainfo.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/metrics/
--rw-r--r--   0 runner    (1001) docker     (122)     3829 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2510 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/accuracy_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     7530 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/action_detection_evaluator.py
--rw-r--r--   0 runner    (1001) docker     (122)     1925 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/audio_noise_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     1454 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     1726 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/bleu_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     3490 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/metrics/ciderD/
--rwxr-xr-x   0 runner    (1001) docker     (122)       21 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/ciderD/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     1926 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/ciderD/ciderD.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     8931 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/ciderD/ciderD_scorer.py
--rw-r--r--   0 runner    (1001) docker     (122)     8664 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/image_color_enhance_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     1875 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/image_colorization_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)    10011 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/image_denoise_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     7612 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/image_inpainting_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)    13318 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/image_instance_segmentation_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     1739 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/image_portrait_enhancement_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     2536 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/image_quality_assessment_degradation_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     1632 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/image_quality_assessment_mos_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     2231 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/inbatch_recall_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     1358 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/loss_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     3002 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/map_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     2032 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/movie_scene_segmentation_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     3197 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/ned_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     2310 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/ocr_recognition_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     2082 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/ppl_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     1163 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/prediction_saving_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (122)     4453 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/referring_video_object_segmentation_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     3112 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/sequence_classification_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     3233 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/text_generation_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     3197 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/text_ranking_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     5765 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/token_classification_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     5435 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/video_frame_interpolation_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     8655 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/video_stabilization_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     3092 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/video_summarization_metric.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/metrics/video_super_resolution_metric/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/video_super_resolution_metric/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7030 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/video_super_resolution_metric/matlab_functions.py
--rw-r--r--   0 runner    (1001) docker     (122)     4771 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/video_super_resolution_metric/metric_util.py
--rw-r--r--   0 runner    (1001) docker     (122)     8932 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/video_super_resolution_metric/niqe.py
--rw-r--r--   0 runner    (1001) docker     (122)     1710 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/metrics/video_super_resolution_metric/video_super_resolution_metric.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/
--rw-r--r--   0 runner    (1001) docker     (122)      519 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/audio/
--rw-r--r--   0 runner    (1001) docker     (122)       93 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/audio/aec/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/aec/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/audio/aec/layers/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/aec/layers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1434 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/aec/layers/activations.py
--rw-r--r--   0 runner    (1001) docker     (122)     2700 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/aec/layers/affine_transform.py
--rw-r--r--   0 runner    (1001) docker     (122)     5630 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/aec/layers/deep_fsmn.py
--rw-r--r--   0 runner    (1001) docker     (122)     1322 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/aec/layers/layer_base.py
--rw-r--r--   0 runner    (1001) docker     (122)    14384 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/aec/layers/uni_deep_fsmn.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/audio/aec/network/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/aec/network/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14438 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/aec/network/loss.py
--rw-r--r--   0 runner    (1001) docker     (122)     9382 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/aec/network/modulation_loss.py
--rw-r--r--   0 runner    (1001) docker     (122)    15396 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/aec/network/se_net.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/audio/ans/
--rw-r--r--   0 runner    (1001) docker     (122)      515 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/ans/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6475 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/ans/complex_nn.py
--rw-r--r--   0 runner    (1001) docker     (122)     3694 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/ans/conv_stft.py
--rw-r--r--   0 runner    (1001) docker     (122)     2513 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/ans/denoise_net.py
--rw-r--r--   0 runner    (1001) docker     (122)    10241 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/ans/frcrn.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/audio/ans/layers/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/ans/layers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1468 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/ans/layers/activations.py
--rw-r--r--   0 runner    (1001) docker     (122)     2414 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/ans/layers/affine_transform.py
--rw-r--r--   0 runner    (1001) docker     (122)      661 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/ans/layers/layer_base.py
--rw-r--r--   0 runner    (1001) docker     (122)     5284 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/ans/layers/uni_deep_fsmn.py
--rw-r--r--   0 runner    (1001) docker     (122)     1038 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/ans/se_module_complex.py
--rw-r--r--   0 runner    (1001) docker     (122)     9833 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/ans/unet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/audio/asr/
--rw-r--r--   0 runner    (1001) docker     (122)      585 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/asr/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3029 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/asr/generic_automatic_speech_recognition.py
--rw-r--r--   0 runner    (1001) docker     (122)     1204 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/asr/wenet_automatic_speech_recognition.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/audio/itn/
--rw-r--r--   0 runner    (1001) docker     (122)      557 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/itn/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1462 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/itn/generic_inverse_text_processing.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/audio/kws/
--rw-r--r--   0 runner    (1001) docker     (122)      735 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/kws/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/audio/kws/farfield/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/kws/farfield/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14349 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/kws/farfield/fsmn.py
--rw-r--r--   0 runner    (1001) docker     (122)     7462 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/kws/farfield/fsmn_sele_v2.py
--rw-r--r--   0 runner    (1001) docker     (122)     2806 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/kws/farfield/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     2608 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/kws/farfield/model_def.py
--rw-r--r--   0 runner    (1001) docker     (122)      908 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/kws/generic_key_word_spotting.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/audio/kws/nearfield/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/kws/nearfield/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3300 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/kws/nearfield/cmvn.py
--rw-r--r--   0 runner    (1001) docker     (122)    17496 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/kws/nearfield/fsmn.py
--rw-r--r--   0 runner    (1001) docker     (122)     6079 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/kws/nearfield/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/audio/punc/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/punc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1466 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/punc/generic_punctuation.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/audio/separation/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/separation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2240 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/separation/layer_norm.py
--rw-r--r--   0 runner    (1001) docker     (122)    13963 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/separation/mossformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     9142 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/separation/mossformer_block.py
--rw-r--r--   0 runner    (1001) docker     (122)     9004 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/separation/mossformer_conv_module.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/audio/sv/
--rw-r--r--   0 runner    (1001) docker     (122)      498 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/sv/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14531 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/sv/ecapa_tdnn.py
--rw-r--r--   0 runner    (1001) docker     (122)     1488 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/sv/generic_speaker_verification.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/audio/tts/
--rw-r--r--   0 runner    (1001) docker     (122)      474 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/tts/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11845 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/tts/sambert_hifi.py
--rw-r--r--   0 runner    (1001) docker     (122)    26394 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/audio/tts/voice.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/base/
--rw-r--r--   0 runner    (1001) docker     (122)      303 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/base/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1091 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/base/base_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     6994 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/base/base_model.py
--rw-r--r--   0 runner    (1001) docker     (122)      605 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/base/base_torch_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     5679 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/base/base_torch_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     3647 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/
--rw-r--r--   0 runner    (1001) docker     (122)     1838 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/abnormal_object_detection/
--rw-r--r--   0 runner    (1001) docker     (122)      490 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/abnormal_object_detection/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3806 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/abnormal_object_detection/mmdet_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/abnormal_object_detection/mmdet_ms/
--rw-r--r--   0 runner    (1001) docker     (122)      113 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/abnormal_object_detection/mmdet_ms/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/
--rw-r--r--   0 runner    (1001) docker     (122)      211 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6414 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/
--rw-r--r--   0 runner    (1001) docker     (122)      145 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6729 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/action_detection/
--rw-r--r--   0 runner    (1001) docker     (122)      493 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/action_detection/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7307 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/action_detection/action_detection_onnx.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/action_detection/modules/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/action_detection/modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9364 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/action_detection/modules/action_detection_pytorch.py
--rw-r--r--   0 runner    (1001) docker     (122)    12132 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/action_detection/modules/resnet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/action_recognition/
--rw-r--r--   0 runner    (1001) docker     (122)      709 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/action_recognition/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4304 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/action_recognition/models.py
--rw-r--r--   0 runner    (1001) docker     (122)    10276 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/action_recognition/s3dg.py
--rw-r--r--   0 runner    (1001) docker     (122)    18618 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/action_recognition/tada_convnext.py
--rw-r--r--   0 runner    (1001) docker     (122)    45441 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/action_recognition/temporal_patch_shift_transformer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/animal_recognition/
--rw-r--r--   0 runner    (1001) docker     (122)      558 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/animal_recognition/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14141 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/animal_recognition/resnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     3955 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/animal_recognition/splat.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/bad_image_detecting/
--rw-r--r--   0 runner    (1001) docker     (122)      496 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/bad_image_detecting/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2589 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/bad_image_detecting/bad_image_detecting.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/body_2d_keypoints/
--rw-r--r--   0 runner    (1001) docker     (122)      502 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/body_2d_keypoints/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12660 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/body_2d_keypoints/hrnet_basic_modules.py
--rw-r--r--   0 runner    (1001) docker     (122)     8776 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/body_2d_keypoints/hrnet_v2.py
--rw-r--r--   0 runner    (1001) docker     (122)     1707 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/body_2d_keypoints/w48.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/
--rw-r--r--   0 runner    (1001) docker     (122)      601 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/cannonical_pose/
--rw-r--r--   0 runner    (1001) docker     (122)       51 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/cannonical_pose/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8957 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/cannonical_pose/body_3d_pose.py
--rw-r--r--   0 runner    (1001) docker     (122)     8196 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/cannonical_pose/canonical_pose_modules.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/hdformer/
--rw-r--r--   0 runner    (1001) docker     (122)       98 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/hdformer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11552 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/hdformer/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)    12870 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/hdformer/block.py
--rw-r--r--   0 runner    (1001) docker     (122)     8086 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/hdformer/directed_graph.py
--rw-r--r--   0 runner    (1001) docker     (122)     2516 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     7719 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer_detector.py
--rw-r--r--   0 runner    (1001) docker     (122)     3401 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/hdformer/skeleton.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/cartoon/
--rw-r--r--   0 runner    (1001) docker     (122)     1216 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/cartoon/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/cartoon/facelib/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/cartoon/facelib/LK/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/cartoon/facelib/LK/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3488 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/cartoon/facelib/LK/lk.py
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/cartoon/facelib/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      713 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/cartoon/facelib/config.py
--rw-r--r--   0 runner    (1001) docker     (122)     3512 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/cartoon/facelib/face_detector.py
--rw-r--r--   0 runner    (1001) docker     (122)     5161 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/cartoon/facelib/face_landmark.py
--rw-r--r--   0 runner    (1001) docker     (122)     4593 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/cartoon/facelib/facer.py
--rw-r--r--   0 runner    (1001) docker     (122)     9075 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/cartoon/loss.py
--rw-r--r--   0 runner    (1001) docker     (122)     2174 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/cartoon/model_tf.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/cartoon/mtcnn_pytorch/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/cartoon/mtcnn_pytorch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/cartoon/mtcnn_pytorch/src/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/cartoon/mtcnn_pytorch/src/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6072 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/cartoon/mtcnn_pytorch/src/align_trans.py
--rw-r--r--   0 runner    (1001) docker     (122)     8519 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/cartoon/mtcnn_pytorch/src/matlab_cp2tform.py
--rw-r--r--   0 runner    (1001) docker     (122)     4404 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/cartoon/network.py
--rw-r--r--   0 runner    (1001) docker     (122)     5265 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/cartoon/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/cmdssl_video_embedding/
--rw-r--r--   0 runner    (1001) docker     (122)      647 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/cmdssl_video_embedding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3876 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/cmdssl_video_embedding/c3d.py
--rw-r--r--   0 runner    (1001) docker     (122)    10785 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/cmdssl_video_embedding/resnet2p1d.py
--rw-r--r--   0 runner    (1001) docker     (122)     8949 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/cmdssl_video_embedding/resnet3d.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/
--rw-r--r--   0 runner    (1001) docker     (122)      414 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    13516 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/annotator.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/midas/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/midas/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4960 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/midas/api.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      504 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/base_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    10152 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/blocks.py
--rw-r--r--   0 runner    (1001) docker     (122)     3307 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/dpt_depth.py
--rw-r--r--   0 runner    (1001) docker     (122)     2746 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net.py
--rw-r--r--   0 runner    (1001) docker     (122)     5829 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net_custom.py
--rw-r--r--   0 runner    (1001) docker     (122)     8045 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/transforms.py
--rw-r--r--   0 runner    (1001) docker     (122)    15438 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/vit.py
--rw-r--r--   0 runner    (1001) docker     (122)     4799 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/midas/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/mlsd/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/mlsd/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9896 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/mlsd/mbv2_mlsd_large.py
--rw-r--r--   0 runner    (1001) docker     (122)    25158 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/mlsd/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/openpose/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/openpose/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12595 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/openpose/body.py
--rw-r--r--   0 runner    (1001) docker     (122)     3894 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/openpose/hand.py
--rw-r--r--   0 runner    (1001) docker     (122)     9024 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/openpose/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     8031 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/openpose/util.py
--rw-r--r--   0 runner    (1001) docker     (122)     8546 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/controlnet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/crowd_counting/
--rw-r--r--   0 runner    (1001) docker     (122)      491 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/crowd_counting/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1100 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/crowd_counting/cc_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    22878 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/crowd_counting/hrnet_aspp_relu.py
--rw-r--r--   0 runner    (1001) docker     (122)      872 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/easycv_base.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_2d_keypoints/
--rw-r--r--   0 runner    (1001) docker     (122)      500 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_2d_keypoints/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      631 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_2d_keypoints/face_2d_keypoints_align.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_attribute_recognition/
--rw-r--r--   0 runner    (1001) docker     (122)      490 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_attribute_recognition/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_attribute_recognition/fair_face/
--rw-r--r--   0 runner    (1001) docker     (122)      115 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_attribute_recognition/fair_face/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2559 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/
--rw-r--r--   0 runner    (1001) docker     (122)      930 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/mogface/
--rw-r--r--   0 runner    (1001) docker     (122)       96 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/mogface/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/mogface/models/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/mogface/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3032 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/mogface/models/detectors.py
--rw-r--r--   0 runner    (1001) docker     (122)     4675 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/mogface/models/mogface.py
--rw-r--r--   0 runner    (1001) docker     (122)     5578 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/mogface/models/mogprednet.py
--rw-r--r--   0 runner    (1001) docker     (122)     6264 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/mogface/models/resnet.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     7072 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/mogface/models/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/mtcnn/
--rw-r--r--   0 runner    (1001) docker     (122)       97 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/mtcnn/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/mtcnn/models/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/mtcnn/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7061 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/mtcnn/models/box_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     5415 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/mtcnn/models/detector.py
--rw-r--r--   0 runner    (1001) docker     (122)     3171 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/mtcnn/models/first_stage.py
--rw-r--r--   0 runner    (1001) docker     (122)     5232 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/mtcnn/models/get_nets.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/peppa_pig_face/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/peppa_pig_face/LK/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/peppa_pig_face/LK/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3444 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/peppa_pig_face/LK/lk.py
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/peppa_pig_face/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3578 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/peppa_pig_face/face_detector.py
--rw-r--r--   0 runner    (1001) docker     (122)     5132 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/peppa_pig_face/face_landmark.py
--rw-r--r--   0 runner    (1001) docker     (122)     4210 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/peppa_pig_face/facer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/retinaface/
--rw-r--r--   0 runner    (1001) docker     (122)       93 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/retinaface/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4832 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/retinaface/detection.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/retinaface/models/
--rwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/retinaface/models/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4766 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/retinaface/models/net.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4590 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/retinaface/models/retinaface.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4120 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/retinaface/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/
--rw-r--r--   0 runner    (1001) docker     (122)      174 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/
--rwxr-xr-x   0 runner    (1001) docker     (122)      192 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/
--rw-r--r--   0 runner    (1001) docker     (122)      325 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     2945 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/transforms.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/
--rwxr-xr-x   0 runner    (1001) docker     (122)      292 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3401 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/bbox_nms.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/
--rw-r--r--   0 runner    (1001) docker     (122)      276 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/
--rwxr-xr-x   0 runner    (1001) docker     (122)      471 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11707 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py
--rw-r--r--   0 runner    (1001) docker     (122)     4169 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py
--rw-r--r--   0 runner    (1001) docker     (122)     7981 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    30670 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     5543 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/retinaface.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/
--rwxr-xr-x   0 runner    (1001) docker     (122)      289 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/
--rwxr-xr-x   0 runner    (1001) docker     (122)      314 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3590 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py
--rw-r--r--   0 runner    (1001) docker     (122)    16146 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/
--rwxr-xr-x   0 runner    (1001) docker     (122)      270 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    46999 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/
--rwxr-xr-x   0 runner    (1001) docker     (122)      295 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11062 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/base.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     6092 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py
--rw-r--r--   0 runner    (1001) docker     (122)     6395 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/single_stage.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     5970 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/tinymog.py
--rw-r--r--   0 runner    (1001) docker     (122)     3451 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     3869 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/scrfd_detect.py
--rw-r--r--   0 runner    (1001) docker     (122)      960 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/tinymog_detect.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/
--rw-r--r--   0 runner    (1001) docker     (122)       90 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     1477 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/detection.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/vision/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/vision/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4126 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/vision/box_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2058 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/vision/mb_tiny.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      571 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/data_preprocessing.py
--rw-r--r--   0 runner    (1001) docker     (122)     1641 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/fd_config.py
--rw-r--r--   0 runner    (1001) docker     (122)     3719 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/mb_tiny_fd.py
--rw-r--r--   0 runner    (1001) docker     (122)     2845 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/predictor.py
--rw-r--r--   0 runner    (1001) docker     (122)     4621 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/ssd.py
--rw-r--r--   0 runner    (1001) docker     (122)     1497 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/vision/transforms.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_emotion/
--rw-r--r--   0 runner    (1001) docker     (122)      540 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_emotion/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_emotion/efficient/
--rw-r--r--   0 runner    (1001) docker     (122)      327 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_emotion/efficient/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    15911 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_emotion/efficient/model.py
--rw-r--r--   0 runner    (1001) docker     (122)    20077 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_emotion/efficient/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2007 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_emotion/emotion_infer.py
--rw-r--r--   0 runner    (1001) docker     (122)     3171 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_emotion/emotion_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_emotion/face_alignment/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_emotion/face_alignment/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2844 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_emotion/face_alignment/face.py
--rw-r--r--   0 runner    (1001) docker     (122)     1674 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_emotion/face_alignment/face_align.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_generation/
--rw-r--r--   0 runner    (1001) docker     (122)      475 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_generation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_generation/op/
--rwxr-xr-x   0 runner    (1001) docker     (122)       89 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_generation/op/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     6497 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_generation/op/conv2d_gradfix.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     3104 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_generation/op/fused_act.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     5922 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_generation/op/upfirdn2d.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    19998 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_generation/stylegan2.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_human_hand_detection/
--rw-r--r--   0 runner    (1001) docker     (122)      544 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_human_hand_detection/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4109 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_human_hand_detection/det_infer.py
--rw-r--r--   0 runner    (1001) docker     (122)    12584 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_human_hand_detection/ghost_pan.py
--rw-r--r--   0 runner    (1001) docker     (122)    16846 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_human_hand_detection/nanodet_plus_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     1833 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_human_hand_detection/one_stage_detector.py
--rw-r--r--   0 runner    (1001) docker     (122)     5791 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_human_hand_detection/shufflenetv2.py
--rw-r--r--   0 runner    (1001) docker     (122)     8874 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_human_hand_detection/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_recognition/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_recognition/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1572 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_recognition/align_face.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_recognition/torchkit/
--rwxr-xr-x   0 runner    (1001) docker     (122)      473 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_recognition/torchkit/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_recognition/torchkit/backbone/
--rwxr-xr-x   0 runner    (1001) docker     (122)     1080 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_recognition/torchkit/backbone/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6507 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_recognition/torchkit/backbone/arcface_backbone.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     1977 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_recognition/torchkit/backbone/common.py
--rw-r--r--   0 runner    (1001) docker     (122)     7572 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_recognition/torchkit/backbone/facemask_backbone.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     8551 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_recognition/torchkit/backbone/model_irse.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4744 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_recognition/torchkit/backbone/model_resnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     7186 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_recognition/torchkit/rts_backbone.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_reconstruction/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_reconstruction/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    23042 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/bfm.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/facelandmark/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/facelandmark/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2583 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/facelandmark/large_base_lmks_infer.py
--rw-r--r--   0 runner    (1001) docker     (122)    14311 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/facelandmark/large_model_infer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6561 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_base_lmks_net.py
--rw-r--r--   0 runner    (1001) docker     (122)     5124 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_eyeball_net.py
--rw-r--r--   0 runner    (1001) docker     (122)    23779 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/facerecon_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    14676 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/losses.py
--rw-r--r--   0 runner    (1001) docker     (122)    21235 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/networks.py
--rw-r--r--   0 runner    (1001) docker     (122)    15159 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/nv_diffrast.py
--rw-r--r--   0 runner    (1001) docker     (122)      266 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/opt.py
--rw-r--r--   0 runner    (1001) docker     (122)    23186 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/face_reconstruction/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/facial_expression_recognition/
--rw-r--r--   0 runner    (1001) docker     (122)      484 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/facial_expression_recognition/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/facial_expression_recognition/fer/
--rw-r--r--   0 runner    (1001) docker     (122)      121 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/facial_expression_recognition/fer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2414 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py
--rw-r--r--   0 runner    (1001) docker     (122)     3277 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/facial_expression_recognition/fer/transforms.py
--rw-r--r--   0 runner    (1001) docker     (122)     1311 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/facial_expression_recognition/fer/vgg.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/facial_landmark_confidence/
--rw-r--r--   0 runner    (1001) docker     (122)      478 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/facial_landmark_confidence/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/facial_landmark_confidence/flc/
--rw-r--r--   0 runner    (1001) docker     (122)      115 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/facial_landmark_confidence/flc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3197 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py
--rw-r--r--   0 runner    (1001) docker     (122)     4932 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/facial_landmark_confidence/flc/manual_landmark_net.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/hand_2d_keypoints/
--rw-r--r--   0 runner    (1001) docker     (122)      488 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/hand_2d_keypoints/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      602 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/hand_2d_keypoints/hand_2d_keypoints.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/hand_static/
--rw-r--r--   0 runner    (1001) docker     (122)      502 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/hand_static/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2480 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/hand_static/hand_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    10891 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/hand_static/networks.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/human_reconstruction/
--rw-r--r--   0 runner    (1001) docker     (122)     5936 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/human_reconstruction/Reconstruction.py
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/human_reconstruction/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/human_reconstruction/models/
--rw-r--r--   0 runner    (1001) docker     (122)     1065 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/human_reconstruction/models/Embedding.py
--rw-r--r--   0 runner    (1001) docker     (122)     4542 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/human_reconstruction/models/PixToMesh.py
--rw-r--r--   0 runner    (1001) docker     (122)    11301 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/human_reconstruction/models/Res_backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     2418 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/human_reconstruction/models/Surface_head.py
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/human_reconstruction/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2716 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/human_reconstruction/models/detectors.py
--rw-r--r--   0 runner    (1001) docker     (122)     1982 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/human_reconstruction/models/geometry.py
--rw-r--r--   0 runner    (1001) docker     (122)     2251 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/human_reconstruction/models/human_segmenter.py
--rw-r--r--   0 runner    (1001) docker     (122)    11865 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/human_reconstruction/models/networks.py
--rw-r--r--   0 runner    (1001) docker     (122)     6111 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/human_reconstruction/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/human_wholebody_keypoint/
--rw-r--r--   0 runner    (1001) docker     (122)      530 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/human_wholebody_keypoint/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      636 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/human_wholebody_keypoint/human_wholebody_keypoint.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_binary_quant_classification/
--rw-r--r--   0 runner    (1001) docker     (122)      573 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_binary_quant_classification/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2859 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_binary_quant_classification/binary_quant_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    19111 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_binary_quant_classification/bnext.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_body_reshaping/
--rw-r--r--   0 runner    (1001) docker     (122)      500 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_body_reshaping/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3920 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_body_reshaping/image_body_reshaping.py
--rw-r--r--   0 runner    (1001) docker     (122)     6541 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_body_reshaping/model.py
--rw-r--r--   0 runner    (1001) docker     (122)    13616 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_body_reshaping/person_info.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_body_reshaping/pose_estimator/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_body_reshaping/pose_estimator/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12027 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_body_reshaping/pose_estimator/body.py
--rw-r--r--   0 runner    (1001) docker     (122)     5673 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_body_reshaping/pose_estimator/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     1311 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_body_reshaping/pose_estimator/util.py
--rw-r--r--   0 runner    (1001) docker     (122)    15760 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_body_reshaping/slim_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_classification/
--rw-r--r--   0 runner    (1001) docker     (122)      598 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_classification/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_classification/backbones/
--rw-r--r--   0 runner    (1001) docker     (122)      107 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_classification/backbones/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    20295 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_classification/backbones/beit_v2.py
--rw-r--r--   0 runner    (1001) docker     (122)    17261 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_classification/backbones/nextvit.py
--rw-r--r--   0 runner    (1001) docker     (122)     2185 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_classification/mmcls_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     1554 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_classification/resnet50_cc.py
--rw-r--r--   0 runner    (1001) docker     (122)     5255 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_classification/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_color_enhance/
--rw-r--r--   0 runner    (1001) docker     (122)      704 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_color_enhance/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_color_enhance/adaint/
--rw-r--r--   0 runner    (1001) docker     (122)       44 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_color_enhance/adaint/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14338 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_color_enhance/adaint/adaint.py
--rw-r--r--   0 runner    (1001) docker     (122)     3753 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_color_enhance/csrnet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_color_enhance/deeplpf/
--rw-r--r--   0 runner    (1001) docker     (122)       66 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_color_enhance/deeplpf/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2575 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_color_enhance/deeplpf/deeplpf_image_color_enhance.py
--rw-r--r--   0 runner    (1001) docker     (122)    31764 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_color_enhance/deeplpf/deeplpfnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     2821 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_color_enhance/image_color_enhance.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_colorization/
--rw-r--r--   0 runner    (1001) docker     (122)      640 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_colorization/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_colorization/ddcolor/
--rw-r--r--   0 runner    (1001) docker     (122)      122 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_colorization/ddcolor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9385 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_colorization/ddcolor/ddcolor.py
--rw-r--r--   0 runner    (1001) docker     (122)     6437 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_colorization/ddcolor/ddcolor_for_image_colorization.py
--rw-r--r--   0 runner    (1001) docker     (122)     9576 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_colorization/ddcolor/loss.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_colorization/ddcolor/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_colorization/ddcolor/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6857 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_colorization/ddcolor/utils/convnext.py
--rw-r--r--   0 runner    (1001) docker     (122)     2196 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_colorization/ddcolor/utils/position_encoding.py
--rw-r--r--   0 runner    (1001) docker     (122)     7796 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_colorization/ddcolor/utils/transformer_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     6465 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_colorization/ddcolor/utils/unet.py
--rw-r--r--   0 runner    (1001) docker     (122)     6510 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_colorization/ddcolor/utils/vgg.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_colorization/unet/
--rw-r--r--   0 runner    (1001) docker     (122)      129 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_colorization/unet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10574 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_colorization/unet/unet.py
--rw-r--r--   0 runner    (1001) docker     (122)    10647 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_colorization/unet/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_debanding/
--rw-r--r--   0 runner    (1001) docker     (122)      483 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_debanding/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_debanding/rrdb/
--rw-r--r--   0 runner    (1001) docker     (122)       53 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_debanding/rrdb/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3005 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_debanding/rrdb/rrdb_image_debanding.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_deblur/
--rw-r--r--   0 runner    (1001) docker     (122)      510 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_deblur/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4241 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_deblur/nafnet_for_image_deblur.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/
--rw-r--r--   0 runner    (1001) docker     (122)      492 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4329 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/defrcn_for_fewshot.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/evaluation/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/evaluation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12474 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/evaluation/coco_evaluation.py
--rw-r--r--   0 runner    (1001) docker     (122)     3141 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/evaluation/evaluator.py
--rw-r--r--   0 runner    (1001) docker     (122)     4354 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/evaluation/pascal_voc_evaluation.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/models/
--rw-r--r--   0 runner    (1001) docker     (122)      448 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6827 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/models/calibration_layer.py
--rw-r--r--   0 runner    (1001) docker     (122)     7064 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/models/defrcn.py
--rw-r--r--   0 runner    (1001) docker     (122)    10779 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/models/fast_rcnn.py
--rw-r--r--   0 runner    (1001) docker     (122)     1220 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/models/gdl.py
--rw-r--r--   0 runner    (1001) docker     (122)     1732 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/models/resnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     4770 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/models/roi_heads.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    23599 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/utils/coco_register.py
--rw-r--r--   0 runner    (1001) docker     (122)     4803 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/utils/configuration_mapper.py
--rw-r--r--   0 runner    (1001) docker     (122)     3595 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/utils/model_surgery_op.py
--rw-r--r--   0 runner    (1001) docker     (122)      583 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/utils/register_data.py
--rw-r--r--   0 runner    (1001) docker     (122)     2617 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/utils/requirements_check.py
--rw-r--r--   0 runner    (1001) docker     (122)    11011 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/utils/voc_register.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_denoise/
--rw-r--r--   0 runner    (1001) docker     (122)      514 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_denoise/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_denoise/nafnet/
--rw-r--r--   0 runner    (1001) docker     (122)     6664 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_denoise/nafnet/NAFNet_arch.py
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_denoise/nafnet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1627 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_denoise/nafnet/arch_util.py
--rw-r--r--   0 runner    (1001) docker     (122)     2475 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_denoise/nafnet_for_image_denoise.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_depth_estimation/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_depth_estimation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_depth_estimation/networks/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_depth_estimation/networks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6757 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_depth_estimation/networks/newcrf_depth.py
--rw-r--r--   0 runner    (1001) docker     (122)    18233 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_depth_estimation/networks/newcrf_layers.py
--rw-r--r--   0 runner    (1001) docker     (122)    10106 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_depth_estimation/networks/newcrf_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    26006 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_depth_estimation/networks/swin_transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)    13560 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_depth_estimation/networks/uper_crf_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     1600 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_depth_estimation/newcrfs_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_depth_estimation_bts/
--rw-r--r--   0 runner    (1001) docker     (122)      536 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_depth_estimation_bts/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2606 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_depth_estimation_bts/depth_estimation_bts_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_depth_estimation_bts/networks/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_depth_estimation_bts/networks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1485 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_depth_estimation_bts/networks/bts_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     2819 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_depth_estimation_bts/networks/decoder.py
--rw-r--r--   0 runner    (1001) docker     (122)     2506 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_depth_estimation_bts/networks/encoder.py
--rw-r--r--   0 runner    (1001) docker     (122)     8545 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_depth_estimation_bts/networks/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_driving_perception/
--rw-r--r--   0 runner    (1001) docker     (122)      999 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_driving_perception/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2022 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_driving_perception/image_driving_percetion_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     4352 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_driving_perception/preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     7475 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_driving_perception/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/
--rw-r--r--   0 runner    (1001) docker     (122)      488 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/facegan/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/facegan/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3124 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/facegan/gan_wrap.py
--rw-r--r--   0 runner    (1001) docker     (122)    21432 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/facegan/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/facegan/op/
--rw-r--r--   0 runner    (1001) docker     (122)      242 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/facegan/op/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6497 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/facegan/op/conv2d_gradfix.py
--rw-r--r--   0 runner    (1001) docker     (122)     3094 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/facegan/op/fused_act.py
--rw-r--r--   0 runner    (1001) docker     (122)     5912 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/facegan/op/upfirdn2d.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/facelib/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/facelib/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10514 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/facelib/align_trans.py
--rw-r--r--   0 runner    (1001) docker     (122)     5955 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/facelib/matlab_cp2tform.py
--rw-r--r--   0 runner    (1001) docker     (122)     9077 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/image_face_fusion.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/network/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/network/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3045 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/network/aad_layer.py
--rw-r--r--   0 runner    (1001) docker     (122)     8555 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/network/aei_flow_net.py
--rw-r--r--   0 runner    (1001) docker     (122)     9366 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/network/bfm.py
--rw-r--r--   0 runner    (1001) docker     (122)    12449 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/network/dense_motion.py
--rw-r--r--   0 runner    (1001) docker     (122)    20604 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/network/facerecon_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     7433 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/network/model_irse.py
--rw-r--r--   0 runner    (1001) docker     (122)     7790 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_face_fusion/network/ops.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_human_parsing/
--rw-r--r--   0 runner    (1001) docker     (122)      575 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_human_parsing/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_human_parsing/backbone/
--rw-r--r--   0 runner    (1001) docker     (122)      525 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_human_parsing/backbone/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11736 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_human_parsing/backbone/deeplab_resnet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_human_parsing/m2fp/
--rw-r--r--   0 runner    (1001) docker     (122)      640 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_human_parsing/m2fp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8257 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_human_parsing/m2fp/m2fp_decoder.py
--rw-r--r--   0 runner    (1001) docker     (122)     8361 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_human_parsing/m2fp/m2fp_encoder.py
--rw-r--r--   0 runner    (1001) docker     (122)    15097 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_human_parsing/m2fp_net.py
--rw-r--r--   0 runner    (1001) docker     (122)     5633 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_human_parsing/parsing_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_inpainting/
--rw-r--r--   0 runner    (1001) docker     (122)      475 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_inpainting/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2690 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_inpainting/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     7605 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_inpainting/default.py
--rw-r--r--   0 runner    (1001) docker     (122)     1253 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_inpainting/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_inpainting/modules/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_inpainting/modules/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_inpainting/modules/ade20k/
--rw-r--r--   0 runner    (1001) docker     (122)       81 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_inpainting/modules/ade20k/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12202 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_inpainting/modules/ade20k/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     5348 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_inpainting/modules/ade20k/resnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     7509 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_inpainting/modules/adversarial.py
--rw-r--r--   0 runner    (1001) docker     (122)     1564 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_inpainting/modules/feature_matching.py
--rw-r--r--   0 runner    (1001) docker     (122)    19052 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_inpainting/modules/ffc.py
--rw-r--r--   0 runner    (1001) docker     (122)    11842 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_inpainting/modules/inception.py
--rw-r--r--   0 runner    (1001) docker     (122)     1511 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_inpainting/modules/perceptual.py
--rw-r--r--   0 runner    (1001) docker     (122)     1965 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_inpainting/modules/pix2pixhd.py
--rw-r--r--   0 runner    (1001) docker     (122)    13350 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_inpainting/refinement.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      984 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/backbones/
--rw-r--r--   0 runner    (1001) docker     (122)      574 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/backbones/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    27174 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/backbones/swin_transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     9305 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/cascade_mask_rcnn_swin.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/datasets/
--rw-r--r--   0 runner    (1001) docker     (122)      101 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/datasets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3992 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/datasets/transforms.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/maskdino/
--rw-r--r--   0 runner    (1001) docker     (122)      600 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/maskdino/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10072 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/maskdino/dino_decoder.py
--rw-r--r--   0 runner    (1001) docker     (122)    15407 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_decoder.py
--rw-r--r--   0 runner    (1001) docker     (122)    18188 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_encoder.py
--rw-r--r--   0 runner    (1001) docker     (122)     7274 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/maskdino/ms_deform_attn.py
--rw-r--r--   0 runner    (1001) docker     (122)     2720 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/maskdino/position_encoding.py
--rw-r--r--   0 runner    (1001) docker     (122)     6712 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/maskdino/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1429 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/maskdino_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    11454 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/maskdino_swin.py
--rw-r--r--   0 runner    (1001) docker     (122)     1549 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     7904 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/postprocess_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/
--rw-r--r--   0 runner    (1001) docker     (122)      553 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/config/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/config/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6991 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/config/default.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/
--rw-r--r--   0 runner    (1001) docker     (122)      171 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/backbone/
--rw-r--r--   0 runner    (1001) docker     (122)      588 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/backbone/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7244 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/backbone/resnet_fpn.py
--rw-r--r--   0 runner    (1001) docker     (122)     3784 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/
--rw-r--r--   0 runner    (1001) docker     (122)      239 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2968 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/fine_preprocess.py
--rw-r--r--   0 runner    (1001) docker     (122)     3012 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/linear_attention.py
--rw-r--r--   0 runner    (1001) docker     (122)     2994 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/quadtree_attention.py
--rw-r--r--   0 runner    (1001) docker     (122)     9677 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/transformer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10531 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/utils/coarse_matching.py
--rw-r--r--   0 runner    (1001) docker     (122)     3048 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/utils/fine_matching.py
--rw-r--r--   0 runner    (1001) docker     (122)     2132 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/utils/position_encoding.py
--rw-r--r--   0 runner    (1001) docker     (122)     2735 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/quadtree_attention_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      344 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_matching/utils/misc.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_mvs_depth_estimation/
--rw-r--r--   0 runner    (1001) docker     (122)      521 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_mvs_depth_estimation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8546 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_mvs_depth_estimation/cas_mvsnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     6274 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_mvs_depth_estimation/casmvs_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    18181 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_mvs_depth_estimation/colmap2mvsnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     9864 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_mvs_depth_estimation/depth_filter.py
--rw-r--r--   0 runner    (1001) docker     (122)     9404 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_mvs_depth_estimation/general_eval_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)    21735 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_mvs_depth_estimation/module.py
--rw-r--r--   0 runner    (1001) docker     (122)     3367 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_mvs_depth_estimation/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_paintbyexample/
--rw-r--r--   0 runner    (1001) docker     (122)      507 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_paintbyexample/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1567 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_paintbyexample/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_panoptic_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      571 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_panoptic_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1694 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_panoptic_segmentation/panseg_model.py
--rw-r--r--   0 runner    (1001) docker     (122)      644 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_panoptic_segmentation/r50_panseg_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/
--rw-r--r--   0 runner    (1001) docker     (122)      538 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     8563 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/align_faces.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/eqface/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/eqface/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     1836 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/eqface/fqa.py
--rw-r--r--   0 runner    (1001) docker     (122)     4710 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/eqface/model_resnet.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    22869 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/gpen.py
--rw-r--r--   0 runner    (1001) docker     (122)     7114 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/image_portrait_enhancement.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/losses/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/losses/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4336 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/losses/helpers.py
--rw-r--r--   0 runner    (1001) docker     (122)     3286 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/losses/losses.py
--rw-r--r--   0 runner    (1001) docker     (122)     3155 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/losses/model_irse.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/retinaface/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/retinaface/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     7313 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/retinaface/detection.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/retinaface/models/
--rwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/retinaface/models/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4845 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/retinaface/models/net.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4676 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/retinaface/models/retinaface.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4120 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/retinaface/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_probing_model/
--rw-r--r--   0 runner    (1001) docker     (122)      533 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_probing_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10076 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_probing_model/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     3319 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_probing_model/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     5206 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_probing_model/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_degradation/
--rw-r--r--   0 runner    (1001) docker     (122)      584 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_degradation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5064 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_degradation/degradation_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     4654 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_man/
--rw-r--r--   0 runner    (1001) docker     (122)      544 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_man/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2600 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_man/image_quality_assessment_man.py
--rw-r--r--   0 runner    (1001) docker     (122)     5163 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_man/maniqa.py
--rw-r--r--   0 runner    (1001) docker     (122)    23230 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_man/swin.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_mos/
--rw-r--r--   0 runner    (1001) docker     (122)      544 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_mos/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_mos/backbones/
--rw-r--r--   0 runner    (1001) docker     (122)      272 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_mos/backbones/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    16222 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_mos/backbones/resnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     1060 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_mos/censeo_ivqa_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_mos/heads/
--rw-r--r--   0 runner    (1001) docker     (122)       36 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_mos/heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      529 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_mos/heads/simple_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     2651 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_mos/image_quality_assessment_mos.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_reid_person/
--rw-r--r--   0 runner    (1001) docker     (122)      467 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_reid_person/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5444 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_reid_person/pass_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    14108 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_reid_person/transreid_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_restoration/
--rw-r--r--   0 runner    (1001) docker     (122)      527 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_restoration/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_restoration/demoire_models/
--rw-r--r--   0 runner    (1001) docker     (122)       69 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_restoration/demoire_models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10243 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_restoration/demoire_models/nets.py
--rw-r--r--   0 runner    (1001) docker     (122)     2640 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_restoration/image_restoration_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      712 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7896 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/data_util.py
--rw-r--r--   0 runner    (1001) docker     (122)     4661 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/feature_extractors.py
--rw-r--r--   0 runner    (1001) docker     (122)     5155 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/pixel_classifier.py
--rw-r--r--   0 runner    (1001) docker     (122)     2273 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     3208 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/pan_merge/
--rw-r--r--   0 runner    (1001) docker     (122)      111 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/pan_merge/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1516 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/pan_merge/base_panoptic_fusion_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     2122 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/pan_merge/maskformer_semantic_head.py
--rw-r--r--   0 runner    (1001) docker     (122)      618 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/segformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     2569 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/semantic_seg_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/
--rw-r--r--   0 runner    (1001) docker     (122)      303 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/
--rw-r--r--   0 runner    (1001) docker     (122)      290 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/
--rw-r--r--   0 runner    (1001) docker     (122)      249 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    17484 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/adapter_modules.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/
--rw-r--r--   0 runner    (1001) docker     (122)      196 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    18241 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/beit.py
--rw-r--r--   0 runner    (1001) docker     (122)     6379 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/beit_adapter.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/
--rw-r--r--   0 runner    (1001) docker     (122)      251 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10785 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/base_decode_head.py
--rw-r--r--   0 runner    (1001) docker     (122)    26514 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/mask2former_head_from_mmseg.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/
--rw-r--r--   0 runner    (1001) docker     (122)      253 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12370 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/base_segmentor.py
--rw-r--r--   0 runner    (1001) docker     (122)    11715 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/encoder_decoder_mask2former.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/
--rw-r--r--   0 runner    (1001) docker     (122)      368 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      398 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/builder.py
--rw-r--r--   0 runner    (1001) docker     (122)     1938 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/data_process_func.py
--rw-r--r--   0 runner    (1001) docker     (122)     1788 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/seg_func.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_skychange/
--rw-r--r--   0 runner    (1001) docker     (122)      650 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_skychange/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9312 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_skychange/preprocessor.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_skychange/ptsemseg/
--rw-r--r--   0 runner    (1001) docker     (122)     3740 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_skychange/ptsemseg/BlockModules.py
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_skychange/ptsemseg/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    21278 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_skychange/ptsemseg/hrnet_backnone.py
--rw-r--r--   0 runner    (1001) docker     (122)    18029 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_skychange/ptsemseg/hrnet_super_and_ocr.py
--rw-r--r--   0 runner    (1001) docker     (122)     8159 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_skychange/ptsemseg/unet.py
--rw-r--r--   0 runner    (1001) docker     (122)    11210 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_skychange/skychange.py
--rw-r--r--   0 runner    (1001) docker     (122)     7874 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_skychange/skychange_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/
--rw-r--r--   0 runner    (1001) docker     (122)      120 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/data/
--rw-r--r--   0 runner    (1001) docker     (122)      561 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3309 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/data/transforms.py
--rw-r--r--   0 runner    (1001) docker     (122)    10476 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/models/
--rw-r--r--   0 runner    (1001) docker     (122)      603 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12670 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/models/autoencoder.py
--rw-r--r--   0 runner    (1001) docker     (122)    12594 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/models/clip.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/ops/
--rw-r--r--   0 runner    (1001) docker     (122)      561 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/ops/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    24469 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/ops/diffusion.py
--rw-r--r--   0 runner    (1001) docker     (122)     1320 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/ops/losses.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/
--rw-r--r--   0 runner    (1001) docker     (122)      536 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/data/
--rw-r--r--   0 runner    (1001) docker     (122)      127 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3309 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/data/transforms.py
--rw-r--r--   0 runner    (1001) docker     (122)    10527 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/model_translation.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/models/
--rw-r--r--   0 runner    (1001) docker     (122)      161 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12670 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/models/autoencoder.py
--rw-r--r--   0 runner    (1001) docker     (122)    12594 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/models/clip.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/ops/
--rw-r--r--   0 runner    (1001) docker     (122)      384 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/ops/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    21828 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/ops/apps.py
--rw-r--r--   0 runner    (1001) docker     (122)    36549 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/ops/degradation.py
--rw-r--r--   0 runner    (1001) docker     (122)    24615 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/ops/diffusion.py
--rw-r--r--   0 runner    (1001) docker     (122)     1320 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/ops/losses.py
--rw-r--r--   0 runner    (1001) docker     (122)     4389 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/ops/metrics.py
--rw-r--r--   0 runner    (1001) docker     (122)     6736 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/ops/random_color.py
--rw-r--r--   0 runner    (1001) docker     (122)     2745 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/ops/random_mask.py
--rw-r--r--   0 runner    (1001) docker     (122)     3777 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/ops/svd.py
--rw-r--r--   0 runner    (1001) docker     (122)     6728 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/ops/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/
--rw-r--r--   0 runner    (1001) docker     (122)      486 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/backbone/
--rw-r--r--   0 runner    (1001) docker     (122)      136 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/backbone/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4099 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/backbone/resnet_DA.py
--rw-r--r--   0 runner    (1001) docker     (122)     6931 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/backbone/vit_horizon_pry_image.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/misc/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/misc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2510 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/misc/fourier.py
--rw-r--r--   0 runner    (1001) docker     (122)     3251 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/misc/panostretch.py
--rw-r--r--   0 runner    (1001) docker     (122)    14887 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/misc/post_proc.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/modality/
--rw-r--r--   0 runner    (1001) docker     (122)       86 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/modality/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5518 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/modality/layout.py
--rw-r--r--   0 runner    (1001) docker     (122)     3406 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/panovit.py
--rw-r--r--   0 runner    (1001) docker     (122)     4808 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1834 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/panovit.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/language_guided_video_summarization/
--rwxr-xr-x   0 runner    (1001) docker     (122)      542 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/language_guided_video_summarization/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     6441 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/language_guided_video_summarization/summarizer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/language_guided_video_summarization/transformer/
--rwxr-xr-x   0 runner    (1001) docker     (122)      508 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/language_guided_video_summarization/transformer/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     1900 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/language_guided_video_summarization/transformer/layers.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     7275 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/language_guided_video_summarization/transformer/models.py
--rwxr-xr-x   0 runner    (1001) docker     (122)      826 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/language_guided_video_summarization/transformer/modules.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     2690 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/language_guided_video_summarization/transformer/sub_layers.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/motion_generation/
--rw-r--r--   0 runner    (1001) docker     (122)      639 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/motion_generation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2151 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/motion_generation/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/motion_generation/modules/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/motion_generation/modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1209 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/motion_generation/modules/cfg_sampler.py
--rw-r--r--   0 runner    (1001) docker     (122)    26783 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/motion_generation/modules/gaussian_diffusion.py
--rw-r--r--   0 runner    (1001) docker     (122)    13594 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/motion_generation/modules/mdm.py
--rw-r--r--   0 runner    (1001) docker     (122)     5412 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/motion_generation/modules/respace.py
--rw-r--r--   0 runner    (1001) docker     (122)     3914 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/motion_generation/modules/rotation2xyz.py
--rw-r--r--   0 runner    (1001) docker     (122)     3189 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/motion_generation/modules/smpl.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/movie_scene_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      615 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/movie_scene_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1567 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/movie_scene_segmentation/get_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     7458 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/movie_scene_segmentation/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/movie_scene_segmentation/utils/
--rw-r--r--   0 runner    (1001) docker     (122)      181 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/movie_scene_segmentation/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      798 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/movie_scene_segmentation/utils/head.py
--rw-r--r--   0 runner    (1001) docker     (122)     4232 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/movie_scene_segmentation/utils/save_op.py
--rw-r--r--   0 runner    (1001) docker     (122)    10325 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/movie_scene_segmentation/utils/shot_encoder.py
--rw-r--r--   0 runner    (1001) docker     (122)     5057 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/movie_scene_segmentation/utils/trn.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/nerf_recon_acc/
--rw-r--r--   0 runner    (1001) docker     (122)      602 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/nerf_recon_acc/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/nerf_recon_acc/dataloader/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/nerf_recon_acc/dataloader/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    18782 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/nerf_recon_acc/dataloader/nerf_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)    20567 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/nerf_recon_acc/dataloader/read_write_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     7297 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/nerf_recon_acc/nerf_preprocess.py
--rw-r--r--   0 runner    (1001) docker     (122)     7876 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/nerf_recon_acc/nerf_recon_acc.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/nerf_recon_acc/network/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/nerf_recon_acc/network/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    13317 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/nerf_recon_acc/network/nerf.py
--rw-r--r--   0 runner    (1001) docker     (122)     1512 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/nerf_recon_acc/network/segmenter.py
--rw-r--r--   0 runner    (1001) docker     (122)     5720 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/nerf_recon_acc/network/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/
--rw-r--r--   0 runner    (1001) docker     (122)      606 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      620 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/dino.py
--rw-r--r--   0 runner    (1001) docker     (122)     3432 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/
--rw-r--r--   0 runner    (1001) docker     (122)      321 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/backbones/
--rw-r--r--   0 runner    (1001) docker     (122)      211 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/backbones/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    21306 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/backbones/vit.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/
--rw-r--r--   0 runner    (1001) docker     (122)      278 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2220 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/anchor_head.py
--rw-r--r--   0 runner    (1001) docker     (122)    11529 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/necks/
--rw-r--r--   0 runner    (1001) docker     (122)      213 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/necks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9021 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/necks/fpn.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/
--rw-r--r--   0 runner    (1001) docker     (122)      426 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/
--rw-r--r--   0 runner    (1001) docker     (122)      375 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8542 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/
--rw-r--r--   0 runner    (1001) docker     (122)      239 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    17739 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/fcn_mask_head.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/utils/
--rw-r--r--   0 runner    (1001) docker     (122)      306 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    21926 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/utils/checkpoint.py
--rw-r--r--   0 runner    (1001) docker     (122)     1204 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/utils/convModule_norm.py
--rw-r--r--   0 runner    (1001) docker     (122)      832 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection/yolox_pai.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/
--rw-r--r--   0 runner    (1001) docker     (122)      467 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/
--rw-r--r--   0 runner    (1001) docker     (122)       86 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2657 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/depe_detect.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/
--rw-r--r--   0 runner    (1001) docker     (122)      605 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/
--rw-r--r--   0 runner    (1001) docker     (122)      299 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6670 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/
--rw-r--r--   0 runner    (1001) docker     (122)      282 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4476 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/
--rw-r--r--   0 runner    (1001) docker     (122)      276 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1064 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/match_cost.py
--rw-r--r--   0 runner    (1001) docker     (122)     2041 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/util.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/
--rw-r--r--   0 runner    (1001) docker     (122)      294 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3095 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/nuscenes_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/
--rw-r--r--   0 runner    (1001) docker     (122)      522 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7936 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/loading.py
--rw-r--r--   0 runner    (1001) docker     (122)     8689 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/
--rw-r--r--   0 runner    (1001) docker     (122)      255 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12874 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/vovnet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/
--rw-r--r--   0 runner    (1001) docker     (122)      282 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7442 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/depth_net.py
--rw-r--r--   0 runner    (1001) docker     (122)    59197 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/petrv2_dednhead.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/
--rw-r--r--   0 runner    (1001) docker     (122)      255 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9533 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/petr3d.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/
--rw-r--r--   0 runner    (1001) docker     (122)      256 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9032 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/cp_fpn.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/
--rw-r--r--   0 runner    (1001) docker     (122)      562 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    18126 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     5175 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/positional_encoding.py
--rw-r--r--   0 runner    (1001) docker     (122)    11059 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/result_vis.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/ocr_detection/
--rw-r--r--   0 runner    (1001) docker     (122)      473 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/ocr_detection/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2870 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/ocr_detection/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/ocr_detection/modules/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/ocr_detection/modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    18335 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/ocr_detection/modules/dbnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     8217 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/ocr_detection/modules/seg_detector_loss.py
--rw-r--r--   0 runner    (1001) docker     (122)     2532 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/ocr_detection/preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     7972 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/ocr_detection/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/ocr_recognition/
--rw-r--r--   0 runner    (1001) docker     (122)      477 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/ocr_recognition/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5848 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/ocr_recognition/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/ocr_recognition/modules/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/ocr_recognition/modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6209 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/ocr_recognition/modules/convnext.py
--rw-r--r--   0 runner    (1001) docker     (122)      517 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/ocr_recognition/modules/convnextvit.py
--rw-r--r--   0 runner    (1001) docker     (122)     3451 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/ocr_recognition/modules/crnn.py
--rw-r--r--   0 runner    (1001) docker     (122)    11644 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/ocr_recognition/modules/timm_tinyc.py
--rw-r--r--   0 runner    (1001) docker     (122)     1955 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/ocr_recognition/modules/vitstr.py
--rw-r--r--   0 runner    (1001) docker     (122)     3603 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/ocr_recognition/preprocessor.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/open_vocabulary_detection_vild/
--rw-r--r--   0 runner    (1001) docker     (122)      501 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/open_vocabulary_detection_vild/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14249 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/open_vocabulary_detection_vild/vild.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/panorama_depth_estimation/
--rw-r--r--   0 runner    (1001) docker     (122)      511 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/panorama_depth_estimation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/panorama_depth_estimation/networks/
--rw-r--r--   0 runner    (1001) docker     (122)      102 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/panorama_depth_estimation/networks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5077 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/panorama_depth_estimation/networks/equi.py
--rw-r--r--   0 runner    (1001) docker     (122)     7503 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/panorama_depth_estimation/networks/layers.py
--rw-r--r--   0 runner    (1001) docker     (122)     7839 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/panorama_depth_estimation/networks/mobilenet.py
--rw-r--r--   0 runner    (1001) docker     (122)    14421 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/panorama_depth_estimation/networks/resnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     9175 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/panorama_depth_estimation/networks/unifuse.py
--rw-r--r--   0 runner    (1001) docker     (122)     3962 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/panorama_depth_estimation/networks/util.py
--rw-r--r--   0 runner    (1001) docker     (122)     3307 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/panorama_depth_estimation/unifuse_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/pointcloud_sceneflow_estimation/
--rw-r--r--   0 runner    (1001) docker     (122)      495 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/pointcloud_sceneflow_estimation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    16147 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/pointcloud_sceneflow_estimation/common.py
--rw-r--r--   0 runner    (1001) docker     (122)    11898 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/pointcloud_sceneflow_estimation/pointnet2_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1797 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/pointcloud_sceneflow_estimation/rcp_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    18618 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/pointcloud_sceneflow_estimation/sf_rcp.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/product_retrieval_embedding/
--rw-r--r--   0 runner    (1001) docker     (122)      548 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/product_retrieval_embedding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    20974 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/product_retrieval_embedding/item_detection.py
--rw-r--r--   0 runner    (1001) docker     (122)     4509 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/product_retrieval_embedding/item_embedding.py
--rw-r--r--   0 runner    (1001) docker     (122)     4340 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/product_retrieval_embedding/item_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/product_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      528 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/product_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7978 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/product_segmentation/net.py
--rw-r--r--   0 runner    (1001) docker     (122)     2194 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/product_segmentation/seg_infer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      514 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5832 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/
--rw-r--r--   0 runner    (1001) docker     (122)      318 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8504 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     9206 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/criterion.py
--rw-r--r--   0 runner    (1001) docker     (122)     7299 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/matcher.py
--rw-r--r--   0 runner    (1001) docker     (122)     7935 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/misc.py
--rw-r--r--   0 runner    (1001) docker     (122)     6242 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/mttr.py
--rw-r--r--   0 runner    (1001) docker     (122)    16610 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/multimodal_transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     2091 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/position_encoding_2d.py
--rw-r--r--   0 runner    (1001) docker     (122)     5629 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/postprocessing.py
--rw-r--r--   0 runner    (1001) docker     (122)     5180 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/segmentation.py
--rw-r--r--   0 runner    (1001) docker     (122)    27941 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/swin_transformer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/robust_image_classification/
--rw-r--r--   0 runner    (1001) docker     (122)      501 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/robust_image_classification/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2902 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/robust_image_classification/easyrobust_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/salient_detection/
--rw-r--r--   0 runner    (1001) docker     (122)      497 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/salient_detection/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/salient_detection/models/
--rw-r--r--   0 runner    (1001) docker     (122)      213 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/salient_detection/models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/salient_detection/models/backbone/
--rw-r--r--   0 runner    (1001) docker     (122)     6558 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/salient_detection/models/backbone/Res2Net_v1b.py
--rw-r--r--   0 runner    (1001) docker     (122)      256 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/salient_detection/models/backbone/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6525 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/salient_detection/models/modules.py
--rw-r--r--   0 runner    (1001) docker     (122)     3047 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/salient_detection/models/senet.py
--rw-r--r--   0 runner    (1001) docker     (122)    12007 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/salient_detection/models/u2net.py
--rw-r--r--   0 runner    (1001) docker     (122)     3524 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/salient_detection/models/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2704 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/salient_detection/salient_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/shop_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      464 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/shop_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2203 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/shop_segmentation/common.py
--rw-r--r--   0 runner    (1001) docker     (122)     4385 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/shop_segmentation/head_fpn.py
--rw-r--r--   0 runner    (1001) docker     (122)    32989 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/shop_segmentation/models.py
--rw-r--r--   0 runner    (1001) docker     (122)     9319 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/shop_segmentation/neck_fpn.py
--rw-r--r--   0 runner    (1001) docker     (122)     5632 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/shop_segmentation/shop_seg_base.py
--rw-r--r--   0 runner    (1001) docker     (122)     4095 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/shop_segmentation/shop_seg_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     6699 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/shop_segmentation/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/skin_retouching/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/skin_retouching/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/skin_retouching/detection_model/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/skin_retouching/detection_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/skin_retouching/detection_model/detection_module.py
--rw-r--r--   0 runner    (1001) docker     (122)     2532 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/skin_retouching/detection_model/detection_unet_in.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/skin_retouching/inpainting_model/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/skin_retouching/inpainting_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5759 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/skin_retouching/inpainting_model/gconv.py
--rw-r--r--   0 runner    (1001) docker     (122)     3233 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/skin_retouching/inpainting_model/inpainting_unet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/skin_retouching/retinaface/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/skin_retouching/retinaface/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10898 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/skin_retouching/retinaface/box_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     3911 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/skin_retouching/retinaface/net.py
--rw-r--r--   0 runner    (1001) docker     (122)     4799 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/skin_retouching/retinaface/network.py
--rw-r--r--   0 runner    (1001) docker     (122)     5003 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/skin_retouching/retinaface/predict_single.py
--rw-r--r--   0 runner    (1001) docker     (122)     1035 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/skin_retouching/retinaface/prior_box.py
--rw-r--r--   0 runner    (1001) docker     (122)     2110 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/skin_retouching/retinaface/utils.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     3890 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/skin_retouching/unet_deploy.py
--rw-r--r--   0 runner    (1001) docker     (122)     9917 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/skin_retouching/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1169 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/skin_retouching/weights_init.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/
--rw-r--r--   0 runner    (1001) docker     (122)      526 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/data/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2094 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/data/data_augment.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/exp/
--rw-r--r--   0 runner    (1001) docker     (122)      165 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/exp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      274 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/exp/base_exp.py
--rw-r--r--   0 runner    (1001) docker     (122)      392 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/exp/build.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/exp/default/
--rw-r--r--   0 runner    (1001) docker     (122)      137 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/exp/default/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1209 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/exp/default/streamyolo.py
--rw-r--r--   0 runner    (1001) docker     (122)     1845 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/exp/yolox_base.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/models/
--rw-r--r--   0 runner    (1001) docker     (122)      238 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6274 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/models/darknet.py
--rw-r--r--   0 runner    (1001) docker     (122)    10911 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/models/dfp_pafpn.py
--rw-r--r--   0 runner    (1001) docker     (122)     6156 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/models/network_blocks.py
--rw-r--r--   0 runner    (1001) docker     (122)     1314 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/models/streamyolo.py
--rw-r--r--   0 runner    (1001) docker     (122)     5669 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/models/tal_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     4352 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/realtime_video_detector.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/utils/
--rw-r--r--   0 runner    (1001) docker     (122)      270 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3681 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/utils/boxes.py
--rw-r--r--   0 runner    (1001) docker     (122)      209 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/stream_yolo/utils/format.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/super_resolution/
--rw-r--r--   0 runner    (1001) docker     (122)      555 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/super_resolution/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7652 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/super_resolution/arch_util.py
--rw-r--r--   0 runner    (1001) docker     (122)    10508 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/super_resolution/ecb.py
--rw-r--r--   0 runner    (1001) docker     (122)     3525 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/super_resolution/ecbsr_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     4945 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/super_resolution/rrdbnet_arch.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/table_recognition/
--rw-r--r--   0 runner    (1001) docker     (122)      462 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/table_recognition/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    15842 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/table_recognition/lineless_table_process.py
--rw-r--r--   0 runner    (1001) docker     (122)     3360 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/table_recognition/model_lore.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/table_recognition/modules/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/table_recognition/modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12614 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/table_recognition/modules/lore_detector.py
--rw-r--r--   0 runner    (1001) docker     (122)    15090 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/table_recognition/modules/lore_processor.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/text_driven_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)       96 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/text_driven_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5488 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/text_driven_segmentation/clip.py
--rw-r--r--   0 runner    (1001) docker     (122)      777 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/text_driven_segmentation/lseg_base.py
--rw-r--r--   0 runner    (1001) docker     (122)     7587 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/text_driven_segmentation/lseg_blocks.py
--rw-r--r--   0 runner    (1001) docker     (122)     4143 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/text_driven_segmentation/lseg_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     6145 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/text_driven_segmentation/lseg_net.py
--rw-r--r--   0 runner    (1001) docker     (122)    18951 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/text_driven_segmentation/lseg_vit.py
--rw-r--r--   0 runner    (1001) docker     (122)    16418 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/text_driven_segmentation/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     5165 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/text_driven_segmentation/simple_tokenizer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_classfication/
--rw-r--r--   0 runner    (1001) docker     (122)      594 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_classfication/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    43681 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_classfication/basic_blocks.py
--rw-r--r--   0 runner    (1001) docker     (122)     2174 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_classfication/global_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2902 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_classfication/master_net.py
--rw-r--r--   0 runner    (1001) docker     (122)      766 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_classfication/model_zoo.py
--rw-r--r--   0 runner    (1001) docker     (122)     2892 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_classfication/plain_net_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     7405 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_classfication/super_blocks.py
--rw-r--r--   0 runner    (1001) docker     (122)    14983 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_classfication/super_res_idwexkx.py
--rw-r--r--   0 runner    (1001) docker     (122)     8709 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_classfication/super_res_k1kxk1.py
--rw-r--r--   0 runner    (1001) docker     (122)     6923 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_classfication/super_res_kxkx.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/
--rw-r--r--   0 runner    (1001) docker     (122)      699 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/apis/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/apis/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2067 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/apis/detector_evaluater.py
--rw-r--r--   0 runner    (1001) docker     (122)     4000 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/apis/detector_inference.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/augmentations/
--rw-r--r--   0 runner    (1001) docker     (122)       49 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/augmentations/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/
--rw-r--r--   0 runner    (1001) docker     (122)       49 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3460 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/box_level_augs.py
--rw-r--r--   0 runner    (1001) docker     (122)     8123 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/color_augs.py
--rw-r--r--   0 runner    (1001) docker     (122)     2345 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/gaussian_maps.py
--rw-r--r--   0 runner    (1001) docker     (122)     5537 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/geometric_augs.py
--rw-r--r--   0 runner    (1001) docker     (122)     2842 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/augmentations/scale_aware_aug.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/
--rw-r--r--   0 runner    (1001) docker     (122)      148 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/
--rw-r--r--   0 runner    (1001) docker     (122)      621 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3701 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/darknet.py
--rw-r--r--   0 runner    (1001) docker     (122)     9356 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_csp.py
--rw-r--r--   0 runner    (1001) docker     (122)     7465 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_res.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14261 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/base_ops.py
--rw-r--r--   0 runner    (1001) docker     (122)    10559 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/neck_ops.py
--rw-r--r--   0 runner    (1001) docker     (122)    13101 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/ops.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    18626 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/ota_assigner.py
--rw-r--r--   0 runner    (1001) docker     (122)     7526 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/repvgg_block.py
--rw-r--r--   0 runner    (1001) docker     (122)     2572 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)      561 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/weight_init.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/heads/
--rw-r--r--   0 runner    (1001) docker     (122)      409 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12602 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/heads/gfocal_v2_tiny.py
--rw-r--r--   0 runner    (1001) docker     (122)    19162 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/heads/zero_head.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/losses/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/losses/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5588 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/losses/distill_loss.py
--rw-r--r--   0 runner    (1001) docker     (122)    12284 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/losses/gfocal_loss.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/necks/
--rw-r--r--   0 runner    (1001) docker     (122)      421 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/necks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7347 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_config.py
--rw-r--r--   0 runner    (1001) docker     (122)    24842 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn.py
--rw-r--r--   0 runner    (1001) docker     (122)     3499 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn_btn.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/detectors/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/detectors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2809 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/detectors/detector.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/structures/
--rw-r--r--   0 runner    (1001) docker     (122)       49 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/structures/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9197 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/structures/bounding_box.py
--rw-r--r--   0 runner    (1001) docker     (122)     2627 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/structures/boxlist_ops.py
--rw-r--r--   0 runner    (1001) docker     (122)     2774 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/structures/image_list.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/utils/
--rw-r--r--   0 runner    (1001) docker     (122)      189 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11594 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/utils/boxes.py
--rw-r--r--   0 runner    (1001) docker     (122)     5838 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/utils/model_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1164 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/utils/scheduler.py
--rw-r--r--   0 runner    (1001) docker     (122)     6400 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/detector.py
--rw-r--r--   0 runner    (1001) docker     (122)      627 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/tinynas_damoyolo.py
--rw-r--r--   0 runner    (1001) docker     (122)      643 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/tinynas_detector.py
--rw-r--r--   0 runner    (1001) docker     (122)     1026 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/tinynas_detection/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_deinterlace/
--rw-r--r--   0 runner    (1001) docker     (122)     2896 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_deinterlace/UNet_for_video_deinterlace.py
--rw-r--r--   0 runner    (1001) docker     (122)      494 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_deinterlace/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      773 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_deinterlace/deinterlace_arch.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_deinterlace/models/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_deinterlace/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3159 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_deinterlace/models/archs.py
--rw-r--r--   0 runner    (1001) docker     (122)     1441 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_deinterlace/models/deep_fourier_upsampling.py
--rw-r--r--   0 runner    (1001) docker     (122)     2972 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_deinterlace/models/enh.py
--rw-r--r--   0 runner    (1001) docker     (122)     3572 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_deinterlace/models/fre.py
--rw-r--r--   0 runner    (1001) docker     (122)     3716 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_deinterlace/models/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/
--rw-r--r--   0 runner    (1001) docker     (122)      483 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/configs/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/configs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12753 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/configs/default_config.py
--rw-r--r--   0 runner    (1001) docker     (122)     6731 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/dro_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/geometry/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/geometry/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5494 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/geometry/camera.py
--rw-r--r--   0 runner    (1001) docker     (122)     2285 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/geometry/camera_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     3737 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/geometry/pose.py
--rw-r--r--   0 runner    (1001) docker     (122)     3685 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/geometry/pose_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/models/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5277 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/models/model_checkpoint.py
--rw-r--r--   0 runner    (1001) docker     (122)     2356 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/models/model_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    10098 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/models/model_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (122)     7204 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/models/sfm_model_mf.py
--rw-r--r--   0 runner    (1001) docker     (122)     4387 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/models/sup_model_mf.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/depth_pose/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/depth_pose/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10532 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/depth_pose/depth_pose_net.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/layers/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/layers/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8902 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/depth_decoder.py
--rw-r--r--   0 runner    (1001) docker     (122)     1607 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/layers.py
--rw-r--r--   0 runner    (1001) docker     (122)     1915 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/pose_decoder.py
--rw-r--r--   0 runner    (1001) docker     (122)     4110 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/resnet_encoder.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/optim/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/optim/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    15781 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/optim/extractor.py
--rw-r--r--   0 runner    (1001) docker     (122)     8039 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/optim/update.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/utils/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6815 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/utils/augmentations.py
--rw-r--r--   0 runner    (1001) docker     (122)    10248 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/utils/config.py
--rw-r--r--   0 runner    (1001) docker     (122)    15071 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/utils/depth.py
--rw-r--r--   0 runner    (1001) docker     (122)     1153 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/utils/horovod.py
--rw-r--r--   0 runner    (1001) docker     (122)    10965 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/utils/image.py
--rw-r--r--   0 runner    (1001) docker     (122)     9407 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/utils/image_gt.py
--rw-r--r--   0 runner    (1001) docker     (122)     6499 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/utils/load.py
--rw-r--r--   0 runner    (1001) docker     (122)     2726 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/utils/misc.py
--rw-r--r--   0 runner    (1001) docker     (122)     1348 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/utils/types.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/
--rw-r--r--   0 runner    (1001) docker     (122)     1894 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/VFINet_arch.py
--rw-r--r--   0 runner    (1001) docker     (122)     3334 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/VFINet_for_video_frame_interpolation.py
--rw-r--r--   0 runner    (1001) docker     (122)      458 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/flow_model/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/flow_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3212 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/flow_model/corr.py
--rw-r--r--   0 runner    (1001) docker     (122)     9228 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/flow_model/extractor.py
--rw-r--r--   0 runner    (1001) docker     (122)     5346 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/flow_model/raft.py
--rw-r--r--   0 runner    (1001) docker     (122)     5540 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/flow_model/update.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/interp_model/
--rw-r--r--   0 runner    (1001) docker     (122)    13745 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/interp_model/IFNet_swin.py
--rw-r--r--   0 runner    (1001) docker     (122)     4049 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/interp_model/UNet.py
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/interp_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3823 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/interp_model/flow_reversal.py
--rw-r--r--   0 runner    (1001) docker     (122)    16050 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/interp_model/refinenet_arch.py
--rw-r--r--   0 runner    (1001) docker     (122)    36415 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/interp_model/transformer_layers.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3317 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/utils/scene_change_detection.py
--rw-r--r--   0 runner    (1001) docker     (122)     2907 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/utils/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_human_matting/
--rw-r--r--   0 runner    (1001) docker     (122)      558 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_human_matting/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1324 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_human_matting/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_human_matting/models/
--rw-r--r--   0 runner    (1001) docker     (122)       36 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_human_matting/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10465 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_human_matting/models/decoder.py
--rw-r--r--   0 runner    (1001) docker     (122)     2662 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_human_matting/models/deep_guided_filter.py
--rw-r--r--   0 runner    (1001) docker     (122)     5506 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_human_matting/models/effv2.py
--rw-r--r--   0 runner    (1001) docker     (122)     2904 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_human_matting/models/lraspp.py
--rw-r--r--   0 runner    (1001) docker     (122)     2234 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_human_matting/models/matting.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_inpainting/
--rw-r--r--   0 runner    (1001) docker     (122)      524 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_inpainting/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11056 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_inpainting/inpainting.py
--rw-r--r--   0 runner    (1001) docker     (122)    13727 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_inpainting/inpainting_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      582 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/head/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/head/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    18236 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/head/kernel_frame_iter_head.py
--rw-r--r--   0 runner    (1001) docker     (122)    21551 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/head/kernel_head.py
--rw-r--r--   0 runner    (1001) docker     (122)    16955 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/head/kernel_iter_head.py
--rw-r--r--   0 runner    (1001) docker     (122)    20983 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/head/kernel_update_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     4136 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/head/kernel_updator.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/neck/
--rw-r--r--   0 runner    (1001) docker     (122)      525 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/neck/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11733 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/neck/msdeformattn_decoder.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/track/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/track/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    26843 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/track/kernel_update_head.py
--rw-r--r--   0 runner    (1001) docker     (122)    10771 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py
--rw-r--r--   0 runner    (1001) docker     (122)     4163 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    18059 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/video_knet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/
--rw-r--r--   0 runner    (1001) docker     (122)      541 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/models/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2970 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/models/common.py
--rw-r--r--   0 runner    (1001) docker     (122)     2420 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/models/decode.py
--rw-r--r--   0 runner    (1001) docker     (122)     1890 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/models/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     4921 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/models/yolo.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/tracker/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/tracker/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1084 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/tracker/basetrack.py
--rw-r--r--   0 runner    (1001) docker     (122)     3129 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/tracker/matching.py
--rw-r--r--   0 runner    (1001) docker     (122)    14998 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/tracker/multitracker.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2230 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/utils/image.py
--rw-r--r--   0 runner    (1001) docker     (122)     9570 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/utils/kalman_filter.py
--rw-r--r--   0 runner    (1001) docker     (122)     7296 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/utils/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2888 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/utils/visualization.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_object_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      497 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_object_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      818 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_object_segmentation/aggregate.py
--rw-r--r--   0 runner    (1001) docker     (122)     3582 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_object_segmentation/cbam.py
--rw-r--r--   0 runner    (1001) docker     (122)     1979 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_object_segmentation/eval_network.py
--rw-r--r--   0 runner    (1001) docker     (122)     3980 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_object_segmentation/inference_core.py
--rw-r--r--   0 runner    (1001) docker     (122)     8255 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_object_segmentation/inference_memory_bank.py
--rw-r--r--   0 runner    (1001) docker     (122)     7037 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_object_segmentation/mod_resnet.py
--rw-r--r--   0 runner    (1001) docker     (122)      974 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_object_segmentation/model.py
--rw-r--r--   0 runner    (1001) docker     (122)    15309 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_object_segmentation/modules.py
--rw-r--r--   0 runner    (1001) docker     (122)     5593 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_object_segmentation/network.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      477 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/backbone/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/backbone/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10040 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_checkpoint.py
--rw-r--r--   0 runner    (1001) docker     (122)    26628 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_transformer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/head/
--rw-r--r--   0 runner    (1001) docker     (122)      515 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/head/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8349 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/head/kernel_head.py
--rw-r--r--   0 runner    (1001) docker     (122)    20677 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/head/kernel_iter_head.py
--rw-r--r--   0 runner    (1001) docker     (122)    28653 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/head/kernel_update_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     4081 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/head/kernel_updator.py
--rw-r--r--   0 runner    (1001) docker     (122)     6696 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/head/mask.py
--rw-r--r--   0 runner    (1001) docker     (122)     8627 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/head/semantic_fpn_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (122)     5885 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/head/track_heads.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/neck/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/neck/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6255 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/neck/fpn.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/track/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/track/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8508 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/track/quasi_dense_embed_tracker.py
--rw-r--r--   0 runner    (1001) docker     (122)    17155 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/video_k_net.py
--rw-r--r--   0 runner    (1001) docker     (122)     5440 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/visualizer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/config/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/config/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1024 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/config/ostrack.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/layers/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/layers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1641 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/layers/attn.py
--rw-r--r--   0 runner    (1001) docker     (122)     5004 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/layers/attn_blocks.py
--rw-r--r--   0 runner    (1001) docker     (122)     4805 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/layers/head.py
--rw-r--r--   0 runner    (1001) docker     (122)     1234 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/layers/patch_embed.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/ostrack/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/ostrack/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3380 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/ostrack/base_backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     3393 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/ostrack/ostrack.py
--rw-r--r--   0 runner    (1001) docker     (122)      653 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/ostrack/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    12278 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/ostrack/vit_ce.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/procontext/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/procontext/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3497 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/procontext/procontext.py
--rw-r--r--   0 runner    (1001) docker     (122)      943 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/procontext/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     4473 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/procontext/vit_ce.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/tracker/
--rw-r--r--   0 runner    (1001) docker     (122)      114 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/tracker/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5507 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/tracker/ostrack.py
--rw-r--r--   0 runner    (1001) docker     (122)     7123 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/tracker/procontext.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8488 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/utils/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/
--rw-r--r--   0 runner    (1001) docker     (122)    14702 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/DUT_raft.py
--rw-r--r--   0 runner    (1001) docker     (122)     4715 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/MotionPro.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/RAFT/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/RAFT/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3290 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/RAFT/corr.py
--rw-r--r--   0 runner    (1001) docker     (122)     9214 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/RAFT/extractor.py
--rw-r--r--   0 runner    (1001) docker     (122)     5275 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/RAFT/raft.py
--rw-r--r--   0 runner    (1001) docker     (122)     5526 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/RAFT/update.py
--rw-r--r--   0 runner    (1001) docker     (122)     3922 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/Smoother.py
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1447 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/config.py
--rw-r--r--   0 runner    (1001) docker     (122)     7292 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/rf_det_module.py
--rw-r--r--   0 runner    (1001) docker     (122)     7981 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/rf_det_so.py
--rw-r--r--   0 runner    (1001) docker     (122)     3452 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUTRAFTStabilizer.py
--rw-r--r--   0 runner    (1001) docker     (122)      492 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/utils/
--rw-r--r--   0 runner    (1001) docker     (122)     4684 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/utils/IterativeSmooth.py
--rw-r--r--   0 runner    (1001) docker     (122)    14212 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/utils/MedianFilter.py
--rw-r--r--   0 runner    (1001) docker     (122)    22544 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/utils/ProjectionUtils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2893 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/utils/RAFTUtils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2817 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/utils/WarpUtils.py
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14474 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/utils/image_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     4977 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_stabilization/utils/math_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_streaming_perception/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_streaming_perception/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_streaming_perception/longshortnet/
--rw-r--r--   0 runner    (1001) docker     (122)      487 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_streaming_perception/longshortnet/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_streaming_perception/longshortnet/exp/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_streaming_perception/longshortnet/exp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2488 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_streaming_perception/longshortnet/exp/longshortnet_base.py
--rw-r--r--   0 runner    (1001) docker     (122)     7097 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_streaming_perception/longshortnet/longshortnet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_streaming_perception/longshortnet/models/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_streaming_perception/longshortnet/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5774 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_long.py
--rw-r--r--   0 runner    (1001) docker     (122)     4848 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_short.py
--rw-r--r--   0 runner    (1001) docker     (122)     9990 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort.py
--rw-r--r--   0 runner    (1001) docker     (122)     3905 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort_backbone_neck.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_summarization/
--rw-r--r--   0 runner    (1001) docker     (122)      536 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_summarization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4959 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_summarization/base_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_summarization/kts/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_summarization/kts/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1221 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_summarization/kts/cpd_auto.py
--rw-r--r--   0 runner    (1001) docker     (122)     3238 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_summarization/kts/cpd_nonlin.py
--rw-r--r--   0 runner    (1001) docker     (122)    13098 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_summarization/pgl_sum.py
--rw-r--r--   0 runner    (1001) docker     (122)     8812 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_summarization/summarizer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/video_super_resolution/
--rw-r--r--   0 runner    (1001) docker     (122)      689 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_super_resolution/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14118 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_super_resolution/basicvsr_net.py
--rw-r--r--   0 runner    (1001) docker     (122)     4727 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_super_resolution/common.py
--rw-r--r--   0 runner    (1001) docker     (122)     4720 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_super_resolution/msrresnet_lite_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     3636 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_super_resolution/real_basicvsr_for_video_super_resolution.py
--rw-r--r--   0 runner    (1001) docker     (122)     3637 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/video_super_resolution/real_basicvsr_net.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/vidt/
--rw-r--r--   0 runner    (1001) docker     (122)      464 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vidt/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    40074 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vidt/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)    25588 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vidt/deformable_transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     7436 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vidt/fpn_fusion.py
--rw-r--r--   0 runner    (1001) docker     (122)    16697 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vidt/head.py
--rw-r--r--   0 runner    (1001) docker     (122)     3651 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vidt/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/virual_tryon/
--rw-r--r--   0 runner    (1001) docker     (122)      464 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/virual_tryon/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    17690 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/virual_tryon/sdafnet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/vision_efficient_tuning/
--rw-r--r--   0 runner    (1001) docker     (122)      502 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vision_efficient_tuning/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    16102 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vision_efficient_tuning/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)      797 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vision_efficient_tuning/head.py
--rw-r--r--   0 runner    (1001) docker     (122)     1786 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vision_efficient_tuning/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     9340 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vision_efficient_tuning/petl.py
--rw-r--r--   0 runner    (1001) docker     (122)     5021 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vision_efficient_tuning/timm_helpers.py
--rw-r--r--   0 runner    (1001) docker     (122)    28715 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vision_efficient_tuning/timm_vision_transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     5040 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vision_efficient_tuning/timm_weight_init.py
--rw-r--r--   0 runner    (1001) docker     (122)     5614 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vision_efficient_tuning/vision_efficient_tuning.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/vision_middleware/
--rw-r--r--   0 runner    (1001) docker     (122)      492 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vision_middleware/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5749 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vision_middleware/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)    28089 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vision_middleware/head.py
--rw-r--r--   0 runner    (1001) docker     (122)     6494 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vision_middleware/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     6192 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vision_middleware/vim.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/cv/vop_retrieval/
--rw-r--r--   0 runner    (1001) docker     (122)      919 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vop_retrieval/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12238 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vop_retrieval/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     5188 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vop_retrieval/basic_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    13520 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vop_retrieval/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     5299 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vop_retrieval/model_se.py
--rw-r--r--   0 runner    (1001) docker     (122)     5177 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/cv/vop_retrieval/tokenization_clip.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/multi_modal/
--rw-r--r--   0 runner    (1001) docker     (122)     1821 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/multi_modal/clip/
--rw-r--r--   0 runner    (1001) docker     (122)       97 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/clip/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14496 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/clip/bert_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (122)     3898 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/clip/configuration_bert.py
--rw-r--r--   0 runner    (1001) docker     (122)    22351 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/clip/model.py
--rw-r--r--   0 runner    (1001) docker     (122)    20998 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/clip/modeling_bert.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/multi_modal/diffusion/
--rw-r--r--   0 runner    (1001) docker     (122)      140 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/diffusion/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    26652 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/diffusion/diffusion.py
--rw-r--r--   0 runner    (1001) docker     (122)    14427 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/diffusion/model.py
--rw-r--r--   0 runner    (1001) docker     (122)    39910 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/diffusion/structbert.py
--rw-r--r--   0 runner    (1001) docker     (122)    11507 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/diffusion/tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (122)    11834 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/diffusion/unet_generator.py
--rw-r--r--   0 runner    (1001) docker     (122)     8566 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/diffusion/unet_upsampler_1024.py
--rw-r--r--   0 runner    (1001) docker     (122)    12361 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/diffusion/unet_upsampler_256.py
--rw-r--r--   0 runner    (1001) docker     (122)    44602 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/dpm_solver_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/multi_modal/gemm/
--rw-r--r--   0 runner    (1001) docker     (122)      139 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/gemm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    21694 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/gemm/gemm_base.py
--rw-r--r--   0 runner    (1001) docker     (122)     3718 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/gemm/gemm_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     6982 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/gemm/tokenizer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/multi_modal/guided_diffusion/
--rw-r--r--   0 runner    (1001) docker     (122)      548 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/guided_diffusion/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    36265 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/guided_diffusion/gaussian_diffusion.py
--rw-r--r--   0 runner    (1001) docker     (122)     2969 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/guided_diffusion/respace.py
--rw-r--r--   0 runner    (1001) docker     (122)     1365 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/guided_diffusion/script.py
--rw-r--r--   0 runner    (1001) docker     (122)    36898 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/guided_diffusion/unet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/multi_modal/mgeo/
--rw-r--r--   0 runner    (1001) docker     (122)     1444 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mgeo/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)   101942 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mgeo/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     8651 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mgeo/text_classification.py
--rw-r--r--   0 runner    (1001) docker     (122)     3268 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mgeo/text_ranking.py
--rw-r--r--   0 runner    (1001) docker     (122)    10331 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mgeo/token_classification.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/multi_modal/mmr/
--rw-r--r--   0 runner    (1001) docker     (122)      141 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mmr/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/multi_modal/mmr/dataloaders/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mmr/dataloaders/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3789 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mmr/dataloaders/rawvideo_util.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/multi_modal/mmr/models/
--rw-r--r--   0 runner    (1001) docker     (122)      162 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mmr/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9797 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mmr/models/clip_for_mm_video_embedding.py
--rw-r--r--   0 runner    (1001) docker     (122)     1442 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mmr/models/dynamic_inverted_softmax.py
--rw-r--r--   0 runner    (1001) docker     (122)    21257 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mmr/models/modeling.py
--rw-r--r--   0 runner    (1001) docker     (122)    18790 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mmr/models/module_clip.py
--rw-r--r--   0 runner    (1001) docker     (122)     3577 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mmr/models/module_cross.py
--rw-r--r--   0 runner    (1001) docker     (122)     5581 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mmr/models/tokenization_clip.py
--rw-r--r--   0 runner    (1001) docker     (122)     4116 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mmr/models/until_module.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/multi_modal/mplug/
--rw-r--r--   0 runner    (1001) docker     (122)      739 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mplug/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/multi_modal/mplug/clip/
--rw-r--r--   0 runner    (1001) docker     (122)       86 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mplug/clip/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    16605 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mplug/clip/clip.py
--rw-r--r--   0 runner    (1001) docker     (122)     6030 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mplug/configuration_mplug.py
--rwxr-xr-x   0 runner    (1001) docker     (122)   120753 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mplug/modeling_mplug.py
--rw-r--r--   0 runner    (1001) docker     (122)    34049 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mplug/mvit.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    21332 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mplug/predictor.py
--rw-r--r--   0 runner    (1001) docker     (122)     5536 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/mplug_for_all_tasks.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/multi_modal/multi_stage_diffusion/
--rw-r--r--   0 runner    (1001) docker     (122)      150 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/multi_stage_diffusion/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10208 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/multi_stage_diffusion/clip.py
--rw-r--r--   0 runner    (1001) docker     (122)    11503 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/multi_stage_diffusion/decoder.py
--rw-r--r--   0 runner    (1001) docker     (122)    28436 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/multi_stage_diffusion/gaussian_diffusion.py
--rw-r--r--   0 runner    (1001) docker     (122)    13487 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/multi_stage_diffusion/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     5276 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/multi_stage_diffusion/prior.py
--rw-r--r--   0 runner    (1001) docker     (122)     6898 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/multi_stage_diffusion/tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (122)    16675 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/multi_stage_diffusion/upsampler.py
--rw-r--r--   0 runner    (1001) docker     (122)     6565 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/multi_stage_diffusion/xglm.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/
--rw-r--r--   0 runner    (1001) docker     (122)      306 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/adaptor/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/adaptor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    18720 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/configuration_mmspeech.py
--rw-r--r--   0 runner    (1001) docker     (122)    16552 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/configuration_ofa.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/generate/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/generate/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1785 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/generate/incremental_decoding_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    21049 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/generate/multihead_attention.py
--rw-r--r--   0 runner    (1001) docker     (122)     5658 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/generate/ngram_repeat_block.py
--rw-r--r--   0 runner    (1001) docker     (122)    43439 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/generate/search.py
--rw-r--r--   0 runner    (1001) docker     (122)    41976 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/generate/sequence_generator.py
--rw-r--r--   0 runner    (1001) docker     (122)    16690 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/generate/token_generation_constraints.py
--rw-r--r--   0 runner    (1001) docker     (122)     4883 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/generate/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    44296 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/modeling_mmspeech.py
--rw-r--r--   0 runner    (1001) docker     (122)   100933 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/modeling_ofa.py
--rw-r--r--   0 runner    (1001) docker     (122)    13226 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/resnet.py
--rw-r--r--   0 runner    (1001) docker     (122)    15028 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/tokenization_ofa.py
--rw-r--r--   0 runner    (1001) docker     (122)     8004 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/tokenization_ofa_fast.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/utils/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      676 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/utils/constant.py
--rw-r--r--   0 runner    (1001) docker     (122)     1877 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/utils/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     8435 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa/vit.py
--rw-r--r--   0 runner    (1001) docker     (122)    24779 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa_for_all_tasks.py
--rw-r--r--   0 runner    (1001) docker     (122)    12983 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/ofa_for_text_to_image_synthesis_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/multi_modal/rleg/
--rw-r--r--   0 runner    (1001) docker     (122)      538 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/rleg/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4865 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/rleg/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     3410 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/rleg/rleg.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/multi_modal/soonet/
--rw-r--r--   0 runner    (1001) docker     (122)      678 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/soonet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9838 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/soonet/blocks.py
--rw-r--r--   0 runner    (1001) docker     (122)    11896 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/soonet/clip.py
--rw-r--r--   0 runner    (1001) docker     (122)     6165 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/soonet/model.py
--rw-r--r--   0 runner    (1001) docker     (122)    22584 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/soonet/swin_transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     4885 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/soonet/tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (122)     1597 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/soonet/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/multi_modal/team/
--rw-r--r--   0 runner    (1001) docker     (122)      140 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/team/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5181 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/team/team_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    11502 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/team/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/multi_modal/video_synthesis/
--rw-r--r--   0 runner    (1001) docker     (122)      538 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/video_synthesis/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    18477 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/video_synthesis/autoencoder.py
--rw-r--r--   0 runner    (1001) docker     (122)     9264 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/video_synthesis/diffusion.py
--rw-r--r--   0 runner    (1001) docker     (122)     9504 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    38498 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/video_synthesis/unet_sd.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/multi_modal/vldoc/
--rw-r--r--   0 runner    (1001) docker     (122)       93 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/vldoc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11339 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/vldoc/conv_fpn_trans.py
--rw-r--r--   0 runner    (1001) docker     (122)     6422 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/vldoc/convnext.py
--rw-r--r--   0 runner    (1001) docker     (122)    16687 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/vldoc/model.py
--rw-r--r--   0 runner    (1001) docker     (122)    47348 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/vldoc/modeling_layout_roberta.py
--rw-r--r--   0 runner    (1001) docker     (122)    20956 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/vldoc/processing.py
--rw-r--r--   0 runner    (1001) docker     (122)     4680 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/vldoc/tokenization.py
--rw-r--r--   0 runner    (1001) docker     (122)     7333 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/multi_modal/vldoc/transformer_local.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/T5/
--rw-r--r--   0 runner    (1001) docker     (122)      599 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/T5/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    67271 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/T5/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     7541 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/T5/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    21467 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/T5/text2text_generation.py
--rw-r--r--   0 runner    (1001) docker     (122)     5927 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/bart/
--rw-r--r--   0 runner    (1001) docker     (122)      112 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/bart/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3058 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/bart/text_error_correction.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/bert/
--rw-r--r--   0 runner    (1001) docker     (122)     1519 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/bert/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    40458 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/bert/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     7308 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/bert/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)     4113 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/bert/document_segmentation.py
--rw-r--r--   0 runner    (1001) docker     (122)      512 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/bert/fill_mask.py
--rw-r--r--   0 runner    (1001) docker     (122)     6074 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/bert/sentence_embedding.py
--rw-r--r--   0 runner    (1001) docker     (122)     7153 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/bert/siamese_uie.py
--rw-r--r--   0 runner    (1001) docker     (122)     1478 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/bert/text_classification.py
--rw-r--r--   0 runner    (1001) docker     (122)     1138 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/bert/text_ranking.py
--rw-r--r--   0 runner    (1001) docker     (122)     2140 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/bert/token_classification.py
--rw-r--r--   0 runner    (1001) docker     (122)     6904 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/bert/word_alignment.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/bloom/
--rw-r--r--   0 runner    (1001) docker     (122)      472 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/bloom/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      505 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/bloom/backbone.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/codegeex/
--rwxr-xr-x   0 runner    (1001) docker     (122)      730 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/codegeex/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    35473 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/codegeex/codegeex.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     3995 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/codegeex/codegeex_for_code_generation.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4008 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/codegeex/codegeex_for_code_translation.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     9997 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/codegeex/inference.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     6111 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/codegeex/tokenizer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/csanmt/
--rw-r--r--   0 runner    (1001) docker     (122)       96 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/csanmt/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    61868 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/csanmt/translation.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/deberta_v2/
--rw-r--r--   0 runner    (1001) docker     (122)     1771 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/deberta_v2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    47998 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/deberta_v2/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     6771 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/deberta_v2/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    10364 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/deberta_v2/fill_mask.py
--rw-r--r--   0 runner    (1001) docker     (122)    21421 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/deberta_v2/tokenization.py
--rw-r--r--   0 runner    (1001) docker     (122)    10698 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/deberta_v2/tokenization_fast.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/dgds/
--rw-r--r--   0 runner    (1001) docker     (122)      942 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/dgds/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7118 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/dgds/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     1656 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/dgds/document_grounded_dialog_generate.py
--rw-r--r--   0 runner    (1001) docker     (122)     1059 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/dgds/document_grounded_dialog_rerank.py
--rw-r--r--   0 runner    (1001) docker     (122)     2180 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/dgds/document_grounded_dialog_retrieval.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/fid_plug/
--rw-r--r--   0 runner    (1001) docker     (122)     1181 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/fid_plug/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    45058 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/fid_plug/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     5120 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/fid_plug/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)     6848 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/fid_plug/text_generation.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/gpt2/
--rw-r--r--   0 runner    (1001) docker     (122)      470 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      498 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt2/backbone.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/gpt3/
--rw-r--r--   0 runner    (1001) docker     (122)      852 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt3/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    15811 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt3/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     8971 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt3/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    51458 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt3/distributed_gpt3.py
--rw-r--r--   0 runner    (1001) docker     (122)     2847 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt3/text_generation.py
--rw-r--r--   0 runner    (1001) docker     (122)     2586 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt3/tokenizer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/gpt_moe/
--rw-r--r--   0 runner    (1001) docker     (122)      874 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt_moe/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    13129 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt_moe/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     5198 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt_moe/checkpointing.py
--rw-r--r--   0 runner    (1001) docker     (122)     4930 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt_moe/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    49316 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt_moe/distributed_gpt_moe.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/gpt_moe/moe/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt_moe/moe/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1194 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt_moe/moe/experts.py
--rw-r--r--   0 runner    (1001) docker     (122)     3504 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt_moe/moe/layer.py
--rw-r--r--   0 runner    (1001) docker     (122)     2553 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt_moe/moe/mappings.py
--rw-r--r--   0 runner    (1001) docker     (122)    23756 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt_moe/moe/sharded_moe.py
--rw-r--r--   0 runner    (1001) docker     (122)     4110 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt_moe/moe/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2717 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt_moe/text_generation.py
--rw-r--r--   0 runner    (1001) docker     (122)     2277 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt_moe/tokenizer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/gpt_neo/
--rw-r--r--   0 runner    (1001) docker     (122)      474 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt_neo/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      518 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/gpt_neo/backbone.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/heads/
--rw-r--r--   0 runner    (1001) docker     (122)      661 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    25248 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/heads/crf_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     4381 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/heads/fill_mask_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     5145 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/heads/infromation_extraction_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     2056 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/heads/text_classification_head.py
--rw-r--r--   0 runner    (1001) docker     (122)      964 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/heads/text_generation_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     2029 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/heads/text_ranking_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     2596 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/heads/token_classification_head.py
--rw-r--r--   0 runner    (1001) docker     (122)      948 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/heads/torch_pretrain_head.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/hf_transformers/
--rw-r--r--   0 runner    (1001) docker     (122)      488 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/hf_transformers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4897 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/hf_transformers/backbone.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/lstm/
--rw-r--r--   0 runner    (1001) docker     (122)      610 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/lstm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1312 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/lstm/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     2187 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/lstm/token_classification.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/megatron_bert/
--rw-r--r--   0 runner    (1001) docker     (122)      688 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/megatron_bert/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    39864 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/megatron_bert/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     6599 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/megatron_bert/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    12452 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/megatron_bert/fill_mask.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/
--rw-r--r--   0 runner    (1001) docker     (122)      572 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    28115 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/arguments.py
--rw-r--r--   0 runner    (1001) docker     (122)    28361 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/blocklm_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    17995 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/configure_data.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/
--rw-r--r--   0 runner    (1001) docker     (122)    13656 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    20350 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/corpora.py
--rw-r--r--   0 runner    (1001) docker     (122)    45773 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/datasets.py
--rw-r--r--   0 runner    (1001) docker     (122)     3342 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/extraction.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     8459 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/file_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     9797 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/lazy_loader.py
--rw-r--r--   0 runner    (1001) docker     (122)     7221 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/samplers.py
--rw-r--r--   0 runner    (1001) docker     (122)     4661 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/sp_tokenizer.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    53304 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/tokenization.py
--rw-r--r--   0 runner    (1001) docker     (122)    14551 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/tokenization_gpt2.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    15773 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/wordpiece.py
--rw-r--r--   0 runner    (1001) docker     (122)    21669 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/generation_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    16247 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/mglm_for_text_summarization.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/model/
--rwxr-xr-x   0 runner    (1001) docker     (122)      984 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/model/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     5443 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/model/distributed.py
--rw-r--r--   0 runner    (1001) docker     (122)     9967 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/model/downstream.py
--rw-r--r--   0 runner    (1001) docker     (122)    70249 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/model/modeling_bert.py
--rw-r--r--   0 runner    (1001) docker     (122)     8743 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/model/modeling_glm.py
--rw-r--r--   0 runner    (1001) docker     (122)     2531 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/model/prompt.py
--rw-r--r--   0 runner    (1001) docker     (122)    48886 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/model/transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     1955 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/process_grid.py
--rw-r--r--   0 runner    (1001) docker     (122)      203 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/run_test.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/test/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/test/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      954 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/test/test_block.py
--rw-r--r--   0 runner    (1001) docker     (122)      708 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/test/test_rel_shift.py
--rw-r--r--   0 runner    (1001) docker     (122)    17815 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/train_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    19029 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/mglm/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/palm_v2/
--rw-r--r--   0 runner    (1001) docker     (122)     1268 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/palm_v2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5532 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/palm_v2/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    29453 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/palm_v2/dureader_eval.py
--rw-r--r--   0 runner    (1001) docker     (122)    55450 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/palm_v2/text_generation.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/peer/
--rw-r--r--   0 runner    (1001) docker     (122)     1194 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/peer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    55780 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/peer/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)    11716 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/peer/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)     6166 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/peer/sas_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     4933 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/peer/text_classification.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/plug/
--rwxr-xr-x   0 runner    (1001) docker     (122)     3321 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/plug/AnnealingLR.py
--rw-r--r--   0 runner    (1001) docker     (122)      660 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/plug/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    41209 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/plug/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)    11797 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/plug/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    10983 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/plug/distributed_plug.py
--rw-r--r--   0 runner    (1001) docker     (122)     8427 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/plug/generator.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/plug_mental/
--rw-r--r--   0 runner    (1001) docker     (122)     1359 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/plug_mental/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7671 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/plug_mental/adv_utils.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    47486 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/plug_mental/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     7956 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/plug_mental/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    10881 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/plug_mental/text_classification.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/ponet/
--rw-r--r--   0 runner    (1001) docker     (122)     1490 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/ponet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    37918 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/ponet/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     6366 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/ponet/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)     4176 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/ponet/document_segmentation.py
--rw-r--r--   0 runner    (1001) docker     (122)    11011 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/ponet/fill_mask.py
--rw-r--r--   0 runner    (1001) docker     (122)     7147 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/ponet/tokenization.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/space/
--rw-r--r--   0 runner    (1001) docker     (122)      987 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1208 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)     3832 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space/dialog_intent_prediction.py
--rw-r--r--   0 runner    (1001) docker     (122)     4354 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space/dialog_modeling.py
--rw-r--r--   0 runner    (1001) docker     (122)    16648 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space/dialog_state_tracking.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/space/model/
--rw-r--r--   0 runner    (1001) docker     (122)      421 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space/model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10281 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space/model/gen_unified_transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)    10757 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space/model/generator.py
--rw-r--r--   0 runner    (1001) docker     (122)     7505 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space/model/intent_unified_transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     3032 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space/model/model_base.py
--rw-r--r--   0 runner    (1001) docker     (122)     1189 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space/model/tokenization_space.py
--rw-r--r--   0 runner    (1001) docker     (122)    11941 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space/model/unified_transformer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/space/modules/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space/modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2398 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space/modules/embedder.py
--rw-r--r--   0 runner    (1001) docker     (122)      956 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space/modules/feedforward.py
--rw-r--r--   0 runner    (1001) docker     (122)     1721 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space/modules/functions.py
--rw-r--r--   0 runner    (1001) docker     (122)     3397 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space/modules/multihead_attention.py
--rw-r--r--   0 runner    (1001) docker     (122)     1922 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space/modules/transformer_block.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/space_T_cn/
--rw-r--r--   0 runner    (1001) docker     (122)      529 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space_T_cn/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    44438 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space_T_cn/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     5194 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space_T_cn/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    30117 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space_T_cn/table_question_answering.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/space_T_en/
--rw-r--r--   0 runner    (1001) docker     (122)      492 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space_T_en/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3715 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/space_T_en/text_to_sql.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/structbert/
--rw-r--r--   0 runner    (1001) docker     (122)     1674 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/structbert/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7673 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/structbert/adv_utils.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    40848 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/structbert/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     7686 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/structbert/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    27203 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/structbert/faq_question_answering.py
--rw-r--r--   0 runner    (1001) docker     (122)    12581 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/structbert/fill_mask.py
--rw-r--r--   0 runner    (1001) docker     (122)    11862 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/structbert/text_classification.py
--rw-r--r--   0 runner    (1001) docker     (122)    11074 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/structbert/token_classification.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/task_models/
--rw-r--r--   0 runner    (1001) docker     (122)     1453 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/task_models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5323 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/task_models/feature_extraction.py
--rw-r--r--   0 runner    (1001) docker     (122)     2698 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/task_models/fill_mask.py
--rw-r--r--   0 runner    (1001) docker     (122)      766 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/task_models/information_extraction.py
--rw-r--r--   0 runner    (1001) docker     (122)    28578 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/task_models/task_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     1912 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/task_models/text_classification.py
--rw-r--r--   0 runner    (1001) docker     (122)     3526 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/task_models/text_generation.py
--rw-r--r--   0 runner    (1001) docker     (122)     2092 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/task_models/text_ranking.py
--rw-r--r--   0 runner    (1001) docker     (122)     5116 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/task_models/token_classification.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/unite/
--rw-r--r--   0 runner    (1001) docker     (122)      622 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/unite/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      412 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/unite/configuration_unite.py
--rw-r--r--   0 runner    (1001) docker     (122)    17494 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/unite/modeling_unite.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/use/
--rw-r--r--   0 runner    (1001) docker     (122)      545 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/use/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5133 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/use/transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     5795 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/use/user_satisfaction_estimation.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/veco/
--rw-r--r--   0 runner    (1001) docker     (122)     1479 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/veco/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4031 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/veco/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     1192 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/veco/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)     4278 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/veco/fill_mask.py
--rw-r--r--   0 runner    (1001) docker     (122)     6643 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/veco/text_classification.py
--rw-r--r--   0 runner    (1001) docker     (122)     4129 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/veco/token_classification.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/nlp/xlm_roberta/
--rw-r--r--   0 runner    (1001) docker     (122)     1156 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/xlm_roberta/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    42932 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/xlm_roberta/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     7697 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/nlp/xlm_roberta/configuration.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/science/
--rw-r--r--   0 runner    (1001) docker     (122)      491 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/science/unifold/
--rw-r--r--   0 runner    (1001) docker     (122)       46 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    24057 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/config.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/science/unifold/data/
--rw-r--r--   0 runner    (1001) docker     (122)      634 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    49139 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/data/data_ops.py
--rw-r--r--   0 runner    (1001) docker     (122)    19694 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/data/msa_pairing.py
--rw-r--r--   0 runner    (1001) docker     (122)     8941 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/data/process.py
--rw-r--r--   0 runner    (1001) docker     (122)    14792 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/data/process_multimer.py
--rw-r--r--   0 runner    (1001) docker     (122)    11422 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/data/protein.py
--rw-r--r--   0 runner    (1001) docker     (122)    42023 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/data/residue_constants.py
--rw-r--r--   0 runner    (1001) docker     (122)     4628 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/data/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    19250 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     2307 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/science/unifold/modules/
--rw-r--r--   0 runner    (1001) docker     (122)      187 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    17066 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/modules/alphafold.py
--rw-r--r--   0 runner    (1001) docker     (122)    12323 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/modules/attentions.py
--rw-r--r--   0 runner    (1001) docker     (122)     5403 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/modules/auxillary_heads.py
--rw-r--r--   0 runner    (1001) docker     (122)    10676 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/modules/common.py
--rw-r--r--   0 runner    (1001) docker     (122)     5814 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/modules/confidence.py
--rw-r--r--   0 runner    (1001) docker     (122)     8771 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/modules/embedders.py
--rw-r--r--   0 runner    (1001) docker     (122)    11078 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/modules/evoformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     6820 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/modules/featurization.py
--rw-r--r--   0 runner    (1001) docker     (122)    18025 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/modules/frame.py
--rw-r--r--   0 runner    (1001) docker     (122)    18476 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/modules/structure_module.py
--rw-r--r--   0 runner    (1001) docker     (122)     9771 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/modules/template.py
--rw-r--r--   0 runner    (1001) docker     (122)     5623 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/modules/triangle_multiplication.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/science/unifold/msa/
--rw-r--r--   0 runner    (1001) docker     (122)       46 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/msa/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    17940 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/msa/mmcif.py
--rw-r--r--   0 runner    (1001) docker     (122)     3128 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/msa/msa_identifiers.py
--rw-r--r--   0 runner    (1001) docker     (122)    23503 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/msa/parsers.py
--rw-r--r--   0 runner    (1001) docker     (122)    11644 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/msa/pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    45466 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/msa/templates.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/models/science/unifold/msa/tools/
--rw-r--r--   0 runner    (1001) docker     (122)      639 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/msa/tools/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6218 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/msa/tools/hhblits.py
--rw-r--r--   0 runner    (1001) docker     (122)     3991 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/msa/tools/hhsearch.py
--rw-r--r--   0 runner    (1001) docker     (122)     5098 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/msa/tools/hmmbuild.py
--rw-r--r--   0 runner    (1001) docker     (122)     5013 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/msa/tools/hmmsearch.py
--rw-r--r--   0 runner    (1001) docker     (122)     8662 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/msa/tools/jackhmmer.py
--rw-r--r--   0 runner    (1001) docker     (122)     3752 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/msa/tools/kalign.py
--rw-r--r--   0 runner    (1001) docker     (122)     1255 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/msa/tools/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2892 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/models/science/unifold/msa/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/
--rw-r--r--   0 runner    (1001) docker     (122)       84 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/audio/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/audio/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      346 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/audio/asr_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/auth/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/auth/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1028 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/auth/auth_config.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/context/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/context/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3321 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/context/dataset_context_config.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/data_files/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/data_files/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5110 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/data_files/data_files_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/data_loader/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/data_loader/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6583 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/data_loader/data_loader.py
--rw-r--r--   0 runner    (1001) docker     (122)     5767 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/data_loader/data_loader_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/
--rw-r--r--   0 runner    (1001) docker     (122)      111 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/
--rw-r--r--   0 runner    (1001) docker     (122)     4817 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/
--rw-r--r--   0 runner    (1001) docker     (122)      730 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1737 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/asr_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     8512 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_farfield_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     6251 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)    13230 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_processor.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/
--rw-r--r--   0 runner    (1001) docker     (122)      541 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1264 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/bad_image_detecting_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)      791 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/
--rw-r--r--   0 runner    (1001) docker     (122)      134 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4708 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/build.py
--rw-r--r--   0 runner    (1001) docker     (122)      945 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/collate_batch.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/
--rw-r--r--   0 runner    (1001) docker     (122)      177 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3815 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/coco.py
--rw-r--r--   0 runner    (1001) docker     (122)    16180 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/mosaic_wrapper.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/
--rw-r--r--   0 runner    (1001) docker     (122)      934 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/
--rw-r--r--   0 runner    (1001) docker     (122)      544 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11572 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/coco_eval.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/
--rw-r--r--   0 runner    (1001) docker     (122)      312 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2622 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/distributed.py
--rw-r--r--   0 runner    (1001) docker     (122)     4858 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/grouped_batch_sampler.py
--rw-r--r--   0 runner    (1001) docker     (122)     1469 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/iteration_based_batch_sampler.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/
--rw-r--r--   0 runner    (1001) docker     (122)      196 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1086 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/build.py
--rw-r--r--   0 runner    (1001) docker     (122)     2509 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/transforms.py
--rw-r--r--   0 runner    (1001) docker     (122)     1511 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/easycv_base.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/face_2d_keypoins/
--rw-r--r--   0 runner    (1001) docker     (122)      512 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/face_2d_keypoins/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1459 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/face_2d_keypoins/face_2d_keypoints_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     2141 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/gopro_image_deblurring_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/hand_2d_keypoints/
--rw-r--r--   0 runner    (1001) docker     (122)      536 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/hand_2d_keypoints/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1498 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/hand_2d_keypoints/hand_2d_keypoints_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/human_wholebody_keypoint/
--rw-r--r--   0 runner    (1001) docker     (122)      556 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/human_wholebody_keypoint/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1565 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/human_wholebody_keypoint/human_wholebody_keypoint_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_classification/
--rw-r--r--   0 runner    (1001) docker     (122)      488 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_classification/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1393 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_classification/classification_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/
--rw-r--r--   0 runner    (1001) docker     (122)      539 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2196 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/image_colorization_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/
--rw-r--r--   0 runner    (1001) docker     (122)      529 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2968 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/aug.py
--rw-r--r--   0 runner    (1001) docker     (122)    12152 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)    12574 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_instance_segmentation_coco_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/
--rw-r--r--   0 runner    (1001) docker     (122)      577 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1091 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/data_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1629 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/image_portrait_enhancement_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/
--rw-r--r--   0 runner    (1001) docker     (122)      615 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1487 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/image_quality_assessment_degradation_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/
--rw-r--r--   0 runner    (1001) docker     (122)      583 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1194 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/image_quality_assessment_mos_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_semantic_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      483 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_semantic_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1792 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_semantic_segmentation/segmentation_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     4849 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/language_guided_video_summarization_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     6225 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/mgeo_ranking_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      559 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6171 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     4013 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/sampler.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/object_detection/
--rw-r--r--   0 runner    (1001) docker     (122)      536 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/object_detection/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4294 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/object_detection/detection_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/
--rw-r--r--   0 runner    (1001) docker     (122)      161 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1589 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/augmenter.py
--rw-r--r--   0 runner    (1001) docker     (122)     4945 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/data_loader.py
--rw-r--r--   0 runner    (1001) docker     (122)     5437 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/image_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/
--rw-r--r--   0 runner    (1001) docker     (122)       40 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7547 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/iou_evaluator.py
--rw-r--r--   0 runner    (1001) docker     (122)     3416 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/quad_measurer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/
--rw-r--r--   0 runner    (1001) docker     (122)      309 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3172 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/augment_data.py
--rw-r--r--   0 runner    (1001) docker     (122)      971 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/data_process.py
--rw-r--r--   0 runner    (1001) docker     (122)     5818 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_border_map.py
--rw-r--r--   0 runner    (1001) docker     (122)     2000 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_icdar_data.py
--rw-r--r--   0 runner    (1001) docker     (122)     3512 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_seg_detection_data.py
--rw-r--r--   0 runner    (1001) docker     (122)      707 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/normalize_image.py
--rw-r--r--   0 runner    (1001) docker     (122)     4912 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/random_crop_data.py
--rw-r--r--   0 runner    (1001) docker     (122)     2297 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_recognition_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     2017 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/reds_image_deblurring_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      599 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    16767 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/referring_video_object_segmentation_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     8725 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/transformers.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/
--rw-r--r--   0 runner    (1001) docker     (122)      545 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1475 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/data_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1984 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/sidd_image_denoising_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     2684 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/transforms.py
--rw-r--r--   0 runner    (1001) docker     (122)     5114 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     1635 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/torch_custom_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     2664 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/veco_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/
--rw-r--r--   0 runner    (1001) docker     (122)      573 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1390 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/data_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/video_frame_interpolation_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/
--rw-r--r--   0 runner    (1001) docker     (122)      543 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      809 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/video_stabilization_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     2624 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_summarization_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/
--rw-r--r--   0 runner    (1001) docker     (122)      553 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2370 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/video_super_resolution_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     4123 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/dataset_cls/dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/download/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/download/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    16700 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/download/dataset_builder.py
--rw-r--r--   0 runner    (1001) docker     (122)      609 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/download/download_config.py
--rw-r--r--   0 runner    (1001) docker     (122)     2487 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/download/download_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/meta/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/meta/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1772 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/meta/data_meta_config.py
--rw-r--r--   0 runner    (1001) docker     (122)     8288 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/meta/data_meta_manager.py
--rw-r--r--   0 runner    (1001) docker     (122)    35443 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/ms_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/task_datasets/
--rw-r--r--   0 runner    (1001) docker     (122)     1072 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/task_datasets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      408 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/task_datasets/gopro_image_deblurring_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)      401 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/task_datasets/reds_image_deblurring_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)      404 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/task_datasets/sidd_image_denoising.py
--rw-r--r--   0 runner    (1001) docker     (122)      410 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/task_datasets/torch_base_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)      404 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/task_datasets/video_summarization_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/msdatasets/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7610 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/utils/dataset_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1026 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/utils/delete_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     6124 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/utils/oss_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2496 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/msdatasets/utils/upload_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/ops/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/ops/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/ops/ailut/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/ops/ailut/Ailut/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/ops/ailut/Ailut/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/ops/ailut/Ailut/csrc/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/ops/ailut/Ailut/csrc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      105 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/ops/ailut/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4314 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/ops/ailut/pyinterfaces.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/ops/quadtree_attention/
--rw-r--r--   0 runner    (1001) docker     (122)      106 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/ops/quadtree_attention/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/ops/quadtree_attention/functions/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/ops/quadtree_attention/functions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2925 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/ops/quadtree_attention/functions/quadtree_attention.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/ops/quadtree_attention/modules/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/ops/quadtree_attention/modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    13956 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/ops/quadtree_attention/modules/quadtree_attention.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/ops/quadtree_attention/src/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/ops/quadtree_attention/src/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/outputs/
--rw-r--r--   0 runner    (1001) docker     (122)      132 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/outputs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      908 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/outputs/cv_outputs.py
--rw-r--r--   0 runner    (1001) docker     (122)    19037 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/outputs/nlp_outputs.py
--rw-r--r--   0 runner    (1001) docker     (122)    41351 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/outputs/outputs.py
--rw-r--r--   0 runner    (1001) docker     (122)     9505 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipeline_inputs.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/pipelines/
--rw-r--r--   0 runner    (1001) docker     (122)      242 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/pipelines/audio/
--rw-r--r--   0 runner    (1001) docker     (122)     1498 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/audio/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7481 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/audio/ans_dfsmn_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5016 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/audio/ans_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    27033 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/audio/asr_inference_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3189 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/audio/asr_wenet_inference_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4357 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/audio/inverse_text_processing_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3830 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/audio/kws_farfield_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     7276 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/audio/kws_kwsbp_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5555 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/audio/linear_aec_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     7716 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/audio/lm_infer_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     6130 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/audio/punctuation_processing_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2560 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/audio/separation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    11293 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/audio/speaker_diarization_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4118 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/audio/speaker_verification_light_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    10172 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/audio/speaker_verification_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1789 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/audio/text_to_speech_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    11582 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/audio/timestamp_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     9340 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/audio/voice_activity_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    22128 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     7279 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/pipelines/cv/
--rw-r--r--   0 runner    (1001) docker     (122)    17270 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2545 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/action_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4852 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/action_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4024 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/animal_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2375 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/arc_face_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2614 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/bad_image_detecting_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     9581 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/body_2d_keypoints_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    14219 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/body_3d_keypoints_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4740 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/card_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5307 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/cmdssl_video_embedding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2528 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/content_check_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5055 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/controllable_image_generation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5942 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/crowd_counting_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5136 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/ddcolor_image_colorization_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1965 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/ddpm_semantic_segmentation_pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/pipelines/cv/easycv_pipelines/
--rw-r--r--   0 runner    (1001) docker     (122)      959 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/easycv_pipelines/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4476 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/easycv_pipelines/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     2241 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/easycv_pipelines/detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     8202 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/easycv_pipelines/face_2d_keypoints_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2491 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/easycv_pipelines/human_wholebody_keypoint_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1599 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/easycv_pipelines/segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2279 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/face_attribute_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3980 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/face_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1531 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/face_emotion_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1648 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/face_human_hand_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2894 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/face_image_generation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3076 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/face_liveness_ir_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3337 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/face_liveness_xc_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     7686 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/face_processing_base_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3630 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/face_quality_assessment_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3309 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/face_recognition_onnx_fm_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3274 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/face_recognition_onnx_ir_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2692 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/face_recognition_ood_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2498 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/face_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    14798 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/face_reconstruction_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2245 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/facial_expression_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2333 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/facial_landmark_confidence_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4030 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/general_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2055 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/hand_2d_keypoints_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1360 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/hand_static_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2982 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/hicossl_video_embedding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4372 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/human_reconstruction_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1399 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_body_reshaping_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2919 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_bts_depth_estimation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5251 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_cartoon_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5857 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_classification_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3296 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_color_enhance_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4441 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_colorization_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2648 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_debanding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4624 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_deblur_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3517 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_defrcn_fewshot_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4160 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_denoise_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1943 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_depth_estimation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1852 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4134 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_driving_perception_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2317 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_face_fusion_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4866 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_human_parsing_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5854 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_inpainting_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4673 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_inpainting_sdv2_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3903 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_instance_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5929 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_matching_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2610 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_matting_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2796 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_mvs_depth_estimation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2770 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_open_vocabulary_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     6113 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_paintbyexample_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4807 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_panoptic_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     8567 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_portrait_enhancement_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3549 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_quality_assessment_degradation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2835 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_quality_assessment_man_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2801 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_quality_assessment_mos_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2026 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_reid_person_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1702 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_restoration_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1644 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_salient_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3207 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_semantic_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2237 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_skychange_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2839 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_structured_model_probing_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4621 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_style_transfer_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3138 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_super_resolution_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     9718 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_to_image_generate_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    12737 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/image_to_image_translation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1883 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/indoor_layout_estimation_pipeline.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     9872 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/language_guided_video_summarization_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4570 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/license_plate_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4182 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/lineless_table_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5276 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/live_category_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2757 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/mask_face_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2787 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/maskdino_instance_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3670 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/mobile_image_super_resolution_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1857 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/mog_face_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4844 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/motion_generation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2403 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/movie_scene_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1953 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/mtcnn_face_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3328 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/nerf_recon_acc_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     6018 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/object_detection_3d_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    11033 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/ocr_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2500 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/ocr_recognition_pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/
--rw-r--r--   0 runner    (1001) docker     (122)      966 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      720 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/model_convnext_transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)    19821 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/model_dla34.py
--rw-r--r--   0 runner    (1001) docker     (122)     8768 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/model_resnet18_half.py
--rw-r--r--   0 runner    (1001) docker     (122)     5286 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/model_resnet_mutex_v4_linewithchar.py
--rw-r--r--   0 runner    (1001) docker     (122)    15421 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/model_vlpt.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/ocr_modules/
--rw-r--r--   0 runner    (1001) docker     (122)      550 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/ocr_modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6209 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/ocr_modules/convnext.py
--rw-r--r--   0 runner    (1001) docker     (122)    11644 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/ocr_modules/timm_tinyc.py
--rw-r--r--   0 runner    (1001) docker     (122)     1560 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/ocr_modules/vitstr.py
--rw-r--r--   0 runner    (1001) docker     (122)    38278 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/ops.py
--rw-r--r--   0 runner    (1001) docker     (122)    19004 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/resnet18_v1.py
--rw-r--r--   0 runner    (1001) docker     (122)    11371 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/resnet_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    11168 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/table_process.py
--rw-r--r--   0 runner    (1001) docker     (122)     7971 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     3029 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/panorama_depth_estimation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4199 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1457 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/product_retrieval_embedding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1451 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/product_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2020 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/realtime_video_object_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     9237 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/referring_video_object_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1956 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/retina_face_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1760 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/shop_segmentation_pipleline.py
--rw-r--r--   0 runner    (1001) docker     (122)    12132 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/skin_retouching_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4446 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/table_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4902 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/tbs_detection_pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/pipelines/cv/tbs_detection_utils/
--rw-r--r--   0 runner    (1001) docker     (122)       10 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/tbs_detection_utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    15542 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/tbs_detection_utils/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1841 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/text_driven_segmentation_pipleline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3246 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/tinynas_classification_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3244 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/tinynas_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1888 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/ulfd_face_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    13776 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/video_category_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     6097 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/video_colorization_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     7757 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/video_deinterlace_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1565 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/video_depth_estimation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    24271 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/video_frame_interpolation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3635 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/video_human_matting_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1719 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/video_inpainting_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     9652 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/video_instance_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3294 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/video_multi_object_tracking_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4724 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/video_object_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4683 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/video_panoptic_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3436 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/video_single_object_tracking_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4647 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/video_stabilization_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4289 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/video_summarization_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     7078 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/video_super_resolution_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     7564 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/vidt_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5240 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/virtual_try_on_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3986 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/vision_efficient_tuning_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2180 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/vision_middleware_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4762 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/vop_retrieval_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5787 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/cv/vop_retrieval_se_pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/
--rw-r--r--   0 runner    (1001) docker     (122)     2927 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2366 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/asr_pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/diffusers_wrapped/
--rw-r--r--   0 runner    (1001) docker     (122)      636 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/diffusers_wrapped/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1993 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/diffusers_wrapped/diffusers_pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/
--rw-r--r--   0 runner    (1001) docker     (122)      718 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11610 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2870 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/
--rw-r--r--   0 runner    (1001) docker     (122)      585 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    15631 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py
--rw-r--r--   0 runner    (1001) docker     (122)    18987 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2311 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/document_vl_embedding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1093 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     9352 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/gridvlp_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2634 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/image_captioning_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1612 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/image_text_retrieval_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     7869 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/mgeo_ranking_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1648 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/multi_modal_embedding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/ocr_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     8561 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1848 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/sudoku_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1059 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/team_multi_modal_similarity_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1789 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/text2sql_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2064 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/text_to_image_synthesis_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3160 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/text_to_video_synthesis_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2150 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/video_captioning_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1329 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/video_multi_modal_embedding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1956 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/video_question_answering_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/visual_entailment_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1781 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/visual_grounding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2396 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/multi_modal/visual_question_answering_pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/pipelines/nlp/
--rw-r--r--   0 runner    (1001) docker     (122)     6502 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6939 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/automatic_post_editing_pipeline.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     2202 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/codegeex_code_generation_pipeline.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     2885 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/codegeex_code_translation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2251 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/conversational_text_to_sql_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2464 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/dialog_intent_prediction_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2308 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/dialog_modeling_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     7550 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/dialog_state_tracking_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2218 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/distributed_gpt3_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1851 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/distributed_gpt_moe_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4425 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/distributed_plug_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2641 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/document_grounded_dialog_generate_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    25886 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/document_grounded_dialog_rerank_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5367 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/document_grounded_dialog_retrieval_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     9648 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/document_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     6156 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/extractive_summarization_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3348 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/faq_question_answering_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2437 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/fasttext_text_classification_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3145 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/feature_extraction_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     8451 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/fid_dialogue_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4542 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/fill_mask_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2383 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/information_extraction_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     6363 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/interactive_translation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     9928 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/language_identification_pipline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1776 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/mglm_text_summarization_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2842 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/named_entity_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3071 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/sentence_embedding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    14210 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/siamese_uie_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2566 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/summarization_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    16202 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/table_question_answering_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     6797 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/text_classification_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3490 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/text_error_correction_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     7622 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/text_generation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2764 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/text_ranking_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     6286 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/token_classification_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4312 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/translation_evaluation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5586 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/translation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2339 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/translation_quality_estimation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4473 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/user_satisfaction_estimation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2713 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/word_alignment_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4523 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/word_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5762 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/nlp/zero_shot_classification_pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/pipelines/science/
--rw-r--r--   0 runner    (1001) docker     (122)      538 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/science/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8255 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/science/protein_structure_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3492 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/pipelines/util.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/preprocessors/
--rw-r--r--   0 runner    (1001) docker     (122)     5402 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12576 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/asr.py
--rw-r--r--   0 runner    (1001) docker     (122)     8680 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/audio.py
--rw-r--r--   0 runner    (1001) docker     (122)    15219 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/base.py
--rw-r--r--   0 runner    (1001) docker     (122)      812 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/builder.py
--rw-r--r--   0 runner    (1001) docker     (122)     6982 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/common.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/preprocessors/cv/
--rw-r--r--   0 runner    (1001) docker     (122)     1795 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/cv/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7446 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/cv/action_detection_mapper.py
--rw-r--r--   0 runner    (1001) docker     (122)     1126 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/cv/bad_image_detecting_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     7789 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/cv/controllable_image_generation.py
--rw-r--r--   0 runner    (1001) docker     (122)    20909 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/cv/cv2_transforms.py
--rw-r--r--   0 runner    (1001) docker     (122)    12826 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/cv/image_classification_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     1250 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/cv/image_quality_assessment_man.py
--rw-r--r--   0 runner    (1001) docker     (122)     2113 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/cv/image_quality_assessment_mos.py
--rw-r--r--   0 runner    (1001) docker     (122)     2790 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/cv/image_restoration_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     2625 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/cv/mmcls_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     3050 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/cv/timer.py
--rw-r--r--   0 runner    (1001) docker     (122)     3606 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/cv/util.py
--rw-r--r--   0 runner    (1001) docker     (122)     1501 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/cv/video_stabilization.py
--rw-r--r--   0 runner    (1001) docker     (122)     8579 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/cv/video_super_resolution.py
--rw-r--r--   0 runner    (1001) docker     (122)    13406 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/image.py
--rw-r--r--   0 runner    (1001) docker     (122)     4884 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/kws.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/preprocessors/movie_scene_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      483 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/movie_scene_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10809 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/movie_scene_segmentation/transforms.py
--rw-r--r--   0 runner    (1001) docker     (122)    23079 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/multi_modal.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/
--rw-r--r--   0 runner    (1001) docker     (122)     5795 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      762 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/bert_seq_cls_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (122)     2572 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/dialog_classification_use_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     4102 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     4391 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     4086 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)    11004 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/document_segmentation_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     6844 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/faq_question_answering_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     3304 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/feature_extraction_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)    12086 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/fill_mask_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     8199 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/mgeo_ranking_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     1168 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/mglm_summarization_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     1799 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/relation_extraction_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     4023 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/sentence_embedding_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     1424 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/siamese_uie_preprocessor.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space/
--rw-r--r--   0 runner    (1001) docker     (122)     1219 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1908 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space/args.py
--rw-r--r--   0 runner    (1001) docker     (122)     1555 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space/batch.py
--rw-r--r--   0 runner    (1001) docker     (122)     3629 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space/data_loader.py
--rw-r--r--   0 runner    (1001) docker     (122)     2701 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space/dialog_intent_prediction_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     2789 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space/dialog_modeling_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     5457 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space/dialog_state_tracking_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)    56779 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space/dst_processors.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space/fields/
--rw-r--r--   0 runner    (1001) docker     (122)      592 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space/fields/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    33884 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space/fields/gen_field.py
--rw-r--r--   0 runner    (1001) docker     (122)    42465 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space/fields/intent_field.py
--rw-r--r--   0 runner    (1001) docker     (122)     1147 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space/lazy_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     1935 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space/preprocess.py
--rw-r--r--   0 runner    (1001) docker     (122)     1610 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space/sampler.py
--rw-r--r--   0 runner    (1001) docker     (122)     2084 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space/tensorlistdataset.py
--rw-r--r--   0 runner    (1001) docker     (122)    24803 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space/tokenizer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_cn/
--rw-r--r--   0 runner    (1001) docker     (122)      654 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_cn/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_cn/fields/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_cn/fields/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4936 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_cn/fields/database.py
--rw-r--r--   0 runner    (1001) docker     (122)    15867 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_cn/fields/schema_link.py
--rw-r--r--   0 runner    (1001) docker     (122)     4888 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_cn/fields/struct.py
--rw-r--r--   0 runner    (1001) docker     (122)     4175 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_cn/table_question_answering_preprocessor.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_en/
--rw-r--r--   0 runner    (1001) docker     (122)      846 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_en/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4902 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_en/conversational_text_to_sql_preprocessor.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_en/fields/
--rw-r--r--   0 runner    (1001) docker     (122)      818 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_en/fields/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    21521 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_en/fields/common_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    12500 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_en/fields/parse.py
--rw-r--r--   0 runner    (1001) docker     (122)     1380 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_en/fields/preprocess_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     2111 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_en/fields/process_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     6422 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/text_classification_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     2298 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/text_error_correction.py
--rw-r--r--   0 runner    (1001) docker     (122)    13896 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/text_generation_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     3834 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/text_ranking_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)    19473 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/token_classification_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     1640 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/token_classification_thai_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     1238 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/token_classification_viet_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     4165 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/transformers_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (122)     3566 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/translation_evaluation_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     3622 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     4962 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/word_alignment_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     2584 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/nlp/zero_shot_classification_preprocessor.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/
--rw-r--r--   0 runner    (1001) docker     (122)      762 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4839 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/asr.py
--rw-r--r--   0 runner    (1001) docker     (122)    11909 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     4172 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/image_captioning.py
--rw-r--r--   0 runner    (1001) docker     (122)     6461 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/image_classification.py
--rw-r--r--   0 runner    (1001) docker     (122)     5740 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/ocr_recognition.py
--rw-r--r--   0 runner    (1001) docker     (122)     4603 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/sudoku.py
--rw-r--r--   0 runner    (1001) docker     (122)     5210 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/summarization.py
--rw-r--r--   0 runner    (1001) docker     (122)    16449 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/text2sql.py
--rw-r--r--   0 runner    (1001) docker     (122)     5368 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/text_classification.py
--rw-r--r--   0 runner    (1001) docker     (122)     2164 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/text_to_image_synthesis.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3221 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/utils/audio_helper.py
--rw-r--r--   0 runner    (1001) docker     (122)     8936 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/utils/bridge_content_encoder.py
--rw-r--r--   0 runner    (1001) docker     (122)     6809 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/utils/collate.py
--rw-r--r--   0 runner    (1001) docker     (122)      660 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/utils/constant.py
--rw-r--r--   0 runner    (1001) docker     (122)     3421 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/utils/get_tables.py
--rw-r--r--   0 runner    (1001) docker     (122)     1302 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/utils/random_help.py
--rw-r--r--   0 runner    (1001) docker     (122)     5459 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/utils/text2phone.py
--rw-r--r--   0 runner    (1001) docker     (122)    19116 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/utils/transforms.py
--rw-r--r--   0 runner    (1001) docker     (122)     9581 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/utils/vision_helper.py
--rw-r--r--   0 runner    (1001) docker     (122)     7664 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/visual_entailment.py
--rw-r--r--   0 runner    (1001) docker     (122)     8484 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/visual_grounding.py
--rw-r--r--   0 runner    (1001) docker     (122)     6796 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/ofa/visual_question_answering.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/preprocessors/science/
--rw-r--r--   0 runner    (1001) docker     (122)      478 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/science/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    21698 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/science/uni_fold.py
--rw-r--r--   0 runner    (1001) docker     (122)     2101 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/tts.py
--rw-r--r--   0 runner    (1001) docker     (122)    13168 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/preprocessors/video.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/tools/
--rw-r--r--   0 runner    (1001) docker     (122)       49 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/tools/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      772 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/tools/eval.py
--rw-r--r--   0 runner    (1001) docker     (122)     5295 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/tools/speech_tts_autolabel.py
--rw-r--r--   0 runner    (1001) docker     (122)      607 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/tools/train.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/
--rw-r--r--   0 runner    (1001) docker     (122)     1595 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/audio/
--rw-r--r--   0 runner    (1001) docker     (122)      826 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/audio/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1873 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/audio/ans_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     6969 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/audio/asr_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    12816 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/audio/kws_farfield_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    20332 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/audio/kws_nearfield_trainer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/audio/kws_utils/
--rw-r--r--   0 runner    (1001) docker     (122)     1768 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/audio/kws_utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    13743 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/audio/kws_utils/batch_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    10996 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/audio/kws_utils/det_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     3497 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/audio/kws_utils/file_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     4509 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/audio/kws_utils/model_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2854 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/audio/kws_utils/runtime_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    21962 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/audio/separation_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    10858 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/audio/tts_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     4389 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/base.py
--rw-r--r--   0 runner    (1001) docker     (122)      681 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/cv/
--rw-r--r--   0 runner    (1001) docker     (122)     1974 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/cv/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7494 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/cv/action_detection_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)      668 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/cv/card_detection_scrfd_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    10044 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/cv/cartoon_translation_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     6737 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/cv/face_detection_scrfd_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    20051 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/cv/image_classifition_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    11655 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/cv/image_defrcn_fewshot_detection_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    22965 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/cv/image_detection_damoyolo_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     4405 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/cv/image_inpainting_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)      816 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/cv/image_instance_segmentation_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     5520 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/cv/image_portrait_enhancement_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)      680 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/cv/movie_scene_segmentation_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    20149 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/cv/nerf_recon_acc_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    16911 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/cv/ocr_detection_db_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     3153 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/cv/ocr_recognition_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     2248 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/cv/referring_video_object_segmentation_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     4602 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/cv/vision_efficient_tuning_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     3280 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/default_config.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/easycv/
--rw-r--r--   0 runner    (1001) docker     (122)      487 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/easycv/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8237 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/easycv/trainer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/easycv/utils/
--rw-r--r--   0 runner    (1001) docker     (122)      523 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/easycv/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1029 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/easycv/utils/hooks.py
--rw-r--r--   0 runner    (1001) docker     (122)     1855 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/easycv/utils/metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     3415 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/easycv/utils/register_util.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/hooks/
--rw-r--r--   0 runner    (1001) docker     (122)     1650 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      296 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/builder.py
--rw-r--r--   0 runner    (1001) docker     (122)    27151 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/checkpoint_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)      764 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/clip_clamp_logit_scale_hook.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/hooks/compression/
--rw-r--r--   0 runner    (1001) docker     (122)      610 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/compression/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4847 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/compression/sparsity_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)     6915 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/compression/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1383 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/ddp_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)     6843 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/deepspeed_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)     4020 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/early_stop_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)     3062 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/evaluation_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)     8101 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/hook.py
--rw-r--r--   0 runner    (1001) docker     (122)      731 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/iter_timer_hook.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/hooks/logger/
--rw-r--r--   0 runner    (1001) docker     (122)      718 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/logger/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4498 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/logger/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     4507 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/logger/tensorboard_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)     7438 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/logger/text_logger_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)     5146 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/lr_scheduler_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)     6176 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/megatron_hook.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/hooks/optimizer/
--rw-r--r--   0 runner    (1001) docker     (122)      743 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/optimizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2845 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/optimizer/apex_optimizer_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)     3585 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/optimizer/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     3301 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/optimizer/torch_optimizer_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)     1707 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/hooks/priority.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/lrscheduler/
--rw-r--r--   0 runner    (1001) docker     (122)      699 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/lrscheduler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2040 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/lrscheduler/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/lrscheduler/warmup/
--rw-r--r--   0 runner    (1001) docker     (122)      614 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/lrscheduler/warmup/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2547 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/lrscheduler/warmup/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     2934 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/lrscheduler/warmup/warmup.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/multi_modal/
--rw-r--r--   0 runner    (1001) docker     (122)      682 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/multi_modal/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/multi_modal/clip/
--rw-r--r--   0 runner    (1001) docker     (122)       89 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/multi_modal/clip/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8618 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/multi_modal/clip/clip_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     4388 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/multi_modal/clip/clip_trainer_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     8086 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/multi_modal/mgeo_ranking_trainer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/multi_modal/mplug/
--rw-r--r--   0 runner    (1001) docker     (122)       91 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/multi_modal/mplug/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1642 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/multi_modal/mplug/mplug_trainer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/multi_modal/ofa/
--rw-r--r--   0 runner    (1001) docker     (122)       87 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/multi_modal/ofa/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10186 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/multi_modal/ofa/ofa_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    17647 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/multi_modal/ofa/ofa_trainer_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/multi_modal/team/
--rw-r--r--   0 runner    (1001) docker     (122)       95 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/multi_modal/team/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5429 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/multi_modal/team/team_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     2443 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/multi_modal/team/team_trainer_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/nlp/
--rw-r--r--   0 runner    (1001) docker     (122)     1179 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14266 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/csanmt_translation_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     9999 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/document_grounded_dialog_generate_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    24284 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/document_grounded_dialog_rerank_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     8159 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/document_grounded_dialog_retrieval_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    12769 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/faq_question_answering_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     3064 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/gpt3_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     2097 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/gpt_moe_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     8171 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/plug_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     3869 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/sentence_embedding_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     8378 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/sequence_classification_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    17110 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/siamese_uie_trainer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/nlp/space/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/space/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5324 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/space/dialog_intent_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     4229 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/space/dialog_modeling_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    36354 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/space/eval.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/nlp/space/metrics/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/space/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2447 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/space/metrics/metrics_tracker.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/nlp/space/trainer/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/space/trainer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    30567 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/space/trainer/gen_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    29570 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/space/trainer/intent_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    20453 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/table_question_answering_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)      991 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/text_generation_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     7514 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp/text_ranking_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     7129 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/nlp_trainer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/optimizer/
--rw-r--r--   0 runner    (1001) docker     (122)      223 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/optimizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1753 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/optimizer/builder.py
--rw-r--r--   0 runner    (1001) docker     (122)     7081 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/optimizer/child_tuning_adamw_optimizer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/parallel/
--rw-r--r--   0 runner    (1001) docker     (122)       80 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/parallel/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      681 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/parallel/builder.py
--rw-r--r--   0 runner    (1001) docker     (122)      754 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/parallel/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    57554 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    24137 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/training_args.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/trainers/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11090 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/utils/inference.py
--rw-r--r--   0 runner    (1001) docker     (122)     1294 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/trainers/utils/log_buffer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)   510651 2023-03-21 07:14:56.000000 modelscope-1.4.2/modelscope/utils/ast_index_file.py
--rw-r--r--   0 runner    (1001) docker     (122)    28846 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/ast_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/utils/audio/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/audio/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10375 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/audio/audio_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2428 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/audio/tts_exceptions.py
--rw-r--r--   0 runner    (1001) docker     (122)    24904 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/checkpoint.py
--rw-r--r--   0 runner    (1001) docker     (122)     2579 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/chinese_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    27433 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/config.py
--rw-r--r--   0 runner    (1001) docker     (122)     1027 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/config_ds.py
--rw-r--r--   0 runner    (1001) docker     (122)    17670 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/constant.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/utils/cv/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/cv/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    22088 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/cv/image_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/utils/cv/motion_utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/cv/motion_utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2307 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/cv/motion_utils/motion_process.py
--rw-r--r--   0 runner    (1001) docker     (122)     3847 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/cv/motion_utils/plot_script.py
--rw-r--r--   0 runner    (1001) docker     (122)     4290 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/cv/motion_utils/rotation_conversions.py
--rw-r--r--   0 runner    (1001) docker     (122)     3153 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/data_collators.py
--rw-r--r--   0 runner    (1001) docker     (122)     1279 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/data_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     9437 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/demo_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     3744 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/device.py
--rw-r--r--   0 runner    (1001) docker     (122)     6147 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/error.py
--rw-r--r--   0 runner    (1001) docker     (122)     1128 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/file_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     5939 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/hub.py
--rw-r--r--   0 runner    (1001) docker     (122)    15752 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/import_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)      478 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/json_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2789 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/logger.py
--rw-r--r--   0 runner    (1001) docker     (122)     1441 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/megatron_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2335 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     5099 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/model_tag.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/utils/nlp/
--rw-r--r--   0 runner    (1001) docker     (122)      499 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/nlp/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4424 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/nlp/distributed.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4526 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/nlp/load_checkpoint.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/utils/nlp/space/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/nlp/space/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1909 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/nlp/space/args.py
--rw-r--r--   0 runner    (1001) docker     (122)    11818 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/nlp/space/clean_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     1439 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/nlp/space/criterions.py
--rw-r--r--   0 runner    (1001) docker     (122)    11252 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/nlp/space/db_ops.py
--rw-r--r--   0 runner    (1001) docker     (122)     6174 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/nlp/space/ontology.py
--rw-r--r--   0 runner    (1001) docker     (122)      197 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/nlp/space/scores.py
--rw-r--r--   0 runner    (1001) docker     (122)     6356 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/nlp/space/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1054 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/nlp/space/utils_dst.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope/utils/nlp/space_T_en/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/nlp/space_T_en/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      859 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/nlp/space_T_en/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2554 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/nlp/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    23452 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/plugins.py
--rw-r--r--   0 runner    (1001) docker     (122)     7833 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/registry.py
--rw-r--r--   0 runner    (1001) docker     (122)    30201 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/regress_test_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     5892 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/service_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2500 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/task_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1593 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/tensor_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    12448 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/test_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1209 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/timer.py
--rw-r--r--   0 runner    (1001) docker     (122)    10959 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/torch_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)      597 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/trie.py
--rw-r--r--   0 runner    (1001) docker     (122)     1696 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/type_assert.py
--rw-r--r--   0 runner    (1001) docker     (122)      296 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/utils/typing.py
--rw-r--r--   0 runner    (1001) docker     (122)      272 2023-03-21 07:14:06.000000 modelscope-1.4.2/modelscope/version.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope.egg-info/
--rw-r--r--   0 runner    (1001) docker     (122)    18732 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (122)   129431 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (122)        1 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (122)       59 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (122)        1 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope.egg-info/not-zip-safe
--rw-r--r--   0 runner    (1001) docker     (122)     4595 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (122)       11 2023-03-21 07:14:57.000000 modelscope-1.4.2/modelscope.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (122)       38 2023-03-21 07:14:57.000000 modelscope-1.4.2/setup.cfg
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/
+-rw-r--r--   0 runner    (1001) docker     (122)       57 2023-04-17 02:47:50.000000 modelscope-1.5.0/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (122)    16425 2023-04-17 02:47:51.000000 modelscope-1.5.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (122)    15442 2023-04-17 02:47:50.000000 modelscope-1.5.0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/
+-rw-r--r--   0 runner    (1001) docker     (122)      156 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/cli/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/cli/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      404 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/cli/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)      829 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/cli/cli.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1230 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/cli/download.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6240 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/cli/modelcard.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4369 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/cli/pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3304 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/cli/plugins.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/configs/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/configs/examples/
+-rw-r--r--   0 runner    (1001) docker     (122)       36 2023-04-17 02:46:44.000000 modelscope-1.5.0/modelscope/configs/examples/configuration.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/exporters/
+-rw-r--r--   0 runner    (1001) docker     (122)     1255 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/exporters/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2667 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/exporters/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)      732 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/exporters/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/exporters/cv/
+-rw-r--r--   0 runner    (1001) docker     (122)      869 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/exporters/cv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2593 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/exporters/cv/cartoon_translation_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3619 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/exporters/cv/face_detection_scrfd_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1245 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/exporters/cv/object_detection_damoyolo_exporter.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/exporters/nlp/
+-rw-r--r--   0 runner    (1001) docker     (122)     1082 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/exporters/nlp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7341 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/exporters/nlp/csanmt_for_translation_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4579 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/exporters/nlp/model_for_token_classification_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3409 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/exporters/nlp/sbert_for_sequence_classification_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2316 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/exporters/nlp/sbert_for_zero_shot_classification_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4809 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/exporters/tf_model_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15145 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/exporters/torch_model_exporter.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/fileio/
+-rw-r--r--   0 runner    (1001) docker     (122)      122 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/fileio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10241 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/fileio/file.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/fileio/format/
+-rw-r--r--   0 runner    (1001) docker     (122)      143 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/fileio/format/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      454 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/fileio/format/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1050 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/fileio/format/json.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13853 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/fileio/format/jsonplus.py
+-rw-r--r--   0 runner    (1001) docker     (122)      669 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/fileio/format/yaml.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4254 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/fileio/io.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/hub/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/hub/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    38838 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/hub/api.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3649 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/hub/check_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1395 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/hub/constants.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11358 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/hub/deploy.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3728 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/hub/errors.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10284 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/hub/file_download.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9101 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/hub/git.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4157 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/hub/push_to_hub.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11794 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/hub/repository.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6535 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/hub/snapshot_download.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/hub/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/hub/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9916 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/hub/utils/caching.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2978 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/hub/utils/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    54236 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metainfo.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/metrics/
+-rw-r--r--   0 runner    (1001) docker     (122)     3829 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2510 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/accuracy_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7530 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/action_detection_evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1925 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/audio_noise_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1454 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1726 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/bleu_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3585 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/metrics/ciderD/
+-rwxr-xr-x   0 runner    (1001) docker     (122)       21 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/ciderD/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1926 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/ciderD/ciderD.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     8931 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/ciderD/ciderD_scorer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8664 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/image_color_enhance_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13387 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/image_colorization_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10011 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/image_denoise_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7612 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/image_inpainting_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13318 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/image_instance_segmentation_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1739 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/image_portrait_enhancement_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2536 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/image_quality_assessment_degradation_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1632 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/image_quality_assessment_mos_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2231 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/inbatch_recall_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1358 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/loss_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3002 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/map_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2032 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/movie_scene_segmentation_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3197 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/ned_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2310 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/ocr_recognition_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2082 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/ppl_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1163 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/prediction_saving_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4453 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/referring_video_object_segmentation_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3112 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/sequence_classification_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3233 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/text_generation_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3197 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/text_ranking_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5765 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/token_classification_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5435 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/video_frame_interpolation_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8655 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/video_stabilization_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3092 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/video_summarization_metric.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/metrics/video_super_resolution_metric/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/video_super_resolution_metric/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7030 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/video_super_resolution_metric/matlab_functions.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4771 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/video_super_resolution_metric/metric_util.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8932 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/video_super_resolution_metric/niqe.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1710 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/metrics/video_super_resolution_metric/video_super_resolution_metric.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      519 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)       93 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/audio/aec/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/aec/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/audio/aec/layers/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/aec/layers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1434 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/aec/layers/activations.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2700 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/aec/layers/affine_transform.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5630 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/aec/layers/deep_fsmn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1322 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/aec/layers/layer_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14384 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/aec/layers/uni_deep_fsmn.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/audio/aec/network/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/aec/network/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14438 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/aec/network/loss.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9382 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/aec/network/modulation_loss.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15396 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/aec/network/se_net.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/audio/ans/
+-rw-r--r--   0 runner    (1001) docker     (122)      515 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/ans/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6475 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/ans/complex_nn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3694 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/ans/conv_stft.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2513 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/ans/denoise_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10241 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/ans/frcrn.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/audio/ans/layers/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/ans/layers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1468 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/ans/layers/activations.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2414 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/ans/layers/affine_transform.py
+-rw-r--r--   0 runner    (1001) docker     (122)      661 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/ans/layers/layer_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5284 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/ans/layers/uni_deep_fsmn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1038 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/ans/se_module_complex.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9833 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/ans/unet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/audio/asr/
+-rw-r--r--   0 runner    (1001) docker     (122)      585 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/asr/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3029 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/asr/generic_automatic_speech_recognition.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1204 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/asr/wenet_automatic_speech_recognition.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/audio/itn/
+-rw-r--r--   0 runner    (1001) docker     (122)      557 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/itn/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1462 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/itn/generic_inverse_text_processing.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/audio/kws/
+-rw-r--r--   0 runner    (1001) docker     (122)      735 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/kws/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/audio/kws/farfield/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/kws/farfield/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14349 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/kws/farfield/fsmn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7462 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/kws/farfield/fsmn_sele_v2.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2806 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/kws/farfield/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2608 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/kws/farfield/model_def.py
+-rw-r--r--   0 runner    (1001) docker     (122)      908 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/kws/generic_key_word_spotting.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/audio/kws/nearfield/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/kws/nearfield/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3300 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/kws/nearfield/cmvn.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17496 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/kws/nearfield/fsmn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6079 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/kws/nearfield/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/audio/punc/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/punc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1466 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/punc/generic_punctuation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/audio/separation/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/separation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2240 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/separation/layer_norm.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13963 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/separation/mossformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9142 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/separation/mossformer_block.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9004 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/separation/mossformer_conv_module.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/audio/sv/
+-rw-r--r--   0 runner    (1001) docker     (122)     6544 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/sv/DTDNN.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8425 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/sv/DTDNN_layers.py
+-rw-r--r--   0 runner    (1001) docker     (122)      498 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/sv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14531 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/sv/ecapa_tdnn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1488 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/sv/generic_speaker_verification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/audio/tts/
+-rw-r--r--   0 runner    (1001) docker     (122)      474 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/tts/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11845 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/tts/sambert_hifi.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26394 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/audio/tts/voice.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/base/
+-rw-r--r--   0 runner    (1001) docker     (122)      303 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/base/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1091 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/base/base_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6994 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/base/base_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)      605 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/base/base_torch_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5679 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/base/base_torch_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3647 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/
+-rw-r--r--   0 runner    (1001) docker     (122)     1872 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/abnormal_object_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      490 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/abnormal_object_detection/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3806 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/abnormal_object_detection/mmdet_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/
+-rw-r--r--   0 runner    (1001) docker     (122)      113 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/
+-rw-r--r--   0 runner    (1001) docker     (122)      211 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6414 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/
+-rw-r--r--   0 runner    (1001) docker     (122)      145 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6729 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/action_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      493 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/action_detection/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7307 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/action_detection/action_detection_onnx.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/action_detection/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/action_detection/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9364 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/action_detection/modules/action_detection_pytorch.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12132 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/action_detection/modules/resnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/action_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)      709 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/action_recognition/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4304 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/action_recognition/models.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10276 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/action_recognition/s3dg.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18618 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/action_recognition/tada_convnext.py
+-rw-r--r--   0 runner    (1001) docker     (122)    45442 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/action_recognition/temporal_patch_shift_transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/animal_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)      558 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/animal_recognition/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14141 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/animal_recognition/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3955 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/animal_recognition/splat.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/bad_image_detecting/
+-rw-r--r--   0 runner    (1001) docker     (122)      496 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/bad_image_detecting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2589 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/bad_image_detecting/bad_image_detecting.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/body_2d_keypoints/
+-rw-r--r--   0 runner    (1001) docker     (122)      502 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/body_2d_keypoints/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12660 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/body_2d_keypoints/hrnet_basic_modules.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8776 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/body_2d_keypoints/hrnet_v2.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1707 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/body_2d_keypoints/w48.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/
+-rw-r--r--   0 runner    (1001) docker     (122)      601 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/
+-rw-r--r--   0 runner    (1001) docker     (122)       51 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8957 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/body_3d_pose.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8196 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/canonical_pose_modules.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/hdformer/
+-rw-r--r--   0 runner    (1001) docker     (122)       98 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/hdformer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11552 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/hdformer/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12870 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/hdformer/block.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8080 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/hdformer/directed_graph.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2516 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7719 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3401 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/hdformer/skeleton.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/cartoon/
+-rw-r--r--   0 runner    (1001) docker     (122)     1216 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/cartoon/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/cartoon/facelib/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/cartoon/facelib/LK/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/cartoon/facelib/LK/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3488 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/cartoon/facelib/LK/lk.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/cartoon/facelib/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      713 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/cartoon/facelib/config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3512 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/cartoon/facelib/face_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5161 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/cartoon/facelib/face_landmark.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4563 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/cartoon/facelib/facer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9075 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/cartoon/loss.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2174 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/cartoon/model_tf.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/cartoon/mtcnn_pytorch/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/cartoon/mtcnn_pytorch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6072 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/align_trans.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8519 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/matlab_cp2tform.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4404 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/cartoon/network.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5265 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/cartoon/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/cmdssl_video_embedding/
+-rw-r--r--   0 runner    (1001) docker     (122)      647 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/cmdssl_video_embedding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3876 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/cmdssl_video_embedding/c3d.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10785 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/cmdssl_video_embedding/resnet2p1d.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8949 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/cmdssl_video_embedding/resnet3d.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/
+-rw-r--r--   0 runner    (1001) docker     (122)      414 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13516 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/annotator.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/midas/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/midas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4960 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/midas/api.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      504 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/base_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10152 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3307 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/dpt_depth.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2746 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5829 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net_custom.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8045 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15438 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/vit.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4799 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/midas/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9896 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/mbv2_mlsd_large.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25158 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12595 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/body.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3894 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/hand.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9024 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8031 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/util.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8546 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/controlnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/crowd_counting/
+-rw-r--r--   0 runner    (1001) docker     (122)      491 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/crowd_counting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1100 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/crowd_counting/cc_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22878 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/crowd_counting/hrnet_aspp_relu.py
+-rw-r--r--   0 runner    (1001) docker     (122)      872 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/easycv_base.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_2d_keypoints/
+-rw-r--r--   0 runner    (1001) docker     (122)      500 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_2d_keypoints/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      631 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_2d_keypoints/face_2d_keypoints_align.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_attribute_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)      490 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_attribute_recognition/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_attribute_recognition/fair_face/
+-rw-r--r--   0 runner    (1001) docker     (122)      115 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_attribute_recognition/fair_face/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2559 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      930 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/mogface/
+-rw-r--r--   0 runner    (1001) docker     (122)       96 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/mogface/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/mogface/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/mogface/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3032 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/mogface/models/detectors.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4675 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/mogface/models/mogface.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5578 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/mogface/models/mogprednet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6264 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/mogface/models/resnet.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     7072 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/mogface/models/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/mtcnn/
+-rw-r--r--   0 runner    (1001) docker     (122)       97 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/mtcnn/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/mtcnn/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/mtcnn/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7061 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/mtcnn/models/box_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5415 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/mtcnn/models/detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3171 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/mtcnn/models/first_stage.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5232 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/mtcnn/models/get_nets.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/peppa_pig_face/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/peppa_pig_face/LK/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/peppa_pig_face/LK/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3444 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/peppa_pig_face/LK/lk.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/peppa_pig_face/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3578 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/peppa_pig_face/face_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5132 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/peppa_pig_face/face_landmark.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4210 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/peppa_pig_face/facer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/retinaface/
+-rw-r--r--   0 runner    (1001) docker     (122)       93 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/retinaface/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4832 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/retinaface/detection.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/retinaface/models/
+-rwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/retinaface/models/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4766 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/retinaface/models/net.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4590 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/retinaface/models/retinaface.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4120 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/retinaface/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/
+-rw-r--r--   0 runner    (1001) docker     (122)      174 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      955 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/damofd_detect.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      192 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/
+-rw-r--r--   0 runner    (1001) docker     (122)      325 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2945 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/transforms.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      292 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3401 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/bbox_nms.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/
+-rw-r--r--   0 runner    (1001) docker     (122)      276 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      471 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11707 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4169 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7981 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    30670 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     5543 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/retinaface.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      289 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      361 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4202 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/master_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3590 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16146 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      270 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    46999 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      295 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11062 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/base.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     6092 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6395 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/single_stage.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     5970 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/tinymog.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3451 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3869 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/scrfd_detect.py
+-rw-r--r--   0 runner    (1001) docker     (122)      960 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/tinymog_detect.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/
+-rw-r--r--   0 runner    (1001) docker     (122)       90 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1477 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/detection.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/vision/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/vision/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4126 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/vision/box_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2058 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/vision/mb_tiny.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      571 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/data_preprocessing.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1641 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/fd_config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3719 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/mb_tiny_fd.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2845 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/predictor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4621 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/ssd.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1497 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/vision/transforms.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_emotion/
+-rw-r--r--   0 runner    (1001) docker     (122)      540 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_emotion/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_emotion/efficient/
+-rw-r--r--   0 runner    (1001) docker     (122)      327 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_emotion/efficient/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15911 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_emotion/efficient/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20077 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_emotion/efficient/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2007 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_emotion/emotion_infer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3171 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_emotion/emotion_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_emotion/face_alignment/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_emotion/face_alignment/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2844 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_emotion/face_alignment/face.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1674 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_emotion/face_alignment/face_align.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_generation/
+-rw-r--r--   0 runner    (1001) docker     (122)      475 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_generation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_generation/op/
+-rwxr-xr-x   0 runner    (1001) docker     (122)       89 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_generation/op/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     6497 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_generation/op/conv2d_gradfix.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     3104 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_generation/op/fused_act.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     5922 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_generation/op/upfirdn2d.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    19998 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_generation/stylegan2.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_human_hand_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      544 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_human_hand_detection/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4109 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_human_hand_detection/det_infer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12584 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_human_hand_detection/ghost_pan.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16846 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_human_hand_detection/nanodet_plus_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1833 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_human_hand_detection/one_stage_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5791 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_human_hand_detection/shufflenetv2.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8874 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_human_hand_detection/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_recognition/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1572 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_recognition/align_face.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_recognition/torchkit/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      473 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_recognition/torchkit/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_recognition/torchkit/backbone/
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1080 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_recognition/torchkit/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6507 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_recognition/torchkit/backbone/arcface_backbone.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1977 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_recognition/torchkit/backbone/common.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7572 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_recognition/torchkit/backbone/facemask_backbone.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     8551 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_recognition/torchkit/backbone/model_irse.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4744 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_recognition/torchkit/backbone/model_resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7186 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_recognition/torchkit/rts_backbone.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    31337 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/bfm.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1428 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/de_retouching_module.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/facelandmark/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/facelandmark/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2583 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/facelandmark/large_base_lmks_infer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6561 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_base_lmks_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5124 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_eyeball_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30403 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/facerecon_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13171 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/losses.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21235 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/networks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8821 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/nv_diffrast.py
+-rw-r--r--   0 runner    (1001) docker     (122)      277 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/opt.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/pix2pix/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/pix2pix/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    31957 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/pix2pix/networks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5908 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/pix2pix/pix2pix_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)      779 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/pix2pix/pix2pix_options.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    13410 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/renderer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4933 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/unet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    31247 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/face_reconstruction/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/facial_expression_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)      484 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/facial_expression_recognition/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/facial_expression_recognition/fer/
+-rw-r--r--   0 runner    (1001) docker     (122)      121 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/facial_expression_recognition/fer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2414 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3277 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/facial_expression_recognition/fer/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1311 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/facial_expression_recognition/fer/vgg.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/facial_landmark_confidence/
+-rw-r--r--   0 runner    (1001) docker     (122)      478 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/facial_landmark_confidence/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/facial_landmark_confidence/flc/
+-rw-r--r--   0 runner    (1001) docker     (122)      115 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/facial_landmark_confidence/flc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3197 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4932 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/facial_landmark_confidence/flc/manual_landmark_net.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/hand_2d_keypoints/
+-rw-r--r--   0 runner    (1001) docker     (122)      488 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/hand_2d_keypoints/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      602 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/hand_2d_keypoints/hand_2d_keypoints.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/hand_static/
+-rw-r--r--   0 runner    (1001) docker     (122)      502 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/hand_static/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2480 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/hand_static/hand_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10891 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/hand_static/networks.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/human_reconstruction/
+-rw-r--r--   0 runner    (1001) docker     (122)     5936 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/human_reconstruction/Reconstruction.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/human_reconstruction/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/human_reconstruction/models/
+-rw-r--r--   0 runner    (1001) docker     (122)     1065 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/human_reconstruction/models/Embedding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4542 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/human_reconstruction/models/PixToMesh.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11301 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/human_reconstruction/models/Res_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2418 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/human_reconstruction/models/Surface_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/human_reconstruction/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2716 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/human_reconstruction/models/detectors.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1982 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/human_reconstruction/models/geometry.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2251 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/human_reconstruction/models/human_segmenter.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11865 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/human_reconstruction/models/networks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6111 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/human_reconstruction/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/human_wholebody_keypoint/
+-rw-r--r--   0 runner    (1001) docker     (122)      530 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/human_wholebody_keypoint/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      636 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/human_wholebody_keypoint/human_wholebody_keypoint.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_binary_quant_classification/
+-rw-r--r--   0 runner    (1001) docker     (122)      573 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_binary_quant_classification/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2859 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_binary_quant_classification/binary_quant_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19111 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_binary_quant_classification/bnext.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_body_reshaping/
+-rw-r--r--   0 runner    (1001) docker     (122)      500 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_body_reshaping/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3920 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_body_reshaping/image_body_reshaping.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6541 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_body_reshaping/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13616 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_body_reshaping/person_info.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_body_reshaping/pose_estimator/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_body_reshaping/pose_estimator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12027 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_body_reshaping/pose_estimator/body.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5673 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_body_reshaping/pose_estimator/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1311 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_body_reshaping/pose_estimator/util.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15760 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_body_reshaping/slim_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_classification/
+-rw-r--r--   0 runner    (1001) docker     (122)      598 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_classification/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_classification/backbones/
+-rw-r--r--   0 runner    (1001) docker     (122)      107 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_classification/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20295 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_classification/backbones/beit_v2.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17261 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_classification/backbones/nextvit.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2185 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_classification/mmcls_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1554 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_classification/resnet50_cc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5255 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_classification/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_color_enhance/
+-rw-r--r--   0 runner    (1001) docker     (122)      704 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_color_enhance/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_color_enhance/adaint/
+-rw-r--r--   0 runner    (1001) docker     (122)       44 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_color_enhance/adaint/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14338 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_color_enhance/adaint/adaint.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3753 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_color_enhance/csrnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_color_enhance/deeplpf/
+-rw-r--r--   0 runner    (1001) docker     (122)       66 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_color_enhance/deeplpf/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2575 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_color_enhance/deeplpf/deeplpf_image_color_enhance.py
+-rw-r--r--   0 runner    (1001) docker     (122)    31764 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_color_enhance/deeplpf/deeplpfnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2821 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_color_enhance/image_color_enhance.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_colorization/
+-rw-r--r--   0 runner    (1001) docker     (122)      640 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_colorization/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_colorization/ddcolor/
+-rw-r--r--   0 runner    (1001) docker     (122)      122 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_colorization/ddcolor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9385 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_colorization/ddcolor/ddcolor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6437 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_colorization/ddcolor/ddcolor_for_image_colorization.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9576 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_colorization/ddcolor/loss.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_colorization/ddcolor/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_colorization/ddcolor/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6857 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_colorization/ddcolor/utils/convnext.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2196 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_colorization/ddcolor/utils/position_encoding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7796 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_colorization/ddcolor/utils/transformer_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6465 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_colorization/ddcolor/utils/unet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6510 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_colorization/ddcolor/utils/vgg.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_colorization/unet/
+-rw-r--r--   0 runner    (1001) docker     (122)      129 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_colorization/unet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10574 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_colorization/unet/unet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10647 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_colorization/unet/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_debanding/
+-rw-r--r--   0 runner    (1001) docker     (122)      483 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_debanding/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_debanding/rrdb/
+-rw-r--r--   0 runner    (1001) docker     (122)       53 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_debanding/rrdb/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3005 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_debanding/rrdb/rrdb_image_debanding.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_deblur/
+-rw-r--r--   0 runner    (1001) docker     (122)      510 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_deblur/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4241 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_deblur/nafnet_for_image_deblur.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/
+-rw-r--r--   0 runner    (1001) docker     (122)      492 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4329 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/defrcn_for_fewshot.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12474 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/coco_evaluation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3141 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4354 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/pascal_voc_evaluation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      448 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6827 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/models/calibration_layer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7064 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/models/defrcn.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10779 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/models/fast_rcnn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1220 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/models/gdl.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1732 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/models/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4770 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/models/roi_heads.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23599 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/utils/coco_register.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4803 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/utils/configuration_mapper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3595 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/utils/model_surgery_op.py
+-rw-r--r--   0 runner    (1001) docker     (122)      583 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/utils/register_data.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2617 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/utils/requirements_check.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11011 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/utils/voc_register.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_denoise/
+-rw-r--r--   0 runner    (1001) docker     (122)      514 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_denoise/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_denoise/nafnet/
+-rw-r--r--   0 runner    (1001) docker     (122)     6664 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_denoise/nafnet/NAFNet_arch.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_denoise/nafnet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1627 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_denoise/nafnet/arch_util.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2475 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_denoise/nafnet_for_image_denoise.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_depth_estimation/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_depth_estimation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_depth_estimation/networks/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_depth_estimation/networks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6757 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_depth_estimation/networks/newcrf_depth.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18233 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_depth_estimation/networks/newcrf_layers.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10106 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_depth_estimation/networks/newcrf_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26006 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_depth_estimation/networks/swin_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13560 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_depth_estimation/networks/uper_crf_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1600 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_depth_estimation/newcrfs_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_depth_estimation_bts/
+-rw-r--r--   0 runner    (1001) docker     (122)      536 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_depth_estimation_bts/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2606 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_depth_estimation_bts/depth_estimation_bts_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_depth_estimation_bts/networks/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_depth_estimation_bts/networks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1485 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_depth_estimation_bts/networks/bts_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2819 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_depth_estimation_bts/networks/decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2506 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_depth_estimation_bts/networks/encoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8545 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_depth_estimation_bts/networks/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_driving_perception/
+-rw-r--r--   0 runner    (1001) docker     (122)      999 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_driving_perception/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2022 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_driving_perception/image_driving_percetion_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4352 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_driving_perception/preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7475 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_driving_perception/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/
+-rw-r--r--   0 runner    (1001) docker     (122)      488 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/facegan/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/facegan/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3124 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/facegan/gan_wrap.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21432 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/facegan/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/facegan/op/
+-rw-r--r--   0 runner    (1001) docker     (122)      242 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/facegan/op/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6497 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/facegan/op/conv2d_gradfix.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3094 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/facegan/op/fused_act.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5912 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/facegan/op/upfirdn2d.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/facelib/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/facelib/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10514 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/facelib/align_trans.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5955 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/facelib/matlab_cp2tform.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9077 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/image_face_fusion.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/network/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/network/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3045 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/network/aad_layer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8555 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/network/aei_flow_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9366 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/network/bfm.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12449 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/network/dense_motion.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20604 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/network/facerecon_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7433 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/network/model_irse.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7790 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_face_fusion/network/ops.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_human_parsing/
+-rw-r--r--   0 runner    (1001) docker     (122)      575 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_human_parsing/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_human_parsing/backbone/
+-rw-r--r--   0 runner    (1001) docker     (122)      525 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_human_parsing/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11736 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_human_parsing/backbone/deeplab_resnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_human_parsing/m2fp/
+-rw-r--r--   0 runner    (1001) docker     (122)      640 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_human_parsing/m2fp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8257 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_human_parsing/m2fp/m2fp_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8361 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_human_parsing/m2fp/m2fp_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15097 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_human_parsing/m2fp_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5633 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_human_parsing/parsing_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_inpainting/
+-rw-r--r--   0 runner    (1001) docker     (122)      475 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_inpainting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2690 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_inpainting/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7605 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_inpainting/default.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1253 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_inpainting/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_inpainting/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_inpainting/modules/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_inpainting/modules/ade20k/
+-rw-r--r--   0 runner    (1001) docker     (122)       81 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_inpainting/modules/ade20k/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12202 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_inpainting/modules/ade20k/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5348 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_inpainting/modules/ade20k/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7509 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_inpainting/modules/adversarial.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1564 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_inpainting/modules/feature_matching.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19052 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_inpainting/modules/ffc.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11842 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_inpainting/modules/inception.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1511 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_inpainting/modules/perceptual.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1965 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_inpainting/modules/pix2pixhd.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13350 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_inpainting/refinement.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      984 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/backbones/
+-rw-r--r--   0 runner    (1001) docker     (122)      574 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    27174 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/backbones/swin_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9305 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/cascade_mask_rcnn_swin.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/datasets/
+-rw-r--r--   0 runner    (1001) docker     (122)      101 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/datasets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3992 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/datasets/transforms.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/maskdino/
+-rw-r--r--   0 runner    (1001) docker     (122)      600 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/maskdino/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10072 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/maskdino/dino_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15407 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18188 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7274 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/maskdino/ms_deform_attn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2720 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/maskdino/position_encoding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6712 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/maskdino/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1429 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/maskdino_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11454 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/maskdino_swin.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1549 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7904 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/postprocess_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/
+-rw-r--r--   0 runner    (1001) docker     (122)      553 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/config/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/config/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6991 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/config/default.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/
+-rw-r--r--   0 runner    (1001) docker     (122)      171 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/backbone/
+-rw-r--r--   0 runner    (1001) docker     (122)      588 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7244 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/backbone/resnet_fpn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3784 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/
+-rw-r--r--   0 runner    (1001) docker     (122)      239 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2968 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/fine_preprocess.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3012 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/linear_attention.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2994 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/quadtree_attention.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9677 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10531 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/coarse_matching.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3048 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/fine_matching.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2132 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/position_encoding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2735 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/quadtree_attention_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      344 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_matching/utils/misc.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_mvs_depth_estimation/
+-rw-r--r--   0 runner    (1001) docker     (122)      521 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_mvs_depth_estimation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8546 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_mvs_depth_estimation/cas_mvsnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6274 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_mvs_depth_estimation/casmvs_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18181 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_mvs_depth_estimation/colmap2mvsnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9864 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_mvs_depth_estimation/depth_filter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9404 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_mvs_depth_estimation/general_eval_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21735 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_mvs_depth_estimation/module.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3367 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_mvs_depth_estimation/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_paintbyexample/
+-rw-r--r--   0 runner    (1001) docker     (122)      507 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_paintbyexample/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1567 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_paintbyexample/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_panoptic_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      571 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_panoptic_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1694 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_panoptic_segmentation/panseg_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)      644 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_panoptic_segmentation/r50_panseg_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/
+-rw-r--r--   0 runner    (1001) docker     (122)      538 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     8563 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/align_faces.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/eqface/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/eqface/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1836 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/eqface/fqa.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4710 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/eqface/model_resnet.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    22869 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/gpen.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7114 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/image_portrait_enhancement.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/losses/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/losses/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4336 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/losses/helpers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3286 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/losses/losses.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3155 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/losses/model_irse.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/retinaface/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/retinaface/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     7313 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/retinaface/detection.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/
+-rwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4845 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/net.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4676 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/retinaface.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4120 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/retinaface/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_probing_model/
+-rw-r--r--   0 runner    (1001) docker     (122)      533 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_probing_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10076 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_probing_model/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3319 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_probing_model/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5206 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_probing_model/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_degradation/
+-rw-r--r--   0 runner    (1001) docker     (122)      584 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_degradation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5064 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_degradation/degradation_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4654 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_man/
+-rw-r--r--   0 runner    (1001) docker     (122)      544 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_man/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2600 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_man/image_quality_assessment_man.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5163 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_man/maniqa.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23230 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_man/swin.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_mos/
+-rw-r--r--   0 runner    (1001) docker     (122)      544 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_mos/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_mos/backbones/
+-rw-r--r--   0 runner    (1001) docker     (122)      272 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_mos/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16222 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_mos/backbones/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1060 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_mos/censeo_ivqa_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_mos/heads/
+-rw-r--r--   0 runner    (1001) docker     (122)       36 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_mos/heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      529 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_mos/heads/simple_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2651 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_mos/image_quality_assessment_mos.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_reid_person/
+-rw-r--r--   0 runner    (1001) docker     (122)      467 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_reid_person/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5444 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_reid_person/pass_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14108 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_reid_person/transreid_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_restoration/
+-rw-r--r--   0 runner    (1001) docker     (122)      527 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_restoration/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_restoration/demoire_models/
+-rw-r--r--   0 runner    (1001) docker     (122)       69 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_restoration/demoire_models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10243 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_restoration/demoire_models/nets.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2640 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_restoration/image_restoration_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      712 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7896 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/data_util.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4661 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/feature_extractors.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5155 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/pixel_classifier.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2273 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3208 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/pan_merge/
+-rw-r--r--   0 runner    (1001) docker     (122)      111 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/pan_merge/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1516 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/pan_merge/base_panoptic_fusion_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2122 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/pan_merge/maskformer_semantic_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)      618 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/segformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2569 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/semantic_seg_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/
+-rw-r--r--   0 runner    (1001) docker     (122)      303 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      290 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/
+-rw-r--r--   0 runner    (1001) docker     (122)      249 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17484 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/adapter_modules.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/
+-rw-r--r--   0 runner    (1001) docker     (122)      196 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18241 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/beit.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6379 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/beit_adapter.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      251 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10785 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/base_decode_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26514 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/mask2former_head_from_mmseg.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/
+-rw-r--r--   0 runner    (1001) docker     (122)      253 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12370 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/base_segmentor.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11715 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/encoder_decoder_mask2former.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      368 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      398 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/builder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1938 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/data_process_func.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1788 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/seg_func.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_skychange/
+-rw-r--r--   0 runner    (1001) docker     (122)      650 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_skychange/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9312 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_skychange/preprocessor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_skychange/ptsemseg/
+-rw-r--r--   0 runner    (1001) docker     (122)     3740 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_skychange/ptsemseg/BlockModules.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_skychange/ptsemseg/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21278 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_skychange/ptsemseg/hrnet_backnone.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18029 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_skychange/ptsemseg/hrnet_super_and_ocr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8159 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_skychange/ptsemseg/unet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11210 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_skychange/skychange.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7874 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_skychange/skychange_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/
+-rw-r--r--   0 runner    (1001) docker     (122)      120 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/data/
+-rw-r--r--   0 runner    (1001) docker     (122)      561 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3309 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/data/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10476 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      603 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12670 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/models/autoencoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12594 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/models/clip.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/ops/
+-rw-r--r--   0 runner    (1001) docker     (122)      561 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/ops/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24469 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/ops/diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1320 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/ops/losses.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/
+-rw-r--r--   0 runner    (1001) docker     (122)      536 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/data/
+-rw-r--r--   0 runner    (1001) docker     (122)      127 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3309 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/data/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10527 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/model_translation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      161 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12670 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/models/autoencoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12594 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/models/clip.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/ops/
+-rw-r--r--   0 runner    (1001) docker     (122)      384 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/ops/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21828 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/ops/apps.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36549 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/ops/degradation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24615 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/ops/diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1320 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/ops/losses.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4389 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/ops/metrics.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6736 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/ops/random_color.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2745 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/ops/random_mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3777 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/ops/svd.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6728 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/ops/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/
+-rw-r--r--   0 runner    (1001) docker     (122)      486 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/
+-rw-r--r--   0 runner    (1001) docker     (122)      136 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4099 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/resnet_DA.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6931 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/vit_horizon_pry_image.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2510 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/fourier.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3251 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/panostretch.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14887 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/post_proc.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/modality/
+-rw-r--r--   0 runner    (1001) docker     (122)       86 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/modality/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5518 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/modality/layout.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3406 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/panovit.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4808 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1834 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/panovit.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/language_guided_video_summarization/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      542 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/language_guided_video_summarization/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     6441 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/language_guided_video_summarization/summarizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/language_guided_video_summarization/transformer/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      508 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/language_guided_video_summarization/transformer/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1900 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/language_guided_video_summarization/transformer/layers.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     7275 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/language_guided_video_summarization/transformer/models.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)      826 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/language_guided_video_summarization/transformer/modules.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2690 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/language_guided_video_summarization/transformer/sub_layers.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/motion_generation/
+-rw-r--r--   0 runner    (1001) docker     (122)      639 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/motion_generation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2151 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/motion_generation/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/motion_generation/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/motion_generation/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1209 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/motion_generation/modules/cfg_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26783 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/motion_generation/modules/gaussian_diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13594 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/motion_generation/modules/mdm.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5412 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/motion_generation/modules/respace.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3914 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/motion_generation/modules/rotation2xyz.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3189 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/motion_generation/modules/smpl.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/movie_scene_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      615 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/movie_scene_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1567 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/movie_scene_segmentation/get_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7458 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/movie_scene_segmentation/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/movie_scene_segmentation/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      181 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/movie_scene_segmentation/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      798 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/movie_scene_segmentation/utils/head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4232 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/movie_scene_segmentation/utils/save_op.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10325 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/movie_scene_segmentation/utils/shot_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5057 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/movie_scene_segmentation/utils/trn.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/nerf_recon_acc/
+-rw-r--r--   0 runner    (1001) docker     (122)      602 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/nerf_recon_acc/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/nerf_recon_acc/dataloader/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/nerf_recon_acc/dataloader/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18782 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/nerf_recon_acc/dataloader/nerf_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20567 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/nerf_recon_acc/dataloader/read_write_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7297 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/nerf_recon_acc/nerf_preprocess.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7876 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/nerf_recon_acc/nerf_recon_acc.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/nerf_recon_acc/network/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/nerf_recon_acc/network/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13317 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/nerf_recon_acc/network/nerf.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1512 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/nerf_recon_acc/network/segmenter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5720 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/nerf_recon_acc/network/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      606 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      620 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/dino.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3432 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/
+-rw-r--r--   0 runner    (1001) docker     (122)      321 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/backbones/
+-rw-r--r--   0 runner    (1001) docker     (122)      211 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21306 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/backbones/vit.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      278 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2220 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/anchor_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11529 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/necks/
+-rw-r--r--   0 runner    (1001) docker     (122)      213 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/necks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9021 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/necks/fpn.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      426 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      375 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8542 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      239 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17739 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/fcn_mask_head.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      306 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21926 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/utils/checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1204 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/utils/convModule_norm.py
+-rw-r--r--   0 runner    (1001) docker     (122)      832 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection/yolox_pai.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/
+-rw-r--r--   0 runner    (1001) docker     (122)      467 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/
+-rw-r--r--   0 runner    (1001) docker     (122)       86 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2657 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/depe_detect.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/
+-rw-r--r--   0 runner    (1001) docker     (122)      605 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/
+-rw-r--r--   0 runner    (1001) docker     (122)      299 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6670 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/
+-rw-r--r--   0 runner    (1001) docker     (122)      282 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4476 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/
+-rw-r--r--   0 runner    (1001) docker     (122)      276 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1064 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/match_cost.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2041 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/util.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/
+-rw-r--r--   0 runner    (1001) docker     (122)      294 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3095 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/nuscenes_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/
+-rw-r--r--   0 runner    (1001) docker     (122)      522 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7936 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/loading.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8689 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/
+-rw-r--r--   0 runner    (1001) docker     (122)      255 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12874 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/vovnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      282 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7442 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/depth_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)    59197 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/petrv2_dednhead.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/
+-rw-r--r--   0 runner    (1001) docker     (122)      255 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9533 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/petr3d.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/
+-rw-r--r--   0 runner    (1001) docker     (122)      256 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9032 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/cp_fpn.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      562 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18126 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5175 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/positional_encoding.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11059 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/result_vis.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/ocr_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      473 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/ocr_detection/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2870 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/ocr_detection/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/ocr_detection/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/ocr_detection/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18335 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/ocr_detection/modules/dbnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8217 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/ocr_detection/modules/seg_detector_loss.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2532 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/ocr_detection/preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7972 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/ocr_detection/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/ocr_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)      477 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/ocr_recognition/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5848 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/ocr_recognition/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/ocr_recognition/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/ocr_recognition/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6209 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/ocr_recognition/modules/convnext.py
+-rw-r--r--   0 runner    (1001) docker     (122)      517 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/ocr_recognition/modules/convnextvit.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3451 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/ocr_recognition/modules/crnn.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11644 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/ocr_recognition/modules/timm_tinyc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1955 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/ocr_recognition/modules/vitstr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3603 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/ocr_recognition/preprocessor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/open_vocabulary_detection_vild/
+-rw-r--r--   0 runner    (1001) docker     (122)      501 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/open_vocabulary_detection_vild/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14249 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/open_vocabulary_detection_vild/vild.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/panorama_depth_estimation/
+-rw-r--r--   0 runner    (1001) docker     (122)      511 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/panorama_depth_estimation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/panorama_depth_estimation/networks/
+-rw-r--r--   0 runner    (1001) docker     (122)      102 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/panorama_depth_estimation/networks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5077 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/panorama_depth_estimation/networks/equi.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7503 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/panorama_depth_estimation/networks/layers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7839 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/panorama_depth_estimation/networks/mobilenet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14421 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/panorama_depth_estimation/networks/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9175 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/panorama_depth_estimation/networks/unifuse.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3962 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/panorama_depth_estimation/networks/util.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3307 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/panorama_depth_estimation/unifuse_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/pedestrian_attribute_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)      488 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/pedestrian_attribute_recognition/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3468 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/pedestrian_attribute_recognition/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/pointcloud_sceneflow_estimation/
+-rw-r--r--   0 runner    (1001) docker     (122)      495 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/pointcloud_sceneflow_estimation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16147 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/pointcloud_sceneflow_estimation/common.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11898 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/pointcloud_sceneflow_estimation/pointnet2_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1797 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/pointcloud_sceneflow_estimation/rcp_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18618 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/pointcloud_sceneflow_estimation/sf_rcp.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/product_retrieval_embedding/
+-rw-r--r--   0 runner    (1001) docker     (122)      548 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/product_retrieval_embedding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20974 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/product_retrieval_embedding/item_detection.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4509 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/product_retrieval_embedding/item_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4340 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/product_retrieval_embedding/item_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/product_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      528 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/product_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7978 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/product_segmentation/net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2194 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/product_segmentation/seg_infer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      514 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5832 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      318 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8504 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9206 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/criterion.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7299 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/matcher.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7935 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/misc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6242 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/mttr.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16610 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/multimodal_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2091 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/position_encoding_2d.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5629 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/postprocessing.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5180 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/segmentation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    27941 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/swin_transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/robust_image_classification/
+-rw-r--r--   0 runner    (1001) docker     (122)      501 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/robust_image_classification/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2902 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/robust_image_classification/easyrobust_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/salient_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      497 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/salient_detection/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/salient_detection/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      213 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/salient_detection/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/salient_detection/models/backbone/
+-rw-r--r--   0 runner    (1001) docker     (122)     6558 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/salient_detection/models/backbone/Res2Net_v1b.py
+-rw-r--r--   0 runner    (1001) docker     (122)      256 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/salient_detection/models/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6525 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/salient_detection/models/modules.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3047 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/salient_detection/models/senet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12007 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/salient_detection/models/u2net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3524 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/salient_detection/models/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2704 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/salient_detection/salient_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/shop_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      464 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/shop_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2203 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/shop_segmentation/common.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4385 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/shop_segmentation/head_fpn.py
+-rw-r--r--   0 runner    (1001) docker     (122)    32989 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/shop_segmentation/models.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9319 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/shop_segmentation/neck_fpn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5632 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/shop_segmentation/shop_seg_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4095 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/shop_segmentation/shop_seg_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6699 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/shop_segmentation/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/skin_retouching/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/skin_retouching/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/skin_retouching/detection_model/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/skin_retouching/detection_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/skin_retouching/detection_model/detection_module.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2532 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/skin_retouching/detection_model/detection_unet_in.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/skin_retouching/inpainting_model/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/skin_retouching/inpainting_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5759 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/skin_retouching/inpainting_model/gconv.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3233 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/skin_retouching/inpainting_model/inpainting_unet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/skin_retouching/retinaface/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/skin_retouching/retinaface/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10898 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/skin_retouching/retinaface/box_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3911 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/skin_retouching/retinaface/net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4799 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/skin_retouching/retinaface/network.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5003 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/skin_retouching/retinaface/predict_single.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1035 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/skin_retouching/retinaface/prior_box.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2110 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/skin_retouching/retinaface/utils.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     3890 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/skin_retouching/unet_deploy.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9917 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/skin_retouching/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1169 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/skin_retouching/weights_init.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/
+-rw-r--r--   0 runner    (1001) docker     (122)      526 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/data/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2094 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/data/data_augment.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/exp/
+-rw-r--r--   0 runner    (1001) docker     (122)      165 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/exp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      274 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/exp/base_exp.py
+-rw-r--r--   0 runner    (1001) docker     (122)      392 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/exp/build.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/exp/default/
+-rw-r--r--   0 runner    (1001) docker     (122)      137 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/exp/default/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1209 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/exp/default/streamyolo.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1845 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/exp/yolox_base.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      238 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6274 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/models/darknet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10911 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/models/dfp_pafpn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6156 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/models/network_blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1314 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/models/streamyolo.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5669 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/models/tal_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4352 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/realtime_video_detector.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      270 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3681 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/utils/boxes.py
+-rw-r--r--   0 runner    (1001) docker     (122)      209 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/stream_yolo/utils/format.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/super_resolution/
+-rw-r--r--   0 runner    (1001) docker     (122)      555 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/super_resolution/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7652 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/super_resolution/arch_util.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10508 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/super_resolution/ecb.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3525 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/super_resolution/ecbsr_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4945 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/super_resolution/rrdbnet_arch.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/table_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)      462 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/table_recognition/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15842 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/table_recognition/lineless_table_process.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3360 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/table_recognition/model_lore.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/table_recognition/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/table_recognition/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12614 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/table_recognition/modules/lore_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15090 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/table_recognition/modules/lore_processor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/text_driven_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)       96 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/text_driven_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5488 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/text_driven_segmentation/clip.py
+-rw-r--r--   0 runner    (1001) docker     (122)      777 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/text_driven_segmentation/lseg_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7587 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/text_driven_segmentation/lseg_blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4143 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/text_driven_segmentation/lseg_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6145 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/text_driven_segmentation/lseg_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18951 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/text_driven_segmentation/lseg_vit.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16418 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/text_driven_segmentation/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5165 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/text_driven_segmentation/simple_tokenizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_classfication/
+-rw-r--r--   0 runner    (1001) docker     (122)      594 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_classfication/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    43681 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_classfication/basic_blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2174 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_classfication/global_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2902 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_classfication/master_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)      766 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_classfication/model_zoo.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2892 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_classfication/plain_net_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7405 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_classfication/super_blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14983 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_classfication/super_res_idwexkx.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8709 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_classfication/super_res_k1kxk1.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6923 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_classfication/super_res_kxkx.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      699 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/apis/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/apis/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2067 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/apis/detector_evaluater.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4000 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/apis/detector_inference.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/augmentations/
+-rw-r--r--   0 runner    (1001) docker     (122)       49 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/augmentations/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/
+-rw-r--r--   0 runner    (1001) docker     (122)       49 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3460 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/box_level_augs.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8123 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/color_augs.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2345 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/gaussian_maps.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5911 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/geometric_augs.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2842 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/augmentations/scale_aware_aug.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/
+-rw-r--r--   0 runner    (1001) docker     (122)      148 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/
+-rw-r--r--   0 runner    (1001) docker     (122)      621 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3701 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/darknet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9356 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_csp.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7465 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_res.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14261 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/base_ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10559 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/neck_ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13101 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/ops.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    18626 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/ota_assigner.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7526 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/repvgg_block.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2572 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)      561 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/weight_init.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      409 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12602 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/gfocal_v2_tiny.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19162 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/zero_head.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5588 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/distill_loss.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12284 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/gfocal_loss.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/
+-rw-r--r--   0 runner    (1001) docker     (122)      421 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7347 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_config.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24842 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3499 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn_btn.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/detectors/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/detectors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2809 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/detectors/detector.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/structures/
+-rw-r--r--   0 runner    (1001) docker     (122)       49 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/structures/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9197 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/structures/bounding_box.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2627 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/structures/boxlist_ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2774 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/structures/image_list.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      189 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11594 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/utils/boxes.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5838 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/utils/model_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1164 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/utils/scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6400 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)      627 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/tinynas_damoyolo.py
+-rw-r--r--   0 runner    (1001) docker     (122)      643 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/tinynas_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1026 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/tinynas_detection/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_deinterlace/
+-rw-r--r--   0 runner    (1001) docker     (122)     2896 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_deinterlace/UNet_for_video_deinterlace.py
+-rw-r--r--   0 runner    (1001) docker     (122)      494 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_deinterlace/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      773 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_deinterlace/deinterlace_arch.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_deinterlace/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_deinterlace/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3159 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_deinterlace/models/archs.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1441 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_deinterlace/models/deep_fourier_upsampling.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2972 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_deinterlace/models/enh.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3572 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_deinterlace/models/fre.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3716 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_deinterlace/models/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/
+-rw-r--r--   0 runner    (1001) docker     (122)      483 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/configs/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/configs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12753 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/configs/default_config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6731 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/dro_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/geometry/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/geometry/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5494 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/geometry/camera.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2285 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/geometry/camera_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3737 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/geometry/pose.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3685 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/geometry/pose_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/models/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5277 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/models/model_checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2356 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/models/model_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10098 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/models/model_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7204 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/models/sfm_model_mf.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4387 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/models/sup_model_mf.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/depth_pose/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/depth_pose/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10532 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/depth_pose/depth_pose_net.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/layers/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/layers/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8902 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/depth_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1607 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/layers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1915 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/pose_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4110 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/resnet_encoder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/optim/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/optim/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15781 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/optim/extractor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8039 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/optim/update.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6815 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/utils/augmentations.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10248 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/utils/config.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15071 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/utils/depth.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1153 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/utils/horovod.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10965 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/utils/image.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9407 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/utils/image_gt.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6499 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/utils/load.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2726 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/utils/misc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1348 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/utils/types.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/
+-rw-r--r--   0 runner    (1001) docker     (122)     1894 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/VFINet_arch.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3334 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/VFINet_for_video_frame_interpolation.py
+-rw-r--r--   0 runner    (1001) docker     (122)      458 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/flow_model/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/flow_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3212 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/flow_model/corr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9228 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/flow_model/extractor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5346 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/flow_model/raft.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5540 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/flow_model/update.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/interp_model/
+-rw-r--r--   0 runner    (1001) docker     (122)    13745 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/interp_model/IFNet_swin.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4049 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/interp_model/UNet.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/interp_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3823 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/interp_model/flow_reversal.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16050 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/interp_model/refinenet_arch.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36415 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/interp_model/transformer_layers.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3317 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/utils/scene_change_detection.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2907 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/utils/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_human_matting/
+-rw-r--r--   0 runner    (1001) docker     (122)      558 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_human_matting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1324 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_human_matting/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_human_matting/models/
+-rw-r--r--   0 runner    (1001) docker     (122)       36 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_human_matting/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10465 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_human_matting/models/decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2662 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_human_matting/models/deep_guided_filter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5506 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_human_matting/models/effv2.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2904 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_human_matting/models/lraspp.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2234 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_human_matting/models/matting.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_inpainting/
+-rw-r--r--   0 runner    (1001) docker     (122)      524 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_inpainting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11056 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_inpainting/inpainting.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13727 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_inpainting/inpainting_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      582 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/head/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/head/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18236 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/head/kernel_frame_iter_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21551 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/head/kernel_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16955 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/head/kernel_iter_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20983 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/head/kernel_update_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4136 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/head/kernel_updator.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/neck/
+-rw-r--r--   0 runner    (1001) docker     (122)      525 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/neck/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11733 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/neck/msdeformattn_decoder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/track/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/track/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26843 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/track/kernel_update_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10771 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4163 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18059 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/video_knet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/
+-rw-r--r--   0 runner    (1001) docker     (122)      541 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2970 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/models/common.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2420 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/models/decode.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1890 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/models/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4921 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/models/yolo.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/tracker/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/tracker/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1084 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/tracker/basetrack.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3129 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/tracker/matching.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14998 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/tracker/multitracker.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2230 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/utils/image.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9570 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/utils/kalman_filter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7296 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/utils/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2888 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/utils/visualization.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_object_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      497 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_object_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      818 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_object_segmentation/aggregate.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3582 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_object_segmentation/cbam.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1979 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_object_segmentation/eval_network.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3980 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_object_segmentation/inference_core.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8255 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_object_segmentation/inference_memory_bank.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7037 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_object_segmentation/mod_resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)      974 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_object_segmentation/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15309 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_object_segmentation/modules.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5593 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_object_segmentation/network.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      477 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/backbone/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10040 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26628 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/head/
+-rw-r--r--   0 runner    (1001) docker     (122)      515 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/head/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8349 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20677 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_iter_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28653 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_update_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4081 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_updator.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6696 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/head/mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8627 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/head/semantic_fpn_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5885 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/head/track_heads.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/neck/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/neck/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6255 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/neck/fpn.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/track/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/track/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8508 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/track/quasi_dense_embed_tracker.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17155 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/video_k_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5440 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/visualizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/config/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/config/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1024 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/config/ostrack.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/layers/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/layers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1641 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/layers/attn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5004 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/layers/attn_blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4805 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/layers/head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1234 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/layers/patch_embed.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3380 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/base_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3393 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/ostrack.py
+-rw-r--r--   0 runner    (1001) docker     (122)      653 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12278 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/vit_ce.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/procontext/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/procontext/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3497 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/procontext/procontext.py
+-rw-r--r--   0 runner    (1001) docker     (122)      943 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/procontext/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4473 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/procontext/vit_ce.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/tracker/
+-rw-r--r--   0 runner    (1001) docker     (122)      114 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/tracker/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5507 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/tracker/ostrack.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7123 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/tracker/procontext.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8488 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/utils/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/
+-rw-r--r--   0 runner    (1001) docker     (122)    14702 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/DUT_raft.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4715 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/MotionPro.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/RAFT/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/RAFT/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3290 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/RAFT/corr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9214 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/RAFT/extractor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5275 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/RAFT/raft.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5526 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/RAFT/update.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3922 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/Smoother.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1447 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7292 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/rf_det_module.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7981 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/rf_det_so.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3452 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUTRAFTStabilizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)      492 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)     4684 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/utils/IterativeSmooth.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14212 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/utils/MedianFilter.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22544 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/utils/ProjectionUtils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2893 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/utils/RAFTUtils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2817 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/utils/WarpUtils.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14474 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/utils/image_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4977 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_stabilization/utils/math_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_streaming_perception/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_streaming_perception/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_streaming_perception/longshortnet/
+-rw-r--r--   0 runner    (1001) docker     (122)      487 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_streaming_perception/longshortnet/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_streaming_perception/longshortnet/exp/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_streaming_perception/longshortnet/exp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2488 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_streaming_perception/longshortnet/exp/longshortnet_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7097 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_streaming_perception/longshortnet/longshortnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5774 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_long.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4848 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_short.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9990 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3905 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort_backbone_neck.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_summarization/
+-rw-r--r--   0 runner    (1001) docker     (122)      536 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_summarization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4959 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_summarization/base_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_summarization/kts/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_summarization/kts/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1221 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_summarization/kts/cpd_auto.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3238 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_summarization/kts/cpd_nonlin.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13098 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_summarization/pgl_sum.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8812 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_summarization/summarizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/video_super_resolution/
+-rw-r--r--   0 runner    (1001) docker     (122)      689 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_super_resolution/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14118 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_super_resolution/basicvsr_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4727 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_super_resolution/common.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4720 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_super_resolution/msrresnet_lite_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3636 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_super_resolution/real_basicvsr_for_video_super_resolution.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3637 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/video_super_resolution/real_basicvsr_net.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/vidt/
+-rw-r--r--   0 runner    (1001) docker     (122)      464 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vidt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    40074 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vidt/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25588 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vidt/deformable_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7436 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vidt/fpn_fusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16697 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vidt/head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3651 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vidt/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/virual_tryon/
+-rw-r--r--   0 runner    (1001) docker     (122)      464 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/virual_tryon/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17690 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/virual_tryon/sdafnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/vision_efficient_tuning/
+-rw-r--r--   0 runner    (1001) docker     (122)      502 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vision_efficient_tuning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16102 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vision_efficient_tuning/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)      797 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vision_efficient_tuning/head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1786 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vision_efficient_tuning/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9340 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vision_efficient_tuning/petl.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5021 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vision_efficient_tuning/timm_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28715 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vision_efficient_tuning/timm_vision_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5040 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vision_efficient_tuning/timm_weight_init.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5614 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vision_efficient_tuning/vision_efficient_tuning.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/vision_middleware/
+-rw-r--r--   0 runner    (1001) docker     (122)      492 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vision_middleware/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5749 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vision_middleware/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28089 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vision_middleware/head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6494 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vision_middleware/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6192 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vision_middleware/vim.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/cv/vop_retrieval/
+-rw-r--r--   0 runner    (1001) docker     (122)      919 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vop_retrieval/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12238 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vop_retrieval/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5188 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vop_retrieval/basic_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13520 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vop_retrieval/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5299 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vop_retrieval/model_se.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5177 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/cv/vop_retrieval/tokenization_clip.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/
+-rw-r--r--   0 runner    (1001) docker     (122)     1957 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/clip/
+-rw-r--r--   0 runner    (1001) docker     (122)       97 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/clip/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14496 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/clip/bert_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3898 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/clip/configuration_bert.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22351 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/clip/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20998 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/clip/modeling_bert.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/diffusion/
+-rw-r--r--   0 runner    (1001) docker     (122)      140 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/diffusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26652 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/diffusion/diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14427 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/diffusion/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    39910 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/diffusion/structbert.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11507 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/diffusion/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11834 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/diffusion/unet_generator.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8566 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/diffusion/unet_upsampler_1024.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12361 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/diffusion/unet_upsampler_256.py
+-rw-r--r--   0 runner    (1001) docker     (122)    44602 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/dpm_solver_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/efficient_diffusion_tuning/
+-rw-r--r--   0 runner    (1001) docker     (122)      540 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/efficient_diffusion_tuning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10302 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/efficient_diffusion_tuning/efficient_stable_diffusion.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/gemm/
+-rw-r--r--   0 runner    (1001) docker     (122)      139 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/gemm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21694 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/gemm/gemm_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3718 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/gemm/gemm_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6982 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/gemm/tokenizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/guided_diffusion/
+-rw-r--r--   0 runner    (1001) docker     (122)      548 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/guided_diffusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36265 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/guided_diffusion/gaussian_diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2969 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/guided_diffusion/respace.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1365 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/guided_diffusion/script.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36898 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/guided_diffusion/unet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/mgeo/
+-rw-r--r--   0 runner    (1001) docker     (122)     1444 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mgeo/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)   101942 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mgeo/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8651 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mgeo/text_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3268 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mgeo/text_ranking.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10331 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mgeo/token_classification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/mmr/
+-rw-r--r--   0 runner    (1001) docker     (122)      141 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mmr/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/mmr/dataloaders/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mmr/dataloaders/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3789 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mmr/dataloaders/rawvideo_util.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/mmr/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      162 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mmr/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9797 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mmr/models/clip_for_mm_video_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1442 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mmr/models/dynamic_inverted_softmax.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21257 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mmr/models/modeling.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18790 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mmr/models/module_clip.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3577 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mmr/models/module_cross.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5581 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mmr/models/tokenization_clip.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4116 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mmr/models/until_module.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/mplug/
+-rw-r--r--   0 runner    (1001) docker     (122)      739 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mplug/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/mplug/clip/
+-rw-r--r--   0 runner    (1001) docker     (122)       86 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mplug/clip/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16605 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mplug/clip/clip.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6030 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mplug/configuration_mplug.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)   120753 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mplug/modeling_mplug.py
+-rw-r--r--   0 runner    (1001) docker     (122)    34049 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mplug/mvit.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    21332 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mplug/predictor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5536 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/mplug_for_all_tasks.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/multi_stage_diffusion/
+-rw-r--r--   0 runner    (1001) docker     (122)      150 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/multi_stage_diffusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10208 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/multi_stage_diffusion/clip.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11503 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/multi_stage_diffusion/decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28436 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/multi_stage_diffusion/gaussian_diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13487 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/multi_stage_diffusion/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5276 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/multi_stage_diffusion/prior.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6898 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/multi_stage_diffusion/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16675 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/multi_stage_diffusion/upsampler.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6565 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/multi_stage_diffusion/xglm.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/
+-rw-r--r--   0 runner    (1001) docker     (122)      306 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/adaptor/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/adaptor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18720 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/configuration_mmspeech.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16552 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/configuration_ofa.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/generate/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/generate/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1785 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/generate/incremental_decoding_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21049 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/generate/multihead_attention.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5658 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/generate/ngram_repeat_block.py
+-rw-r--r--   0 runner    (1001) docker     (122)    43439 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/generate/search.py
+-rw-r--r--   0 runner    (1001) docker     (122)    41976 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/generate/sequence_generator.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16690 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/generate/token_generation_constraints.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4883 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/generate/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    44296 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/modeling_mmspeech.py
+-rw-r--r--   0 runner    (1001) docker     (122)   100933 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/modeling_ofa.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13226 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15028 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/tokenization_ofa.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8004 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/tokenization_ofa_fast.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      676 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/utils/constant.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1877 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/utils/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8435 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa/vit.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24779 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa_for_all_tasks.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12983 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/ofa_for_text_to_image_synthesis_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/rleg/
+-rw-r--r--   0 runner    (1001) docker     (122)      538 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/rleg/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4865 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/rleg/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3410 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/rleg/rleg.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/soonet/
+-rw-r--r--   0 runner    (1001) docker     (122)      678 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/soonet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9838 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/soonet/blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11896 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/soonet/clip.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6165 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/soonet/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22584 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/soonet/swin_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4885 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/soonet/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1597 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/soonet/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/team/
+-rw-r--r--   0 runner    (1001) docker     (122)      140 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/team/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5181 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/team/team_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11502 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/team/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/video_synthesis/
+-rw-r--r--   0 runner    (1001) docker     (122)      538 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/video_synthesis/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18477 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/video_synthesis/autoencoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9264 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/video_synthesis/diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9504 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    38498 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/video_synthesis/unet_sd.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/multi_modal/vldoc/
+-rw-r--r--   0 runner    (1001) docker     (122)       93 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/vldoc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11339 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/vldoc/conv_fpn_trans.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6422 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/vldoc/convnext.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16687 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/vldoc/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    47348 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/vldoc/modeling_layout_roberta.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20956 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/vldoc/processing.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4680 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/vldoc/tokenization.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7333 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/multi_modal/vldoc/transformer_local.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/T5/
+-rw-r--r--   0 runner    (1001) docker     (122)      599 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/T5/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    67374 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/T5/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7541 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/T5/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21467 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/T5/text2text_generation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6368 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/bart/
+-rw-r--r--   0 runner    (1001) docker     (122)      112 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/bart/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3058 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/bart/text_error_correction.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/bert/
+-rw-r--r--   0 runner    (1001) docker     (122)     1519 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/bert/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    40458 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/bert/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7308 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/bert/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4113 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/bert/document_segmentation.py
+-rw-r--r--   0 runner    (1001) docker     (122)      512 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/bert/fill_mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6074 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/bert/sentence_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7153 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/bert/siamese_uie.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1478 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/bert/text_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1138 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/bert/text_ranking.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2140 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/bert/token_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6904 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/bert/word_alignment.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/bloom/
+-rw-r--r--   0 runner    (1001) docker     (122)      472 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/bloom/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      505 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/bloom/backbone.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/canmt/
+-rw-r--r--   0 runner    (1001) docker     (122)      102 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/canmt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    52235 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/canmt/canmt_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2940 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/canmt/canmt_translation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    35688 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/canmt/sequence_generator.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/codegeex/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      730 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/codegeex/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    35473 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/codegeex/codegeex.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     3995 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/codegeex/codegeex_for_code_generation.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4008 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/codegeex/codegeex_for_code_translation.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     9997 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/codegeex/inference.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     6111 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/codegeex/tokenizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/csanmt/
+-rw-r--r--   0 runner    (1001) docker     (122)       96 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/csanmt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    61868 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/csanmt/translation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/deberta_v2/
+-rw-r--r--   0 runner    (1001) docker     (122)     1771 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/deberta_v2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    47998 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/deberta_v2/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6771 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/deberta_v2/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10364 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/deberta_v2/fill_mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21421 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/deberta_v2/tokenization.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10698 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/deberta_v2/tokenization_fast.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/dgds/
+-rw-r--r--   0 runner    (1001) docker     (122)      939 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/dgds/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7092 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/dgds/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1656 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/dgds/document_grounded_dialog_generate.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1059 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/dgds/document_grounded_dialog_rerank.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2180 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/dgds/document_grounded_dialog_retrieval.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/fid_T5/
+-rw-r--r--   0 runner    (1001) docker     (122)     1082 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/fid_T5/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8108 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/fid_T5/text_generation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/fid_plug/
+-rw-r--r--   0 runner    (1001) docker     (122)     1181 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/fid_plug/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    45082 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/fid_plug/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5045 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/fid_plug/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6709 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/fid_plug/text_generation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/glm_130b/
+-rw-r--r--   0 runner    (1001) docker     (122)      540 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/glm_130b/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/glm_130b/generation/
+-rw-r--r--   0 runner    (1001) docker     (122)       87 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/glm_130b/generation/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    10112 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/glm_130b/generation/strategies.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     5152 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/glm_130b/initialize.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/glm_130b/kernels/
+-rw-r--r--   0 runner    (1001) docker     (122)     3263 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/glm_130b/kernels/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/glm_130b/quantization/
+-rw-r--r--   0 runner    (1001) docker     (122)     2635 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/glm_130b/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1189 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/glm_130b/quantization/functional.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4510 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/glm_130b/quantization/layers.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    13504 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/glm_130b/text_generation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/gpt2/
+-rw-r--r--   0 runner    (1001) docker     (122)      470 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      498 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt2/backbone.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/gpt3/
+-rw-r--r--   0 runner    (1001) docker     (122)      852 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt3/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15811 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt3/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8971 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt3/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    51600 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt3/distributed_gpt3.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2847 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt3/text_generation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2586 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt3/tokenizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/gpt_moe/
+-rw-r--r--   0 runner    (1001) docker     (122)      874 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt_moe/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13129 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt_moe/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5198 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt_moe/checkpointing.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4930 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt_moe/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    49316 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt_moe/distributed_gpt_moe.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/gpt_moe/moe/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt_moe/moe/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1194 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt_moe/moe/experts.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3504 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt_moe/moe/layer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2553 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt_moe/moe/mappings.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23756 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt_moe/moe/sharded_moe.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4110 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt_moe/moe/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2717 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt_moe/text_generation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2277 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt_moe/tokenizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/gpt_neo/
+-rw-r--r--   0 runner    (1001) docker     (122)      474 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt_neo/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      518 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/gpt_neo/backbone.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      661 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25248 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/heads/crf_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6947 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/heads/fill_mask_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5145 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/heads/infromation_extraction_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2056 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/heads/text_classification_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)      964 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/heads/text_generation_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2029 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/heads/text_ranking_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2596 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/heads/token_classification_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)      948 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/heads/torch_pretrain_head.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/hf_transformers/
+-rw-r--r--   0 runner    (1001) docker     (122)      488 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/hf_transformers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4897 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/hf_transformers/backbone.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/llama/
+-rw-r--r--   0 runner    (1001) docker     (122)      866 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/llama/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    29197 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/llama/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4695 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/llama/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11349 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/llama/convert_llama_weights_to_hf.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7212 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/llama/text_generation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10333 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/llama/tokenization.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4720 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/llama/tokenization_fast.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/lstm/
+-rw-r--r--   0 runner    (1001) docker     (122)      610 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/lstm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1312 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/lstm/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2187 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/lstm/token_classification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/megatron_bert/
+-rw-r--r--   0 runner    (1001) docker     (122)      688 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/megatron_bert/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    39864 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/megatron_bert/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6599 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/megatron_bert/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12452 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/megatron_bert/fill_mask.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/
+-rw-r--r--   0 runner    (1001) docker     (122)      572 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    28115 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/arguments.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28361 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/blocklm_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17995 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/configure_data.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/
+-rw-r--r--   0 runner    (1001) docker     (122)    13656 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    20350 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/corpora.py
+-rw-r--r--   0 runner    (1001) docker     (122)    45773 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/datasets.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3342 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/extraction.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     8459 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/file_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9797 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/lazy_loader.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7221 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/samplers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4661 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/sp_tokenizer.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    53304 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/tokenization.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14551 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/tokenization_gpt2.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    15773 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/wordpiece.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21669 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/generation_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16247 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/mglm_for_text_summarization.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/model/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      984 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/model/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     5443 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/model/distributed.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9967 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/model/downstream.py
+-rw-r--r--   0 runner    (1001) docker     (122)    70249 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/model/modeling_bert.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8743 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/model/modeling_glm.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2531 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/model/prompt.py
+-rw-r--r--   0 runner    (1001) docker     (122)    48886 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/model/transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1955 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/process_grid.py
+-rw-r--r--   0 runner    (1001) docker     (122)      203 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/run_test.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/test/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/test/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      954 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/test/test_block.py
+-rw-r--r--   0 runner    (1001) docker     (122)      708 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/test/test_rel_shift.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17815 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/train_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19029 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/mglm/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/palm_v2/
+-rw-r--r--   0 runner    (1001) docker     (122)     1268 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/palm_v2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5532 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/palm_v2/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    29453 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/palm_v2/dureader_eval.py
+-rw-r--r--   0 runner    (1001) docker     (122)    55450 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/palm_v2/text_generation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/peer/
+-rw-r--r--   0 runner    (1001) docker     (122)     1194 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/peer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    55780 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/peer/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11716 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/peer/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6166 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/peer/sas_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4933 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/peer/text_classification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/plug/
+-rwxr-xr-x   0 runner    (1001) docker     (122)     3321 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/plug/AnnealingLR.py
+-rw-r--r--   0 runner    (1001) docker     (122)      660 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/plug/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    41209 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/plug/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11797 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/plug/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10983 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/plug/distributed_plug.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8427 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/plug/generator.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/plug_mental/
+-rw-r--r--   0 runner    (1001) docker     (122)     1359 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/plug_mental/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7671 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/plug_mental/adv_utils.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    47486 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/plug_mental/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7956 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/plug_mental/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10881 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/plug_mental/text_classification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/ponet/
+-rw-r--r--   0 runner    (1001) docker     (122)     1490 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/ponet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    37918 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/ponet/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6366 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/ponet/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4176 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/ponet/document_segmentation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11011 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/ponet/fill_mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7147 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/ponet/tokenization.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/space/
+-rw-r--r--   0 runner    (1001) docker     (122)      987 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1208 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3832 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space/dialog_intent_prediction.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4354 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space/dialog_modeling.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16648 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space/dialog_state_tracking.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/space/model/
+-rw-r--r--   0 runner    (1001) docker     (122)      421 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space/model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10281 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space/model/gen_unified_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10757 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space/model/generator.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7505 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space/model/intent_unified_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3032 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space/model/model_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1189 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space/model/tokenization_space.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11941 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space/model/unified_transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/space/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2398 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space/modules/embedder.py
+-rw-r--r--   0 runner    (1001) docker     (122)      956 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space/modules/feedforward.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1721 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space/modules/functions.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3397 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space/modules/multihead_attention.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1922 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space/modules/transformer_block.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/space_T_cn/
+-rw-r--r--   0 runner    (1001) docker     (122)      529 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space_T_cn/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    44438 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space_T_cn/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5194 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space_T_cn/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30117 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space_T_cn/table_question_answering.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/space_T_en/
+-rw-r--r--   0 runner    (1001) docker     (122)      492 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space_T_en/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3715 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/space_T_en/text_to_sql.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/structbert/
+-rw-r--r--   0 runner    (1001) docker     (122)     1674 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/structbert/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7673 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/structbert/adv_utils.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    40848 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/structbert/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7686 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/structbert/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    27203 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/structbert/faq_question_answering.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12581 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/structbert/fill_mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11862 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/structbert/text_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11074 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/structbert/token_classification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/task_models/
+-rw-r--r--   0 runner    (1001) docker     (122)     1453 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/task_models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5323 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/task_models/feature_extraction.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2698 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/task_models/fill_mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)      766 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/task_models/information_extraction.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28991 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/task_models/task_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1912 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/task_models/text_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8678 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/task_models/text_generation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2092 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/task_models/text_ranking.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5116 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/task_models/token_classification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/unite/
+-rw-r--r--   0 runner    (1001) docker     (122)      622 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/unite/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      412 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/unite/configuration_unite.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17494 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/unite/modeling_unite.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/use/
+-rw-r--r--   0 runner    (1001) docker     (122)      545 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/use/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5133 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/use/transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5795 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/use/user_satisfaction_estimation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/veco/
+-rw-r--r--   0 runner    (1001) docker     (122)     1479 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/veco/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4031 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/veco/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1192 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/veco/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4278 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/veco/fill_mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6643 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/veco/text_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4129 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/veco/token_classification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/nlp/xlm_roberta/
+-rw-r--r--   0 runner    (1001) docker     (122)     1156 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/xlm_roberta/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    42932 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/xlm_roberta/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7697 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/nlp/xlm_roberta/configuration.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/science/
+-rw-r--r--   0 runner    (1001) docker     (122)      491 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/science/unifold/
+-rw-r--r--   0 runner    (1001) docker     (122)       46 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24057 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/config.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/science/unifold/data/
+-rw-r--r--   0 runner    (1001) docker     (122)      634 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    49139 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/data/data_ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19694 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/data/msa_pairing.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8941 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/data/process.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14792 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/data/process_multimer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11422 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/data/protein.py
+-rw-r--r--   0 runner    (1001) docker     (122)    42023 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/data/residue_constants.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4628 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/data/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19250 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2307 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/science/unifold/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)      187 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17066 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/modules/alphafold.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12323 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/modules/attentions.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5403 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/modules/auxillary_heads.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10676 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/modules/common.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5814 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/modules/confidence.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8771 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/modules/embedders.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11078 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/modules/evoformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6820 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/modules/featurization.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18025 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/modules/frame.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18476 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/modules/structure_module.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9771 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/modules/template.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5623 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/modules/triangle_multiplication.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/science/unifold/msa/
+-rw-r--r--   0 runner    (1001) docker     (122)       46 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/msa/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17940 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/msa/mmcif.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3128 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/msa/msa_identifiers.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23503 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/msa/parsers.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11644 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/msa/pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    45466 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/msa/templates.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/models/science/unifold/msa/tools/
+-rw-r--r--   0 runner    (1001) docker     (122)      639 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/msa/tools/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6218 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/msa/tools/hhblits.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3991 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/msa/tools/hhsearch.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5098 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/msa/tools/hmmbuild.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5013 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/msa/tools/hmmsearch.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8662 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/msa/tools/jackhmmer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3752 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/msa/tools/kalign.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1255 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/msa/tools/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2892 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/models/science/unifold/msa/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/
+-rw-r--r--   0 runner    (1001) docker     (122)       84 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/audio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      346 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/audio/asr_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/auth/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/auth/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1028 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/auth/auth_config.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/context/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/context/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3321 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/context/dataset_context_config.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/data_files/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/data_files/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5110 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/data_files/data_files_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/data_loader/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/data_loader/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6583 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/data_loader/data_loader.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5767 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/data_loader/data_loader_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/
+-rw-r--r--   0 runner    (1001) docker     (122)      111 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/
+-rw-r--r--   0 runner    (1001) docker     (122)     4817 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)      730 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1737 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/asr_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8512 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_farfield_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7723 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11689 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_processor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/
+-rw-r--r--   0 runner    (1001) docker     (122)      541 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1264 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/bad_image_detecting_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)      791 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/
+-rw-r--r--   0 runner    (1001) docker     (122)      134 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4708 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/build.py
+-rw-r--r--   0 runner    (1001) docker     (122)      945 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/collate_batch.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/
+-rw-r--r--   0 runner    (1001) docker     (122)      177 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3815 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/coco.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16180 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/mosaic_wrapper.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/
+-rw-r--r--   0 runner    (1001) docker     (122)      934 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/
+-rw-r--r--   0 runner    (1001) docker     (122)      544 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11572 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/coco_eval.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/
+-rw-r--r--   0 runner    (1001) docker     (122)      312 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2622 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/distributed.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4858 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/grouped_batch_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1469 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/iteration_based_batch_sampler.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/
+-rw-r--r--   0 runner    (1001) docker     (122)      196 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1086 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/build.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2509 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1511 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/easycv_base.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/face_2d_keypoins/
+-rw-r--r--   0 runner    (1001) docker     (122)      512 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/face_2d_keypoins/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1459 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/face_2d_keypoins/face_2d_keypoints_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2141 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/gopro_image_deblurring_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/hand_2d_keypoints/
+-rw-r--r--   0 runner    (1001) docker     (122)      536 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/hand_2d_keypoints/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1498 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/hand_2d_keypoints/hand_2d_keypoints_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/human_wholebody_keypoint/
+-rw-r--r--   0 runner    (1001) docker     (122)      556 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/human_wholebody_keypoint/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1565 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/human_wholebody_keypoint/human_wholebody_keypoint_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_classification/
+-rw-r--r--   0 runner    (1001) docker     (122)      488 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_classification/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1393 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_classification/classification_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/
+-rw-r--r--   0 runner    (1001) docker     (122)      539 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2196 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/image_colorization_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/
+-rw-r--r--   0 runner    (1001) docker     (122)      529 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2968 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/aug.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12152 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12574 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_instance_segmentation_coco_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/
+-rw-r--r--   0 runner    (1001) docker     (122)      577 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1091 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1629 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/image_portrait_enhancement_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/
+-rw-r--r--   0 runner    (1001) docker     (122)      615 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1487 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/image_quality_assessment_degradation_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/
+-rw-r--r--   0 runner    (1001) docker     (122)      583 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1194 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/image_quality_assessment_mos_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_semantic_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      483 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_semantic_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1792 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_semantic_segmentation/segmentation_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4849 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/language_guided_video_summarization_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6225 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/mgeo_ranking_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      559 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6171 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4013 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/sampler.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/object_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      536 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/object_detection/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4294 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/object_detection/detection_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      161 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1589 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/augmenter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4945 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/data_loader.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5437 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/image_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/
+-rw-r--r--   0 runner    (1001) docker     (122)       40 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7547 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/iou_evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3416 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/quad_measurer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/
+-rw-r--r--   0 runner    (1001) docker     (122)      309 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3172 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/augment_data.py
+-rw-r--r--   0 runner    (1001) docker     (122)      971 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/data_process.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5818 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_border_map.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2000 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_icdar_data.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3512 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_seg_detection_data.py
+-rw-r--r--   0 runner    (1001) docker     (122)      707 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/normalize_image.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4912 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/random_crop_data.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2297 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_recognition_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2017 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/reds_image_deblurring_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      599 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16767 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/referring_video_object_segmentation_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8725 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/transformers.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/
+-rw-r--r--   0 runner    (1001) docker     (122)      545 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1475 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1984 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/sidd_image_denoising_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2684 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5114 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1635 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/torch_custom_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2664 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/veco_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/
+-rw-r--r--   0 runner    (1001) docker     (122)      573 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1390 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/video_frame_interpolation_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/
+-rw-r--r--   0 runner    (1001) docker     (122)      543 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      809 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/video_stabilization_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2624 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_summarization_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/
+-rw-r--r--   0 runner    (1001) docker     (122)      553 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2370 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/video_super_resolution_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4123 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/dataset_cls/dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/download/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/download/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16700 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/download/dataset_builder.py
+-rw-r--r--   0 runner    (1001) docker     (122)      609 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/download/download_config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2487 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/download/download_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/meta/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/meta/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1772 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/meta/data_meta_config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8288 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/meta/data_meta_manager.py
+-rw-r--r--   0 runner    (1001) docker     (122)    35780 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/ms_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/task_datasets/
+-rw-r--r--   0 runner    (1001) docker     (122)     1072 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/task_datasets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      408 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/task_datasets/gopro_image_deblurring_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)      401 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/task_datasets/reds_image_deblurring_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)      404 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/task_datasets/sidd_image_denoising.py
+-rw-r--r--   0 runner    (1001) docker     (122)      410 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/task_datasets/torch_base_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)      404 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/task_datasets/video_summarization_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/msdatasets/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7610 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/utils/dataset_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1026 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/utils/delete_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6124 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/utils/oss_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2496 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/msdatasets/utils/upload_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/ops/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/ops/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/ops/ailut/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/ops/ailut/Ailut/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/ops/ailut/Ailut/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/ops/ailut/Ailut/csrc/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/ops/ailut/Ailut/csrc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6065 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/ops/ailut/Ailut/csrc/ailut_transform.cpp
+-rw-r--r--   0 runner    (1001) docker     (122)    27053 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/ops/ailut/Ailut/csrc/ailut_transform_cpu.cpp
+-rw-r--r--   0 runner    (1001) docker     (122)    31866 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/ops/ailut/Ailut/csrc/ailut_transform_cuda.cu
+-rw-r--r--   0 runner    (1001) docker     (122)      105 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/ops/ailut/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4314 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/ops/ailut/pyinterfaces.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/ops/quadtree_attention/
+-rw-r--r--   0 runner    (1001) docker     (122)      106 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/ops/quadtree_attention/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/ops/quadtree_attention/functions/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/ops/quadtree_attention/functions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2925 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/ops/quadtree_attention/functions/quadtree_attention.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/ops/quadtree_attention/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/ops/quadtree_attention/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13956 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/ops/quadtree_attention/modules/quadtree_attention.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/ops/quadtree_attention/src/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/ops/quadtree_attention/src/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1425 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/ops/quadtree_attention/src/score_computation.cpp
+-rw-r--r--   0 runner    (1001) docker     (122)     1008 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/ops/quadtree_attention/src/score_computation.h
+-rw-r--r--   0 runner    (1001) docker     (122)     6033 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/ops/quadtree_attention/src/score_computation_kernal.cu
+-rw-r--r--   0 runner    (1001) docker     (122)      612 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/ops/quadtree_attention/src/utils.h
+-rw-r--r--   0 runner    (1001) docker     (122)     2369 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/ops/quadtree_attention/src/value_aggregation.cpp
+-rw-r--r--   0 runner    (1001) docker     (122)      815 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/ops/quadtree_attention/src/value_aggregation.h
+-rw-r--r--   0 runner    (1001) docker     (122)     3600 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/ops/quadtree_attention/src/value_aggregation_kernel.cu
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/outputs/
+-rw-r--r--   0 runner    (1001) docker     (122)      132 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/outputs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      908 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/outputs/cv_outputs.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19638 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/outputs/nlp_outputs.py
+-rw-r--r--   0 runner    (1001) docker     (122)    42453 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/outputs/outputs.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9630 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipeline_inputs.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/pipelines/
+-rw-r--r--   0 runner    (1001) docker     (122)      242 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/pipelines/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)     1567 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/audio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7481 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/audio/ans_dfsmn_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5016 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/audio/ans_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    27456 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/audio/asr_inference_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3189 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/audio/asr_wenet_inference_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4357 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/audio/inverse_text_processing_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3830 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/audio/kws_farfield_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7276 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/audio/kws_kwsbp_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5555 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/audio/linear_aec_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7916 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/audio/lm_infer_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6329 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/audio/punctuation_processing_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2560 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/audio/separation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10993 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/audio/speaker_diarization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4118 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/audio/speaker_verification_light_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10369 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/audio/speaker_verification_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1789 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/audio/text_to_speech_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11785 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/audio/timestamp_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9544 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/audio/voice_activity_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22330 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7279 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/pipelines/cv/
+-rw-r--r--   0 runner    (1001) docker     (122)    17487 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2545 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/action_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4852 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/action_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4024 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/animal_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2576 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/arc_face_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2614 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/bad_image_detecting_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9581 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/body_2d_keypoints_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14219 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/body_3d_keypoints_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4740 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/card_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5307 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/cmdssl_video_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2528 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/content_check_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5055 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/controllable_image_generation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5942 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/crowd_counting_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5136 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/ddcolor_image_colorization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1965 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/ddpm_semantic_segmentation_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/pipelines/cv/easycv_pipelines/
+-rw-r--r--   0 runner    (1001) docker     (122)      959 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/easycv_pipelines/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4476 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/easycv_pipelines/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2241 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/easycv_pipelines/detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8202 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/easycv_pipelines/face_2d_keypoints_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2491 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/easycv_pipelines/human_wholebody_keypoint_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1599 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/easycv_pipelines/segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2498 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/face_attribute_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3980 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/face_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1531 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/face_emotion_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1648 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/face_human_hand_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2894 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/face_image_generation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3312 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/face_liveness_ir_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3573 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/face_liveness_xc_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7748 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/face_processing_base_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3872 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/face_quality_assessment_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3528 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/face_recognition_onnx_fm_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3493 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/face_recognition_onnx_ir_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2878 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/face_recognition_ood_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2498 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/face_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19633 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/face_reconstruction_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2400 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/facial_expression_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2681 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/facial_landmark_confidence_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4030 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/general_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2055 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/hand_2d_keypoints_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1360 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/hand_static_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2982 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/hicossl_video_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4372 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/human_reconstruction_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1399 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_body_reshaping_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2919 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_bts_depth_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5251 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_cartoon_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5857 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_classification_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3296 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_color_enhance_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4441 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_colorization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2648 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_debanding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4624 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_deblur_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3517 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_defrcn_fewshot_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4160 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_denoise_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1943 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_depth_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1852 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4134 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_driving_perception_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2317 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_face_fusion_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4866 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_human_parsing_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5854 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_inpainting_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4673 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_inpainting_sdv2_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3903 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_instance_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5929 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_matching_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2610 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_matting_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2796 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_mvs_depth_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2770 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_open_vocabulary_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6113 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_paintbyexample_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4807 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_panoptic_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8567 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_portrait_enhancement_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3549 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_quality_assessment_degradation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2835 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_quality_assessment_man_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2801 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_quality_assessment_mos_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2026 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_reid_person_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1702 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_restoration_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1644 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_salient_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3207 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_semantic_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2237 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_skychange_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2839 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_structured_model_probing_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4621 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_style_transfer_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3138 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_super_resolution_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9718 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_to_image_generate_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12737 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/image_to_image_translation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1883 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/indoor_layout_estimation_pipeline.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     9872 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/language_guided_video_summarization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4570 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/license_plate_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4182 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/lineless_table_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5276 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/live_category_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2757 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/mask_face_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2787 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/maskdino_instance_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3670 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/mobile_image_super_resolution_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1857 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/mog_face_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4844 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/motion_generation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2403 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/movie_scene_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1953 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/mtcnn_face_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3328 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/nerf_recon_acc_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6018 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/object_detection_3d_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11033 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/ocr_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2500 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/ocr_recognition_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      966 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      720 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/model_convnext_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19821 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/model_dla34.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8768 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/model_resnet18_half.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5286 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/model_resnet_mutex_v4_linewithchar.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15421 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/model_vlpt.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/
+-rw-r--r--   0 runner    (1001) docker     (122)      550 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6209 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/convnext.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11644 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/timm_tinyc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1560 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/vitstr.py
+-rw-r--r--   0 runner    (1001) docker     (122)    38278 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19004 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/resnet18_v1.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11371 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/resnet_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11168 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/table_process.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7971 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3029 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/panorama_depth_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9781 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/pedestrian_attribute_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4199 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1457 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/product_retrieval_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1451 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/product_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2020 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/realtime_video_object_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9237 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/referring_video_object_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1956 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/retina_face_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1760 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/shop_segmentation_pipleline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12132 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/skin_retouching_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4446 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/table_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4902 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/tbs_detection_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/pipelines/cv/tbs_detection_utils/
+-rw-r--r--   0 runner    (1001) docker     (122)       10 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/tbs_detection_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15542 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/tbs_detection_utils/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1841 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/text_driven_segmentation_pipleline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3246 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/tinynas_classification_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3244 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/tinynas_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1888 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/ulfd_face_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13776 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/video_category_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6097 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/video_colorization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7757 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/video_deinterlace_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1565 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/video_depth_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24271 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/video_frame_interpolation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3635 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/video_human_matting_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1719 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/video_inpainting_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9652 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/video_instance_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3294 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/video_multi_object_tracking_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4724 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/video_object_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4683 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/video_panoptic_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3436 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/video_single_object_tracking_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4647 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/video_stabilization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4289 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/video_summarization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7078 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/video_super_resolution_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7564 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/vidt_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5240 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/virtual_try_on_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3986 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/vision_efficient_tuning_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2180 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/vision_middleware_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4762 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/vop_retrieval_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5787 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/cv/vop_retrieval_se_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/
+-rw-r--r--   0 runner    (1001) docker     (122)     2927 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2366 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/asr_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/diffusers_wrapped/
+-rw-r--r--   0 runner    (1001) docker     (122)      636 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/diffusers_wrapped/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1993 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/diffusers_wrapped/diffusers_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/
+-rw-r--r--   0 runner    (1001) docker     (122)      718 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11610 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2870 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/
+-rw-r--r--   0 runner    (1001) docker     (122)      585 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15631 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18987 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2311 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/document_vl_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2931 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/efficient_diffusion_tuning_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1093 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9352 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/gridvlp_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2634 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/image_captioning_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1612 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/image_text_retrieval_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7869 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/mgeo_ranking_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1648 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/multi_modal_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/ocr_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8561 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1848 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/sudoku_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1059 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/team_multi_modal_similarity_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1789 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/text2sql_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2064 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/text_to_image_synthesis_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3160 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/text_to_video_synthesis_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2150 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/video_captioning_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1329 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/video_multi_modal_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1956 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/video_question_answering_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/visual_entailment_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1781 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/visual_grounding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2396 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/multi_modal/visual_question_answering_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/pipelines/nlp/
+-rw-r--r--   0 runner    (1001) docker     (122)     6798 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6939 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/automatic_post_editing_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3696 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/canmt_translation_pipeline.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2202 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/codegeex_code_generation_pipeline.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2885 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/codegeex_code_translation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2251 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/conversational_text_to_sql_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2464 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/dialog_intent_prediction_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2308 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/dialog_modeling_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7550 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/dialog_state_tracking_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2218 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/distributed_gpt3_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1851 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/distributed_gpt_moe_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4425 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/distributed_plug_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2641 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/document_grounded_dialog_generate_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25886 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/document_grounded_dialog_rerank_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5367 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/document_grounded_dialog_retrieval_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9648 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/document_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6156 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/extractive_summarization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3348 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/faq_question_answering_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2437 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/fasttext_text_classification_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3145 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/feature_extraction_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10173 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/fid_dialogue_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4542 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/fill_mask_pipeline.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)      999 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/glm130b_text_generation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2383 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/information_extraction_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6363 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/interactive_translation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9928 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/language_identification_pipline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1776 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/mglm_text_summarization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2842 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/named_entity_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3071 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/sentence_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14210 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/siamese_uie_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2566 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/summarization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16202 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/table_question_answering_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6797 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/text_classification_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3490 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/text_error_correction_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7622 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/text_generation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2764 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/text_ranking_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6286 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/token_classification_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4312 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/translation_evaluation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5586 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/translation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2339 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/translation_quality_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4473 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/user_satisfaction_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2713 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/word_alignment_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4523 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/word_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5762 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/nlp/zero_shot_classification_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/pipelines/science/
+-rw-r--r--   0 runner    (1001) docker     (122)      538 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/science/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8255 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/science/protein_structure_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3492 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/pipelines/util.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/preprocessors/
+-rw-r--r--   0 runner    (1001) docker     (122)     5610 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12580 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/asr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8680 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/audio.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15312 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)      812 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/builder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6982 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/common.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/preprocessors/cv/
+-rw-r--r--   0 runner    (1001) docker     (122)     1896 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/cv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7446 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/cv/action_detection_mapper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1126 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/cv/bad_image_detecting_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7789 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/cv/controllable_image_generation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20909 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/cv/cv2_transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12826 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/cv/image_classification_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1250 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/cv/image_quality_assessment_man.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2113 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/cv/image_quality_assessment_mos.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2790 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/cv/image_restoration_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2625 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/cv/mmcls_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3050 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/cv/timer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3606 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/cv/util.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1501 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/cv/video_stabilization.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8579 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/cv/video_super_resolution.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13406 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/image.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4884 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/kws.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/preprocessors/movie_scene_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      483 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/movie_scene_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10809 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/movie_scene_segmentation/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24768 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/multi_modal.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/
+-rw-r--r--   0 runner    (1001) docker     (122)     5942 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      762 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/bert_seq_cls_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3929 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/canmt_translation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2572 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/dialog_classification_use_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4102 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4391 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4086 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11004 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/document_segmentation_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6844 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/faq_question_answering_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3304 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/feature_extraction_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12086 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/fill_mask_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8199 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/mgeo_ranking_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1168 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/mglm_summarization_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1799 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/relation_extraction_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4023 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/sentence_embedding_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1424 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/siamese_uie_preprocessor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space/
+-rw-r--r--   0 runner    (1001) docker     (122)     1219 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1908 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space/args.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1555 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space/batch.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3629 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space/data_loader.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2701 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space/dialog_intent_prediction_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2789 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space/dialog_modeling_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5457 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space/dialog_state_tracking_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)    56779 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space/dst_processors.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space/fields/
+-rw-r--r--   0 runner    (1001) docker     (122)      592 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space/fields/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    33884 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space/fields/gen_field.py
+-rw-r--r--   0 runner    (1001) docker     (122)    42465 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space/fields/intent_field.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1147 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space/lazy_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1935 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space/preprocess.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1610 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space/sampler.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2084 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space/tensorlistdataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24803 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space/tokenizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_cn/
+-rw-r--r--   0 runner    (1001) docker     (122)      654 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_cn/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_cn/fields/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_cn/fields/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4936 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_cn/fields/database.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15867 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_cn/fields/schema_link.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4888 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_cn/fields/struct.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4175 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_cn/table_question_answering_preprocessor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_en/
+-rw-r--r--   0 runner    (1001) docker     (122)      846 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_en/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4902 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_en/conversational_text_to_sql_preprocessor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_en/fields/
+-rw-r--r--   0 runner    (1001) docker     (122)      818 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_en/fields/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21521 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_en/fields/common_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12500 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_en/fields/parse.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1380 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_en/fields/preprocess_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2111 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_en/fields/process_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6422 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/text_classification_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1641 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/text_clean.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2298 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/text_error_correction.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13896 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/text_generation_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3834 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/text_ranking_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19473 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/token_classification_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1640 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/token_classification_thai_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1238 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/token_classification_viet_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4482 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/transformers_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3566 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/translation_evaluation_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3622 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4962 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/word_alignment_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2584 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/nlp/zero_shot_classification_preprocessor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/
+-rw-r--r--   0 runner    (1001) docker     (122)      762 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4839 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/asr.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11909 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4172 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/image_captioning.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6461 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/image_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5740 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/ocr_recognition.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4603 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/sudoku.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5210 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/summarization.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16449 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/text2sql.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5368 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/text_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2164 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/text_to_image_synthesis.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3221 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/utils/audio_helper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8936 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/utils/bridge_content_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6809 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/utils/collate.py
+-rw-r--r--   0 runner    (1001) docker     (122)      660 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/utils/constant.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3421 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/utils/get_tables.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1302 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/utils/random_help.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5459 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/utils/text2phone.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19116 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/utils/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9581 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/utils/vision_helper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7664 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/visual_entailment.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8484 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/visual_grounding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6796 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/ofa/visual_question_answering.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/preprocessors/science/
+-rw-r--r--   0 runner    (1001) docker     (122)      478 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/science/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21698 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/science/uni_fold.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2101 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/tts.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13168 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/preprocessors/video.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/tools/
+-rw-r--r--   0 runner    (1001) docker     (122)       49 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/tools/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      772 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/tools/eval.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5362 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/tools/speech_tts_autolabel.py
+-rw-r--r--   0 runner    (1001) docker     (122)      607 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/tools/train.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/
+-rw-r--r--   0 runner    (1001) docker     (122)     1595 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)      826 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/audio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1873 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/audio/ans_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6969 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/audio/asr_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12816 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/audio/kws_farfield_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22065 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/audio/kws_nearfield_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/audio/kws_utils/
+-rw-r--r--   0 runner    (1001) docker     (122)     1768 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/audio/kws_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22114 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/audio/kws_utils/batch_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11499 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/audio/kws_utils/det_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7022 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/audio/kws_utils/file_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4509 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/audio/kws_utils/model_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2854 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/audio/kws_utils/runtime_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21962 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/audio/separation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10858 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/audio/tts_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4389 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1808 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/cv/
+-rw-r--r--   0 runner    (1001) docker     (122)     1974 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/cv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7494 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/cv/action_detection_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)      668 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/cv/card_detection_scrfd_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10044 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/cv/cartoon_translation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6737 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/cv/face_detection_scrfd_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20051 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/cv/image_classifition_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11655 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/cv/image_defrcn_fewshot_detection_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22965 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/cv/image_detection_damoyolo_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4405 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/cv/image_inpainting_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)      816 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/cv/image_instance_segmentation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5520 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/cv/image_portrait_enhancement_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)      680 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/cv/movie_scene_segmentation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20149 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/cv/nerf_recon_acc_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16911 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/cv/ocr_detection_db_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3153 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/cv/ocr_recognition_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2248 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/cv/referring_video_object_segmentation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4602 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/cv/vision_efficient_tuning_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3229 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/default_config.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/easycv/
+-rw-r--r--   0 runner    (1001) docker     (122)      487 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/easycv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8237 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/easycv/trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/easycv/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      523 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/easycv/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1029 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/easycv/utils/hooks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1855 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/easycv/utils/metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3415 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/easycv/utils/register_util.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/hooks/
+-rw-r--r--   0 runner    (1001) docker     (122)     1650 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      296 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/builder.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30111 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/checkpoint_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)      764 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/clip_clamp_logit_scale_hook.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/hooks/compression/
+-rw-r--r--   0 runner    (1001) docker     (122)      610 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/compression/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4847 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/compression/sparsity_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6915 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/compression/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1383 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/ddp_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6843 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/deepspeed_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4020 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/early_stop_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3062 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/evaluation_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8101 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)      731 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/iter_timer_hook.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/hooks/logger/
+-rw-r--r--   0 runner    (1001) docker     (122)      718 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/logger/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4498 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/logger/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4507 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/logger/tensorboard_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7443 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/logger/text_logger_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5146 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/lr_scheduler_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6166 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/megatron_hook.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/hooks/optimizer/
+-rw-r--r--   0 runner    (1001) docker     (122)      743 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/optimizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2845 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/optimizer/apex_optimizer_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3585 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/optimizer/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3301 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/optimizer/torch_optimizer_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1707 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/hooks/priority.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/lrscheduler/
+-rw-r--r--   0 runner    (1001) docker     (122)      699 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/lrscheduler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2040 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/lrscheduler/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/lrscheduler/warmup/
+-rw-r--r--   0 runner    (1001) docker     (122)      614 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/lrscheduler/warmup/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2547 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/lrscheduler/warmup/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2934 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/lrscheduler/warmup/warmup.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/multi_modal/
+-rw-r--r--   0 runner    (1001) docker     (122)      682 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/multi_modal/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/multi_modal/clip/
+-rw-r--r--   0 runner    (1001) docker     (122)       89 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/multi_modal/clip/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9742 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/multi_modal/clip/clip_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4388 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/multi_modal/clip/clip_trainer_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/multi_modal/efficient_diffusion_tuning/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/multi_modal/efficient_diffusion_tuning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2874 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/multi_modal/efficient_diffusion_tuning/efficient_diffusion_tuning_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8086 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/multi_modal/mgeo_ranking_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/multi_modal/mplug/
+-rw-r--r--   0 runner    (1001) docker     (122)       91 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/multi_modal/mplug/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1642 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/multi_modal/mplug/mplug_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/multi_modal/ofa/
+-rw-r--r--   0 runner    (1001) docker     (122)       87 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/multi_modal/ofa/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10186 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/multi_modal/ofa/ofa_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17647 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/multi_modal/ofa/ofa_trainer_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/multi_modal/team/
+-rw-r--r--   0 runner    (1001) docker     (122)       95 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/multi_modal/team/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5429 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/multi_modal/team/team_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2443 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/multi_modal/team/team_trainer_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/nlp/
+-rw-r--r--   0 runner    (1001) docker     (122)     1179 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14266 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/csanmt_translation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9999 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/document_grounded_dialog_generate_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24284 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/document_grounded_dialog_rerank_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8159 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/document_grounded_dialog_retrieval_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12769 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/faq_question_answering_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3064 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/gpt3_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2097 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/gpt_moe_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8171 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/plug_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3869 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/sentence_embedding_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8378 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/sequence_classification_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17110 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/siamese_uie_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/nlp/space/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/space/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5324 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/space/dialog_intent_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4229 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/space/dialog_modeling_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36354 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/space/eval.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/nlp/space/metrics/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/space/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2447 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/space/metrics/metrics_tracker.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/nlp/space/trainer/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/space/trainer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30567 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/space/trainer/gen_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    29570 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/space/trainer/intent_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20453 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/table_question_answering_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)      991 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/text_generation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7514 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp/text_ranking_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7129 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/nlp_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/optimizer/
+-rw-r--r--   0 runner    (1001) docker     (122)      223 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/optimizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1753 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/optimizer/builder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7081 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/optimizer/child_tuning_adamw_optimizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/parallel/
+-rw-r--r--   0 runner    (1001) docker     (122)       80 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/parallel/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      681 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/parallel/builder.py
+-rw-r--r--   0 runner    (1001) docker     (122)      754 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/parallel/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    58132 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28082 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/training_args.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/trainers/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11090 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/utils/inference.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1294 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/trainers/utils/log_buffer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/tuners/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/tuners/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    38904 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/tuners/control_sd_lora.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24058 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/tuners/lora.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8840 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/tuners/sd_lora.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)   521265 2023-04-17 02:47:50.000000 modelscope-1.5.0/modelscope/utils/ast_index_file.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28886 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/ast_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/utils/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/audio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12122 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/audio/audio_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2428 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/audio/tts_exceptions.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26452 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2573 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/chinese_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    27433 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1027 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/config_ds.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17934 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/constant.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/utils/cv/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/cv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23195 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/cv/image_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/utils/cv/motion_utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/cv/motion_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2307 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/cv/motion_utils/motion_process.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3847 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/cv/motion_utils/plot_script.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4290 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/cv/motion_utils/rotation_conversions.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3153 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/data_collators.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1279 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9437 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/demo_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3744 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/device.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6147 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/error.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1128 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/file_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5939 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/hub.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15752 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/import_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)      478 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/json_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2789 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/logger.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7561 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/megatron_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2335 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5099 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/model_tag.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/utils/nlp/
+-rw-r--r--   0 runner    (1001) docker     (122)      499 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/nlp/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4424 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/nlp/distributed.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4526 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/nlp/load_checkpoint.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/utils/nlp/space/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/nlp/space/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1909 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/nlp/space/args.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11818 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/nlp/space/clean_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1439 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/nlp/space/criterions.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11252 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/nlp/space/db_ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6174 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/nlp/space/ontology.py
+-rw-r--r--   0 runner    (1001) docker     (122)      197 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/nlp/space/scores.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6356 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/nlp/space/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1054 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/nlp/space/utils_dst.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope/utils/nlp/space_T_en/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/nlp/space_T_en/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      859 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/nlp/space_T_en/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2554 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/nlp/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    35005 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/plugins.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7833 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/registry.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30201 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/regress_test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5892 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/service_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2500 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/task_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1593 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/tensor_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12584 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1209 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/timer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10966 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/torch_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)      597 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/trie.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1696 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/type_assert.py
+-rw-r--r--   0 runner    (1001) docker     (122)      296 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/utils/typing.py
+-rw-r--r--   0 runner    (1001) docker     (122)      272 2023-04-17 02:46:45.000000 modelscope-1.5.0/modelscope/version.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (122)    16425 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (122)   132676 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (122)        1 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (122)       59 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (122)        1 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope.egg-info/not-zip-safe
+-rw-r--r--   0 runner    (1001) docker     (122)     4703 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (122)       11 2023-04-17 02:47:51.000000 modelscope-1.5.0/modelscope.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (122)       38 2023-04-17 02:47:51.000000 modelscope-1.5.0/setup.cfg
```

### Comparing `modelscope-1.4.2/PKG-INFO` & `modelscope-1.5.0/PKG-INFO`

 * *Files 19% similar despite different names*

```diff
@@ -1,303 +1,15 @@
 Metadata-Version: 2.1
 Name: modelscope
-Version: 1.4.2
+Version: 1.5.0
 Summary: UNKNOWN
 Home-page: https://github.com/modelscope/modelscope
 Author: Alibaba ModelScope team
 Author-email: modelscope@list.alibaba-inc.com
 License: Apache License 2.0
-Description: 
-        <p align="center">
-            <br>
-            <img src="https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif" width="400"/>
-            <br>
-        <p>
-        
-        <div align="center">
-        
-        [![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/modelscope/)
-        <!-- [![Documentation Status](https://readthedocs.org/projects/easy-cv/badge/?version=latest)](https://easy-cv.readthedocs.io/en/latest/) -->
-        [![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)
-        [![open issues](https://isitmaintained.com/badge/open/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/issues)
-        [![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/pull/)
-        [![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/modelscope)](https://GitHub.com/modelscope/modelscope/commit/)
-        [![Leaderboard](https://img.shields.io/badge/ModelScope-Check%20Your%20Contribution-orange)](https://opensource.alibaba.com/contribution_leaderboard/details?projectValue=modelscope)
-        
-        <!-- [![GitHub contributors](https://img.shields.io/github/contributors/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/graphs/contributors/) -->
-        <!-- [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) -->
-        
-        <h4 align="center">
-            <p>
-                <b>English</b> |
-                <a href="https://github.com/modelscope/modelscope/blob/master/README_zh.md">中文</a>
-            <p>
-        </h4>
-        
-        
-        </div>
-        
-        # Introduction
-        
-        [ModelScope]( https://www.modelscope.cn) is built upon the notion of “Model-as-a-Service” (MaaS). It seeks to bring together most advanced machine learning models from the AI community, and streamlines the process of leveraging AI models in real-world applications. The core ModelScope library open-sourced in this repository provides the interfaces and implementations that allow developers to perform  model inference, training and evaluation.
-        
-        
-        In particular, with rich layers of API-abstraction, the ModelScope library offers unified experience to explore state-of-the-art models spanning across domains such as CV, NLP, Speech, Multi-Modality, and Scientific-computation. Model contributors of different areas can integrate models into the ModelScope ecosystem through the layered-APIs, allowing easy and unified access to their models. Once integrated, model inference, fine-tuning, and evaluations can be done with only a few lines of codes. In the meantime, flexibilities are also provided so that different components in the model applications can be customized wherever necessary.
-        
-        Apart from harboring implementations of a wide range of different models, ModelScope library also enables the necessary interactions with ModelScope backend services, particularly with the Model-Hub and Dataset-Hub. Such interactions facilitate management of  various entities (models and datasets) to be performed seamlessly under-the-hood, including entity lookup, version control, cache management, and many others.
-        
-        # Models and Online Accessibility
-        
-        Hundreds of models are made publicly available on [ModelScope]( https://www.modelscope.cn)  (700+ and counting), covering the latest development in areas such as NLP, CV, Audio, Multi-modality, and AI for Science, etc. Many of these models represent the SOTA in their specific fields, and made their open-sourced debut on ModelScope. Users can visit ModelScope([modelscope.cn](http://www.modelscope.cn)) and experience first-hand how these models perform via online experience, with just a few clicks. Immediate developer-experience is also possible through the ModelScope Notebook, which is backed by ready-to-use CPU/GPU development environment in the cloud - only one click away on [ModelScope](https://www.modelscope.cn).
-        
-        
-        <p align="center">
-            <br>
-            <img src="data/resource/inference.gif" width="1024"/>
-            <br>
-        <p>
-        
-        Some representative examples include:
-        
-        NLP:
-        
-        * [nlp_gpt3_text-generation_2.7B](https://modelscope.cn/models/damo/nlp_gpt3_text-generation_2.7B)
-        
-        * [ChatYuan-large](https://modelscope.cn/models/ClueAI/ChatYuan-large)
-        
-        * [mengzi-t5-base](https://modelscope.cn/models/langboat/mengzi-t5-base)
-        
-        * [nlp_csanmt_translation_en2zh](https://modelscope.cn/models/damo/nlp_csanmt_translation_en2zh)
-        
-        * [nlp_raner_named-entity-recognition_chinese-base-news](https://modelscope.cn/models/damo/nlp_raner_named-entity-recognition_chinese-base-news)
-        
-        * [nlp_structbert_word-segmentation_chinese-base](https://modelscope.cn/models/damo/nlp_structbert_word-segmentation_chinese-base)
-        
-        * [Erlangshen-RoBERTa-330M-Sentiment](https://modelscope.cn/models/fengshenbang/Erlangshen-RoBERTa-330M-Sentiment)
-        
-        * [nlp_convai_text2sql_pretrain_cn](https://modelscope.cn/models/damo/nlp_convai_text2sql_pretrain_cn)
-        
-        Multi-Modal:
-        
-        * [multi-modal_clip-vit-base-patch16_zh](https://modelscope.cn/models/damo/multi-modal_clip-vit-base-patch16_zh)
-        
-        * [ofa_pretrain_base_zh](https://modelscope.cn/models/damo/ofa_pretrain_base_zh)
-        
-        * [Taiyi-Stable-Diffusion-1B-Chinese-v0.1](https://modelscope.cn/models/fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-v0.1)
-        
-        * [mplug_visual-question-answering_coco_large_en](https://modelscope.cn/models/damo/mplug_visual-question-answering_coco_large_en)
-        
-        CV:
-        
-        * [cv_controlnet_controllable-image-generation_nine-annotators](https://modelscope.cn/models/dienstag/cv_controlnet_controllable-image-generation_nine-annotators/summary)
-        
-        * [cv_tinynas_object-detection_damoyolo](https://modelscope.cn/models/damo/cv_tinynas_object-detection_damoyolo)
-        
-        * [cv_unet_person-image-cartoon_compound-models](https://modelscope.cn/models/damo/cv_unet_person-image-cartoon_compound-models)
-        
-        * [cv_convnextTiny_ocr-recognition-general_damo](https://modelscope.cn/models/damo/cv_convnextTiny_ocr-recognition-general_damo)
-        
-        * [cv_resnet18_human-detection](https://modelscope.cn/models/damo/cv_resnet18_human-detection)
-        
-        * [cv_resnet50_face-detection_retinaface](https://modelscope.cn/models/damo/cv_resnet50_face-detection_retinaface)
-        
-        * [cv_unet_image-matting](https://modelscope.cn/models/damo/cv_unet_image-matting)
-        
-        * [cv_F3Net_product-segmentation](https://modelscope.cn/models/damo/cv_F3Net_product-segmentation)
-        
-        * [cv_resnest101_general_recognition](https://modelscope.cn/models/damo/cv_resnest101_general_recognition)
-        
-        
-        Audio:
-        
-        * [speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch)
-        
-        * [speech_sambert-hifigan_tts_zh-cn_16k](https://modelscope.cn/models/damo/speech_sambert-hifigan_tts_zh-cn_16k)
-        
-        * [speech_charctc_kws_phone-xiaoyun](https://modelscope.cn/models/damo/speech_charctc_kws_phone-xiaoyun)
-        
-        * [u2pp_conformer-asr-cn-16k-online](https://modelscope.cn/models/wenet/u2pp_conformer-asr-cn-16k-online)
-        
-        * [speech_frcrn_ans_cirm_16k](https://modelscope.cn/models/damo/speech_frcrn_ans_cirm_16k)
-        
-        * [speech_dfsmn_aec_psm_16k](https://modelscope.cn/models/damo/speech_dfsmn_aec_psm_16k)
-        
-        
-        
-        AI for Science:
-        
-        * [uni-fold-monomer](https://modelscope.cn/models/DPTech/uni-fold-monomer/summary)
-        
-        * [uni-fold-multimer](https://modelscope.cn/models/DPTech/uni-fold-multimer/summary)
-        
-        **Note:** Most models on ModelScope are public and can be downloaded without account registration on modelscope website([www.modelscope.cn](www.modelscope.cn)), please refer to instructions for [model download](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD), for dowloading models with api provided by modelscope library or git.
-        
-        # QuickTour
-        
-        We provide unified interface for inference using `pipeline`, fine-tuning and evaluation using `Trainer` for different tasks.
-        
-        For any given task with any type of input (image, text, audio, video...), inference pipeline can be implemented with only a few lines of code, which will automatically load the underlying model to get inference result, as is exemplified below:
-        
-        ```python
-        >>> from modelscope.pipelines import pipeline
-        >>> word_segmentation = pipeline('word-segmentation',model='damo/nlp_structbert_word-segmentation_chinese-base')
-        >>> word_segmentation('今天天气不错，适合出去游玩')
-        {'output': '今天 天气 不错 ， 适合 出去 游玩'}
-        ```
-        
-        Given an image, portrait matting (aka. background-removal) can be accomplished with the following code snippet:
-        
-        ![image](data/resource/portrait_input.png)
-        
-        ```python
-        >>> import cv2
-        >>> from modelscope.pipelines import pipeline
-        
-        >>> portrait_matting = pipeline('portrait-matting')
-        >>> result = portrait_matting('https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/image_matting.png')
-        >>> cv2.imwrite('result.png', result['output_img'])
-        ```
-        
-        The output image with the background removed is:
-        ![image](data/resource/portrait_output.png)
-        
-        
-        Fine-tuning and evaluation can also be done with a few more lines of code to set up training dataset and trainer, with the heavy-lifting work of training and evaluation a model encapsulated in the implementation of  `traner.train()` and
-        `trainer.evaluate()`  interfaces.
-        
-        For example, the gpt3 base model (1.3B) can be fine-tuned with the chinese-poetry dataset, resulting in a model that can be used for chinese-poetry generation.
-        
-        ```python
-        >>> from modelscope.metainfo import Trainers
-        >>> from modelscope.msdatasets import MsDataset
-        >>> from modelscope.trainers import build_trainer
-        
-        >>> train_dataset = MsDataset.load('chinese-poetry-collection', split='train'). remap_columns({'text1': 'src_txt'})
-        >>> eval_dataset = MsDataset.load('chinese-poetry-collection', split='test').remap_columns({'text1': 'src_txt'})
-        >>> max_epochs = 10
-        >>> tmp_dir = './gpt3_poetry'
-        
-        >>> kwargs = dict(
-             model='damo/nlp_gpt3_text-generation_1.3B',
-             train_dataset=train_dataset,
-             eval_dataset=eval_dataset,
-             max_epochs=max_epochs,
-             work_dir=tmp_dir)
-        
-        >>> trainer = build_trainer(name=Trainers.gpt3_trainer, default_args=kwargs)
-        >>> trainer.train()
-        ```
-        
-        # Why should I use ModelScope library
-        
-        1. A unified and concise user interface is abstracted for different tasks and different models. Model inferences and training can be implemented by as few as 3 and 10 lines of code, respectively. It is convenient for users to explore models in different fields in the ModelScope community. All models integrated into ModelScope are ready to use, which makes it easy to get started with AI, in both educational and industrial settings.
-        
-        2. ModelScope offers a model-centric development and application experience. It streamlines the support for model training, inference, export and deployment, and facilitates users to build their own MLOps based on the ModelScope ecosystem.
-        
-        3. For the model inference and training process, a modular design is put in place, and a wealth of functional module implementations are provided, which is convenient for users to customize their own model inference, training and other processes.
-        
-        4. For distributed model training, especially for large models, it provides rich training strategy support, including data parallel, model parallel, hybrid parallel and so on.
-        
-        # Installation
-        
-        ## Docker
-        
-        ModelScope Library currently supports popular deep learning framework for model training and inference, including PyTorch, TensorFlow and ONNX. All releases are tested and run on Python 3.7+, Pytorch 1.8+, Tensorflow1.15 or Tensorflow2.0+.
-        
-        To allow out-of-box usage for all the models on ModelScope, official docker images are provided for all releases. Based on the docker image, developers can skip all environment installation and configuration and use it directly. Currently, the latest version of the CPU image and GPU image can be obtained from:
-        
-        CPU docker image
-        ```shell
-        registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py37-torch1.11.0-tf1.15.5-1.3.0
-        ```
-        
-        GPU docker image
-        ```shell
-        registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.3.0-py37-torch1.11.0-tf1.15.5-1.3.0
-        ```
-        
-        ## Setup Local Python Environment
-        
-        One can also set up local ModelScope environment using pip and conda.  We suggest [anaconda](https://docs.anaconda.com/anaconda/install/) for creating local python environment:
-        
-        ```shell
-        conda create -n modelscope python=3.7
-        conda activate modelscope
-        ```
-        
-        PyTorch or TensorFlow can be installed separately according to each model's requirements.
-        * Install pytorch [doc](https://pytorch.org/get-started/locally/)
-        * Install tensorflow [doc](https://www.tensorflow.org/install/pip)
-        
-        After installing the necessary machine-learning framework, you can install modelscope library as follows:
-        
-        If you only want to play around with the modelscope framework, of trying out model/dataset download, you can install the core modelscope components:
-        ```shell
-        pip install modelscope
-        ```
-        
-        If you want to use multi-modal models:
-        ```shell
-        pip install modelscope[multi-modal]
-        ```
-        
-        If you want to use nlp models:
-        ```shell
-        pip install modelscope[nlp] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
-        ```
-        
-        If you want to use cv models:
-        ```shell
-        pip install modelscope[cv] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
-        ```
-        
-        If you want to use audio models:
-        ```shell
-        pip install modelscope[audio] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
-        ```
-        
-        If you want to use science models:
-        ```shell
-        pip install modelscope[science] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
-        ```
-        
-        `Notes`:
-        1. Currently, some audio-task models only support python3.7, tensorflow1.15.4 Linux environments. Most other models can be installed and used on Windows and Mac (x86).
-        
-        2. Some models in the audio field use the third-party library SoundFile for wav file processing. On the Linux system, users need to manually install libsndfile of SoundFile([doc link](https://github.com/bastibe/python-soundfile#installation)). On Windows and MacOS, it will be installed automatically without user operation. For example, on Ubuntu, you can use following commands:
-            ```shell
-            sudo apt-get update
-            sudo apt-get install libsndfile1
-            ```
-        
-        3. Some models in computer vision need mmcv-full, you can refer to mmcv [installation guide](https://github.com/open-mmlab/mmcv#installation), a minimal installation is as follows:
-        
-            ```shell
-            pip uninstall mmcv # if you have installed mmcv, uninstall it
-            pip install -U openmim
-            mim install mmcv-full
-            ```
-        
-        
-        
-        # Learn More
-        
-        We  provide additional documentations including:
-        * [More detailed Installation Guide](https://modelscope.cn/docs/%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85)
-        * [Introduction to tasks](https://modelscope.cn/docs/%E4%BB%BB%E5%8A%A1%E7%9A%84%E4%BB%8B%E7%BB%8D)
-        * [Use pipeline for model inference](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86Pipeline)
-        * [Finetuning example](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83Train)
-        * [Preprocessing of data](https://modelscope.cn/docs/%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86)
-        * [Evaluation](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0)
-        * [Contribute your own model to ModelScope](https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)
-        
-        # License
-        
-        This project is licensed under the [Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE).
-        
 Keywords: python,nlp,science,cv,speech,multi-modal
 Platform: UNKNOWN
 Classifier: Development Status :: 4 - Beta
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.7
@@ -311,7 +23,297 @@
 Provides-Extra: nlp
 Provides-Extra: science
 Provides-Extra: audio_asr
 Provides-Extra: audio_kws
 Provides-Extra: audio_signal
 Provides-Extra: audio_tts
 Provides-Extra: all
+
+
+<p align="center">
+    <br>
+    <img src="https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif" width="400"/>
+    <br>
+<p>
+
+<div align="center">
+
+[![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/modelscope/)
+<!-- [![Documentation Status](https://readthedocs.org/projects/easy-cv/badge/?version=latest)](https://easy-cv.readthedocs.io/en/latest/) -->
+[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)
+[![open issues](https://isitmaintained.com/badge/open/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/issues)
+[![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/pull/)
+[![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/modelscope)](https://GitHub.com/modelscope/modelscope/commit/)
+[![Leaderboard](https://img.shields.io/badge/ModelScope-Check%20Your%20Contribution-orange)](https://opensource.alibaba.com/contribution_leaderboard/details?projectValue=modelscope)
+
+<!-- [![GitHub contributors](https://img.shields.io/github/contributors/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/graphs/contributors/) -->
+<!-- [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) -->
+
+<h4 align="center">
+    <p>
+        <b>English</b> |
+        <a href="https://github.com/modelscope/modelscope/blob/master/README_zh.md">中文</a>
+    <p>
+</h4>
+
+
+</div>
+
+# Introduction
+
+[ModelScope]( https://www.modelscope.cn) is built upon the notion of “Model-as-a-Service” (MaaS). It seeks to bring together most advanced machine learning models from the AI community, and streamlines the process of leveraging AI models in real-world applications. The core ModelScope library open-sourced in this repository provides the interfaces and implementations that allow developers to perform  model inference, training and evaluation.
+
+
+In particular, with rich layers of API-abstraction, the ModelScope library offers unified experience to explore state-of-the-art models spanning across domains such as CV, NLP, Speech, Multi-Modality, and Scientific-computation. Model contributors of different areas can integrate models into the ModelScope ecosystem through the layered-APIs, allowing easy and unified access to their models. Once integrated, model inference, fine-tuning, and evaluations can be done with only a few lines of codes. In the meantime, flexibilities are also provided so that different components in the model applications can be customized wherever necessary.
+
+Apart from harboring implementations of a wide range of different models, ModelScope library also enables the necessary interactions with ModelScope backend services, particularly with the Model-Hub and Dataset-Hub. Such interactions facilitate management of  various entities (models and datasets) to be performed seamlessly under-the-hood, including entity lookup, version control, cache management, and many others.
+
+# Models and Online Accessibility
+
+Hundreds of models are made publicly available on [ModelScope]( https://www.modelscope.cn)  (700+ and counting), covering the latest development in areas such as NLP, CV, Audio, Multi-modality, and AI for Science, etc. Many of these models represent the SOTA in their specific fields, and made their open-sourced debut on ModelScope. Users can visit ModelScope([modelscope.cn](http://www.modelscope.cn)) and experience first-hand how these models perform via online experience, with just a few clicks. Immediate developer-experience is also possible through the ModelScope Notebook, which is backed by ready-to-use CPU/GPU development environment in the cloud - only one click away on [ModelScope](https://www.modelscope.cn).
+
+
+<p align="center">
+    <br>
+    <img src="data/resource/inference.gif" width="1024"/>
+    <br>
+<p>
+
+Some representative examples include:
+
+NLP:
+
+* [nlp_gpt3_text-generation_2.7B](https://modelscope.cn/models/damo/nlp_gpt3_text-generation_2.7B)
+
+* [ChatYuan-large](https://modelscope.cn/models/ClueAI/ChatYuan-large)
+
+* [mengzi-t5-base](https://modelscope.cn/models/langboat/mengzi-t5-base)
+
+* [nlp_csanmt_translation_en2zh](https://modelscope.cn/models/damo/nlp_csanmt_translation_en2zh)
+
+* [nlp_raner_named-entity-recognition_chinese-base-news](https://modelscope.cn/models/damo/nlp_raner_named-entity-recognition_chinese-base-news)
+
+* [nlp_structbert_word-segmentation_chinese-base](https://modelscope.cn/models/damo/nlp_structbert_word-segmentation_chinese-base)
+
+* [Erlangshen-RoBERTa-330M-Sentiment](https://modelscope.cn/models/fengshenbang/Erlangshen-RoBERTa-330M-Sentiment)
+
+* [nlp_convai_text2sql_pretrain_cn](https://modelscope.cn/models/damo/nlp_convai_text2sql_pretrain_cn)
+
+Multi-Modal:
+
+* [multi-modal_clip-vit-base-patch16_zh](https://modelscope.cn/models/damo/multi-modal_clip-vit-base-patch16_zh)
+
+* [ofa_pretrain_base_zh](https://modelscope.cn/models/damo/ofa_pretrain_base_zh)
+
+* [Taiyi-Stable-Diffusion-1B-Chinese-v0.1](https://modelscope.cn/models/fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-v0.1)
+
+* [mplug_visual-question-answering_coco_large_en](https://modelscope.cn/models/damo/mplug_visual-question-answering_coco_large_en)
+
+CV:
+
+* [cv_controlnet_controllable-image-generation_nine-annotators](https://modelscope.cn/models/dienstag/cv_controlnet_controllable-image-generation_nine-annotators/summary)
+
+* [cv_tinynas_object-detection_damoyolo](https://modelscope.cn/models/damo/cv_tinynas_object-detection_damoyolo)
+
+* [cv_unet_person-image-cartoon_compound-models](https://modelscope.cn/models/damo/cv_unet_person-image-cartoon_compound-models)
+
+* [cv_convnextTiny_ocr-recognition-general_damo](https://modelscope.cn/models/damo/cv_convnextTiny_ocr-recognition-general_damo)
+
+* [cv_resnet18_human-detection](https://modelscope.cn/models/damo/cv_resnet18_human-detection)
+
+* [cv_resnet50_face-detection_retinaface](https://modelscope.cn/models/damo/cv_resnet50_face-detection_retinaface)
+
+* [cv_unet_image-matting](https://modelscope.cn/models/damo/cv_unet_image-matting)
+
+* [cv_F3Net_product-segmentation](https://modelscope.cn/models/damo/cv_F3Net_product-segmentation)
+
+* [cv_resnest101_general_recognition](https://modelscope.cn/models/damo/cv_resnest101_general_recognition)
+
+
+Audio:
+
+* [speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch)
+
+* [speech_sambert-hifigan_tts_zh-cn_16k](https://modelscope.cn/models/damo/speech_sambert-hifigan_tts_zh-cn_16k)
+
+* [speech_charctc_kws_phone-xiaoyun](https://modelscope.cn/models/damo/speech_charctc_kws_phone-xiaoyun)
+
+* [u2pp_conformer-asr-cn-16k-online](https://modelscope.cn/models/wenet/u2pp_conformer-asr-cn-16k-online)
+
+* [speech_frcrn_ans_cirm_16k](https://modelscope.cn/models/damo/speech_frcrn_ans_cirm_16k)
+
+* [speech_dfsmn_aec_psm_16k](https://modelscope.cn/models/damo/speech_dfsmn_aec_psm_16k)
+
+
+
+AI for Science:
+
+* [uni-fold-monomer](https://modelscope.cn/models/DPTech/uni-fold-monomer/summary)
+
+* [uni-fold-multimer](https://modelscope.cn/models/DPTech/uni-fold-multimer/summary)
+
+**Note:** Most models on ModelScope are public and can be downloaded without account registration on modelscope website([www.modelscope.cn](www.modelscope.cn)), please refer to instructions for [model download](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD), for dowloading models with api provided by modelscope library or git.
+
+# QuickTour
+
+We provide unified interface for inference using `pipeline`, fine-tuning and evaluation using `Trainer` for different tasks.
+
+For any given task with any type of input (image, text, audio, video...), inference pipeline can be implemented with only a few lines of code, which will automatically load the underlying model to get inference result, as is exemplified below:
+
+```python
+>>> from modelscope.pipelines import pipeline
+>>> word_segmentation = pipeline('word-segmentation',model='damo/nlp_structbert_word-segmentation_chinese-base')
+>>> word_segmentation('今天天气不错，适合出去游玩')
+{'output': '今天 天气 不错 ， 适合 出去 游玩'}
+```
+
+Given an image, portrait matting (aka. background-removal) can be accomplished with the following code snippet:
+
+![image](data/resource/portrait_input.png)
+
+```python
+>>> import cv2
+>>> from modelscope.pipelines import pipeline
+
+>>> portrait_matting = pipeline('portrait-matting')
+>>> result = portrait_matting('https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/image_matting.png')
+>>> cv2.imwrite('result.png', result['output_img'])
+```
+
+The output image with the background removed is:
+![image](data/resource/portrait_output.png)
+
+
+Fine-tuning and evaluation can also be done with a few more lines of code to set up training dataset and trainer, with the heavy-lifting work of training and evaluation a model encapsulated in the implementation of  `traner.train()` and
+`trainer.evaluate()`  interfaces.
+
+For example, the gpt3 base model (1.3B) can be fine-tuned with the chinese-poetry dataset, resulting in a model that can be used for chinese-poetry generation.
+
+```python
+>>> from modelscope.metainfo import Trainers
+>>> from modelscope.msdatasets import MsDataset
+>>> from modelscope.trainers import build_trainer
+
+>>> train_dataset = MsDataset.load('chinese-poetry-collection', split='train'). remap_columns({'text1': 'src_txt'})
+>>> eval_dataset = MsDataset.load('chinese-poetry-collection', split='test').remap_columns({'text1': 'src_txt'})
+>>> max_epochs = 10
+>>> tmp_dir = './gpt3_poetry'
+
+>>> kwargs = dict(
+     model='damo/nlp_gpt3_text-generation_1.3B',
+     train_dataset=train_dataset,
+     eval_dataset=eval_dataset,
+     max_epochs=max_epochs,
+     work_dir=tmp_dir)
+
+>>> trainer = build_trainer(name=Trainers.gpt3_trainer, default_args=kwargs)
+>>> trainer.train()
+```
+
+# Why should I use ModelScope library
+
+1. A unified and concise user interface is abstracted for different tasks and different models. Model inferences and training can be implemented by as few as 3 and 10 lines of code, respectively. It is convenient for users to explore models in different fields in the ModelScope community. All models integrated into ModelScope are ready to use, which makes it easy to get started with AI, in both educational and industrial settings.
+
+2. ModelScope offers a model-centric development and application experience. It streamlines the support for model training, inference, export and deployment, and facilitates users to build their own MLOps based on the ModelScope ecosystem.
+
+3. For the model inference and training process, a modular design is put in place, and a wealth of functional module implementations are provided, which is convenient for users to customize their own model inference, training and other processes.
+
+4. For distributed model training, especially for large models, it provides rich training strategy support, including data parallel, model parallel, hybrid parallel and so on.
+
+# Installation
+
+## Docker
+
+ModelScope Library currently supports popular deep learning framework for model training and inference, including PyTorch, TensorFlow and ONNX. All releases are tested and run on Python 3.7+, Pytorch 1.8+, Tensorflow1.15 or Tensorflow2.0+.
+
+To allow out-of-box usage for all the models on ModelScope, official docker images are provided for all releases. Based on the docker image, developers can skip all environment installation and configuration and use it directly. Currently, the latest version of the CPU image and GPU image can be obtained from:
+
+CPU docker image
+```shell
+registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py37-torch1.11.0-tf1.15.5-1.3.0
+```
+
+GPU docker image
+```shell
+registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.3.0-py37-torch1.11.0-tf1.15.5-1.3.0
+```
+
+## Setup Local Python Environment
+
+One can also set up local ModelScope environment using pip and conda.  We suggest [anaconda](https://docs.anaconda.com/anaconda/install/) for creating local python environment:
+
+```shell
+conda create -n modelscope python=3.7
+conda activate modelscope
+```
+
+PyTorch or TensorFlow can be installed separately according to each model's requirements.
+* Install pytorch [doc](https://pytorch.org/get-started/locally/)
+* Install tensorflow [doc](https://www.tensorflow.org/install/pip)
+
+After installing the necessary machine-learning framework, you can install modelscope library as follows:
+
+If you only want to play around with the modelscope framework, of trying out model/dataset download, you can install the core modelscope components:
+```shell
+pip install modelscope
+```
+
+If you want to use multi-modal models:
+```shell
+pip install modelscope[multi-modal]
+```
+
+If you want to use nlp models:
+```shell
+pip install modelscope[nlp] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
+```
+
+If you want to use cv models:
+```shell
+pip install modelscope[cv] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
+```
+
+If you want to use audio models:
+```shell
+pip install modelscope[audio] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
+```
+
+If you want to use science models:
+```shell
+pip install modelscope[science] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
+```
+
+`Notes`:
+1. Currently, some audio-task models only support python3.7, tensorflow1.15.4 Linux environments. Most other models can be installed and used on Windows and Mac (x86).
+
+2. Some models in the audio field use the third-party library SoundFile for wav file processing. On the Linux system, users need to manually install libsndfile of SoundFile([doc link](https://github.com/bastibe/python-soundfile#installation)). On Windows and MacOS, it will be installed automatically without user operation. For example, on Ubuntu, you can use following commands:
+    ```shell
+    sudo apt-get update
+    sudo apt-get install libsndfile1
+    ```
+
+3. Some models in computer vision need mmcv-full, you can refer to mmcv [installation guide](https://github.com/open-mmlab/mmcv#installation), a minimal installation is as follows:
+
+    ```shell
+    pip uninstall mmcv # if you have installed mmcv, uninstall it
+    pip install -U openmim
+    mim install mmcv-full
+    ```
+
+
+
+# Learn More
+
+We  provide additional documentations including:
+* [More detailed Installation Guide](https://modelscope.cn/docs/%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85)
+* [Introduction to tasks](https://modelscope.cn/docs/%E4%BB%BB%E5%8A%A1%E7%9A%84%E4%BB%8B%E7%BB%8D)
+* [Use pipeline for model inference](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86Pipeline)
+* [Finetuning example](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83Train)
+* [Preprocessing of data](https://modelscope.cn/docs/%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86)
+* [Evaluation](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0)
+* [Contribute your own model to ModelScope](https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)
+
+# License
+
+This project is licensed under the [Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE).
+
+
```

#### html2text {}

```diff
@@ -1,11 +1,20 @@
-Metadata-Version: 2.1 Name: modelscope Version: 1.4.2 Summary: UNKNOWN Home-
+Metadata-Version: 2.1 Name: modelscope Version: 1.5.0 Summary: UNKNOWN Home-
 page: https://github.com/modelscope/modelscope Author: Alibaba ModelScope team
 Author-email: modelscope@list.alibaba-inc.com License: Apache License 2.0
-Description:
+Keywords: python,nlp,science,cv,speech,multi-modal Platform: UNKNOWN
+Classifier: Development Status :: 4 - Beta Classifier: License :: OSI Approved
+:: Apache Software License Classifier: Operating System :: OS Independent
+Classifier: Programming Language :: Python :: 3 Classifier: Programming
+Language :: Python :: 3.7 Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9 Classifier: Programming
+Language :: Python :: 3.10 Description-Content-Type: text/markdown Provides-
+Extra: audio Provides-Extra: cv Provides-Extra: multi-modal Provides-Extra: nlp
+Provides-Extra: science Provides-Extra: audio_asr Provides-Extra: audio_kws
+Provides-Extra: audio_signal Provides-Extra: audio_tts Provides-Extra: all
 
        [https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif]
  [![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/
   modelscope/)  [![license](https://img.shields.io/github/license/modelscope/
 modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)
        [![open issues](https://isitmaintained.com/badge/open/modelscope/
   modelscope.svg)](https://github.com/modelscope/modelscope/issues) [![GitHub
@@ -199,18 +208,8 @@
 (https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83Train)
 * [Preprocessing of data](https://modelscope.cn/docs/
 %E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86) * [Evaluation](https://
 modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0) * [Contribute
 your own model to ModelScope](https://modelscope.cn/docs/
 ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)
 # License This project is licensed under the [Apache License (Version 2.0)]
-(https://github.com/modelscope/modelscope/blob/master/LICENSE). Keywords:
-python,nlp,science,cv,speech,multi-modal Platform: UNKNOWN Classifier:
-Development Status :: 4 - Beta Classifier: License :: OSI Approved :: Apache
-Software License Classifier: Operating System :: OS Independent Classifier:
-Programming Language :: Python :: 3 Classifier: Programming Language :: Python
-:: 3.7 Classifier: Programming Language :: Python :: 3.8 Classifier:
-Programming Language :: Python :: 3.9 Classifier: Programming Language ::
-Python :: 3.10 Description-Content-Type: text/markdown Provides-Extra: audio
-Provides-Extra: cv Provides-Extra: multi-modal Provides-Extra: nlp Provides-
-Extra: science Provides-Extra: audio_asr Provides-Extra: audio_kws Provides-
-Extra: audio_signal Provides-Extra: audio_tts Provides-Extra: all
+(https://github.com/modelscope/modelscope/blob/master/LICENSE).
```

### Comparing `modelscope-1.4.2/README.md` & `modelscope-1.5.0/README.md`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/cli/cli.py` & `modelscope-1.5.0/modelscope/cli/cli.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/cli/download.py` & `modelscope-1.5.0/modelscope/cli/download.py`

 * *Files 8% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 from argparse import ArgumentParser
 
 from modelscope.cli.base import CLICommand
 from modelscope.hub.snapshot_download import snapshot_download
 
 
 def subparser_func(args):
-    """ Fuction which will be called for a specific sub parser.
+    """ Function which will be called for a specific sub parser.
     """
     return DownloadCMD(args)
 
 
 class DownloadCMD(CLICommand):
     name = 'download'
```

### Comparing `modelscope-1.4.2/modelscope/cli/modelcard.py` & `modelscope-1.5.0/modelscope/cli/modelcard.py`

 * *Files 0% similar despite different names*

```diff
@@ -14,15 +14,15 @@
 logger = get_logger()
 
 curren_path = os.path.dirname(os.path.abspath(__file__))
 template_path = os.path.join(curren_path, 'template')
 
 
 def subparser_func(args):
-    """ Fuction which will be called for a specific sub parser.
+    """ Function which will be called for a specific sub parser.
     """
     return ModelCardCMD(args)
 
 
 class ModelCardCMD(CLICommand):
     name = 'modelcard'
```

### Comparing `modelscope-1.4.2/modelscope/cli/pipeline.py` & `modelscope-1.5.0/modelscope/cli/pipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 logger = get_logger()
 
 curren_path = os.path.dirname(os.path.abspath(__file__))
 template_path = os.path.join(curren_path, 'template')
 
 
 def subparser_func(args):
-    """ Fuction which will be called for a specific sub parser.
+    """ Function which will be called for a specific sub parser.
     """
     return PipelineCMD(args)
 
 
 class PipelineCMD(CLICommand):
     name = 'pipeline'
```

### Comparing `modelscope-1.4.2/modelscope/cli/plugins.py` & `modelscope-1.5.0/modelscope/cli/plugins.py`

 * *Files 1% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 from modelscope.cli.base import CLICommand
 from modelscope.utils.plugins import PluginsManager
 
 plugins_manager = PluginsManager()
 
 
 def subparser_func(args):
-    """ Fuction which will be called for a specific sub parser.
+    """ Function which will be called for a specific sub parser.
     """
     return PluginsCMD(args)
 
 
 class PluginsCMD(CLICommand):
     name = 'plugin'
```

### Comparing `modelscope-1.4.2/modelscope/exporters/__init__.py` & `modelscope-1.5.0/modelscope/exporters/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/exporters/base.py` & `modelscope-1.5.0/modelscope/exporters/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/exporters/builder.py` & `modelscope-1.5.0/modelscope/exporters/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/exporters/cv/__init__.py` & `modelscope-1.5.0/modelscope/exporters/cv/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/exporters/cv/cartoon_translation_exporter.py` & `modelscope-1.5.0/modelscope/exporters/cv/cartoon_translation_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/exporters/cv/face_detection_scrfd_exporter.py` & `modelscope-1.5.0/modelscope/exporters/cv/face_detection_scrfd_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/exporters/cv/object_detection_damoyolo_exporter.py` & `modelscope-1.5.0/modelscope/exporters/cv/object_detection_damoyolo_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/exporters/nlp/__init__.py` & `modelscope-1.5.0/modelscope/exporters/nlp/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/exporters/nlp/csanmt_for_translation_exporter.py` & `modelscope-1.5.0/modelscope/exporters/nlp/csanmt_for_translation_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/exporters/nlp/model_for_token_classification_exporter.py` & `modelscope-1.5.0/modelscope/exporters/nlp/model_for_token_classification_exporter.py`

 * *Files 0% similar despite different names*

```diff
@@ -85,15 +85,15 @@
         if isinstance(outputs_origin, (Mapping, ModelOutputBase)):
             outputs_origin = list(
                 numpify_tensor_nested(outputs_origin).values())
         elif isinstance(outputs_origin, (tuple, list)):
             outputs_origin = list(numpify_tensor_nested(outputs_origin))
 
         outputs_origin = [outputs_origin[0]
-                          ]  # keeo `predictions`, drop other outputs
+                          ]  # keep `predictions`, drop other outputs
 
         np_dummy_inputs = numpify_tensor_nested(dummy_inputs)
         np_dummy_inputs['label_mask'] = np_dummy_inputs['label_mask'].astype(
             bool)
         outputs = ort_session.run(onnx_outputs, np_dummy_inputs)
         outputs = numpify_tensor_nested(outputs)
         if isinstance(outputs, dict):
```

### Comparing `modelscope-1.4.2/modelscope/exporters/nlp/sbert_for_sequence_classification_exporter.py` & `modelscope-1.5.0/modelscope/exporters/nlp/sbert_for_sequence_classification_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/exporters/nlp/sbert_for_zero_shot_classification_exporter.py` & `modelscope-1.5.0/modelscope/exporters/nlp/sbert_for_zero_shot_classification_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/exporters/tf_model_exporter.py` & `modelscope-1.5.0/modelscope/exporters/tf_model_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/exporters/torch_model_exporter.py` & `modelscope-1.5.0/modelscope/exporters/torch_model_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/fileio/file.py` & `modelscope-1.5.0/modelscope/fileio/file.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/fileio/format/json.py` & `modelscope-1.5.0/modelscope/fileio/format/json.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import numpy as np
 
+from . import jsonplus
 from .base import FormatHandler
 
 
 def set_default(obj):
     """Set default json values for non-serializable values.
 
     It helps convert ``set``, ``range`` and ``np.ndarray`` data types to list.
@@ -20,17 +21,14 @@
     raise TypeError(f'{type(obj)} is unsupported for json dump')
 
 
 class JsonHandler(FormatHandler):
     """Use jsonplus, serialization of Python types to JSON that "just works"."""
 
     def load(self, file):
-        import jsonplus
         return jsonplus.loads(file.read())
 
     def dump(self, obj, file, **kwargs):
         file.write(self.dumps(obj, **kwargs))
 
     def dumps(self, obj, **kwargs):
-        import jsonplus
-        kwargs.setdefault('default', set_default)
         return jsonplus.dumps(obj, **kwargs)
```

### Comparing `modelscope-1.4.2/modelscope/fileio/format/yaml.py` & `modelscope-1.5.0/modelscope/fileio/format/yaml.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/fileio/io.py` & `modelscope-1.5.0/modelscope/fileio/io.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/hub/api.py` & `modelscope-1.5.0/modelscope/hub/api.py`

 * *Files 1% similar despite different names*

```diff
@@ -120,22 +120,24 @@
         return d[API_RESPONSE_FIELD_DATA][
             API_RESPONSE_FIELD_GIT_ACCESS_TOKEN], cookies
 
     def create_model(self,
                      model_id: str,
                      visibility: Optional[int] = ModelVisibility.PUBLIC,
                      license: Optional[str] = Licenses.APACHE_V2,
-                     chinese_name: Optional[str] = None) -> str:
+                     chinese_name: Optional[str] = None,
+                     original_model_id: Optional[str] = '') -> str:
         """Create model repo at ModelScope Hub.
 
         Args:
             model_id (str): The model id
             visibility (int, optional): visibility of the model(1-private, 5-public), default 5.
             license (str, optional): license of the model, default none.
             chinese_name (str, optional): chinese name of the model.
+            original_model_id (str, optional): the base model id which this model is trained from
 
         Returns:
             Name of the model created
 
         Raises:
             InvalidParameter: If model_id is invalid.
             ValueError: If not login.
@@ -152,15 +154,16 @@
         path = f'{self.endpoint}/api/v1/models'
         owner_or_group, name = model_id_to_group_owner_name(model_id)
         body = {
             'Path': owner_or_group,
             'Name': name,
             'ChineseName': chinese_name,
             'Visibility': visibility,  # server check
-            'License': license
+            'License': license,
+            'OriginalModelId': original_model_id,
         }
         r = self.session.post(
             path, json=body, cookies=cookies, headers=self.headers)
         handle_http_post_error(r, path, body)
         raise_on_error(r.json())
         model_repo_url = f'{get_endpoint()}/{model_id}'
         return model_repo_url
@@ -229,15 +232,16 @@
     def push_model(self,
                    model_id: str,
                    model_dir: str,
                    visibility: Optional[int] = ModelVisibility.PUBLIC,
                    license: Optional[str] = Licenses.APACHE_V2,
                    chinese_name: Optional[str] = None,
                    commit_message: Optional[str] = 'upload model',
-                   revision: Optional[str] = DEFAULT_REPOSITORY_REVISION):
+                   revision: Optional[str] = DEFAULT_REPOSITORY_REVISION,
+                   original_model_id: Optional[str] = None):
         """Upload model from a given directory to given repository. A valid model directory
         must contain a configuration.json file.
 
         This function upload the files in given directory to given repository. If the
         given repository is not exists in remote, it will automatically create it with
         given visibility, license and chinese_name parameters. If the revision is also
         not exists in remote repository, it will create a new branch for it.
@@ -263,14 +267,15 @@
             chinese_name(`str`, *optional*, defaults to `None`):
                 chinese name of the new created model.
             commit_message(`str`, *optional*, defaults to `None`):
                 commit message of the push request.
             revision (`str`, *optional*, default to DEFAULT_MODEL_REVISION):
                 which branch to push. If the branch is not exists, It will create a new
                 branch and push to it.
+            original_model_id (str, optional): The base model id which this model is trained from
 
         Raises:
             InvalidParameter: Parameter invalid.
             NotLoginException: Not login
             ValueError: No configuration.json
             Exception: Create failed.
         """
@@ -295,15 +300,16 @@
                     'visibility and license cannot be empty if want to create new repo'
                 )
             logger.info('Create new model %s' % model_id)
             self.create_model(
                 model_id=model_id,
                 visibility=visibility,
                 license=license,
-                chinese_name=chinese_name)
+                chinese_name=chinese_name,
+                original_model_id=original_model_id)
         tmp_dir = tempfile.mkdtemp()
         git_wrapper = GitCommandWrapper()
         try:
             repo = Repository(model_dir=tmp_dir, clone_from=model_id)
             branches = git_wrapper.get_remote_branches(tmp_dir)
             if revision not in branches:
                 logger.info('Create new branch %s' % revision)
```

### Comparing `modelscope-1.4.2/modelscope/hub/check_model.py` & `modelscope-1.5.0/modelscope/hub/check_model.py`

 * *Files 4% similar despite different names*

```diff
@@ -87,7 +87,23 @@
                     else:
                         logger.info(
                             'Model is updated from modelscope hub, you can verify from https://www.modelscope.cn.'
                         )
                         break
     except:  # noqa: E722
         pass  # ignore
+
+
+def check_model_is_id(model_id: str, token=None):
+    if token is None:
+        token = os.environ.get('MODELSCOPE_API_TOKEN')
+    if model_id is None or os.path.exists(model_id):
+        return False
+    else:
+        _api = HubApi()
+        if token is not None:
+            _api.login(token)
+        try:
+            _api.get_model(model_id=model_id, )
+            return True
+        except Exception:
+            return False
```

### Comparing `modelscope-1.4.2/modelscope/hub/constants.py` & `modelscope-1.5.0/modelscope/hub/constants.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/hub/deploy.py` & `modelscope-1.5.0/modelscope/hub/deploy.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/hub/errors.py` & `modelscope-1.5.0/modelscope/hub/errors.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/hub/file_download.py` & `modelscope-1.5.0/modelscope/hub/file_download.py`

 * *Files 0% similar despite different names*

```diff
@@ -156,15 +156,15 @@
     return cache.put_file(file_to_download_info,
                           os.path.join(temporary_cache_dir, temp_file_name))
 
 
 def get_file_download_url(model_id: str, file_path: str, revision: str):
     """Format file download url according to `model_id`, `revision` and `file_path`.
     e.g., Given `model_id=john/bert`, `revision=master`, `file_path=README.md`,
-    the resulted download url is: https://modelscope.co/api/v1/models/john/bert/repo?Revision=master&FilePath=README.md
+    the resulted download url is: https://modelscope.cn/api/v1/models/john/bert/repo?Revision=master&FilePath=README.md
 
     Args:
         model_id (str): The model_id.
         file_path (str): File path
         revision (str): File revision.
 
     Returns:
```

### Comparing `modelscope-1.4.2/modelscope/hub/git.py` & `modelscope-1.5.0/modelscope/hub/git.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/hub/repository.py` & `modelscope-1.5.0/modelscope/hub/repository.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/hub/snapshot_download.py` & `modelscope-1.5.0/modelscope/hub/snapshot_download.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/hub/utils/caching.py` & `modelscope-1.5.0/modelscope/hub/utils/caching.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/hub/utils/utils.py` & `modelscope-1.5.0/modelscope/hub/utils/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metainfo.py` & `modelscope-1.5.0/modelscope/metainfo.py`

 * *Files 1% similar despite different names*

```diff
@@ -56,14 +56,15 @@
     unifuse_depth_estimation = 'unifuse-depth-estimation'
     dro_resnet18_depth_estimation = 'dro-resnet18-depth-estimation'
     resnet50_bert = 'resnet50-bert'
     referring_video_object_segmentation = 'swinT-referring-video-object-segmentation'
     fer = 'fer'
     fairface = 'fairface'
     retinaface = 'retinaface'
+    damofd = 'damofd'
     shop_segmentation = 'shop-segmentation'
     mogface = 'mogface'
     mtcnn = 'mtcnn'
     ulfd = 'ulfd'
     rts = 'rts'
     flir = 'flir'
     arcface = 'arcface'
@@ -111,14 +112,15 @@
     m2fp = 'm2fp'
     nerf_recon_acc = 'nerf-recon-acc'
     bts_depth_estimation = 'bts-depth-estimation'
     vision_efficient_tuning = 'vision-efficient-tuning'
     bad_image_detecting = 'bad-image-detecting'
     controllable_image_generation = 'controllable-image-generation'
     longshortnet = 'longshortnet'
+    pedestrian_attribute_recognition = 'pedestrian-attribute-recognition'
 
     # EasyCV models
     yolox = 'YOLOX'
     segformer = 'Segformer'
     hand_2d_keypoints = 'HRNet-Hand2D-Keypoints'
     image_object_detection_auto = 'image-object-detection-auto'
     dino = 'DINO'
@@ -126,14 +128,15 @@
     # nlp models
     bert = 'bert'
     palm = 'palm-v2'
     structbert = 'structbert'
     deberta_v2 = 'deberta_v2'
     veco = 'veco'
     translation = 'csanmt-translation'
+    canmt = 'canmt'
     space_dst = 'space-dst'
     space_intent = 'space-intent'
     space_modeling = 'space-modeling'
     space_T_en = 'space-T-en'
     space_T_cn = 'space-T-cn'
     tcrf = 'transformer-crf'
     token_classification_for_ner = 'token-classification-for-ner'
@@ -150,25 +153,28 @@
     plug = 'plug'
     bert_for_ds = 'bert-for-document-segmentation'
     ponet_for_ds = 'ponet-for-document-segmentation'
     ponet = 'ponet'
     T5 = 'T5'
     mglm = 'mglm'
     codegeex = 'codegeex'
+    glm130b = 'glm130b'
     bloom = 'bloom'
     unite = 'unite'
     megatron_bert = 'megatron-bert'
     use = 'user-satisfaction-estimation'
     fid_plug = 'fid-plug'
+    fid_T5 = 'fid-T5'
     lstm = 'lstm'
     xlm_roberta = 'xlm-roberta'
     transformers = 'transformers'
     plug_mental = 'plug-mental'
     doc2bot = 'doc2bot'
     peer = 'peer'
+    llama = 'llama'
 
     # audio models
     sambert_hifigan = 'sambert-hifigan'
     speech_frcrn_ans_cirm_16k = 'speech_frcrn_ans_cirm_16k'
     speech_dfsmn_ans = 'speech_dfsmn_ans'
     speech_dfsmn_kws_char_farfield = 'speech_dfsmn_kws_char_farfield'
     speech_kws_fsmn_char_ctc_nearfield = 'speech_kws_fsmn_char_ctc_nearfield'
@@ -176,14 +182,15 @@
     kws_kwsbp = 'kws-kwsbp'
     generic_asr = 'generic-asr'
     wenet_asr = 'wenet-asr'
     generic_itn = 'generic-itn'
     generic_punc = 'generic-punc'
     generic_sv = 'generic-sv'
     ecapa_tdnn_sv = 'ecapa-tdnn-sv'
+    campplus_sv = 'cam++-sv'
     generic_lm = 'generic-lm'
 
     # multi-modal models
     ofa = 'ofa'
     clip = 'clip-multi-modal-embedding'
     gemm = 'gemm-generative-multi-modal'
     rleg = 'rleg-generative-multi-modal'
@@ -193,14 +200,15 @@
     video_synthesis = 'latent-text-to-video-synthesis'
     team = 'team-multi-modal-similarity'
     video_clip = 'video-clip-multi-modal-embedding'
     mgeo = 'mgeo'
     vldoc = 'vldoc'
     hitea = 'hitea'
     soonet = 'soonet'
+    efficient_diffusion_tuning = 'efficient-diffusion-tuning'
 
     # science models
     unifold = 'unifold'
     unifold_symmetry = 'unifold-symmetry'
 
 
 class TaskModels(object):
@@ -219,14 +227,15 @@
 
     # text cls
     text_classification = 'text-classification'
     # fill mask
     fill_mask = 'fill-mask'
     bert_mlm = 'bert-mlm'
     roberta_mlm = 'roberta-mlm'
+    xlm_roberta_mlm = 'xlm-roberta-mlm'
     # token cls
     token_classification = 'token-classification'
     # extraction
     information_extraction = 'information-extraction'
     # text gen
     text_generation = 'text-generation'
     # text ranking
@@ -395,14 +404,15 @@
     controllable_image_generation = 'controllable-image-generation'
 
     image_quality_assessment_mos = 'image-quality-assessment-mos'
     image_quality_assessment_man = 'image-quality-assessment-man'
     image_quality_assessment_degradation = 'image-quality-assessment-degradation'
     vision_efficient_tuning = 'vision-efficient-tuning'
     image_bts_depth_estimation = 'image-bts-depth-estimation'
+    pedestrian_attribute_recognition = 'resnet50_pedestrian-attribute-recognition_image'
 
     # nlp tasks
     automatic_post_editing = 'automatic-post-editing'
     translation_quality_estimation = 'translation-quality-estimation'
     domain_classification = 'domain-classification'
     sentence_similarity = 'sentence-similarity'
     word_segmentation = 'word-segmentation'
@@ -417,14 +427,15 @@
     text2text_generation = 'text2text-generation'
     sentiment_analysis = 'sentiment-analysis'
     sentiment_classification = 'sentiment-classification'
     text_classification = 'text-classification'
     fill_mask = 'fill-mask'
     fill_mask_ponet = 'fill-mask-ponet'
     csanmt_translation = 'csanmt-translation'
+    canmt_translation = 'canmt-translation'
     interactive_translation = 'interactive-translation'
     nli = 'nli'
     dialog_intent_prediction = 'dialog-intent-prediction'
     dialog_modeling = 'dialog-modeling'
     dialog_state_tracking = 'dialog-state-tracking'
     zero_shot_classification = 'zero-shot-classification'
     text_error_correction = 'text-error-correction'
@@ -441,14 +452,15 @@
     relation_extraction = 'relation-extraction'
     document_segmentation = 'document-segmentation'
     extractive_summarization = 'extractive-summarization'
     feature_extraction = 'feature-extraction'
     mglm_text_summarization = 'mglm-text-summarization'
     codegeex_code_translation = 'codegeex-code-translation'
     codegeex_code_generation = 'codegeex-code-generation'
+    glm130b_text_generation = 'glm130b-text-generation'
     translation_en_to_de = 'translation_en_to_de'  # keep it underscore
     translation_en_to_ro = 'translation_en_to_ro'  # keep it underscore
     translation_en_to_fr = 'translation_en_to_fr'  # keep it underscore
     token_classification = 'token-classification'
     translation_evaluation = 'translation-evaluation'
     user_satisfaction_estimation = 'user-satisfaction-estimation'
     siamese_uie = 'siamese-uie'
@@ -497,14 +509,15 @@
     disco_guided_diffusion = 'disco_guided_diffusion'
     document_vl_embedding = 'document-vl-embedding'
     chinese_stable_diffusion = 'chinese-stable-diffusion'
     text_to_video_synthesis = 'latent-text-to-video-synthesis'  # latent-text-to-video-synthesis
     gridvlp_multi_modal_classification = 'gridvlp-multi-modal-classification'
     gridvlp_multi_modal_embedding = 'gridvlp-multi-modal-embedding'
     soonet_video_temporal_grounding = 'soonet-video-temporal-grounding'
+    efficient_diffusion_tuning = 'efficient-diffusion-tuning'
 
     # science tasks
     protein_structure = 'unifold-protein-structure'
 
 
 DEFAULT_MODEL_FOR_PIPELINE = {
     # TaskName: (pipeline_module_name, model_repo)
@@ -531,14 +544,16 @@
      'damo/nlp_bert_relation-extraction_chinese-base'),
     Tasks.information_extraction:
     (Pipelines.relation_extraction,
      'damo/nlp_bert_relation-extraction_chinese-base'),
     Tasks.sentence_similarity:
     (Pipelines.sentence_similarity,
      'damo/nlp_structbert_sentence-similarity_chinese-base'),
+    Tasks.competency_aware_translation:
+    (Pipelines.canmt_translation, 'damo/nlp_canmt_translation_zh2en_large'),
     Tasks.translation: (Pipelines.csanmt_translation,
                         'damo/nlp_csanmt_translation_zh2en'),
     Tasks.nli: (Pipelines.nli, 'damo/nlp_structbert_nli_chinese-base'),
     Tasks.sentiment_classification:
     (Pipelines.sentiment_classification,
      'damo/nlp_structbert_sentiment-classification_chinese-base'
      ),  # TODO: revise back after passing the pr
@@ -729,17 +744,17 @@
     Tasks.skin_retouching: (Pipelines.skin_retouching,
                             'damo/cv_unet_skin-retouching'),
     Tasks.faq_question_answering:
     (Pipelines.faq_question_answering,
      'damo/nlp_structbert_faq-question-answering_chinese-base'),
     Tasks.crowd_counting: (Pipelines.crowd_counting,
                            'damo/cv_hrnet_crowd-counting_dcanet'),
-    Tasks.video_single_object_tracking:
-    (Pipelines.video_single_object_tracking,
-     'damo/cv_vitb_video-single-object-tracking_ostrack'),
+    Tasks.video_single_object_tracking: (
+        Pipelines.video_single_object_tracking,
+        'damo/cv_vitb_video-single-object-tracking_ostrack'),
     Tasks.image_reid_person: (Pipelines.image_reid_person,
                               'damo/cv_passvitb_image-reid-person_market'),
     Tasks.text_driven_segmentation: (
         Pipelines.text_driven_segmentation,
         'damo/cv_vitl16_segmentation_text-driven-seg'),
     Tasks.movie_scene_segmentation: (
         Pipelines.movie_scene_segmentation,
@@ -817,14 +832,17 @@
                                 'damo/cv_object-detection-3d_depe'),
     Tasks.bad_image_detecting: (Pipelines.bad_image_detecting,
                                 'damo/cv_mobilenet-v2_bad-image-detecting'),
     Tasks.nerf_recon_acc: (Pipelines.nerf_recon_acc,
                            'damo/cv_nerf-3d-reconstruction-accelerate_damo'),
     Tasks.siamese_uie: (Pipelines.siamese_uie,
                         'damo/nlp_structbert_siamese-uie_chinese-base'),
+    Tasks.pedestrian_attribute_recognition: (
+        Pipelines.pedestrian_attribute_recognition,
+        'damo/cv_resnet50_pedestrian-attribute-recognition_image'),
 }
 
 
 class CVTrainers(object):
     # cv trainers
     image_instance_segmentation = 'image-instance-segmentation'
     image_portrait_enhancement = 'image-portrait-enhancement'
@@ -866,14 +884,15 @@
 
 
 class MultiModalTrainers(object):
     clip_multi_modal_embedding = 'clip-multi-modal-embedding'
     ofa = 'ofa'
     mplug = 'mplug'
     mgeo_ranking_trainer = 'mgeo-ranking-trainer'
+    efficient_diffusion_tuning = 'efficient-diffusion-tuning'
 
 
 class AudioTrainers(object):
     speech_frcrn_ans_cirm_16k = 'speech_frcrn_ans_cirm_16k'
     speech_dfsmn_kws_char_farfield = 'speech_dfsmn_kws_char_farfield'
     speech_kws_fsmn_char_ctc_nearfield = 'speech_kws_fsmn_char_ctc_nearfield'
     speech_kantts_trainer = 'speech-kantts-trainer'
@@ -989,14 +1008,15 @@
     table_question_answering_preprocessor = 'table-question-answering-preprocessor'
     re_tokenizer = 're-tokenizer'
     document_segmentation = 'document-segmentation'
     feature_extraction = 'feature-extraction'
     mglm_summarization = 'mglm-summarization'
     sentence_piece = 'sentence-piece'
     translation_evaluation = 'translation-evaluation-preprocessor'
+    canmt_translation = 'canmt-translation'
     dialog_use_preprocessor = 'dialog-use-preprocessor'
     siamese_uie_preprocessor = 'siamese-uie-preprocessor'
     document_grounded_dialog_retrieval = 'document-grounded-dialog-retrieval'
     document_grounded_dialog_rerank = 'document-grounded-dialog-rerank'
     document_grounded_dialog_generate = 'document-grounded-dialog-generate'
 
     # audio preprocessor
@@ -1009,14 +1029,15 @@
     # multi-modal preprocessor
     ofa_tasks_preprocessor = 'ofa-tasks-preprocessor'
     clip_preprocessor = 'clip-preprocessor'
     mplug_tasks_preprocessor = 'mplug-tasks-preprocessor'
     mgeo_ranking = 'mgeo-ranking'
     vldoc_preprocessor = 'vldoc-preprocessor'
     hitea_tasks_preprocessor = 'hitea-tasks-preprocessor'
+    diffusion_image_generation_preprocessor = 'diffusion-image-generation-preprocessor'
 
     # science preprocessor
     unifold_preprocessor = 'unifold-preprocessor'
 
 
 class Metrics(object):
     """ Names for different metrics.
```

### Comparing `modelscope-1.4.2/modelscope/metrics/__init__.py` & `modelscope-1.5.0/modelscope/metrics/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/accuracy_metric.py` & `modelscope-1.5.0/modelscope/metrics/accuracy_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/action_detection_evaluator.py` & `modelscope-1.5.0/modelscope/metrics/action_detection_evaluator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/audio_noise_metric.py` & `modelscope-1.5.0/modelscope/metrics/audio_noise_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/base.py` & `modelscope-1.5.0/modelscope/metrics/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/bleu_metric.py` & `modelscope-1.5.0/modelscope/metrics/bleu_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/builder.py` & `modelscope-1.5.0/modelscope/metrics/builder.py`

 * *Files 2% similar despite different names*

```diff
@@ -37,14 +37,15 @@
     PPL = 'ppl'
     PLCC = 'plcc'
     SRCC = 'srcc'
     RMSE = 'rmse'
     MRR = 'mrr'
     NDCG = 'ndcg'
     AR = 'AR'
+    Colorfulness = 'colorfulness'
 
 
 task_default_metrics = {
     Tasks.image_segmentation: [Metrics.image_ins_seg_coco_metric],
     Tasks.sentence_similarity: [Metrics.seq_cls_metric],
     Tasks.nli: [Metrics.seq_cls_metric],
     Tasks.sentiment_classification: [Metrics.seq_cls_metric],
@@ -70,14 +71,15 @@
     Tasks.video_stabilization: [Metrics.video_stabilization_metric],
     Tasks.image_quality_assessment_degradation:
     [Metrics.image_quality_assessment_degradation_metric],
     Tasks.image_quality_assessment_mos:
     [Metrics.image_quality_assessment_mos_metric],
     Tasks.bad_image_detecting: [Metrics.accuracy],
     Tasks.ocr_recognition: [Metrics.ocr_recognition_metric],
+    Tasks.efficient_diffusion_tuning: [Metrics.loss_metric],
 }
 
 
 def build_metric(metric_cfg: Union[str, Dict],
                  field: str = default_group,
                  default_args: dict = None):
     """ Build metric given metric_name and field.
```

### Comparing `modelscope-1.4.2/modelscope/metrics/ciderD/ciderD.py` & `modelscope-1.5.0/modelscope/metrics/ciderD/ciderD.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/ciderD/ciderD_scorer.py` & `modelscope-1.5.0/modelscope/metrics/ciderD/ciderD_scorer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/image_color_enhance_metric.py` & `modelscope-1.5.0/modelscope/metrics/image_color_enhance_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/image_colorization_metric.py` & `modelscope-1.5.0/modelscope/metrics/image_portrait_enhancement_metric.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,56 +1,61 @@
-# Copyright (c) Alibaba, Inc. and its affiliates.
+# Part of the implementation is borrowed and modified from BasicSR, publicly available at
+# https://github.com/XPixelGroup/BasicSR/blob/master/basicsr/metrics/psnr_ssim.py
 from typing import Dict
 
+import cv2
 import numpy as np
-import torch
-import torch.nn.functional as F
-from scipy import linalg
 
 from modelscope.metainfo import Metrics
-from modelscope.models.cv.image_inpainting.modules.inception import InceptionV3
 from modelscope.utils.registry import default_group
-from modelscope.utils.tensor_utils import (torch_nested_detach,
-                                           torch_nested_numpify)
 from .base import Metric
 from .builder import METRICS, MetricKeys
-from .image_denoise_metric import calculate_psnr
-from .image_inpainting_metric import FIDScore
+
+
+def calculate_psnr(img, img2):
+    assert img.shape == img2.shape, (
+        f'Image shapes are different: {img.shape}, {img2.shape}.')
+
+    img = img.astype(np.float64)
+    img2 = img2.astype(np.float64)
+
+    mse = np.mean((img - img2)**2)
+    if mse == 0:
+        return float('inf')
+    return 10. * np.log10(255. * 255. / mse)
 
 
 @METRICS.register_module(
-    group_key=default_group, module_name=Metrics.image_colorization_metric)
-class ImageColorizationMetric(Metric):
-    """The metric computation class for image colorization.
+    group_key=default_group,
+    module_name=Metrics.image_portrait_enhancement_metric)
+class ImagePortraitEnhancementMetric(Metric):
+    """The metric for image-portrait-enhancement task.
     """
 
     def __init__(self):
         self.preds = []
         self.targets = []
 
-        device = 'cuda' if torch.cuda.is_available() else 'cpu'
-        self.FID = FIDScore().to(device)
-
     def add(self, outputs: Dict, inputs: Dict):
-        ground_truths = outputs['preds']
-        eval_results = outputs['targets']
-        self.preds.append(eval_results)
-        self.targets.append(ground_truths)
+        ground_truths = outputs['target']
+        eval_results = outputs['pred']
+
+        self.preds.extend(eval_results)
+        self.targets.extend(ground_truths)
 
     def evaluate(self):
-        psnr_list = []
-        for (pred, target) in zip(self.preds, self.targets):
-            self.FID(pred, target)
-            psnr_list.append(calculate_psnr(target[0], pred[0], crop_border=0))
-        fid = self.FID.get_value()
-        return {MetricKeys.PSNR: np.mean(psnr_list), MetricKeys.FID: fid}
+        psnrs = [
+            calculate_psnr(pred, target)
+            for pred, target in zip(self.preds, self.targets)
+        ]
+
+        return {MetricKeys.PSNR: sum(psnrs) / len(psnrs)}
 
-    def merge(self, other: 'ImageColorizationMetric'):
+    def merge(self, other: 'ImagePortraitEnhancementMetric'):
         self.preds.extend(other.preds)
         self.targets.extend(other.targets)
 
     def __getstate__(self):
         return self.preds, self.targets
 
     def __setstate__(self, state):
-        self.__init__()
         self.preds, self.targets = state
```

### Comparing `modelscope-1.4.2/modelscope/metrics/image_denoise_metric.py` & `modelscope-1.5.0/modelscope/metrics/image_denoise_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/image_inpainting_metric.py` & `modelscope-1.5.0/modelscope/metrics/image_inpainting_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/image_instance_segmentation_metric.py` & `modelscope-1.5.0/modelscope/metrics/image_instance_segmentation_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/image_quality_assessment_degradation_metric.py` & `modelscope-1.5.0/modelscope/metrics/image_quality_assessment_degradation_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/image_quality_assessment_mos_metric.py` & `modelscope-1.5.0/modelscope/metrics/image_quality_assessment_mos_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/inbatch_recall_metric.py` & `modelscope-1.5.0/modelscope/metrics/inbatch_recall_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/loss_metric.py` & `modelscope-1.5.0/modelscope/metrics/loss_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/map_metric.py` & `modelscope-1.5.0/modelscope/metrics/map_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/movie_scene_segmentation_metric.py` & `modelscope-1.5.0/modelscope/metrics/movie_scene_segmentation_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/ned_metric.py` & `modelscope-1.5.0/modelscope/metrics/ned_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/ocr_recognition_metric.py` & `modelscope-1.5.0/modelscope/metrics/ocr_recognition_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/ppl_metric.py` & `modelscope-1.5.0/modelscope/metrics/ppl_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/prediction_saving_wrapper.py` & `modelscope-1.5.0/modelscope/metrics/prediction_saving_wrapper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/referring_video_object_segmentation_metric.py` & `modelscope-1.5.0/modelscope/metrics/referring_video_object_segmentation_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/sequence_classification_metric.py` & `modelscope-1.5.0/modelscope/metrics/sequence_classification_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/text_generation_metric.py` & `modelscope-1.5.0/modelscope/metrics/text_generation_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/text_ranking_metric.py` & `modelscope-1.5.0/modelscope/metrics/text_ranking_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/token_classification_metric.py` & `modelscope-1.5.0/modelscope/metrics/token_classification_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/video_frame_interpolation_metric.py` & `modelscope-1.5.0/modelscope/metrics/video_frame_interpolation_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/video_stabilization_metric.py` & `modelscope-1.5.0/modelscope/metrics/video_stabilization_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/video_summarization_metric.py` & `modelscope-1.5.0/modelscope/metrics/video_summarization_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/video_super_resolution_metric/matlab_functions.py` & `modelscope-1.5.0/modelscope/metrics/video_super_resolution_metric/matlab_functions.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/video_super_resolution_metric/metric_util.py` & `modelscope-1.5.0/modelscope/metrics/video_super_resolution_metric/metric_util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/video_super_resolution_metric/niqe.py` & `modelscope-1.5.0/modelscope/metrics/video_super_resolution_metric/niqe.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/metrics/video_super_resolution_metric/video_super_resolution_metric.py` & `modelscope-1.5.0/modelscope/metrics/video_super_resolution_metric/video_super_resolution_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/__init__.py` & `modelscope-1.5.0/modelscope/models/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/aec/layers/activations.py` & `modelscope-1.5.0/modelscope/models/audio/aec/layers/activations.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/aec/layers/affine_transform.py` & `modelscope-1.5.0/modelscope/models/audio/aec/layers/affine_transform.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/aec/layers/deep_fsmn.py` & `modelscope-1.5.0/modelscope/models/audio/aec/layers/deep_fsmn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/aec/layers/layer_base.py` & `modelscope-1.5.0/modelscope/models/audio/aec/layers/layer_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/aec/layers/uni_deep_fsmn.py` & `modelscope-1.5.0/modelscope/models/audio/aec/layers/uni_deep_fsmn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/aec/network/loss.py` & `modelscope-1.5.0/modelscope/models/audio/aec/network/loss.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/aec/network/modulation_loss.py` & `modelscope-1.5.0/modelscope/models/audio/aec/network/modulation_loss.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/aec/network/se_net.py` & `modelscope-1.5.0/modelscope/models/audio/aec/network/se_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/ans/__init__.py` & `modelscope-1.5.0/modelscope/models/audio/ans/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/ans/complex_nn.py` & `modelscope-1.5.0/modelscope/models/audio/ans/complex_nn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/ans/conv_stft.py` & `modelscope-1.5.0/modelscope/models/audio/ans/conv_stft.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/ans/denoise_net.py` & `modelscope-1.5.0/modelscope/models/audio/ans/denoise_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/ans/frcrn.py` & `modelscope-1.5.0/modelscope/models/audio/ans/frcrn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/ans/layers/activations.py` & `modelscope-1.5.0/modelscope/models/audio/ans/layers/activations.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/ans/layers/affine_transform.py` & `modelscope-1.5.0/modelscope/models/audio/ans/layers/affine_transform.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/ans/layers/layer_base.py` & `modelscope-1.5.0/modelscope/models/audio/ans/layers/layer_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/ans/layers/uni_deep_fsmn.py` & `modelscope-1.5.0/modelscope/models/audio/ans/layers/uni_deep_fsmn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/ans/se_module_complex.py` & `modelscope-1.5.0/modelscope/models/audio/ans/se_module_complex.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/ans/unet.py` & `modelscope-1.5.0/modelscope/models/audio/ans/unet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/asr/__init__.py` & `modelscope-1.5.0/modelscope/models/audio/asr/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/asr/generic_automatic_speech_recognition.py` & `modelscope-1.5.0/modelscope/models/audio/asr/generic_automatic_speech_recognition.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/asr/wenet_automatic_speech_recognition.py` & `modelscope-1.5.0/modelscope/models/audio/asr/wenet_automatic_speech_recognition.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/itn/__init__.py` & `modelscope-1.5.0/modelscope/models/audio/itn/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/itn/generic_inverse_text_processing.py` & `modelscope-1.5.0/modelscope/models/audio/itn/generic_inverse_text_processing.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/kws/__init__.py` & `modelscope-1.5.0/modelscope/models/audio/kws/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/kws/farfield/fsmn.py` & `modelscope-1.5.0/modelscope/models/audio/kws/farfield/fsmn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/kws/farfield/fsmn_sele_v2.py` & `modelscope-1.5.0/modelscope/models/audio/kws/farfield/fsmn_sele_v2.py`

 * *Files 0% similar despite different names*

```diff
@@ -14,15 +14,15 @@
     """
 
     def __init__(self, dimlinear=128, dimproj=64, lorder=20, rorder=1):
         """
         Args:
             dimlinear:              input / output dimension
             dimproj:                fsmn input / output dimension
-            lorder:                 left ofder
+            lorder:                 left order
             rorder:                 right order
         """
         super(FSMNUnit, self).__init__()
 
         self.shrink = LinearTransform(dimlinear, dimproj)
         self.fsmn = Fsmn(dimproj, dimproj, lorder, rorder, 1, 1)
         self.expand = AffineTransform(dimproj, dimlinear)
```

### Comparing `modelscope-1.4.2/modelscope/models/audio/kws/farfield/model.py` & `modelscope-1.5.0/modelscope/models/audio/kws/farfield/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/kws/farfield/model_def.py` & `modelscope-1.5.0/modelscope/models/audio/kws/farfield/model_def.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/kws/generic_key_word_spotting.py` & `modelscope-1.5.0/modelscope/models/audio/kws/generic_key_word_spotting.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/kws/nearfield/cmvn.py` & `modelscope-1.5.0/modelscope/models/audio/kws/nearfield/cmvn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/kws/nearfield/fsmn.py` & `modelscope-1.5.0/modelscope/models/audio/kws/nearfield/fsmn.py`

 * *Files 0% similar despite different names*

```diff
@@ -431,15 +431,15 @@
         self,
         input: torch.Tensor,
         in_cache: torch.Tensor = torch.zeros(0, 0, 0, dtype=torch.float)
     ) -> Tuple[torch.Tensor, torch.Tensor]:
         """
         Args:
             input (torch.Tensor): Input tensor (B, T, D)
-            in_cache(torhc.Tensor): (B, D, C), C is the accumulated cache size
+            in_cache(torch.Tensor): (B, D, C), C is the accumulated cache size
         """
 
         # print("FSMN forward!!!!")
         # print(input.shape)
         # print(input)
         # print(self.in_linear1.input_dim)
         # print(self.in_linear1.output_dim)
```

### Comparing `modelscope-1.4.2/modelscope/models/audio/kws/nearfield/model.py` & `modelscope-1.5.0/modelscope/models/audio/kws/nearfield/model.py`

 * *Files 1% similar despite different names*

```diff
@@ -35,16 +35,16 @@
                  **kwargs):
         """initialize the fsmn model from the `model_dir` path.
 
         Args:
             model_dir (str): the model path.
             cmvn_file (str): cmvn file
             backbone (dict): params related to backbone
-            input_dim (int): input dimention of network
-            output_dim (int): output dimention of network
+            input_dim (int): input dimension of network
+            output_dim (int): output dimension of network
             training (bool): training or inference mode
         """
         super().__init__(model_dir, *args, **kwargs)
 
         self.model = None
         self.model_cfg = None
 
@@ -104,15 +104,15 @@
                              preprocessing, backbone, classifier, activation)
         return kws_model
 
 
 class KWSModel(nn.Module):
     """Our model consists of four parts:
     1. global_cmvn: Optional, (idim, idim)
-    2. preprocessing: feature dimention projection, (idim, hdim)
+    2. preprocessing: feature dimension projection, (idim, hdim)
     3. backbone: backbone or feature extractor of the whole network, (hdim, hdim)
     4. classifier: output layer or classifier of KWS model, (hdim, odim)
     5. activation:
         nn.Sigmoid for wakeup word
         nn.Identity for speech command dataset
     """
 
@@ -129,15 +129,15 @@
     ):
         """
         Args:
             idim (int): input dimension of network
             odim (int): output dimension of network
             hdim (int): hidden dimension of network
             global_cmvn (nn.Module): cmvn for input feature, (idim, idim)
-            preprocessing (nn.Module): feature dimention projection, (idim, hdim)
+            preprocessing (nn.Module): feature dimension projection, (idim, hdim)
             backbone (nn.Module): backbone or feature extractor of the whole network, (hdim, hdim)
             classifier (nn.Module): output layer or classifier of KWS model, (hdim, odim)
             activation (nn.Module): nn.Identity for training, nn.Sigmoid for inference
         """
         super().__init__()
         self.idim = idim
         self.odim = odim
```

### Comparing `modelscope-1.4.2/modelscope/models/audio/punc/generic_punctuation.py` & `modelscope-1.5.0/modelscope/models/audio/punc/generic_punctuation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/separation/layer_norm.py` & `modelscope-1.5.0/modelscope/models/audio/separation/layer_norm.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/separation/mossformer.py` & `modelscope-1.5.0/modelscope/models/audio/separation/mossformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/separation/mossformer_block.py` & `modelscope-1.5.0/modelscope/models/audio/separation/mossformer_block.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/separation/mossformer_conv_module.py` & `modelscope-1.5.0/modelscope/models/audio/separation/mossformer_conv_module.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/sv/ecapa_tdnn.py` & `modelscope-1.5.0/modelscope/models/audio/sv/ecapa_tdnn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/sv/generic_speaker_verification.py` & `modelscope-1.5.0/modelscope/models/audio/sv/generic_speaker_verification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/tts/sambert_hifi.py` & `modelscope-1.5.0/modelscope/models/audio/tts/sambert_hifi.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/audio/tts/voice.py` & `modelscope-1.5.0/modelscope/models/audio/tts/voice.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/base/base_head.py` & `modelscope-1.5.0/modelscope/models/base/base_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/base/base_model.py` & `modelscope-1.5.0/modelscope/models/base/base_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/base/base_torch_head.py` & `modelscope-1.5.0/modelscope/models/base/base_torch_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/base/base_torch_model.py` & `modelscope-1.5.0/modelscope/models/base/base_torch_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/builder.py` & `modelscope-1.5.0/modelscope/models/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -13,16 +13,16 @@
                image_panoptic_segmentation, image_portrait_enhancement,
                image_probing_model, image_quality_assessment_degradation,
                image_quality_assessment_man, image_quality_assessment_mos,
                image_reid_person, image_restoration,
                image_semantic_segmentation, image_to_image_generation,
                image_to_image_translation, language_guided_video_summarization,
                movie_scene_segmentation, object_detection,
-               panorama_depth_estimation, pointcloud_sceneflow_estimation,
-               product_retrieval_embedding,
+               panorama_depth_estimation, pedestrian_attribute_recognition,
+               pointcloud_sceneflow_estimation, product_retrieval_embedding,
                referring_video_object_segmentation,
                robust_image_classification, salient_detection,
                shop_segmentation, stream_yolo, super_resolution,
                table_recognition, video_deinterlace, video_frame_interpolation,
                video_object_segmentation, video_panoptic_segmentation,
                video_single_object_tracking, video_stabilization,
                video_summarization, video_super_resolution, vidt, virual_tryon,
```

### Comparing `modelscope-1.4.2/modelscope/models/cv/abnormal_object_detection/mmdet_model.py` & `modelscope-1.5.0/modelscope/models/cv/abnormal_object_detection/mmdet_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py` & `modelscope-1.5.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py` & `modelscope-1.5.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/action_detection/action_detection_onnx.py` & `modelscope-1.5.0/modelscope/models/cv/action_detection/action_detection_onnx.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/action_detection/modules/action_detection_pytorch.py` & `modelscope-1.5.0/modelscope/models/cv/action_detection/modules/action_detection_pytorch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/action_detection/modules/resnet.py` & `modelscope-1.5.0/modelscope/models/cv/action_detection/modules/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/action_recognition/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/action_recognition/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/action_recognition/models.py` & `modelscope-1.5.0/modelscope/models/cv/action_recognition/models.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/action_recognition/s3dg.py` & `modelscope-1.5.0/modelscope/models/cv/action_recognition/s3dg.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/action_recognition/tada_convnext.py` & `modelscope-1.5.0/modelscope/models/cv/action_recognition/tada_convnext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/action_recognition/temporal_patch_shift_transformer.py` & `modelscope-1.5.0/modelscope/models/cv/action_recognition/temporal_patch_shift_transformer.py`

 * *Files 0% similar despite different names*

```diff
@@ -867,15 +867,15 @@
         The differences between swin3d and swin2d mainly lie in an extra
         axis. To utilize the pretrained parameters in 2d model,
         the weight of swin2d models should be inflated to fit in the shapes of
         the 3d counterpart.
 
         Args:
             logger (logging.Logger): The logger used to print
-                debugging infomation.
+                debugging information.
         """
         checkpoint = torch.load(self.pretrained, map_location='cpu')
         state_dict = checkpoint['model']
 
         # delete relative_position_index since we always re-init it
         relative_position_index_keys = [
             k for k in state_dict.keys() if 'relative_position_index' in k
```

### Comparing `modelscope-1.4.2/modelscope/models/cv/animal_recognition/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/animal_recognition/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/animal_recognition/resnet.py` & `modelscope-1.5.0/modelscope/models/cv/animal_recognition/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/animal_recognition/splat.py` & `modelscope-1.5.0/modelscope/models/cv/animal_recognition/splat.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/bad_image_detecting/bad_image_detecting.py` & `modelscope-1.5.0/modelscope/models/cv/bad_image_detecting/bad_image_detecting.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/body_2d_keypoints/hrnet_basic_modules.py` & `modelscope-1.5.0/modelscope/models/cv/body_2d_keypoints/hrnet_basic_modules.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/body_2d_keypoints/hrnet_v2.py` & `modelscope-1.5.0/modelscope/models/cv/body_2d_keypoints/hrnet_v2.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/body_2d_keypoints/w48.py` & `modelscope-1.5.0/modelscope/models/cv/body_2d_keypoints/w48.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/cannonical_pose/body_3d_pose.py` & `modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/body_3d_pose.py`

 * *Files 0% similar despite different names*

```diff
@@ -177,15 +177,15 @@
                 NUM_FRAME = max(receptive_filed + video_frame_number, video_frame_number)
 
         Returns:
             Dict[str, Any]:
                 "camera_pose": Tensor, [1, NUM_FRAME, OUT_NUM_JOINTS, OUT_3D_FEATURE_DIM],
                     3D human pose keypoints in camera frame.
                 "camera_traj": Tensor, [1, NUM_FRAME, 1, 3],
-                    root keypoints coordinates in camere frame.
+                    root keypoints coordinates in camera frame.
         """
         inputs_2d = input['inputs_2d']
         pose2d_rr = input['pose2d_rr']
         pose2d_canonical = input['pose2d_canonical']
         with torch.no_grad():
             # predict 3D pose keypoints
             predicted_3d_pos = self.model_pos(inputs_2d)
```

### Comparing `modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/cannonical_pose/canonical_pose_modules.py` & `modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/canonical_pose_modules.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/hdformer/backbone.py` & `modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/hdformer/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/hdformer/block.py` & `modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/hdformer/block.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/hdformer/directed_graph.py` & `modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/hdformer/directed_graph.py`

 * *Files 2% similar despite different names*

```diff
@@ -42,22 +42,22 @@
 
 class DiGraph():
 
     def __init__(self, skeleton):
         super().__init__()
         self.num_nodes = len(skeleton.parents())
         self.directed_edges_hop1 = [
-            (parrent, child)
-            for child, parrent in enumerate(skeleton.parents()) if parrent >= 0
+            (parent, child) for child, parent in enumerate(skeleton.parents())
+            if parent >= 0
         ]
         self.directed_edges_hop2 = [(0, 1, 2), (0, 4, 5), (0, 7, 8), (1, 2, 3),
                                     (4, 5, 6), (7, 8, 9),
                                     (7, 8, 11), (7, 8, 14), (8, 9, 10),
                                     (8, 11, 12), (8, 14, 15), (11, 12, 13),
-                                    (14, 15, 16)]  # (parrent, child)
+                                    (14, 15, 16)]  # (parent, child)
         self.directed_edges_hop3 = [(0, 1, 2, 3), (0, 4, 5, 6), (0, 7, 8, 9),
                                     (7, 8, 9, 10), (7, 8, 11, 12),
                                     (7, 8, 14, 15), (8, 11, 12, 13),
                                     (8, 14, 15, 16)]
         self.directed_edges_hop4 = [(0, 7, 8, 9, 10), (0, 7, 8, 11, 12),
                                     (0, 7, 8, 14, 15), (7, 8, 11, 12, 13),
                                     (7, 8, 14, 15, 16)]
@@ -108,16 +108,16 @@
     def __str__(self):
         return self.A
 
     def get_edge(self, skeleton):
         # edge is a list of [child, parent] paris
         self.num_node = len(skeleton.parents())
         self_link = [(i, i) for i in range(self.num_node)]
-        neighbor_link = [(child, parrent)
-                         for child, parrent in enumerate(skeleton.parents())]
+        neighbor_link = [(child, parent)
+                         for child, parent in enumerate(skeleton.parents())]
         self.self_link = self_link
         self.neighbor_link = neighbor_link
         self.edge = self_link + neighbor_link
         self.center = 0  # for h36m data skeleton, root node idx
 
     def get_adjacency(self, strategy):
         valid_hop = range(0, self.max_hop + 1, self.dilation)
```

### Comparing `modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer.py` & `modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer_detector.py` & `modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer_detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/body_3d_keypoints/hdformer/skeleton.py` & `modelscope-1.5.0/modelscope/models/cv/body_3d_keypoints/hdformer/skeleton.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/cartoon/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/cartoon/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/cartoon/facelib/LK/lk.py` & `modelscope-1.5.0/modelscope/models/cv/cartoon/facelib/LK/lk.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/cartoon/facelib/config.py` & `modelscope-1.5.0/modelscope/models/cv/cartoon/facelib/config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/cartoon/facelib/face_detector.py` & `modelscope-1.5.0/modelscope/models/cv/cartoon/facelib/face_detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/cartoon/facelib/face_landmark.py` & `modelscope-1.5.0/modelscope/models/cv/cartoon/facelib/face_landmark.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/cartoon/facelib/facer.py` & `modelscope-1.5.0/modelscope/models/cv/cartoon/facelib/facer.py`

 * *Files 1% similar despite different names*

```diff
@@ -90,15 +90,15 @@
 
         area = np.array(area)
 
         picked = area.argsort()[-self.top_k:][::-1]
         sorted_bboxes = [bboxes[x] for x in picked]
         return np.array(sorted_bboxes)
 
-    def judge_boxs(self, previuous_bboxs, now_bboxs):
+    def judge_boxs(self, previous_bboxs, now_bboxs):
 
         def iou(rec1, rec2):
 
             # computing area of each rectangles
             S_rec1 = (rec1[2] - rec1[0]) * (rec1[3] - rec1[1])
             S_rec2 = (rec2[2] - rec2[0]) * (rec2[3] - rec2[1])
 
@@ -112,25 +112,24 @@
             y2 = min(rec1[3], rec2[3])
 
             # judge if there is an intersect
             intersect = max(0, x2 - x1) * max(0, y2 - y1)
 
             return intersect / (sum_area - intersect)
 
-        if previuous_bboxs is None:
+        if previous_bboxs is None:
             return now_bboxs
 
         result = []
 
         for i in range(now_bboxs.shape[0]):
             contain = False
-            for j in range(previuous_bboxs.shape[0]):
-                if iou(now_bboxs[i], previuous_bboxs[j]) > self.iou_thres:
-                    result.append(
-                        self.smooth(now_bboxs[i], previuous_bboxs[j]))
+            for j in range(previous_bboxs.shape[0]):
+                if iou(now_bboxs[i], previous_bboxs[j]) > self.iou_thres:
+                    result.append(self.smooth(now_bboxs[i], previous_bboxs[j]))
                     contain = True
                     break
             if not contain:
                 result.append(now_bboxs[i])
 
         return np.array(result)
```

### Comparing `modelscope-1.4.2/modelscope/models/cv/cartoon/loss.py` & `modelscope-1.5.0/modelscope/models/cv/cartoon/loss.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/cartoon/model_tf.py` & `modelscope-1.5.0/modelscope/models/cv/cartoon/model_tf.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/cartoon/mtcnn_pytorch/src/align_trans.py` & `modelscope-1.5.0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/align_trans.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/cartoon/mtcnn_pytorch/src/matlab_cp2tform.py` & `modelscope-1.5.0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/matlab_cp2tform.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/cartoon/network.py` & `modelscope-1.5.0/modelscope/models/cv/cartoon/network.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/cartoon/utils.py` & `modelscope-1.5.0/modelscope/models/cv/cartoon/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/cmdssl_video_embedding/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/cmdssl_video_embedding/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/cmdssl_video_embedding/c3d.py` & `modelscope-1.5.0/modelscope/models/cv/cmdssl_video_embedding/c3d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/cmdssl_video_embedding/resnet2p1d.py` & `modelscope-1.5.0/modelscope/models/cv/cmdssl_video_embedding/resnet2p1d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/cmdssl_video_embedding/resnet3d.py` & `modelscope-1.5.0/modelscope/models/cv/cmdssl_video_embedding/resnet3d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/annotator.py` & `modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/annotator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/midas/api.py` & `modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/midas/api.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/blocks.py` & `modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/blocks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/dpt_depth.py` & `modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/dpt_depth.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net.py` & `modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net_custom.py` & `modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net_custom.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/transforms.py` & `modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/vit.py` & `modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/vit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/midas/utils.py` & `modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/midas/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/mlsd/mbv2_mlsd_large.py` & `modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/mbv2_mlsd_large.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/mlsd/utils.py` & `modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/openpose/body.py` & `modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/body.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/openpose/hand.py` & `modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/hand.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/openpose/model.py` & `modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/annotator/openpose/util.py` & `modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/controllable_image_generation/controlnet.py` & `modelscope-1.5.0/modelscope/models/cv/controllable_image_generation/controlnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/crowd_counting/cc_model.py` & `modelscope-1.5.0/modelscope/models/cv/crowd_counting/cc_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/crowd_counting/hrnet_aspp_relu.py` & `modelscope-1.5.0/modelscope/models/cv/crowd_counting/hrnet_aspp_relu.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/easycv_base.py` & `modelscope-1.5.0/modelscope/models/cv/easycv_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_2d_keypoints/face_2d_keypoints_align.py` & `modelscope-1.5.0/modelscope/models/cv/face_2d_keypoints/face_2d_keypoints_align.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py` & `modelscope-1.5.0/modelscope/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/mogface/models/detectors.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/mogface/models/detectors.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/mogface/models/mogface.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/mogface/models/mogface.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/mogface/models/mogprednet.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/mogface/models/mogprednet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/mogface/models/resnet.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/mogface/models/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/mogface/models/utils.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/mogface/models/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/mtcnn/models/box_utils.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/mtcnn/models/box_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/mtcnn/models/detector.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/mtcnn/models/detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/mtcnn/models/first_stage.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/mtcnn/models/first_stage.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/mtcnn/models/get_nets.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/mtcnn/models/get_nets.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/peppa_pig_face/LK/lk.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/peppa_pig_face/LK/lk.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/peppa_pig_face/face_detector.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/peppa_pig_face/face_detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/peppa_pig_face/face_landmark.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/peppa_pig_face/face_landmark.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/peppa_pig_face/facer.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/peppa_pig_face/facer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/retinaface/detection.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/retinaface/detection.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/retinaface/models/net.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/retinaface/models/net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/retinaface/models/retinaface.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/retinaface/models/retinaface.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/retinaface/utils.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/retinaface/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/transforms.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/bbox_nms.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/bbox_nms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/retinaface.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/retinaface.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/base.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/single_stage.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/single_stage.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/tinymog.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/tinymog.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/preprocessor.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/scrfd_detect.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/scrfd_detect.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/scrfd/tinymog_detect.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/scrfd/tinymog_detect.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/detection.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/detection.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/vision/box_utils.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/vision/box_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/vision/mb_tiny.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/vision/mb_tiny.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/data_preprocessing.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/data_preprocessing.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/fd_config.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/fd_config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/mb_tiny_fd.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/mb_tiny_fd.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/predictor.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/predictor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/ssd.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/ssd.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_detection/ulfd_slim/vision/transforms.py` & `modelscope-1.5.0/modelscope/models/cv/face_detection/ulfd_slim/vision/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_emotion/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/face_emotion/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_emotion/efficient/model.py` & `modelscope-1.5.0/modelscope/models/cv/face_emotion/efficient/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_emotion/efficient/utils.py` & `modelscope-1.5.0/modelscope/models/cv/face_emotion/efficient/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_emotion/emotion_infer.py` & `modelscope-1.5.0/modelscope/models/cv/face_emotion/emotion_infer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_emotion/emotion_model.py` & `modelscope-1.5.0/modelscope/models/cv/face_emotion/emotion_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_emotion/face_alignment/face.py` & `modelscope-1.5.0/modelscope/models/cv/face_emotion/face_alignment/face.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_emotion/face_alignment/face_align.py` & `modelscope-1.5.0/modelscope/models/cv/face_emotion/face_alignment/face_align.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_generation/op/conv2d_gradfix.py` & `modelscope-1.5.0/modelscope/models/cv/face_generation/op/conv2d_gradfix.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_generation/op/fused_act.py` & `modelscope-1.5.0/modelscope/models/cv/face_generation/op/fused_act.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_generation/op/upfirdn2d.py` & `modelscope-1.5.0/modelscope/models/cv/face_generation/op/upfirdn2d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_generation/stylegan2.py` & `modelscope-1.5.0/modelscope/models/cv/face_generation/stylegan2.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_human_hand_detection/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/face_human_hand_detection/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_human_hand_detection/det_infer.py` & `modelscope-1.5.0/modelscope/models/cv/face_human_hand_detection/det_infer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_human_hand_detection/ghost_pan.py` & `modelscope-1.5.0/modelscope/models/cv/face_human_hand_detection/ghost_pan.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_human_hand_detection/nanodet_plus_head.py` & `modelscope-1.5.0/modelscope/models/cv/face_human_hand_detection/nanodet_plus_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_human_hand_detection/one_stage_detector.py` & `modelscope-1.5.0/modelscope/models/cv/face_human_hand_detection/one_stage_detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_human_hand_detection/shufflenetv2.py` & `modelscope-1.5.0/modelscope/models/cv/face_human_hand_detection/shufflenetv2.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_human_hand_detection/utils.py` & `modelscope-1.5.0/modelscope/models/cv/face_human_hand_detection/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_recognition/align_face.py` & `modelscope-1.5.0/modelscope/models/cv/face_recognition/align_face.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_recognition/torchkit/backbone/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/face_recognition/torchkit/backbone/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_recognition/torchkit/backbone/arcface_backbone.py` & `modelscope-1.5.0/modelscope/models/cv/face_recognition/torchkit/backbone/arcface_backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_recognition/torchkit/backbone/common.py` & `modelscope-1.5.0/modelscope/models/cv/face_recognition/torchkit/backbone/common.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_recognition/torchkit/backbone/facemask_backbone.py` & `modelscope-1.5.0/modelscope/models/cv/face_recognition/torchkit/backbone/facemask_backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_recognition/torchkit/backbone/model_irse.py` & `modelscope-1.5.0/modelscope/models/cv/face_recognition/torchkit/backbone/model_irse.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_recognition/torchkit/backbone/model_resnet.py` & `modelscope-1.5.0/modelscope/models/cv/face_recognition/torchkit/backbone/model_resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_recognition/torchkit/rts_backbone.py` & `modelscope-1.5.0/modelscope/models/cv/face_recognition/torchkit/rts_backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/facelandmark/large_base_lmks_infer.py` & `modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/facelandmark/large_base_lmks_infer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_base_lmks_net.py` & `modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_base_lmks_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_eyeball_net.py` & `modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_eyeball_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/facerecon_model.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/ops/apps.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,564 +1,664 @@
-# Part of the implementation is borrowed and modified from Deep3DFaceRecon_pytorch,
-# publicly available at https://github.com/sicxu/Deep3DFaceRecon_pytorch
-import os
+# Copyright 2021-2022 The Alibaba Fundamental Vision Team Authors. All rights reserved.
+# APPs that facilitate the use of pretrained neural networks.
 
-import cv2
+import os.path as osp
+
+import artist.data as data
+import artist.models as models
 import numpy as np
 import torch
+import torch.cuda.amp as amp
+import torch.nn.functional as F
+import torchvision.transforms as T
+from artist import DOWNLOAD_TO_CACHE
+from PIL import Image
+from torch.utils.data import DataLoader, Dataset
+
+from .utils import parallel, read_image
+
+__all__ = [
+    'FeatureExtractor', 'Classifier', 'Text2Image', 'Sole2Shoe', 'ImageParser',
+    'TextImageMatch', 'taobao_feature_extractor', 'singleton_classifier',
+    'orientation_classifier', 'fashion_text2image', 'mindalle_text2image',
+    'sole2shoe', 'sole_parser', 'sod_foreground_parser',
+    'fashion_text_image_match'
+]
+
+
+class ImageFolder(Dataset):
+
+    def __init__(self, paths, transforms=None):
+        self.paths = paths
+        self.transforms = transforms
+
+    def __getitem__(self, index):
+        img = read_image(self.paths[index])
+        if img.mode != 'RGB':
+            img = img.convert('RGB')
+        if self.transforms is not None:
+            img = self.transforms(img)
+        return img
+
+    def __len__(self):
+        return len(self.paths)
+
+
+class FeatureExtractor(object):
+
+    def __init__(
+        self,
+        model='InceptionV1',
+        checkpoint='models/inception-v1/1218shoes.v9_7.140.0.1520000',
+        resolution=224,
+        mean=[0.485, 0.456, 0.406],
+        std=[0.229, 0.224, 0.225],
+        batch_size=64,
+        device=torch.device(
+            'cuda' if torch.cuda.is_available() else 'cpu')):  # noqa E125
+        self.resolution = resolution
+        self.batch_size = batch_size
+        self.device = device
+
+        # init model
+        self.net = getattr(
+            models,
+            model)(num_classes=None).eval().requires_grad_(False).to(device)
+        self.net.load_state_dict(
+            torch.load(DOWNLOAD_TO_CACHE(checkpoint), map_location=device))
+
+        # data transforms
+        self.transforms = T.Compose([
+            data.PadToSquare(),
+            T.Resize(resolution),
+            T.ToTensor(),
+            T.Normalize(mean, std)
+        ])
 
-from modelscope.models import MODELS, TorchModel
-from modelscope.models.cv.face_reconstruction.models import opt
-from .. import utils
-from . import networks
-from .bfm import ParametricFaceModel
-from .losses import (CLIPLoss_relative, TVLoss, TVLoss_std, landmark_loss,
-                     perceptual_loss, photo_loss, points_loss_horizontal,
-                     reflectance_loss, reg_loss)
-from .nv_diffrast import MeshRenderer
-
-
-@MODELS.register_module('face-reconstruction', 'face_reconstruction')
-class FaceReconModel(TorchModel):
-
-    def __init__(self,
-                 model_dir,
-                 w_color=1.92,
-                 w_exp=0.8,
-                 w_gamma=10.0,
-                 w_id=1.0,
-                 w_lm=0.0016,
-                 w_reg=0.0003,
-                 w_tex=0.017,
-                 *args,
-                 **kwargs):
-        """The FaceReconModel is implemented based on Deep3DFaceRecon_pytorch, publicly available at
-        https://github.com/sicxu/Deep3DFaceRecon_pytorch
-
-        Args:
-            model_dir: the root directory of the model files
-            w_color: the weight of color loss
-            w_exp: the regularization weight of expression
-            w_gamma: the regularization weight of lighting
-            w_id: the regularization weight of identity
-            w_lm: the weight of landmark loss
-            w_reg: the weight of regularization loss
-            w_tex: the regularization weight of texture
+    def __call__(self, imgs, num_workers=0):
+        r"""imgs:   Either a PIL.Image or a list of PIL.Image instances.
         """
-        super().__init__(model_dir, *args, **kwargs)
-
-        opt.bfm_folder = os.path.join(model_dir, 'assets')
-        self.opt = opt
-        self.w_color = w_color
-        self.w_exp = w_exp
-        self.w_gamma = w_gamma
-        self.w_id = w_id
-        self.w_lm = w_lm
-        self.w_reg = w_reg
-        self.w_tex = w_tex
-        self.device = torch.device('cpu')
-        self.isTrain = opt.isTrain
-        self.visual_names = ['output_vis']
-        self.model_names = ['net_recon']
-        self.parallel_names = self.model_names + ['renderer']
-
-        self.net_recon = networks.define_net_recon(
-            net_recon=opt.net_recon,
-            use_last_fc=opt.use_last_fc,
-            init_path=None)
-
-        self.facemodel = ParametricFaceModel(
-            bfm_folder=opt.bfm_folder,
-            camera_distance=opt.camera_d,
-            focal=opt.focal,
-            center=opt.center,
-            is_train=self.isTrain,
-            default_name=opt.bfm_model)
-
-        self.facemodel_front = ParametricFaceModel(
-            bfm_folder=opt.bfm_folder,
-            camera_distance=opt.camera_d,
-            focal=opt.focal,
-            center=opt.center,
-            is_train=self.isTrain,
-            default_name='face_model_for_maas.mat')
-
-        fov = 2 * np.arctan(opt.center / opt.focal) * 180 / np.pi
-        self.renderer = MeshRenderer(
-            rasterize_fov=fov,
-            znear=opt.z_near,
-            zfar=opt.z_far,
-            rasterize_size=int(2 * opt.center))
-
-        self.renderer_fitting = MeshRenderer(
-            rasterize_fov=fov,
-            znear=opt.z_near,
-            zfar=opt.z_far,
-            rasterize_size=int(2 * opt.center))
-
-        self.nonlinear_UVs = self.facemodel.nonlinear_UVs
-        self.nonlinear_UVs = torch.from_numpy(self.nonlinear_UVs).to(
-            torch.device('cuda'))
-
-        template_obj_path = os.path.join(opt.bfm_folder, 'template_mesh.obj')
-        self.template_mesh = utils.read_obj(template_obj_path)
-
-        self.input_imgs = []
-        self.input_img_hds = []
-        self.input_fat_img_hds = []
-        self.atten_masks = []
-        self.gt_lms = []
-        self.gt_lm_hds = []
-        self.trans_ms = []
-        self.img_names = []
-        self.face_masks = []
-        self.head_masks = []
-        self.input_imgs_coeff = []
-        self.gt_lms_coeff = []
-
-        self.loss_names = [
-            'all', 'feat', 'color', 'lm', 'reg', 'gamma', 'reflc'
-        ]
-
-        # loss func name: (compute_%s_loss) % loss_name
-        self.compute_feat_loss = perceptual_loss
-        self.comupte_color_loss = photo_loss
-        self.compute_lm_loss = landmark_loss
-        self.compute_reg_loss = reg_loss
-        self.compute_reflc_loss = reflectance_loss
-
-    def load_networks(self, load_path):
-        state_dict = torch.load(load_path, map_location=self.device)
-        print('loading the model from %s' % load_path)
-
-        for name in self.model_names:
-            if isinstance(name, str):
-                net = getattr(self, name)
-                if isinstance(net, torch.nn.DataParallel):
-                    net = net.module
-                net.load_state_dict(state_dict[name], strict=False)
-
-        if self.opt.phase != 'test':
-            if self.opt.continue_train:
-
-                try:
-                    for i, sched in enumerate(self.schedulers):
-                        sched.load_state_dict(state_dict['sched_%02d' % i])
-                except Exception as e:
-                    print(e)
-                    for i, sched in enumerate(self.schedulers):
-                        sched.last_epoch = self.opt.epoch_count - 1
-
-    def setup(self, checkpoint_path):
-        """Load and print networks; create schedulers
+        # preprocess
+        if isinstance(imgs, Image.Image):
+            imgs = [imgs]
+        assert isinstance(imgs,
+                          (tuple, list)) and isinstance(imgs[0], Image.Image)
+        imgs = torch.stack(parallel(self.transforms, imgs, num_workers), dim=0)
+
+        # forward
+        feats = []
+        for batch in imgs.split(self.batch_size, dim=0):
+            batch = batch.to(self.device, non_blocking=True)
+            feats.append(self.net(batch))
+        return torch.cat(feats, dim=0)
+
+    def batch_process(self, paths):
+        # init dataloader
+        dataloader = DataLoader(
+            dataset=ImageFolder(paths, self.transforms),
+            batch_size=self.batch_size,
+            shuffle=False,
+            drop_last=False,
+            pin_memory=True,
+            num_workers=8,
+            prefetch_factor=2)
+
+        # forward
+        feats = []
+        for step, batch in enumerate(dataloader, 1):
+            print(f'Step: {step}/{len(dataloader)}', flush=True)
+            batch = batch.to(self.device, non_blocking=True)
+            feats.append(self.net(batch))
+        return torch.cat(feats)
+
+
+class Classifier(object):
+
+    def __init__(
+        self,
+        model='InceptionV1',
+        checkpoint='models/classifier/shoes+apparel+bag-sgdetect-211230.pth',
+        num_classes=1,
+        resolution=224,
+        mean=[0.485, 0.456, 0.406],
+        std=[0.229, 0.224, 0.225],
+        batch_size=64,
+        device=torch.device(
+            'cuda' if torch.cuda.is_available() else 'cpu')):  # noqa E125
+        self.num_classes = num_classes
+        self.resolution = resolution
+        self.batch_size = batch_size
+        self.device = device
+
+        # init model
+        self.net = getattr(models, model)(
+            num_classes=num_classes).eval().requires_grad_(False).to(device)
+        self.net.load_state_dict(
+            torch.load(DOWNLOAD_TO_CACHE(checkpoint), map_location=device))
+
+        # data transforms
+        self.transforms = T.Compose([
+            data.PadToSquare(),
+            T.Resize(resolution),
+            T.ToTensor(),
+            T.Normalize(mean, std)
+        ])
 
-        Parameters:
-            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions
+    def __call__(self, imgs, num_workers=0):
+        r"""imgs:   Either a PIL.Image or a list of PIL.Image instances.
         """
-        self.load_networks(checkpoint_path)
+        # preprocess
+        if isinstance(imgs, Image.Image):
+            imgs = [imgs]
+        assert isinstance(imgs,
+                          (tuple, list)) and isinstance(imgs[0], Image.Image)
+        imgs = torch.stack(parallel(self.transforms, imgs, num_workers), dim=0)
+
+        # forward
+        scores = []
+        for batch in imgs.split(self.batch_size, dim=0):
+            batch = batch.to(self.device, non_blocking=True)
+            logits = self.net(batch)
+            scores.append(logits.sigmoid() if self.num_classes ==  # noqa W504
+                          1 else logits.softmax(dim=1))
+        return torch.cat(scores, dim=0)
+
+
+class Text2Image(object):
+
+    def __init__(
+        self,
+        vqgan_dim=128,
+        vqgan_z_dim=256,
+        vqgan_dim_mult=[1, 1, 2, 2, 4],
+        vqgan_num_res_blocks=2,
+        vqgan_attn_scales=[1.0 / 16],
+        vqgan_codebook_size=975,
+        vqgan_beta=0.25,
+        gpt_txt_vocab_size=21128,
+        gpt_txt_seq_len=64,
+        gpt_img_seq_len=1024,
+        gpt_dim=1024,
+        gpt_num_heads=16,
+        gpt_num_layers=24,
+        vqgan_checkpoint='models/vqgan/vqgan_shoes+apparels_step10k_vocab975.pth',
+        gpt_checkpoint='models/seq2seq_gpt/text2image_shoes+apparels_step400k.pth',
+        tokenizer=data.BertTokenizer(name='bert-base-chinese', length=64),
+        batch_size=16,
+        device=torch.device(
+            'cuda' if torch.cuda.is_available() else 'cpu')):  # noqa E125
+        self.tokenizer = tokenizer
+        self.batch_size = batch_size
+        self.device = device
+
+        # init VQGAN model
+        self.vqgan = models.VQGAN(
+            dim=vqgan_dim,
+            z_dim=vqgan_z_dim,
+            dim_mult=vqgan_dim_mult,
+            num_res_blocks=vqgan_num_res_blocks,
+            attn_scales=vqgan_attn_scales,
+            codebook_size=vqgan_codebook_size,
+            beta=vqgan_beta).eval().requires_grad_(False).to(device)
+        self.vqgan.load_state_dict(
+            torch.load(
+                DOWNLOAD_TO_CACHE(vqgan_checkpoint), map_location=device))
+
+        # init GPT model
+        self.gpt = models.Seq2SeqGPT(
+            src_vocab_size=gpt_txt_vocab_size,
+            tar_vocab_size=vqgan_codebook_size,
+            src_seq_len=gpt_txt_seq_len,
+            tar_seq_len=gpt_img_seq_len,
+            dim=gpt_dim,
+            num_heads=gpt_num_heads,
+            num_layers=gpt_num_layers).eval().requires_grad_(False).to(device)
+        self.gpt.load_state_dict(
+            torch.load(DOWNLOAD_TO_CACHE(gpt_checkpoint), map_location=device))
+
+    def __call__(self,
+                 txts,
+                 top_k=64,
+                 top_p=None,
+                 temperature=0.6,
+                 use_fp16=True):
+        # preprocess
+        if isinstance(txts, str):
+            txts = [txts]
+        assert isinstance(txts, (tuple, list)) and isinstance(txts[0], str)
+        txt_tokens = torch.LongTensor([self.tokenizer(u) for u in txts])
+
+        # forward
+        out_imgs = []
+        for batch in txt_tokens.split(self.batch_size, dim=0):
+            # sample
+            batch = batch.to(self.device, non_blocking=True)
+            with amp.autocast(enabled=use_fp16):
+                img_tokens = self.gpt.sample(batch, top_k, top_p, temperature)
+
+            # decode
+            imgs = self.vqgan.decode_from_tokens(img_tokens)
+            imgs = self._whiten_borders(imgs)
+            imgs = imgs.clamp_(-1, 1).add_(1).mul_(125.0).permute(
+                0, 2, 3, 1).cpu().numpy().astype(np.uint8)
+            imgs = [Image.fromarray(u) for u in imgs]
+
+            # append
+            out_imgs += imgs
+        return out_imgs
 
-    def parallelize(self, convert_sync_batchnorm=True):
-        if not self.opt.use_ddp:
-            for name in self.parallel_names:
-                if isinstance(name, str):
-                    module = getattr(self, name)
-                    setattr(self, name, module.to(self.device))
-        else:
-            for name in self.model_names:
-                if isinstance(name, str):
-                    module = getattr(self, name)
-                    if convert_sync_batchnorm:
-                        module = torch.nn.SyncBatchNorm.convert_sync_batchnorm(
-                            module)
-                    setattr(
-                        self, name,
-                        torch.nn.parallel.DistributedDataParallel(
-                            module.to(self.device),
-                            device_ids=[self.device.index],
-                            find_unused_parameters=True,
-                            broadcast_buffers=True))
-
-            # DistributedDataParallel is not needed when a module doesn't have any parameter that requires a gradient.
-            for name in self.parallel_names:
-                if isinstance(name, str) and name not in self.model_names:
-                    module = getattr(self, name)
-                    setattr(self, name, module.to(self.device))
-
-        # put state_dict of optimizer to gpu device
-        if self.opt.phase != 'test':
-            if self.opt.continue_train:
-                for optim in self.optimizers:
-                    for state in optim.state.values():
-                        for k, v in state.items():
-                            if isinstance(v, torch.Tensor):
-                                state[k] = v.to(self.device)
-
-    def eval(self):
-        """Make models eval mode"""
-        for name in self.model_names:
-            if isinstance(name, str):
-                net = getattr(self, name)
-                net.eval()
-
-    def set_render(self, image_res):
-        fov = 2 * np.arctan(self.opt.center / self.opt.focal) * 180 / np.pi
-        if image_res is None:
-            image_res = int(2 * self.opt.center)
-
-        self.renderer = MeshRenderer(
-            rasterize_fov=fov,
-            znear=self.opt.z_near,
-            zfar=self.opt.z_far,
-            rasterize_size=image_res)
-
-    def set_input(self, input):
-        """Unpack input data from the dataloader and perform necessary pre-processing steps.
-
-        Parameters:
-            input: a dictionary that contains the data itself and its metadata information.
+    def _whiten_borders(self, imgs):
+        r"""Remove border artifacts.
         """
-        self.input_img = input['imgs'].to(self.device)
-        self.input_img_hd = input['imgs_hd'].to(
-            self.device) if 'imgs_hd' in input else None
+        imgs[:, :, :18, :] = 1
+        imgs[:, :, :, :18] = 1
+        imgs[:, :, -18:, :] = 1
+        imgs[:, :, :, -18:] = 1
+        return imgs
+
+
+class Sole2Shoe(object):
+
+    def __init__(
+        self,
+        vqgan_dim=128,
+        vqgan_z_dim=256,
+        vqgan_dim_mult=[1, 1, 2, 2, 4],
+        vqgan_num_res_blocks=2,
+        vqgan_attn_scales=[1.0 / 16],
+        vqgan_codebook_size=975,
+        vqgan_beta=0.25,
+        src_resolution=256,
+        tar_resolution=512,
+        gpt_dim=1024,
+        gpt_num_heads=16,
+        gpt_num_layers=24,
+        vqgan_checkpoint='models/vqgan/vqgan_shoes+apparels_step10k_vocab975.pth',
+        gpt_checkpoint='models/seq2seq_gpt/sole2shoe-step300k-220104.pth',
+        batch_size=12,
+        device=torch.device(
+            'cuda' if torch.cuda.is_available() else 'cpu')):  # noqa E125
+        self.batch_size = batch_size
+        self.device = device
+        src_seq_len = (src_resolution // 16)**2
+        tar_seq_len = (tar_resolution // 16)**2
+
+        # init VQGAN model
+        self.vqgan = models.VQGAN(
+            dim=vqgan_dim,
+            z_dim=vqgan_z_dim,
+            dim_mult=vqgan_dim_mult,
+            num_res_blocks=vqgan_num_res_blocks,
+            attn_scales=vqgan_attn_scales,
+            codebook_size=vqgan_codebook_size,
+            beta=vqgan_beta).eval().requires_grad_(False).to(device)
+        self.vqgan.load_state_dict(
+            torch.load(
+                DOWNLOAD_TO_CACHE(vqgan_checkpoint), map_location=device))
+
+        # init GPT model
+        self.gpt = models.Seq2SeqGPT(
+            src_vocab_size=vqgan_codebook_size,
+            tar_vocab_size=vqgan_codebook_size,
+            src_seq_len=src_seq_len,
+            tar_seq_len=tar_seq_len,
+            dim=gpt_dim,
+            num_heads=gpt_num_heads,
+            num_layers=gpt_num_layers).eval().requires_grad_(False).to(device)
+        self.gpt.load_state_dict(
+            torch.load(DOWNLOAD_TO_CACHE(gpt_checkpoint), map_location=device))
+
+        # data transforms
+        self.transforms = T.Compose([
+            data.PadToSquare(),
+            T.Resize(src_resolution),
+            T.ToTensor(),
+            T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
+        ])
+
+    def __call__(self,
+                 sole_imgs,
+                 top_k=64,
+                 top_p=None,
+                 temperature=0.6,
+                 use_fp16=True,
+                 num_workers=0):
+        # preprocess
+        if isinstance(sole_imgs, Image.Image):
+            sole_imgs = [sole_imgs]
+        assert isinstance(sole_imgs, (tuple, list)) and isinstance(
+            sole_imgs[0], Image.Image)
+        sole_imgs = torch.stack(
+            parallel(self.transforms, sole_imgs, num_workers), dim=0)
+
+        # forward
+        out_imgs = []
+        for batch in sole_imgs.split(self.batch_size, dim=0):
+            # sample
+            batch = batch.to(self.device)
+            with amp.autocast(enabled=use_fp16):
+                sole_tokens = self.vqgan.encode_to_tokens(batch)
+                shoe_tokens = self.gpt.sample(sole_tokens, top_k, top_p,
+                                              temperature)
+
+            # decode
+            shoe_imgs = self.vqgan.decode_from_tokens(shoe_tokens)
+            shoe_imgs = self._whiten_borders(shoe_imgs)
+            shoe_imgs = shoe_imgs.clamp_(-1, 1).add_(1).mul_(125.0).permute(
+                0, 2, 3, 1).cpu().numpy().astype(np.uint8)
+            shoe_imgs = [Image.fromarray(u) for u in shoe_imgs]
+
+            # append
+            out_imgs += shoe_imgs
+        return out_imgs
 
-        if 'imgs_fat_hd' not in input or input['imgs_fat_hd'] is None:
-            self.input_fat_img_hd = self.input_img_hd
+    def _whiten_borders(self, imgs):
+        r"""Remove border artifacts.
+        """
+        imgs[:, :, :18, :] = 1
+        imgs[:, :, :, :18] = 1
+        imgs[:, :, -18:, :] = 1
+        imgs[:, :, :, -18:] = 1
+        return imgs
+
+
+class ImageParser(object):
+
+    def __init__(
+        self,
+        model='SPNet',
+        num_classes=2,
+        resolution=800,
+        mean=[0.485, 0.456, 0.406],
+        std=[0.229, 0.224, 0.225],
+        model_with_softmax=False,
+        checkpoint='models/spnet/sole_segmentation_211219.pth',
+        batch_size=16,
+        device=torch.device(
+            'cuda' if torch.cuda.is_available() else 'cpu')):  # noqa E125
+        self.batch_size = batch_size
+        self.device = device
+
+        # init model
+        if checkpoint.endswith('.pt'):
+            self.net = torch.jit.load(
+                DOWNLOAD_TO_CACHE(checkpoint)).eval().to(device)
+            [p.requires_grad_(False) for p in self.net.parameters()]
         else:
-            self.input_fat_img_hd = input['imgs_fat_hd'].to(self.device)
-
-        self.atten_mask = input['msks'].to(
-            self.device) if 'msks' in input else None
-        self.gt_lm = input['lms'].to(self.device) if 'lms' in input else None
-        self.gt_lm_hd = input['lms_hd'].to(
-            self.device) if 'lms_hd' in input else None
-        self.trans_m = input['M'].to(self.device) if 'M' in input else None
-        self.image_paths = input['im_paths'] if 'im_paths' in input else None
-        self.img_name = input['img_name'] if 'img_name' in input else None
-        self.face_mask = input['face_mask'].to(
-            self.device) if 'face_mask' in input else None
-        self.head_mask = input['head_mask'].to(
-            self.device) if 'head_mask' in input else None
-        self.gt_normals = input['normals'].to(
-            self.device) if 'normals' in input else None
-        self.input_img_coeff = input['imgs_coeff'].to(
-            self.device) if 'imgs_coeff' in input else None
-        self.gt_lm_coeff = input['lms_coeff'].to(
-            self.device) if 'lms_coeff' in input else None
-
-    def get_edge_points_horizontal(self):
-        left_points = []
-        right_points = []
-        for i in range(self.face_mask.shape[2]):
-            inds = torch.where(self.face_mask[0, 0, i, :] > 0.5)  # 0.9
-            if len(inds[0]) > 0:  # i > 112 and len(inds[0]) > 0
-                left_points.append(int(inds[0][0]) + 1)
-                right_points.append(int(inds[0][-1]))
-            else:
-                left_points.append(0)
-                right_points.append(self.face_mask.shape[3] - 1)
-        self.left_points = torch.tensor(left_points).long().to(self.device)
-        self.right_points = torch.tensor(right_points).long().to(self.device)
-
-    def get_edge_points_vertical(self):
-        top_points = []
-        bottom_points = []
-        for i in range(self.face_mask.shape[3]):
-            inds = torch.where(self.face_mask[0, 0, :, i] > 0.9)
-            if len(inds[0]) > 0:
-                top_points.append(int(inds[0][0]))
-                bottom_points.append(int(inds[0][-1]))
-            else:
-                top_points.append(0)
-                bottom_points.append(self.face_mask.shape[2] - 1)
-        self.top_points = torch.tensor(top_points).long().to(self.device)
-        self.bottom_points = torch.tensor(bottom_points).long().to(self.device)
-
-    def blur_shape_offset_uv(self, global_blur=False, blur_size=3):
-        if self.edge_mask is not None:
-            shape_offset_uv_blur = self.shape_offset_uv[0].detach().cpu(
-            ).numpy()
-            shape_offset_uv_blur = cv2.blur(shape_offset_uv_blur, (15, 15))
-            shape_offset_uv_blur = torch.from_numpy(
-                shape_offset_uv_blur).float().to(self.device)[None, ...]
-            value_1 = shape_offset_uv_blur * self.edge_mask[None, ..., None]
-            value_2 = self.shape_offset_uv * (
-                1 - self.edge_mask[None, ..., None])
-            self.shape_offset_uv = value_1 + value_2
-
-        self.shape_offset_uv = self.shape_offset_uv * self.fusion_mask[None,
-                                                                       ...,
-                                                                       None]
-
-        if global_blur and blur_size > 0:
-            shape_offset_uv_blur = self.shape_offset_uv[0].detach().cpu(
-            ).numpy()
-            shape_offset_uv_blur = cv2.blur(shape_offset_uv_blur,
-                                            (blur_size, blur_size))
-            shape_offset_uv_blur = torch.from_numpy(
-                shape_offset_uv_blur).float().to(self.device)[None, ...]
-            self.shape_offset_uv = shape_offset_uv_blur
-
-    def get_fusion_mask(self):
-
-        h, w = self.shape_offset_uv.shape[1:3]
-        self.fusion_mask = torch.zeros((h, w)).to(self.device).float()
-        UVs_coords = self.nonlinear_UVs.clone()[:35709]
-        UVs_coords[:, 0] *= w
-        UVs_coords[:, 1] *= h
-        UVs_coords_int = torch.floor(UVs_coords)
-        UVs_coords_int = UVs_coords_int.long()
-
-        self.fusion_mask[h - 1 - UVs_coords_int[:, 1], UVs_coords_int[:,
-                                                                      0]] = 1
-
-        # blur mask
-        self.fusion_mask = self.fusion_mask.cpu().numpy()
-        new_kernel1 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
-        new_kernel2 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (8, 8))
-        self.fusion_mask = cv2.dilate(self.fusion_mask, new_kernel1, 1)
-        self.fusion_mask = cv2.erode(self.fusion_mask, new_kernel2, 1)
-        self.fusion_mask = cv2.blur(self.fusion_mask, (17, 17))
-        self.fusion_mask = torch.from_numpy(self.fusion_mask).float().to(
-            self.device)
-
-    def get_edge_mask(self):
-
-        h, w = self.shape_offset_uv.shape[1:3]
-        self.edge_mask = torch.zeros((h, w)).to(self.device).float()
-        UVs_coords = self.nonlinear_UVs.clone()[self.edge_points_inds]
-        UVs_coords[:, 0] *= w
-        UVs_coords[:, 1] *= h
-        UVs_coords_int = torch.floor(UVs_coords)
-        UVs_coords_int = UVs_coords_int.long()
-
-        self.edge_mask[h - 1 - UVs_coords_int[:, 1], UVs_coords_int[:, 0]] = 1
-
-        # blur mask
-        self.edge_mask = self.edge_mask.cpu().numpy()
-        new_kernel1 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (8, 8))
-        self.edge_mask = cv2.dilate(self.edge_mask, new_kernel1, 1)
-        self.edge_mask = cv2.blur(self.edge_mask, (5, 5))
-        self.edge_mask = torch.from_numpy(self.edge_mask).float().to(
-            self.device)
-
-    def fitting_nonlinear(self, coeff, debug=False, n_iters=100, out_dir=None):
-        output_coeff = coeff.detach().clone()
-
-        output_coeff = self.facemodel_front.split_coeff(output_coeff)
-        output_coeff['id'].requires_grad = True
-        output_coeff['exp'].requires_grad = True
-        output_coeff['tex'].requires_grad = True
-        output_coeff['angle'].requires_grad = True
-        output_coeff['gamma'].requires_grad = True
-        output_coeff['trans'].requires_grad = True
-
-        self.shape_offset_uv = torch.zeros(
-            (1, 300, 300, 3),
-            dtype=torch.float32).to(self.device)  # (1, 180, 256, 3)
-        self.shape_offset_uv.requires_grad = True
-
-        self.texture_offset_uv = torch.zeros(
-            (1, 300, 300, 3),
-            dtype=torch.float32).to(self.device)  # (1, 180, 256, 3)
-        self.texture_offset_uv.requires_grad = True
-
-        value_list = [
-            self.shape_offset_uv, self.texture_offset_uv, output_coeff['id'],
-            output_coeff['exp'], output_coeff['tex'], output_coeff['angle'],
-            output_coeff['gamma'], output_coeff['trans']
+            self.net = getattr(models, model)(
+                num_classes=num_classes,
+                pretrained=False).eval().requires_grad_(False).to(device)
+            self.net.load_state_dict(
+                torch.load(DOWNLOAD_TO_CACHE(checkpoint), map_location=device))
+        self.softmax = (lambda x, dim: x) if model_with_softmax else F.softmax
+
+        # data transforms
+        self.transforms = T.Compose([
+            data.PadToSquare(),
+            T.Resize(resolution),
+            T.ToTensor(),
+            T.Normalize(mean, std)
+        ])
+
+    def __call__(self, imgs, num_workers=0):
+        # preprocess
+        if isinstance(imgs, Image.Image):
+            imgs = [imgs]
+        assert isinstance(imgs,
+                          (tuple, list)) and isinstance(imgs[0], Image.Image)
+        sizes = [u.size for u in imgs]
+        imgs = torch.stack(parallel(self.transforms, imgs, num_workers), dim=0)
+
+        # forward
+        masks = []
+        for batch in imgs.split(self.batch_size, dim=0):
+            batch = batch.to(self.device, non_blocking=True)
+            masks.append(self.softmax(self.net(batch), dim=1))
+
+        # postprocess
+        masks = torch.cat(masks, dim=0).unsqueeze(1)
+        masks = [
+            F.interpolate(u, v, mode='bilinear', align_corners=False)
+            for u, v in zip(masks, sizes)
         ]
-        optim = torch.optim.Adam(value_list, lr=1e-3)
+        return masks
 
-        self.get_edge_points_horizontal()
-        self.get_edge_points_vertical()
 
-        self.cur_iter = 0
-        for i in range(n_iters):  # 500
-            self.pred_vertex, _, self.pred_color, self.pred_lm, _, face_shape_offset, self.verts_proj = \
-                self.facemodel_front.compute_for_render_train_nonlinear(output_coeff, self.shape_offset_uv,
-                                                                        self.texture_offset_uv,
-                                                                        self.nonlinear_UVs[:35709, ...])
-            self.pred_mask, _, self.pred_face, self.occ = self.renderer_fitting(
-                self.pred_vertex,
-                self.facemodel_front.face_buf,
-                feat=self.pred_color)
-
-            self.pred_coeffs_dict = self.facemodel_front.split_coeff(
-                output_coeff)
-            self.compute_losses_fitting()
-            if debug and i % 10 == 0:
-                print('{}: total loss: {:.6f}'.format(i, self.loss_all.item()))
-
-            optim.zero_grad()
-            self.loss_all.backward()
-            optim.step()
-
-            self.cur_iter += 1
-
-        output_coeff = self.facemodel_front.merge_coeff(output_coeff)
-
-        self.get_edge_mask()
-        self.get_fusion_mask()
-        self.blur_shape_offset_uv()
-
-        self.pred_vertex, _, self.pred_color, self.pred_lm, _, face_shape_offset, self.verts_proj = \
-            self.facemodel_front.compute_for_render_train_nonlinear(output_coeff, self.shape_offset_uv,
-                                                                    self.texture_offset_uv,
-                                                                    self.nonlinear_UVs[:35709, ...])
-
-        if out_dir is not None:
-            input_img_numpy = 255. * (self.input_img).detach().cpu().permute(
-                0, 2, 3, 1).numpy()
-            input_img_numpy = np.squeeze(input_img_numpy)
-
-            output_vis = self.pred_face
-            output_vis_numpy_raw = 255. * output_vis.detach().cpu().permute(
-                0, 2, 3, 1).numpy()
-            output_vis_numpy_raw = np.squeeze(output_vis_numpy_raw)
-
-            output_vis_numpy = np.concatenate(
-                (input_img_numpy, output_vis_numpy_raw), axis=-2)
-
-            output_vis = np.squeeze(output_vis_numpy)
-            output_vis = output_vis[..., ::-1]  # rgb->bgr
-            output_face_mask = self.pred_mask.detach().cpu().permute(
-                0, 2, 3, 1).squeeze().numpy() * 255.0
-            output_vis = np.column_stack(
-                (output_vis, cv2.cvtColor(output_face_mask,
-                                          cv2.COLOR_GRAY2BGR)))
-            output_input_vis = output_vis[:, :224]
-            output_pred_vis = output_vis[:, 224:448]
-            output_mask_vis = output_vis[:, 448:]
-
-            face_mask_vis = 255. * self.face_mask.detach().cpu()[0, 0].numpy()
-
-            shape_offset_vis = self.shape_offset_uv.detach().cpu().numpy()[0]
-            shape_offset_vis = (shape_offset_vis - shape_offset_vis.min()) / (
-                shape_offset_vis.max() - shape_offset_vis.min()) * 255.0
-
-            cv2.imwrite(
-                os.path.join(out_dir, 'fitting_01_input.jpg'),
-                output_input_vis)
-            cv2.imwrite(
-                os.path.join(out_dir, 'fitting_02_pred.jpg'), output_pred_vis)
-            cv2.imwrite(
-                os.path.join(out_dir, 'fitting_03_mask.jpg'), output_mask_vis)
-            cv2.imwrite(
-                os.path.join(out_dir, 'fitting_04_facemask.jpg'),
-                face_mask_vis)
-            cv2.imwrite(
-                os.path.join(out_dir, 'fitting_05_shape_offset.jpg'),
-                shape_offset_vis)
-
-        recon_shape_offset = face_shape_offset
-        recon_shape_offset[..., -1] = 10 - recon_shape_offset[
-            ..., -1]  # from camera space to world space
-        recon_shape_offset = recon_shape_offset.detach().cpu().numpy()[0]
-
-        tri = self.facemodel_front.face_buf.cpu().numpy()
-        pred_color = self.pred_color.detach().cpu().numpy()[0].clip(0, 1)
-
-        output = {
-            'coeffs': output_coeff,
-            'face_vertices': recon_shape_offset,
-            'face_faces': tri + 1,
-            'face_colors': pred_color
-        }
-        return output
-
-    def forward(self, out_dir=None):
-        self.facemodel.to(self.device)
-        self.facemodel_front.to(self.device)
-        with torch.no_grad():
-
-            output_coeff = self.net_recon(self.input_img)
-
-        with torch.enable_grad():
-            output = self.fitting_nonlinear(
-                output_coeff, debug=True, out_dir=out_dir)
-
-        output_coeff = output['coeffs']
-        output_coeff = self.facemodel.split_coeff(output_coeff)
-        eye_coeffs = output_coeff['exp'][0, 16] + output_coeff['exp'][
-            0, 17] + output_coeff['exp'][0, 19]
-        if eye_coeffs > 1.0:
-            degree = 0.5
-        else:
-            degree = 1.0
-        output_coeff['exp'][0, 16] += 1 * degree
-        output_coeff['exp'][0, 17] += 1 * degree
-        output_coeff['exp'][0, 19] += 1.5 * degree
-        output_coeff = self.facemodel.merge_coeff(output_coeff)
-
-        self.pred_vertex, face_shape_ori, head_shape = \
-            self.facemodel.compute_for_render_nonlinear_full(output_coeff, self.shape_offset_uv.detach(),
-                                                             self.nonlinear_UVs, nose_coeff=0.1)
-
-        UVs_tensor = torch.tensor(
-            self.template_mesh['uvs'],
-            dtype=torch.float32)[None, ...].to(self.pred_vertex.device)
-        target_img = self.input_fat_img_hd.permute(0, 2, 3, 1)
-        with torch.enable_grad():
-            _, _, _, texture_map, _ = self.renderer.pred_shape_and_texture(
-                self.pred_vertex, self.facemodel.face_buf, UVs_tensor,
-                target_img)
-
-        recon_shape = head_shape
-        recon_shape[
-            ...,
-            -1] = 10 - recon_shape[..., -1]  # from camera space to world space
-        recon_shape = recon_shape.cpu().numpy()[0]
-        tri = self.facemodel.face_buf.cpu().numpy()
-        normals = utils.estimate_normals(recon_shape, tri)
-
-        output['head_vertices'] = recon_shape
-        output['head_faces'] = tri + 1
-        output['head_tex_map'] = texture_map
-        output['head_UVs'] = self.template_mesh['uvs']
-        output['head_faces_uv'] = self.template_mesh['faces_uv']
-        output['head_normals'] = normals
-
-        return output
-
-    def compute_losses_fitting(self):
-        face_mask = self.pred_mask
-
-        face_mask = face_mask.detach()
-        self.loss_color = self.w_color * self.comupte_color_loss(
-            self.pred_face, self.input_img, face_mask)  # 1.0
-
-        self.loss_color_nose = torch.tensor(0.0).float().to(self.device)
-
-        loss_reg, loss_gamma = self.compute_reg_loss(self.pred_coeffs_dict,
-                                                     self.w_id, self.w_exp,
-                                                     self.w_tex)
-        self.loss_reg = self.w_reg * loss_reg  # 1.0
-        self.loss_gamma = self.w_gamma * loss_gamma  # 1.0
-
-        self.loss_lm = self.w_lm * self.compute_lm_loss(
-            self.pred_lm, self.gt_lm) * 0.1  # 0.1
-
-        self.loss_smooth_offset = TVLoss()(self.shape_offset_uv.permute(
-            0, 3, 1, 2)) * 10000  # 10000
-
-        self.loss_reg_offset = torch.tensor(0.0).float().to(self.device)
-
-        self.loss_reg_textureOff = torch.mean(
-            torch.abs(self.texture_offset_uv)) * 10  # 10
-
-        self.loss_smooth_offset_std = TVLoss_std()(
-            self.shape_offset_uv.permute(0, 3, 1, 2)) * 50000  # 50000
-
-        self.loss_points_horizontal, self.edge_points_inds = points_loss_horizontal(
-            self.verts_proj, self.left_points, self.right_points)  # 20
-        self.loss_points_horizontal *= 20
-        self.loss_points_horizontal_jaw = torch.tensor(0.0).float().to(
-            self.device)
-        self.loss_points_vertical = torch.tensor(0.0).float().to(self.device)
-        self.loss_normals = torch.tensor(0.0).float().to(self.device)
-
-        self.loss_all = self.loss_color + self.loss_lm + self.loss_reg + self.loss_gamma + self.loss_smooth_offset
-        self.loss_all += self.loss_reg_offset + self.loss_smooth_offset_std + self.loss_points_horizontal
-        self.loss_all += self.loss_points_vertical + self.loss_reg_textureOff
-        self.loss_all += self.loss_color_nose + self.loss_normals + self.loss_points_horizontal_jaw
+class TextImageMatch(object):
 
-        self.loss_mask = torch.tensor(0.0).float().to(self.device)
+    def __init__(
+        self,
+        embed_dim=512,
+        image_size=224,
+        patch_size=32,
+        vision_dim=768,
+        vision_heads=12,
+        vision_layers=12,
+        vocab_size=21128,
+        text_len=77,
+        text_dim=512,
+        text_heads=8,
+        text_layers=12,
+        mean=[0.48145466, 0.4578275, 0.40821073],
+        std=[0.26862954, 0.26130258, 0.27577711],
+        checkpoint='models/clip/clip_shoes+apparels_step84k_210105.pth',
+        tokenizer=data.BertTokenizer(name='bert-base-chinese', length=77),
+        batch_size=64,
+        device=torch.device(
+            'cuda' if torch.cuda.is_available() else 'cpu')):  # noqa E125
+        self.tokenizer = tokenizer
+        self.batch_size = batch_size
+        self.device = device
+
+        # init model
+        self.clip = models.CLIP(
+            embed_dim=embed_dim,
+            image_size=image_size,
+            patch_size=patch_size,
+            vision_dim=vision_dim,
+            vision_heads=vision_heads,
+            vision_layers=vision_layers,
+            vocab_size=vocab_size,
+            text_len=text_len,
+            text_dim=text_dim,
+            text_heads=text_heads,
+            text_layers=text_layers).eval().requires_grad_(False).to(device)
+        self.clip.load_state_dict(
+            torch.load(DOWNLOAD_TO_CACHE(checkpoint), map_location=device))
+
+        # transforms
+        scale_size = int(image_size * 8 / 7)
+        self.transforms = T.Compose([
+            data.PadToSquare(),
+            T.Resize(scale_size),
+            T.CenterCrop(image_size),
+            T.ToTensor(),
+            T.Normalize(mean, std)
+        ])
+
+    def __call__(self, imgs, txts, num_workers=0):
+        # preprocess
+        assert isinstance(imgs,
+                          (tuple, list)) and isinstance(imgs[0], Image.Image)
+        assert isinstance(txts, (tuple, list)) and isinstance(txts[0], str)
+        txt_tokens = torch.LongTensor([self.tokenizer(u) for u in txts])
+        imgs = torch.stack(parallel(self.transforms, imgs, num_workers), dim=0)
+
+        # forward
+        scores = []
+        for img_batch, txt_batch in zip(
+                imgs.split(self.batch_size, dim=0),
+                txt_tokens.split(self.batch_size, dim=0)):
+            img_batch = img_batch.to(self.device)
+            txt_batch = txt_batch.to(self.device)
+            xi = F.normalize(self.clip.visual(img_batch), p=2, dim=1)
+            xt = F.normalize(self.clip.textual(txt_batch), p=2, dim=1)
+            scores.append((xi * xt).sum(dim=1))
+        return torch.cat(scores, dim=0)
+
+
+def taobao_feature_extractor(category='shoes', **kwargs):
+    r"""Pretrained taobao-search feature extractors.
+    """
+    assert category in ['softall', 'shoes', 'bag']
+    checkpoint = osp.join(
+        'models/inception-v1', {
+            'softall': '1214softall_10.10.0.5000',
+            'shoes': '1218shoes.v9_7.140.0.1520000',
+            'bag': '0926bag.v9_6.29.0.140000'
+        }[category])
+    app = FeatureExtractor(
+        model='InceptionV1',
+        checkpoint=checkpoint,
+        resolution=224,
+        mean=[0.485, 0.456, 0.406],
+        std=[0.229, 0.224, 0.225],
+        **kwargs)
+    return app
+
+
+def singleton_classifier(**kwargs):
+    r"""Pretrained classifier that finds single-object images.
+        Supports shoes, apparel, and bag images.
+    """
+    app = Classifier(
+        model='InceptionV1',
+        checkpoint='models/classifier/shoes+apparel+bag-sgdetect-211230.pth',
+        num_classes=1,
+        resolution=224,
+        mean=[0.485, 0.456, 0.406],
+        std=[0.229, 0.224, 0.225],
+        **kwargs)
+    return app
+
+
+def orientation_classifier(**kwargs):
+    r"""Shoes orientation classifier.
+    """
+    app = Classifier(
+        model='InceptionV1',
+        checkpoint='models/classifier/shoes-oriendetect-20211026.pth',
+        num_classes=1,
+        resolution=224,
+        mean=[0.485, 0.456, 0.406],
+        std=[0.229, 0.224, 0.225],
+        **kwargs)
+    return app
+
+
+def fashion_text2image(**kwargs):
+    r"""Fashion text-to-image generator.
+        Supports shoe and apparel image generation.
+    """
+    app = Text2Image(
+        vqgan_dim=128,
+        vqgan_z_dim=256,
+        vqgan_dim_mult=[1, 1, 2, 2, 4],
+        vqgan_num_res_blocks=2,
+        vqgan_attn_scales=[1.0 / 16],
+        vqgan_codebook_size=975,
+        vqgan_beta=0.25,
+        gpt_txt_vocab_size=21128,
+        gpt_txt_seq_len=64,
+        gpt_img_seq_len=1024,
+        gpt_dim=1024,
+        gpt_num_heads=16,
+        gpt_num_layers=24,
+        vqgan_checkpoint=  # noqa E251
+        'models/vqgan/vqgan_shoes+apparels_step10k_vocab975.pth',
+        gpt_checkpoint=  # noqa E251
+        'models/seq2seq_gpt/text2image_shoes+apparels_step400k.pth',
+        tokenizer=data.BertTokenizer(name='bert-base-chinese', length=64),
+        **kwargs)
+    return app
+
+
+def mindalle_text2image(**kwargs):
+    r"""Pretrained text2image generator with weights copied from minDALL-E.
+    """
+    app = Text2Image(
+        vqgan_dim=128,
+        vqgan_z_dim=256,
+        vqgan_dim_mult=[1, 1, 2, 2, 4],
+        vqgan_num_res_blocks=2,
+        vqgan_attn_scales=[1.0 / 16],
+        vqgan_codebook_size=16384,
+        vqgan_beta=0.25,
+        gpt_txt_vocab_size=16384,
+        gpt_txt_seq_len=64,
+        gpt_img_seq_len=256,
+        gpt_dim=1536,
+        gpt_num_heads=24,
+        gpt_num_layers=42,
+        vqgan_checkpoint='models/minDALLE/1.3B_vqgan.pth',
+        gpt_checkpoint='models/minDALLE/1.3B_gpt.pth',
+        tokenizer=data.BPETokenizer(length=64),
+        **kwargs)
+    return app
+
+
+def sole2shoe(**kwargs):
+    app = Sole2Shoe(
+        vqgan_dim=128,
+        vqgan_z_dim=256,
+        vqgan_dim_mult=[1, 1, 2, 2, 4],
+        vqgan_num_res_blocks=2,
+        vqgan_attn_scales=[1.0 / 16],
+        vqgan_codebook_size=975,
+        vqgan_beta=0.25,
+        src_resolution=256,
+        tar_resolution=512,
+        gpt_dim=1024,
+        gpt_num_heads=16,
+        gpt_num_layers=24,
+        vqgan_checkpoint=  # noqa E251
+        'models/vqgan/vqgan_shoes+apparels_step10k_vocab975.pth',
+        gpt_checkpoint='models/seq2seq_gpt/sole2shoe-step300k-220104.pth',
+        **kwargs)
+    return app
+
+
+def sole_parser(**kwargs):
+    app = ImageParser(
+        model='SPNet',
+        num_classes=2,
+        resolution=800,
+        mean=[0.485, 0.456, 0.406],
+        std=[0.229, 0.224, 0.225],
+        model_with_softmax=False,
+        checkpoint='models/spnet/sole_segmentation_211219.pth',
+        **kwargs)
+    return app
+
+
+def sod_foreground_parser(**kwargs):
+    app = ImageParser(
+        model=None,
+        num_classes=None,
+        resolution=448,
+        mean=[0.488431, 0.466275, 0.403686],
+        std=[0.222627, 0.21949, 0.22549],
+        model_with_softmax=True,
+        checkpoint='models/semseg/sod_model_20201228.pt',
+        **kwargs)
+    return app
+
+
+def fashion_text_image_match(**kwargs):
+    app = TextImageMatch(
+        embed_dim=512,
+        image_size=224,
+        patch_size=32,
+        vision_dim=768,
+        vision_heads=12,
+        vision_layers=12,
+        vocab_size=21128,
+        text_len=77,
+        text_dim=512,
+        text_heads=8,
+        text_layers=12,
+        mean=[0.48145466, 0.4578275, 0.40821073],
+        std=[0.26862954, 0.26130258, 0.27577711],
+        checkpoint='models/clip/clip_shoes+apparels_step84k_210105.pth',
+        tokenizer=data.BertTokenizer(name='bert-base-chinese', length=77),
+        **kwargs)
+    return app
```

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/losses.py` & `modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/losses.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,67 +1,21 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
-import clip
 import numpy as np
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from kornia.geometry import warp_affine
 
 
 def resize_n_crop(image, M, dsize=112):
     # image: (b, c, h, w)
     # M   :  (b, 2, 3)
     return warp_affine(image, M, dsize=(dsize, dsize))
 
 
-class CLIPLoss(torch.nn.Module):
-
-    def __init__(self):
-        super(CLIPLoss, self).__init__()
-        self.model, self.preprocess = clip.load('ViT-B/32', device='cuda')
-
-    def forward(self, image, text):
-        similarity = 1 - self.model(image, text)[0] / 100
-        return similarity
-
-
-class CLIPLoss_relative(torch.nn.Module):
-
-    def __init__(self):
-        super(CLIPLoss_relative, self).__init__()
-        self.model, self.preprocess = clip.load('ViT-B/32', device='cuda')
-
-    def forward(self, image, text, image_ori, text_ori):
-
-        image_features = self.model.encode_image(image)
-        text_features = self.model.encode_text(text)
-
-        # normalized features
-        image_features = image_features / image_features.norm(
-            dim=1, keepdim=True)
-        text_features = text_features / text_features.norm(dim=1, keepdim=True)
-
-        image_features_ori = self.model.encode_image(image_ori)
-        text_features_ori = self.model.encode_text(text_ori)
-
-        # normalized features
-        image_features_ori = image_features_ori / image_features_ori.norm(
-            dim=1, keepdim=True)
-        text_features_ori = text_features_ori / text_features_ori.norm(
-            dim=1, keepdim=True)
-
-        delta_image = image_features - image_features_ori
-        delta_text = text_features - text_features_ori
-
-        loss = 1 - torch.sum(delta_image * delta_text) / (
-            torch.norm(delta_image) * torch.norm(delta_text))
-
-        return loss
-
-
 # perceptual level loss
 class PerceptualLoss(nn.Module):
 
     def __init__(self, recog_net, input_size=112):
         super(PerceptualLoss, self).__init__()
         self.recog_net = recog_net
         self.preprocess = lambda x: 2 * x - 1
```

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/networks.py` & `modelscope-1.5.0/modelscope/models/cv/face_reconstruction/models/networks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_reconstruction/models/nv_diffrast.py` & `modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/interp_model/refinenet_arch.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,400 +1,488 @@
-# Part of the implementation is borrowed and modified from Deep3DFaceRecon_pytorch,
-# publicly available at https://github.com/sicxu/Deep3DFaceRecon_pytorch
-import warnings
-from typing import List
+# Part of the implementation is borrowed and modified from QVI, publicly available at https://github.com/xuxy09/QVI
 
 import numpy as np
-import nvdiffrast.torch as dr
 import torch
+import torch.nn as nn
 import torch.nn.functional as F
-from torch import nn
 
-from .losses import TVLoss, TVLoss_std
+from modelscope.models.cv.video_frame_interpolation.interp_model.flow_reversal import \
+    FlowReversal
+from modelscope.models.cv.video_frame_interpolation.interp_model.IFNet_swin import \
+    IFNet
+from modelscope.models.cv.video_frame_interpolation.interp_model.UNet import \
+    Small_UNet_Ds
+
+
+class AcFusionLayer(nn.Module):
+
+    def __init__(self, ):
+        super(AcFusionLayer, self).__init__()
+
+    def forward(self, flo10, flo12, flo21, flo23, t=0.5):
+        return 0.5 * ((t + t**2) * flo12 - (t - t**2) * flo10), \
+            0.5 * (((1 - t) + (1 - t)**2) * flo21 - ((1 - t) - (1 - t)**2) * flo23)
+        # return 0.375 * flo12 - 0.125 * flo10, 0.375 * flo21 - 0.125 * flo23
+
+
+class Get_gradient(nn.Module):
+
+    def __init__(self):
+        super(Get_gradient, self).__init__()
+        kernel_v = [[0, -1, 0], [0, 0, 0], [0, 1, 0]]
+        kernel_h = [[0, 0, 0], [-1, 0, 1], [0, 0, 0]]
+        kernel_h = torch.FloatTensor(kernel_h).unsqueeze(0).unsqueeze(0)
+        kernel_v = torch.FloatTensor(kernel_v).unsqueeze(0).unsqueeze(0)
+        self.weight_h = nn.Parameter(data=kernel_h, requires_grad=False)
+        self.weight_v = nn.Parameter(data=kernel_v, requires_grad=False)
+
+    def forward(self, x):
+        x0 = x[:, 0]  # R
+        x1 = x[:, 1]  # G
+        x2 = x[:, 2]  # B
+        x0_v = F.conv2d(x0.unsqueeze(1), self.weight_v, padding=1)
+        x0_h = F.conv2d(x0.unsqueeze(1), self.weight_h, padding=1)
+
+        x1_v = F.conv2d(x1.unsqueeze(1), self.weight_v, padding=1)
+        x1_h = F.conv2d(x1.unsqueeze(1), self.weight_h, padding=1)
+
+        x2_v = F.conv2d(x2.unsqueeze(1), self.weight_v, padding=1)
+        x2_h = F.conv2d(x2.unsqueeze(1), self.weight_h, padding=1)
+
+        x0 = torch.sqrt(torch.pow(x0_v, 2) + torch.pow(x0_h, 2) + 1e-6)
+        x1 = torch.sqrt(torch.pow(x1_v, 2) + torch.pow(x1_h, 2) + 1e-6)
+        x2 = torch.sqrt(torch.pow(x2_v, 2) + torch.pow(x2_h, 2) + 1e-6)
+
+        x = torch.cat([x0, x1, x2], dim=1)
+        return x
+
+
+class LowPassFilter(nn.Module):
+
+    def __init__(self):
+        super(LowPassFilter, self).__init__()
+        kernel_lpf = [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1],
+                      [1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1],
+                      [1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1],
+                      [1, 1, 1, 1, 1, 1, 1]]
+
+        kernel_lpf = torch.FloatTensor(kernel_lpf).unsqueeze(0).unsqueeze(
+            0) / 49
+
+        self.weight_lpf = nn.Parameter(data=kernel_lpf, requires_grad=False)
+
+    def forward(self, x):
+        x0 = x[:, 0]
+        x1 = x[:, 1]
+        y0 = F.conv2d(x0.unsqueeze(1), self.weight_lpf, padding=3)
+        y1 = F.conv2d(x1.unsqueeze(1), self.weight_lpf, padding=3)
+
+        y = torch.cat([y0, y1], dim=1)
+
+        return y
+
+
+def backwarp(img, flow):
+    _, _, H, W = img.size()
+
+    u = flow[:, 0, :, :]
+    v = flow[:, 1, :, :]
+
+    gridX, gridY = np.meshgrid(np.arange(W), np.arange(H))
+
+    gridX = torch.tensor(
+        gridX,
+        requires_grad=False,
+    ).cuda()
+    gridY = torch.tensor(
+        gridY,
+        requires_grad=False,
+    ).cuda()
+    x = gridX.unsqueeze(0).expand_as(u).float() + u
+    y = gridY.unsqueeze(0).expand_as(v).float() + v
+
+    x = 2 * (x / (W - 1) - 0.5)
+    y = 2 * (y / (H - 1) - 0.5)
+
+    grid = torch.stack((x, y), dim=3)
+
+    imgOut = torch.nn.functional.grid_sample(
+        img, grid, padding_mode='border', align_corners=True)
+
+    return imgOut
+
+
+class SmallMaskNet(nn.Module):
+    """A three-layer network for predicting mask"""
+
+    def __init__(self, input, output):
+        super(SmallMaskNet, self).__init__()
+        self.conv1 = nn.Conv2d(input, 32, 5, padding=2)
+        self.conv2 = nn.Conv2d(32, 16, 3, padding=1)
+        self.conv3 = nn.Conv2d(16, output, 3, padding=1)
+
+    def forward(self, x):
+        x = F.leaky_relu(self.conv1(x), negative_slope=0.1)
+        x = F.leaky_relu(self.conv2(x), negative_slope=0.1)
+        x = self.conv3(x)
+        return x
+
+
+class StaticMaskNet(nn.Module):
+    """static mask"""
+
+    def __init__(self, input, output):
+        super(StaticMaskNet, self).__init__()
+
+        modules_body = []
+        modules_body.append(
+            nn.Conv2d(
+                in_channels=input,
+                out_channels=32,
+                kernel_size=3,
+                stride=1,
+                padding=1))
+        modules_body.append(nn.LeakyReLU(inplace=False, negative_slope=0.1))
+        modules_body.append(
+            nn.Conv2d(
+                in_channels=32,
+                out_channels=32,
+                kernel_size=3,
+                stride=1,
+                padding=1))
+        modules_body.append(nn.LeakyReLU(inplace=False, negative_slope=0.1))
+        modules_body.append(
+            nn.Conv2d(
+                in_channels=32,
+                out_channels=16,
+                kernel_size=3,
+                stride=1,
+                padding=1))
+        modules_body.append(nn.LeakyReLU(inplace=False, negative_slope=0.1))
+        modules_body.append(
+            nn.Conv2d(
+                in_channels=16,
+                out_channels=16,
+                kernel_size=3,
+                stride=1,
+                padding=1))
+        modules_body.append(nn.LeakyReLU(inplace=False, negative_slope=0.1))
+        modules_body.append(
+            nn.Conv2d(
+                in_channels=16,
+                out_channels=output,
+                kernel_size=3,
+                stride=1,
+                padding=1))
+        modules_body.append(nn.Sigmoid())
+
+        self.body = nn.Sequential(*modules_body)
+
+    def forward(self, x):
+        y = self.body(x)
+        return y
+
+
+def tensor_erode(bin_img, ksize=5):
+    B, C, H, W = bin_img.shape
+    pad = (ksize - 1) // 2
+    bin_img = F.pad(bin_img, [pad, pad, pad, pad], mode='constant', value=0)
+
+    patches = bin_img.unfold(dimension=2, size=ksize, step=1)
+    patches = patches.unfold(dimension=3, size=ksize, step=1)
+
+    eroded, _ = patches.reshape(B, C, H, W, -1).max(dim=-1)
+    return eroded
+
+
+class QVI_inter_Ds(nn.Module):
+    """Given flow, implement Quadratic Video Interpolation"""
+
+    def __init__(self, debug_en=False, is_training=False):
+        super(QVI_inter_Ds, self).__init__()
+        self.acc = AcFusionLayer()
+        self.fwarp = FlowReversal()
+        self.refinenet = Small_UNet_Ds(20, 8)
+        self.masknet = SmallMaskNet(38, 1)
+
+        self.staticnet = StaticMaskNet(56, 1)
+        self.lpfilter = LowPassFilter()
+
+        self.get_grad = Get_gradient()
+        self.debug_en = debug_en
+        self.is_training = is_training
+
+    def fill_flow_hole(self, ft, norm, ft_fill):
+        (N, C, H, W) = ft.shape
+        ft[norm == 0] = ft_fill[norm == 0]
+
+        ft_1 = self.lpfilter(ft.clone())
+        ft_ds = torch.nn.functional.interpolate(
+            input=ft_1,
+            size=(H // 4, W // 4),
+            mode='bilinear',
+            align_corners=False)
+        ft_up = torch.nn.functional.interpolate(
+            input=ft_ds, size=(H, W), mode='bilinear', align_corners=False)
+
+        ft[norm == 0] = ft_up[norm == 0]
+
+        return ft
+
+    def forward(self, F10_Ds, F12_Ds, F21_Ds, F23_Ds, I1_Ds, I2_Ds, I1, I2, t):
+        if F12_Ds is None or F21_Ds is None:
+            return I1
+
+        if F10_Ds is not None and F23_Ds is not None:
+            F1t_Ds, F2t_Ds = self.acc(F10_Ds, F12_Ds, F21_Ds, F23_Ds, t)
+
+        else:
+            F1t_Ds = t * F12_Ds
+            F2t_Ds = (1 - t) * F21_Ds
+
+        # Flow Reversal
+        F1t_Ds2 = F.interpolate(
+            F1t_Ds, scale_factor=1.0 / 3, mode='nearest') / 3
+        F2t_Ds2 = F.interpolate(
+            F2t_Ds, scale_factor=1.0 / 3, mode='nearest') / 3
+        Ft1_Ds2, norm1_Ds2 = self.fwarp(F1t_Ds2, F1t_Ds2)
+        Ft1_Ds2 = -Ft1_Ds2
+        Ft2_Ds2, norm2_Ds2 = self.fwarp(F2t_Ds2, F2t_Ds2)
+        Ft2_Ds2 = -Ft2_Ds2
+
+        Ft1_Ds2[norm1_Ds2 > 0] \
+            = Ft1_Ds2[norm1_Ds2 > 0] / norm1_Ds2[norm1_Ds2 > 0].clone()
+        Ft2_Ds2[norm2_Ds2 > 0] \
+            = Ft2_Ds2[norm2_Ds2 > 0] / norm2_Ds2[norm2_Ds2 > 0].clone()
+        if 1:
+            Ft1_Ds2_fill = -F1t_Ds2
+            Ft2_Ds2_fill = -F2t_Ds2
+            Ft1_Ds2 = self.fill_flow_hole(Ft1_Ds2, norm1_Ds2, Ft1_Ds2_fill)
+            Ft2_Ds2 = self.fill_flow_hole(Ft2_Ds2, norm2_Ds2, Ft2_Ds2_fill)
+
+        Ft1_Ds = F.interpolate(
+            Ft1_Ds2, size=[F1t_Ds.size(2), F1t_Ds.size(3)], mode='nearest') * 3
+        Ft2_Ds = F.interpolate(
+            Ft2_Ds2, size=[F2t_Ds.size(2), F2t_Ds.size(3)], mode='nearest') * 3
+
+        I1t_Ds = backwarp(I1_Ds, Ft1_Ds)
+        I2t_Ds = backwarp(I2_Ds, Ft2_Ds)
+
+        output_Ds, feature_Ds = self.refinenet(
+            torch.cat(
+                [I1_Ds, I2_Ds, I1t_Ds, I2t_Ds, F12_Ds, F21_Ds, Ft1_Ds, Ft2_Ds],
+                dim=1))
+
+        # Adaptive filtering
+        Ft1r_Ds = backwarp(
+            Ft1_Ds, 10 * torch.tanh(output_Ds[:, 4:6])) + output_Ds[:, :2]
+        Ft2r_Ds = backwarp(
+            Ft2_Ds, 10 * torch.tanh(output_Ds[:, 6:8])) + output_Ds[:, 2:4]
+
+        # warping and fusing
+        I1tf_Ds = backwarp(I1_Ds, Ft1r_Ds)
+        I2tf_Ds = backwarp(I2_Ds, Ft2r_Ds)
+
+        G1_Ds = self.get_grad(I1_Ds)
+        G2_Ds = self.get_grad(I2_Ds)
+        G1tf_Ds = backwarp(G1_Ds, Ft1r_Ds)
+        G2tf_Ds = backwarp(G2_Ds, Ft2r_Ds)
+
+        M_Ds = torch.sigmoid(
+            self.masknet(torch.cat([I1tf_Ds, I2tf_Ds, feature_Ds],
+                                   dim=1))).repeat(1, 3, 1, 1)
+
+        Ft1r = F.interpolate(
+            Ft1r_Ds * 2, scale_factor=2, mode='bilinear', align_corners=False)
+        Ft2r = F.interpolate(
+            Ft2r_Ds * 2, scale_factor=2, mode='bilinear', align_corners=False)
+
+        I1tf = backwarp(I1, Ft1r)
+        I2tf = backwarp(I2, Ft2r)
+
+        M = F.interpolate(
+            M_Ds, scale_factor=2, mode='bilinear', align_corners=False)
+
+        # fuse
+        It_warp = ((1 - t) * M * I1tf + t * (1 - M) * I2tf) \
+            / ((1 - t) * M + t * (1 - M)).clone()
+
+        # static blending
+        It_static = (1 - t) * I1 + t * I2
+        tmp = torch.cat((I1tf_Ds, I2tf_Ds, G1tf_Ds, G2tf_Ds, I1_Ds, I2_Ds,
+                         G1_Ds, G2_Ds, feature_Ds),
+                        dim=1)
+        M_static_Ds = self.staticnet(tmp)
+        M_static_dilate = tensor_erode(M_static_Ds)
+        M_static_dilate = tensor_erode(M_static_dilate)
+        M_static = F.interpolate(
+            M_static_dilate,
+            scale_factor=2,
+            mode='bilinear',
+            align_corners=False)
+
+        It_warp = (1 - M_static) * It_warp + M_static * It_static
+
+        if self.is_training:
+            return It_warp, Ft1r, Ft2r
+        else:
+            if self.debug_en:
+                return It_warp, M, M_static, I1tf, I2tf, Ft1r, Ft2r
+            else:
+                return It_warp
+
+
+class QVI_inter(nn.Module):
+    """Given flow, implement Quadratic Video Interpolation"""
+
+    def __init__(self, debug_en=False, is_training=False):
+        super(QVI_inter, self).__init__()
+        self.acc = AcFusionLayer()
+        self.fwarp = FlowReversal()
+        self.refinenet = Small_UNet_Ds(20, 8)
+        self.masknet = SmallMaskNet(38, 1)
+
+        self.staticnet = StaticMaskNet(56, 1)
+        self.lpfilter = LowPassFilter()
+
+        self.get_grad = Get_gradient()
+        self.debug_en = debug_en
+        self.is_training = is_training
+
+    def fill_flow_hole(self, ft, norm, ft_fill):
+        (N, C, H, W) = ft.shape
+        ft[norm == 0] = ft_fill[norm == 0]
+
+        ft_1 = self.lpfilter(ft.clone())
+        ft_ds = torch.nn.functional.interpolate(
+            input=ft_1,
+            size=(H // 4, W // 4),
+            mode='bilinear',
+            align_corners=False)
+        ft_up = torch.nn.functional.interpolate(
+            input=ft_ds, size=(H, W), mode='bilinear', align_corners=False)
+
+        ft[norm == 0] = ft_up[norm == 0]
+
+        return ft
+
+    def forward(self, F10, F12, F21, F23, I1, I2, t):
+        if F12 is None or F21 is None:
+            return I1
+
+        if F10 is not None and F23 is not None:
+            F1t, F2t = self.acc(F10, F12, F21, F23, t)
+
+        else:
+            F1t = t * F12
+            F2t = (1 - t) * F21
+
+        # Flow Reversal
+        F1t_Ds = F.interpolate(F1t, scale_factor=1.0 / 3, mode='nearest') / 3
+        F2t_Ds = F.interpolate(F2t, scale_factor=1.0 / 3, mode='nearest') / 3
+        Ft1_Ds, norm1_Ds = self.fwarp(F1t_Ds, F1t_Ds)
+        Ft1_Ds = -Ft1_Ds
+        Ft2_Ds, norm2_Ds = self.fwarp(F2t_Ds, F2t_Ds)
+        Ft2_Ds = -Ft2_Ds
+
+        Ft1_Ds[norm1_Ds > 0] \
+            = Ft1_Ds[norm1_Ds > 0] / norm1_Ds[norm1_Ds > 0].clone()
+        Ft2_Ds[norm2_Ds > 0] \
+            = Ft2_Ds[norm2_Ds > 0] / norm2_Ds[norm2_Ds > 0].clone()
+        if 1:
+            Ft1_fill = -F1t_Ds
+            Ft2_fill = -F2t_Ds
+            Ft1_Ds = self.fill_flow_hole(Ft1_Ds, norm1_Ds, Ft1_fill)
+            Ft2_Ds = self.fill_flow_hole(Ft2_Ds, norm2_Ds, Ft2_fill)
+
+        Ft1 = F.interpolate(
+            Ft1_Ds, size=[F1t.size(2), F1t.size(3)], mode='nearest') * 3
+        Ft2 = F.interpolate(
+            Ft2_Ds, size=[F2t.size(2), F2t.size(3)], mode='nearest') * 3
+
+        I1t = backwarp(I1, Ft1)
+        I2t = backwarp(I2, Ft2)
+
+        output, feature = self.refinenet(
+            torch.cat([I1, I2, I1t, I2t, F12, F21, Ft1, Ft2], dim=1))
+
+        # Adaptive filtering
+        Ft1r = backwarp(Ft1, 10 * torch.tanh(output[:, 4:6])) + output[:, :2]
+        Ft2r = backwarp(Ft2, 10 * torch.tanh(output[:, 6:8])) + output[:, 2:4]
+
+        # warping and fusing
+        I1tf = backwarp(I1, Ft1r)
+        I2tf = backwarp(I2, Ft2r)
+
+        M = torch.sigmoid(
+            self.masknet(torch.cat([I1tf, I2tf, feature],
+                                   dim=1))).repeat(1, 3, 1, 1)
+
+        It_warp = ((1 - t) * M * I1tf + t * (1 - M) * I2tf) \
+            / ((1 - t) * M + t * (1 - M)).clone()
+
+        G1 = self.get_grad(I1)
+        G2 = self.get_grad(I2)
+        G1tf = backwarp(G1, Ft1r)
+        G2tf = backwarp(G2, Ft2r)
+
+        # static blending
+        It_static = (1 - t) * I1 + t * I2
+        M_static = self.staticnet(
+            torch.cat([I1tf, I2tf, G1tf, G2tf, I1, I2, G1, G2, feature],
+                      dim=1))
+        M_static_dilate = tensor_erode(M_static)
+        M_static_dilate = tensor_erode(M_static_dilate)
+        It_warp = (1 - M_static_dilate) * It_warp + M_static_dilate * It_static
+
+        if self.is_training:
+            return It_warp, Ft1r, Ft2r
+        else:
+            if self.debug_en:
+                return It_warp, M, M_static, I1tf, I2tf, Ft1r, Ft2r
+            else:
+                return It_warp
+
+
+class InterpNetDs(nn.Module):
+
+    def __init__(self, debug_en=False, is_training=False):
+        super(InterpNetDs, self).__init__()
+        self.ifnet = IFNet()
+        self.internet = QVI_inter_Ds(
+            debug_en=debug_en, is_training=is_training)
+
+    def forward(self,
+                img1,
+                img2,
+                F10_up,
+                F12_up,
+                F21_up,
+                F23_up,
+                UHD=2,
+                timestep=0.5):
+        F12, F21 = self.ifnet(img1, img2, F12_up, F21_up, UHD)
+        It_warp = self.internet(F10_up, F12, F21, F23_up, img1, img2, timestep)
+
+        return It_warp
+
+
+class InterpNet(nn.Module):
+
+    def __init__(self, debug_en=False, is_training=False):
+        super(InterpNet, self).__init__()
+        self.ifnet = IFNet()
+        self.internet = QVI_inter(debug_en=debug_en, is_training=is_training)
+
+    def forward(self,
+                img1,
+                img2,
+                F10_up,
+                F12_up,
+                F21_up,
+                F23_up,
+                UHD=2,
+                timestep=0.5):
+        F12, F21 = self.ifnet(img1, img2, F12_up, F21_up, UHD)
+        It_warp = self.internet(F10_up, F12, F21, F23_up, img1, img2, timestep)
 
-warnings.filterwarnings('ignore')
-
-
-def ndc_projection(x=0.1, n=1.0, f=50.0):
-    return np.array([[n / x, 0, 0, 0], [0, n / -x, 0, 0],
-                     [0, 0, -(f + n) / (f - n), -(2 * f * n) / (f - n)],
-                     [0, 0, -1, 0]]).astype(np.float32)
-
-
-def to_image(face_shape):
-    """
-    Return:
-        face_proj        -- torch.tensor, size (B, N, 2), y direction is opposite to v direction
-
-    Parameters:
-        face_shape       -- torch.tensor, size (B, N, 3)
-    """
-
-    focal = 1015.
-    center = 112.
-    persc_proj = np.array([focal, 0, center, 0, focal, center, 0, 0,
-                           1]).reshape([3, 3]).astype(np.float32).transpose()
-
-    persc_proj = torch.tensor(persc_proj).to(face_shape.device)
-
-    face_proj = face_shape @ persc_proj
-    face_proj = face_proj[..., :2] / face_proj[..., 2:]
-
-    return face_proj
-
-
-class MeshRenderer(nn.Module):
-
-    def __init__(self, rasterize_fov, znear=0.1, zfar=10, rasterize_size=224):
-        super(MeshRenderer, self).__init__()
-
-        x = np.tan(np.deg2rad(rasterize_fov * 0.5)) * znear
-        self.ndc_proj = torch.tensor(ndc_projection(
-            x=x, n=znear,
-            f=zfar)).matmul(torch.diag(torch.tensor([1., -1, -1, 1])))
-        self.rasterize_size = rasterize_size
-        self.glctx = None
-
-    def forward(self, vertex, tri, feat=None):
-        """
-        Return:
-            mask               -- torch.tensor, size (B, 1, H, W)
-            depth              -- torch.tensor, size (B, 1, H, W)
-            features(optional) -- torch.tensor, size (B, C, H, W) if feat is not None
-
-        Parameters:
-            vertex          -- torch.tensor, size (B, N, 3)
-            tri             -- torch.tensor, size (B, M, 3) or (M, 3), triangles
-            feat(optional)  -- torch.tensor, size (B, C), features
-        """
-        device = vertex.device
-        rsize = int(self.rasterize_size)
-        ndc_proj = self.ndc_proj.to(device)
-        verts_proj = to_image(vertex)
-        # trans to homogeneous coordinates of 3d vertices, the direction of y is the same as v
-        if vertex.shape[-1] == 3:
-            vertex = torch.cat(
-                [vertex, torch.ones([*vertex.shape[:2], 1]).to(device)],
-                dim=-1)
-            vertex[..., 1] = -vertex[..., 1]
-
-        vertex_ndc = vertex @ ndc_proj.t()
-        if self.glctx is None:
-            self.glctx = dr.RasterizeCudaContext(device=device)
-
-        ranges = None
-        if isinstance(tri, List) or len(tri.shape) == 3:
-            vum = vertex_ndc.shape[1]
-            fnum = torch.tensor([f.shape[0]
-                                 for f in tri]).unsqueeze(1).to(device)
-
-            print('fnum shape:{}'.format(fnum.shape))
-
-            fstartidx = torch.cumsum(fnum, dim=0) - fnum
-            ranges = torch.cat([fstartidx, fnum],
-                               axis=1).type(torch.int32).cpu()
-            for i in range(tri.shape[0]):
-                tri[i] = tri[i] + i * vum
-            vertex_ndc = torch.cat(vertex_ndc, dim=0)
-            tri = torch.cat(tri, dim=0)
-
-        # for range_mode vetex: [B*N, 4], tri: [B*M, 3], for instance_mode vetex: [B, N, 4], tri: [M, 3]
-        tri = tri.type(torch.int32).contiguous()
-        rast_out, _ = dr.rasterize(
-            self.glctx,
-            vertex_ndc.contiguous(),
-            tri,
-            resolution=[rsize, rsize],
-            ranges=ranges)
-
-        depth, _ = dr.interpolate(
-            vertex.reshape([-1, 4])[..., 2].unsqueeze(1).contiguous(),
-            rast_out, tri)
-        depth = depth.permute(0, 3, 1, 2)
-        mask = (rast_out[..., 3] > 0).float().unsqueeze(1)
-        depth = mask * depth
-
-        image = None
-
-        verts_x = verts_proj[0, :, 0]
-        verts_y = 224 - verts_proj[0, :, 1]
-        verts_int = torch.ceil(verts_proj[0]).long()  # (n, 2)
-        verts_xr_int = verts_int[:, 0].clamp(1, 224 - 1)
-        verts_yt_int = 224 - verts_int[:, 1].clamp(2, 224)
-        verts_right_float = verts_xr_int - verts_x
-        verts_left_float = 1 - verts_right_float
-        verts_top_float = verts_y - verts_yt_int
-        verts_bottom_float = 1 - verts_top_float
-
-        rast_lt = rast_out[0, verts_yt_int, verts_xr_int - 1, 3]
-        rast_lb = rast_out[0, verts_yt_int + 1, verts_xr_int - 1, 3]
-        rast_rt = rast_out[0, verts_yt_int, verts_xr_int, 3]
-        rast_rb = rast_out[0, verts_yt_int + 1, verts_xr_int, 3]
-
-        occ_feat = (rast_lt > 0) * 1.0 * (verts_left_float + verts_top_float) + \
-                   (rast_lb > 0) * 1.0 * (verts_left_float + verts_bottom_float) + \
-                   (rast_rt > 0) * 1.0 * (verts_right_float + verts_top_float) + \
-                   (rast_rb > 0) * 1.0 * (verts_right_float + verts_bottom_float)
-        occ_feat = occ_feat[None, :, None] / 4.0
-
-        occ, _ = dr.interpolate(occ_feat, rast_out, tri)
-        occ = occ.permute(0, 3, 1, 2)
-
-        if feat is not None:
-            image, _ = dr.interpolate(feat, rast_out, tri)
-            image = image.permute(0, 3, 1, 2)
-            image = mask * image
-
-        return mask, depth, image, occ
-
-    def render_uv_texture(self, vertex, tri, uv, uv_texture):
-        """
-        Return:
-            mask               -- torch.tensor, size (B, 1, H, W)
-            depth              -- torch.tensor, size (B, 1, H, W)
-            features(optional) -- torch.tensor, size (B, C, H, W) if feat is not None
-
-        Parameters:
-            vertex          -- torch.tensor, size (B, N, 3)
-            tri             -- torch.tensor, size (M, 3), triangles
-            uv                -- torch.tensor, size (B,N, 2),  uv mapping
-            base_tex   -- torch.tensor, size (B,H,W,C)
-        """
-        device = vertex.device
-        rsize = int(self.rasterize_size)
-        ndc_proj = self.ndc_proj.to(device)
-        # trans to homogeneous coordinates of 3d vertices, the direction of y is the same as v
-        if vertex.shape[-1] == 3:
-            vertex = torch.cat(
-                [vertex, torch.ones([*vertex.shape[:2], 1]).to(device)],
-                dim=-1)
-            vertex[..., 1] = -vertex[..., 1]
-
-        vertex_ndc = vertex @ ndc_proj.t()
-        if self.glctx is None:
-            self.glctx = dr.RasterizeCudaContext(device=device)
-
-        ranges = None
-        if isinstance(tri, List) or len(tri.shape) == 3:
-            vum = vertex_ndc.shape[1]
-            fnum = torch.tensor([f.shape[0]
-                                 for f in tri]).unsqueeze(1).to(device)
-
-            print('fnum shape:{}'.format(fnum.shape))
-
-            fstartidx = torch.cumsum(fnum, dim=0) - fnum
-            ranges = torch.cat([fstartidx, fnum],
-                               axis=1).type(torch.int32).cpu()
-            for i in range(tri.shape[0]):
-                tri[i] = tri[i] + i * vum
-            vertex_ndc = torch.cat(vertex_ndc, dim=0)
-            tri = torch.cat(tri, dim=0)
-
-        # for range_mode vetex: [B*N, 4], tri: [B*M, 3], for instance_mode vetex: [B, N, 4], tri: [M, 3]
-        tri = tri.type(torch.int32).contiguous()
-        rast_out, _ = dr.rasterize(
-            self.glctx,
-            vertex_ndc.contiguous(),
-            tri,
-            resolution=[rsize, rsize],
-            ranges=ranges)
-
-        depth, _ = dr.interpolate(
-            vertex.reshape([-1, 4])[..., 2].unsqueeze(1).contiguous(),
-            rast_out, tri)
-        depth = depth.permute(0, 3, 1, 2)
-        mask = (rast_out[..., 3] > 0).float().unsqueeze(1)
-        depth = mask * depth
-        uv[..., -1] = 1.0 - uv[..., -1]
-
-        rast_out, rast_db = dr.rasterize(
-            self.glctx,
-            vertex_ndc.contiguous(),
-            tri,
-            resolution=[rsize, rsize],
-            ranges=ranges)
-
-        interp_out, uv_da = dr.interpolate(
-            uv, rast_out, tri, rast_db, diff_attrs='all')
-
-        uv_texture = uv_texture.permute(0, 2, 3, 1).contiguous()
-        img = dr.texture(
-            uv_texture, interp_out, filter_mode='linear')  # , uv_da)
-        img = img * torch.clamp(rast_out[..., -1:], 0,
-                                1)  # Mask out background.
-
-        tex_map = uv_texture[0].detach().cpu().numpy()[..., ::-1] * 255.0
-
-        image = img.permute(0, 3, 1, 2)
-
-        return mask, depth, image, tex_map
-
-    def pred_shape_and_texture(self,
-                               vertex,
-                               tri,
-                               uv,
-                               target_img,
-                               base_tex=None):
-        """
-        Return:
-            mask               -- torch.tensor, size (B, 1, H, W)
-            depth              -- torch.tensor, size (B, 1, H, W)
-            features(optional) -- torch.tensor, size (B, C, H, W) if feat is not None
-
-        Parameters:
-            vertex          -- torch.tensor, size (B, N, 3)
-            tri             -- torch.tensor, size (B, M, 3) or (M, 3), triangles
-            uv                -- torch.tensor, size (B,N, 2),  uv mapping
-            base_tex   -- torch.tensor, size (B,H,W,C)
-        """
-        vertex = torch.cat([vertex[:, :35241, :], vertex[:, 37082:, :]],
-                           dim=1)  # BFM front
-        tri = torch.cat([tri[:69732, :], tri[73936:, ]], dim=0)
-        uv = torch.cat([uv[:, :35241, :], uv[:, 37082:, :]], dim=1)
-        tri[69732:, :] = tri[69732:, :] - (37082 - 35241)
-
-        device = vertex.device
-        rsize = int(self.rasterize_size)
-        ndc_proj = self.ndc_proj.to(device)
-        # trans to homogeneous coordinates of 3d vertices, the direction of y is the same as v
-        if vertex.shape[-1] == 3:
-            vertex = torch.cat(
-                [vertex, torch.ones([*vertex.shape[:2], 1]).to(device)],
-                dim=-1)
-            vertex[..., 1] = -vertex[..., 1]
-
-        vertex_ndc = vertex @ ndc_proj.t()
-        if self.glctx is None:
-            self.glctx = dr.RasterizeCudaContext(device=device)
-
-        ranges = None
-        if isinstance(tri, List) or len(tri.shape) == 3:
-            vum = vertex_ndc.shape[1]
-            fnum = torch.tensor([f.shape[0]
-                                 for f in tri]).unsqueeze(1).to(device)
-
-            fstartidx = torch.cumsum(fnum, dim=0) - fnum
-            ranges = torch.cat([fstartidx, fnum],
-                               axis=1).type(torch.int32).cpu()
-            for i in range(tri.shape[0]):
-                tri[i] = tri[i] + i * vum
-            vertex_ndc = torch.cat(vertex_ndc, dim=0)
-            tri = torch.cat(tri, dim=0)
-
-        # for range_mode vetex: [B*N, 4], tri: [B*M, 3], for instance_mode vetex: [B, N, 4], tri: [M, 3]
-        tri = tri.type(torch.int32).contiguous()
-        rast_out, _ = dr.rasterize(
-            self.glctx,
-            vertex_ndc.contiguous(),
-            tri,
-            resolution=[rsize, rsize],
-            ranges=ranges)
-
-        depth, _ = dr.interpolate(
-            vertex.reshape([-1, 4])[..., 2].unsqueeze(1).contiguous(),
-            rast_out, tri)
-        depth = depth.permute(0, 3, 1, 2)
-        mask = (rast_out[..., 3] > 0).float().unsqueeze(1)
-        depth = mask * depth
-        uv[..., -1] = 1.0 - uv[..., -1]
-
-        rast_out, rast_db = dr.rasterize(
-            self.glctx,
-            vertex_ndc.contiguous(),
-            tri,
-            resolution=[rsize, rsize],
-            ranges=ranges)
-
-        interp_out, uv_da = dr.interpolate(
-            uv, rast_out, tri, rast_db, diff_attrs='all')
-
-        mask_3c = mask.permute(0, 2, 3, 1)
-        mask_3c = torch.cat((mask_3c, mask_3c, mask_3c), dim=-1)
-        maskout_img = mask_3c * target_img
-        mean_color = torch.sum(maskout_img, dim=(1, 2))
-        valid_pixel_count = torch.sum(mask)
-
-        mean_color = mean_color / valid_pixel_count
-
-        tex = torch.zeros((1, 128 * 5 // 4, 128, 3), dtype=torch.float32)
-        tex[:, :, :, 0] = mean_color[0, 0]
-        tex[:, :, :, 1] = mean_color[0, 1]
-        tex[:, :, :, 2] = mean_color[0, 2]
-
-        tex = tex.cuda()
-
-        tex_mask = torch.zeros((1, 2048 * 5 // 4, 2048, 3),
-                               dtype=torch.float32)
-        tex_mask[:, :, :, 1] = 1.0
-        tex_mask = tex_mask.cuda()
-        tex_mask.requires_grad = True
-        tex_mask = tex_mask.contiguous()
-
-        criterionTV = TVLoss()
-
-        if base_tex is not None:
-            base_tex = base_tex.cuda()
-
-        for tex_resolution in [64, 128, 256, 512, 1024, 2048]:
-            tex = tex.detach()
-            tex = tex.permute(0, 3, 1, 2)
-            tex = F.interpolate(tex, (tex_resolution * 5 // 4, tex_resolution))
-            tex = tex.permute(0, 2, 3, 1).contiguous()
-
-            if base_tex is not None:
-                _base_tex = base_tex.permute(0, 3, 1, 2)
-                _base_tex = F.interpolate(
-                    _base_tex, (tex_resolution * 5 // 4, tex_resolution))
-                _base_tex = _base_tex.permute(0, 2, 3, 1).contiguous()
-                tex += _base_tex
-
-            tex.requires_grad = True
-            optim = torch.optim.Adam([tex], lr=1e-2)
-
-            texture_opt_iters = 100
-
-            if tex_resolution == 2048:
-                optim_mask = torch.optim.Adam([tex_mask], lr=1e-2)
-
-            for i in range(int(texture_opt_iters)):
-
-                if tex_resolution == 2048:
-                    optim_mask.zero_grad()
-                    rendered = dr.texture(
-                        tex_mask, interp_out, filter_mode='linear')  # , uv_da)
-                    rendered = rendered * torch.clamp(
-                        rast_out[..., -1:], 0, 1)  # Mask out background.
-                    tex_loss = torch.mean((target_img - rendered)**2)
-
-                    tex_loss.backward()
-                    optim_mask.step()
-
-                optim.zero_grad()
-
-                img = dr.texture(
-                    tex, interp_out, filter_mode='linear')  # , uv_da)
-                img = img * torch.clamp(rast_out[..., -1:], 0,
-                                        1)  # Mask out background.
-                recon_loss = torch.mean((target_img - img)**2)
-
-                if tex_resolution < 2048:
-                    tv_loss = criterionTV(tex.permute(0, 3, 1, 2))
-
-                    total_loss = recon_loss + tv_loss * 0.01
-                else:
-
-                    total_loss = recon_loss
-
-                total_loss.backward()
-                optim.step()
-
-        tex_map = tex[0].detach().cpu().numpy()[..., ::-1] * 255.0
-
-        image = img.permute(0, 3, 1, 2)
-
-        tex_mask = tex_mask[0].detach().cpu().numpy() * 255.0
-        tex_mask = np.where(tex_mask[..., 1] > 250, 1.0, 0.0) * np.where(
-            tex_mask[..., 0] < 10, 1.0, 0) * np.where(tex_mask[..., 2] < 10,
-                                                      1.0, 0)
-        tex_mask = 1.0 - tex_mask
-
-        return mask, depth, image, tex_map, tex_mask
+        return It_warp
```

### Comparing `modelscope-1.4.2/modelscope/models/cv/face_reconstruction/utils.py` & `modelscope-1.5.0/modelscope/models/cv/face_reconstruction/utils.py`

 * *Files 23% similar despite different names*

```diff
@@ -379,58 +379,106 @@
     ]
     Lm3D = np.stack(value_list, axis=0)
     Lm3D = Lm3D[[1, 2, 0, 3, 4], :]
 
     return Lm3D
 
 
+def mesh_to_string(mesh):
+    out_string = ''
+    out_string += '# Create by HRN\n'
+
+    if 'colors' in mesh:
+        for i, v in enumerate(mesh['vertices']):
+            out_string += \
+                'v {:.6f} {:.6f} {:.6f} {:.6f} {:.6f} {:.6f}\n'.format(
+                    v[0], v[1], v[2], mesh['colors'][i][0],
+                    mesh['colors'][i][1], mesh['colors'][i][2])
+    else:
+        for v in mesh['vertices']:
+            out_string += 'v {:.6f} {:.6f} {:.6f}\n'.format(v[0], v[1], v[2])
+
+    if 'UVs' in mesh:
+        for uv in mesh['UVs']:
+            out_string += 'vt {:.6f} {:.6f}\n'.format(uv[0], uv[1])
+
+    if 'normals' in mesh:
+        for vn in mesh['normals']:
+            out_string += 'vn {:.6f} {:.6f} {:.6f}\n'.format(
+                vn[0], vn[1], vn[2])
+
+    if 'faces' in mesh:
+        for ind, face in enumerate(mesh['faces']):
+            if 'faces_uv' in mesh or 'faces_normal' in mesh or 'UVs' in mesh:
+                if 'faces_uv' in mesh:
+                    face_uv = mesh['faces_uv'][ind]
+                else:
+                    face_uv = face
+                if 'faces_normal' in mesh:
+                    face_normal = mesh['faces_normal'][ind]
+                else:
+                    face_normal = face
+                row = 'f ' + ' '.join([
+                    '{}/{}/{}'.format(face[i], face_uv[i], face_normal[i])
+                    for i in range(len(face))
+                ]) + '\n'
+            else:
+                row = 'f ' + ' '.join(
+                    ['{}'.format(face[i]) for i in range(len(face))]) + '\n'
+            out_string += row
+
+    return out_string
+
+
 def write_obj(save_path, mesh):
     save_dir = os.path.dirname(save_path)
     save_name = os.path.splitext(os.path.basename(save_path))[0]
 
     if 'texture_map' in mesh:
         cv2.imwrite(
             os.path.join(save_dir, save_name + '.jpg'), mesh['texture_map'])
 
         with open(os.path.join(save_dir, save_name + '.mtl'), 'w') as wf:
-            wf.write('# Created by ModelScope\n')
+            wf.write('# Created by HRN\n')
             wf.write('newmtl material_0\n')
             wf.write('Ka 1.000000 0.000000 0.000000\n')
             wf.write('Kd 1.000000 1.000000 1.000000\n')
             wf.write('Ks 0.000000 0.000000 0.000000\n')
             wf.write('Tr 0.000000\n')
             wf.write('illum 0\n')
             wf.write('Ns 0.000000\n')
             wf.write('map_Kd {}\n'.format(save_name + '.jpg'))
 
     with open(save_path, 'w') as wf:
         if 'texture_map' in mesh:
-            wf.write('# Create by ModelScope\n')
+            wf.write('# Create by HRN\n')
             wf.write('mtllib ./{}.mtl\n'.format(save_name))
 
         if 'colors' in mesh:
             for i, v in enumerate(mesh['vertices']):
-                wf.write('v {} {} {} {} {} {}\n'.format(
-                    v[0], v[1], v[2], mesh['colors'][i][0],
-                    mesh['colors'][i][1], mesh['colors'][i][2]))
+                wf.write(
+                    'v {:.6f} {:.6f} {:.6f} {:.6f} {:.6f} {:.6f}\n'.format(
+                        v[0], v[1], v[2], mesh['colors'][i][0],
+                        mesh['colors'][i][1], mesh['colors'][i][2]))
         else:
             for v in mesh['vertices']:
-                wf.write('v {} {} {}\n'.format(v[0], v[1], v[2]))
+                wf.write('v {:.6f} {:.6f} {:.6f}\n'.format(v[0], v[1], v[2]))
 
         if 'UVs' in mesh:
             for uv in mesh['UVs']:
-                wf.write('vt {} {}\n'.format(uv[0], uv[1]))
+                wf.write('vt {:.6f} {:.6f}\n'.format(uv[0], uv[1]))
 
         if 'normals' in mesh:
             for vn in mesh['normals']:
-                wf.write('vn {} {} {}\n'.format(vn[0], vn[1], vn[2]))
+                wf.write('vn {:.6f} {:.6f} {:.6f}\n'.format(
+                    vn[0], vn[1], vn[2]))
 
         if 'faces' in mesh:
             for ind, face in enumerate(mesh['faces']):
-                if 'faces_uv' in mesh or 'faces_normal' in mesh:
+                if 'faces_uv' in mesh or 'faces_normal' in mesh or 'UVs' in mesh:
                     if 'faces_uv' in mesh:
                         face_uv = mesh['faces_uv'][ind]
                     else:
                         face_uv = face
                     if 'faces_normal' in mesh:
                         face_normal = mesh['faces_normal'][ind]
                     else:
@@ -746,7 +794,207 @@
         for j in range(faces.shape[0]):
             norm[faces[j, i]] += n[j]
 
     inds = (norm[:, 0] == 0) * (norm[:, 1] == 0) * (norm[:, 2] == 0)
     norm[inds] = [0, 0, 1.0]
     result = normalize_v3(norm)
     return result
+
+
+def draw_landmarks(img, landmark, color='r', step=2):
+    """
+    Return:
+        img              -- numpy.array, (B, H, W, 3) img with landmark, RGB order, range (0, 255)
+
+
+    Parameters:
+        img              -- numpy.array, (B, H, W, 3), RGB order, range (0, 255)
+        landmark         -- numpy.array, (B, 68, 2), y direction is opposite to v direction
+        color            -- str, 'r' or 'b' (red or blue)
+    """
+    if color == 'r':
+        c = np.array([255., 0, 0])
+    else:
+        c = np.array([0, 0, 255.])
+
+    _, H, W, _ = img.shape
+    img, landmark = img.copy(), landmark.copy()
+    landmark[..., 1] = H - 1 - landmark[..., 1]
+    landmark = np.round(landmark).astype(np.int32)
+    for i in range(landmark.shape[1]):
+        x, y = landmark[:, i, 0], landmark[:, i, 1]
+        for j in range(-step, step):
+            for k in range(-step, step):
+                u = np.clip(x + j, 0, W - 1)
+                v = np.clip(y + k, 0, H - 1)
+                for m in range(landmark.shape[0]):
+                    img[m, v[m], u[m]] = c
+    return img
+
+
+def split_vis(img_path, target_dir=None):
+    img = cv2.imread(img_path)
+    h, w = img.shape[:2]
+    n_split = w // h
+    if target_dir is None:
+        target_dir = os.path.dirname(img_path)
+    base_name = os.path.splitext(os.path.basename(img_path))[0]
+    for i in range(n_split):
+        img_i = img[:, i * h:(i + 1) * h, :]
+        cv2.imwrite(
+            os.path.join(target_dir, '{}_{:0>2d}.jpg'.format(base_name,
+                                                             i + 1)), img_i)
+
+
+def write_video(image_list, save_path, fps=20.0):
+    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
+    # fourcc = cv2.VideoWriter_fourcc(*'MJPG')  # avi格式
+
+    h, w = image_list[0].shape[:2]
+
+    out = cv2.VideoWriter(save_path, fourcc, fps, (w, h), True)
+
+    for frame in image_list:
+        out.write(frame)
+
+    out.release()
+
+
+# ---------------------------- process/generate vertices, normals, faces
+def generate_triangles(h, w, margin_x=2, margin_y=5, mask=None):
+    # quad layout:
+    # 0 1 ... w-1
+    # w w+1
+    # .
+    # w*h
+    triangles = []
+    for x in range(margin_x, w - 1 - margin_x):
+        for y in range(margin_y, h - 1 - margin_y):
+            triangle0 = [y * w + x, y * w + x + 1, (y + 1) * w + x]
+            triangle1 = [y * w + x + 1, (y + 1) * w + x + 1, (y + 1) * w + x]
+            triangles.append(triangle0)
+            triangles.append(triangle1)
+    triangles = np.array(triangles)
+    triangles = triangles[:, [0, 2, 1]]
+    return triangles
+
+
+def face_vertices(vertices, faces):
+    """
+    :param vertices: [batch size, number of vertices, 3]
+    :param faces: [batch size, number of faces, 3]
+    :return: [batch size, number of faces, 3, 3]
+    """
+    assert (vertices.ndimension() == 3)
+    assert (faces.ndimension() == 3)
+    assert (vertices.shape[0] == faces.shape[0])
+    assert (vertices.shape[2] == 3)
+    assert (faces.shape[2] == 3)
+
+    bs, nv = vertices.shape[:2]
+    bs, nf = faces.shape[:2]
+    device = vertices.device
+    faces = faces + (torch.arange(bs, dtype=torch.int32).to(device)
+                     * nv)[:, None, None]
+    vertices = vertices.reshape((bs * nv, 3))
+    # pytorch only supports long and byte tensors for indexing
+    return vertices[faces.long()]
+
+
+def vertex_normals(vertices, faces):
+    """
+    :param vertices: [batch size, number of vertices, 3]
+    :param faces: [batch size, number of faces, 3]
+    :return: [batch size, number of vertices, 3]
+    """
+    assert (vertices.ndimension() == 3)
+    assert (faces.ndimension() == 3)
+    assert (vertices.shape[0] == faces.shape[0])
+    assert (vertices.shape[2] == 3)
+    assert (faces.shape[2] == 3)
+    bs, nv = vertices.shape[:2]
+    bs, nf = faces.shape[:2]
+    device = vertices.device
+    normals = torch.zeros(bs * nv, 3).to(device)
+
+    faces = faces + (torch.arange(bs, dtype=torch.int32).to(device)
+                     * nv)[:, None, None]  # expanded faces
+    vertices_faces = vertices.reshape((bs * nv, 3))[faces.long()]
+
+    faces = faces.reshape(-1, 3)
+    vertices_faces = vertices_faces.reshape(-1, 3, 3)
+
+    normals.index_add_(
+        0, faces[:, 1].long(),
+        torch.cross(vertices_faces[:, 2] - vertices_faces[:, 1],
+                    vertices_faces[:, 0] - vertices_faces[:, 1]))
+    normals.index_add_(
+        0, faces[:, 2].long(),
+        torch.cross(vertices_faces[:, 0] - vertices_faces[:, 2],
+                    vertices_faces[:, 1] - vertices_faces[:, 2]))
+    normals.index_add_(
+        0, faces[:, 0].long(),
+        torch.cross(vertices_faces[:, 1] - vertices_faces[:, 0],
+                    vertices_faces[:, 2] - vertices_faces[:, 0]))
+
+    normals = F.normalize(normals, eps=1e-6, dim=1)
+    normals = normals.reshape((bs, nv, 3))
+    # pytorch only supports long and byte tensors for indexing
+    return normals
+
+
+def dict2obj(d):
+    # if isinstance(d, list):
+    #     d = [dict2obj(x) for x in d]
+    if not isinstance(d, dict):
+        return d
+
+    class C(object):
+        pass
+
+    o = C()
+    for k in d:
+        o.__dict__[k] = dict2obj(d[k])
+    return o
+
+
+def enlarged_bbox(bbox, img_width, img_height, enlarge_ratio=0.2):
+    '''
+    :param bbox: [xmin,ymin,xmax,ymax]
+    :return: bbox: [xmin,ymin,xmax,ymax]
+    '''
+
+    left = bbox[0]
+    top = bbox[1]
+
+    right = bbox[2]
+    bottom = bbox[3]
+
+    roi_width = right - left
+    roi_height = bottom - top
+
+    new_left = left - int(roi_width * enlarge_ratio)
+    new_left = 0 if new_left < 0 else new_left
+
+    new_top = top - int(roi_height * enlarge_ratio)
+    new_top = 0 if new_top < 0 else new_top
+
+    new_right = right + int(roi_width * enlarge_ratio)
+    new_right = img_width if new_right > img_width else new_right
+
+    new_bottom = bottom + int(roi_height * enlarge_ratio)
+    new_bottom = img_height if new_bottom > img_height else new_bottom
+
+    bbox = [new_left, new_top, new_right, new_bottom]
+
+    bbox = [int(x) for x in bbox]
+
+    return bbox
+
+
+def draw_line(im, points, color, stroke_size=2, closed=False):
+    points = points.astype(np.int32)
+    for i in range(len(points) - 1):
+        cv2.line(im, tuple(points[i]), tuple(points[i + 1]), color,
+                 stroke_size)
+    if closed:
+        cv2.line(im, tuple(points[0]), tuple(points[-1]), color, stroke_size)
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `modelscope-1.4.2/modelscope/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py` & `modelscope-1.5.0/modelscope/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/facial_expression_recognition/fer/transforms.py` & `modelscope-1.5.0/modelscope/models/cv/facial_expression_recognition/fer/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/facial_expression_recognition/fer/vgg.py` & `modelscope-1.5.0/modelscope/models/cv/facial_expression_recognition/fer/vgg.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py` & `modelscope-1.5.0/modelscope/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/facial_landmark_confidence/flc/manual_landmark_net.py` & `modelscope-1.5.0/modelscope/models/cv/facial_landmark_confidence/flc/manual_landmark_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/hand_2d_keypoints/hand_2d_keypoints.py` & `modelscope-1.5.0/modelscope/models/cv/hand_2d_keypoints/hand_2d_keypoints.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/hand_static/hand_model.py` & `modelscope-1.5.0/modelscope/models/cv/hand_static/hand_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/hand_static/networks.py` & `modelscope-1.5.0/modelscope/models/cv/hand_static/networks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/human_reconstruction/Reconstruction.py` & `modelscope-1.5.0/modelscope/models/cv/human_reconstruction/Reconstruction.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/human_reconstruction/models/Embedding.py` & `modelscope-1.5.0/modelscope/models/cv/human_reconstruction/models/Embedding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/human_reconstruction/models/PixToMesh.py` & `modelscope-1.5.0/modelscope/models/cv/human_reconstruction/models/PixToMesh.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/human_reconstruction/models/Res_backbone.py` & `modelscope-1.5.0/modelscope/models/cv/human_reconstruction/models/Res_backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/human_reconstruction/models/Surface_head.py` & `modelscope-1.5.0/modelscope/models/cv/human_reconstruction/models/Surface_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/human_reconstruction/models/detectors.py` & `modelscope-1.5.0/modelscope/models/cv/human_reconstruction/models/detectors.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/human_reconstruction/models/geometry.py` & `modelscope-1.5.0/modelscope/models/cv/human_reconstruction/models/geometry.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/human_reconstruction/models/human_segmenter.py` & `modelscope-1.5.0/modelscope/models/cv/human_reconstruction/models/human_segmenter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/human_reconstruction/models/networks.py` & `modelscope-1.5.0/modelscope/models/cv/human_reconstruction/models/networks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/human_reconstruction/utils.py` & `modelscope-1.5.0/modelscope/models/cv/human_reconstruction/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/human_wholebody_keypoint/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/human_wholebody_keypoint/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/human_wholebody_keypoint/human_wholebody_keypoint.py` & `modelscope-1.5.0/modelscope/models/cv/human_wholebody_keypoint/human_wholebody_keypoint.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_binary_quant_classification/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_binary_quant_classification/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_binary_quant_classification/binary_quant_model.py` & `modelscope-1.5.0/modelscope/models/cv/image_binary_quant_classification/binary_quant_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_binary_quant_classification/bnext.py` & `modelscope-1.5.0/modelscope/models/cv/image_binary_quant_classification/bnext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_body_reshaping/image_body_reshaping.py` & `modelscope-1.5.0/modelscope/models/cv/image_body_reshaping/image_body_reshaping.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_body_reshaping/model.py` & `modelscope-1.5.0/modelscope/models/cv/image_body_reshaping/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_body_reshaping/person_info.py` & `modelscope-1.5.0/modelscope/models/cv/image_body_reshaping/person_info.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_body_reshaping/pose_estimator/body.py` & `modelscope-1.5.0/modelscope/models/cv/image_body_reshaping/pose_estimator/body.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_body_reshaping/pose_estimator/model.py` & `modelscope-1.5.0/modelscope/models/cv/image_body_reshaping/pose_estimator/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_body_reshaping/pose_estimator/util.py` & `modelscope-1.5.0/modelscope/models/cv/image_body_reshaping/pose_estimator/util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_body_reshaping/slim_utils.py` & `modelscope-1.5.0/modelscope/models/cv/image_body_reshaping/slim_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_classification/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_classification/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_classification/backbones/beit_v2.py` & `modelscope-1.5.0/modelscope/models/cv/image_classification/backbones/beit_v2.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_classification/backbones/nextvit.py` & `modelscope-1.5.0/modelscope/models/cv/image_classification/backbones/nextvit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_classification/mmcls_model.py` & `modelscope-1.5.0/modelscope/models/cv/image_classification/mmcls_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_classification/resnet50_cc.py` & `modelscope-1.5.0/modelscope/models/cv/image_classification/resnet50_cc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_classification/utils.py` & `modelscope-1.5.0/modelscope/models/cv/image_classification/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_color_enhance/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_color_enhance/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_color_enhance/adaint/adaint.py` & `modelscope-1.5.0/modelscope/models/cv/image_color_enhance/adaint/adaint.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_color_enhance/csrnet.py` & `modelscope-1.5.0/modelscope/models/cv/image_color_enhance/csrnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_color_enhance/deeplpf/deeplpf_image_color_enhance.py` & `modelscope-1.5.0/modelscope/models/cv/image_color_enhance/deeplpf/deeplpf_image_color_enhance.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_color_enhance/deeplpf/deeplpfnet.py` & `modelscope-1.5.0/modelscope/models/cv/image_color_enhance/deeplpf/deeplpfnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_color_enhance/image_color_enhance.py` & `modelscope-1.5.0/modelscope/models/cv/image_color_enhance/image_color_enhance.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_colorization/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_colorization/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_colorization/ddcolor/ddcolor.py` & `modelscope-1.5.0/modelscope/models/cv/image_colorization/ddcolor/ddcolor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_colorization/ddcolor/ddcolor_for_image_colorization.py` & `modelscope-1.5.0/modelscope/models/cv/image_colorization/ddcolor/ddcolor_for_image_colorization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_colorization/ddcolor/loss.py` & `modelscope-1.5.0/modelscope/models/cv/image_colorization/ddcolor/loss.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_colorization/ddcolor/utils/convnext.py` & `modelscope-1.5.0/modelscope/models/cv/image_colorization/ddcolor/utils/convnext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_colorization/ddcolor/utils/position_encoding.py` & `modelscope-1.5.0/modelscope/models/cv/image_colorization/ddcolor/utils/position_encoding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_colorization/ddcolor/utils/transformer_utils.py` & `modelscope-1.5.0/modelscope/models/cv/image_colorization/ddcolor/utils/transformer_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_colorization/ddcolor/utils/unet.py` & `modelscope-1.5.0/modelscope/models/cv/image_colorization/ddcolor/utils/unet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_colorization/ddcolor/utils/vgg.py` & `modelscope-1.5.0/modelscope/models/cv/image_colorization/ddcolor/utils/vgg.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_colorization/unet/unet.py` & `modelscope-1.5.0/modelscope/models/cv/image_colorization/unet/unet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_colorization/unet/utils.py` & `modelscope-1.5.0/modelscope/models/cv/image_colorization/unet/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_debanding/rrdb/rrdb_image_debanding.py` & `modelscope-1.5.0/modelscope/models/cv/image_debanding/rrdb/rrdb_image_debanding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_deblur/nafnet_for_image_deblur.py` & `modelscope-1.5.0/modelscope/models/cv/image_deblur/nafnet_for_image_deblur.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/defrcn_for_fewshot.py` & `modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/defrcn_for_fewshot.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/evaluation/coco_evaluation.py` & `modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/coco_evaluation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/evaluation/evaluator.py` & `modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/evaluator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/evaluation/pascal_voc_evaluation.py` & `modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/pascal_voc_evaluation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/models/calibration_layer.py` & `modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/models/calibration_layer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/models/defrcn.py` & `modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/models/defrcn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/models/fast_rcnn.py` & `modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/models/fast_rcnn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/models/gdl.py` & `modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/models/gdl.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/models/resnet.py` & `modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/models/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/models/roi_heads.py` & `modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/models/roi_heads.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/utils/coco_register.py` & `modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/utils/coco_register.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/utils/configuration_mapper.py` & `modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/utils/configuration_mapper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/utils/model_surgery_op.py` & `modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/utils/model_surgery_op.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/utils/register_data.py` & `modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/utils/register_data.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/utils/requirements_check.py` & `modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/utils/requirements_check.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_defrcn_fewshot/utils/voc_register.py` & `modelscope-1.5.0/modelscope/models/cv/image_defrcn_fewshot/utils/voc_register.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_denoise/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_denoise/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_denoise/nafnet/NAFNet_arch.py` & `modelscope-1.5.0/modelscope/models/cv/image_denoise/nafnet/NAFNet_arch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_denoise/nafnet/arch_util.py` & `modelscope-1.5.0/modelscope/models/cv/image_denoise/nafnet/arch_util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_denoise/nafnet_for_image_denoise.py` & `modelscope-1.5.0/modelscope/models/cv/image_denoise/nafnet_for_image_denoise.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_depth_estimation/networks/newcrf_depth.py` & `modelscope-1.5.0/modelscope/models/cv/image_depth_estimation/networks/newcrf_depth.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_depth_estimation/networks/newcrf_layers.py` & `modelscope-1.5.0/modelscope/models/cv/image_depth_estimation/networks/newcrf_layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_depth_estimation/networks/newcrf_utils.py` & `modelscope-1.5.0/modelscope/models/cv/image_depth_estimation/networks/newcrf_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_depth_estimation/networks/swin_transformer.py` & `modelscope-1.5.0/modelscope/models/cv/image_depth_estimation/networks/swin_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_depth_estimation/networks/uper_crf_head.py` & `modelscope-1.5.0/modelscope/models/cv/image_depth_estimation/networks/uper_crf_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_depth_estimation/newcrfs_model.py` & `modelscope-1.5.0/modelscope/models/cv/image_depth_estimation/newcrfs_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_depth_estimation_bts/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_depth_estimation_bts/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_depth_estimation_bts/depth_estimation_bts_model.py` & `modelscope-1.5.0/modelscope/models/cv/image_depth_estimation_bts/depth_estimation_bts_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_depth_estimation_bts/networks/bts_model.py` & `modelscope-1.5.0/modelscope/models/cv/image_depth_estimation_bts/networks/bts_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_depth_estimation_bts/networks/decoder.py` & `modelscope-1.5.0/modelscope/models/cv/image_depth_estimation_bts/networks/decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_depth_estimation_bts/networks/encoder.py` & `modelscope-1.5.0/modelscope/models/cv/image_depth_estimation_bts/networks/encoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_depth_estimation_bts/networks/utils.py` & `modelscope-1.5.0/modelscope/models/cv/image_depth_estimation_bts/networks/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_driving_perception/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_driving_perception/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_driving_perception/image_driving_percetion_model.py` & `modelscope-1.5.0/modelscope/models/cv/image_driving_perception/image_driving_percetion_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_driving_perception/preprocessor.py` & `modelscope-1.5.0/modelscope/models/cv/image_driving_perception/preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_driving_perception/utils.py` & `modelscope-1.5.0/modelscope/models/cv/image_driving_perception/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_face_fusion/facegan/gan_wrap.py` & `modelscope-1.5.0/modelscope/models/cv/image_face_fusion/facegan/gan_wrap.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_face_fusion/facegan/model.py` & `modelscope-1.5.0/modelscope/models/cv/image_face_fusion/facegan/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_face_fusion/facegan/op/conv2d_gradfix.py` & `modelscope-1.5.0/modelscope/models/cv/image_face_fusion/facegan/op/conv2d_gradfix.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_face_fusion/facegan/op/fused_act.py` & `modelscope-1.5.0/modelscope/models/cv/image_face_fusion/facegan/op/fused_act.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_face_fusion/facegan/op/upfirdn2d.py` & `modelscope-1.5.0/modelscope/models/cv/image_face_fusion/facegan/op/upfirdn2d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_face_fusion/facelib/align_trans.py` & `modelscope-1.5.0/modelscope/models/cv/image_face_fusion/facelib/align_trans.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_face_fusion/facelib/matlab_cp2tform.py` & `modelscope-1.5.0/modelscope/models/cv/image_face_fusion/facelib/matlab_cp2tform.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_face_fusion/image_face_fusion.py` & `modelscope-1.5.0/modelscope/models/cv/image_face_fusion/image_face_fusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_face_fusion/network/aad_layer.py` & `modelscope-1.5.0/modelscope/models/cv/image_face_fusion/network/aad_layer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_face_fusion/network/aei_flow_net.py` & `modelscope-1.5.0/modelscope/models/cv/image_face_fusion/network/aei_flow_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_face_fusion/network/bfm.py` & `modelscope-1.5.0/modelscope/models/cv/image_face_fusion/network/bfm.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_face_fusion/network/dense_motion.py` & `modelscope-1.5.0/modelscope/models/cv/image_face_fusion/network/dense_motion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_face_fusion/network/facerecon_model.py` & `modelscope-1.5.0/modelscope/models/cv/image_face_fusion/network/facerecon_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_face_fusion/network/model_irse.py` & `modelscope-1.5.0/modelscope/models/cv/image_face_fusion/network/model_irse.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_face_fusion/network/ops.py` & `modelscope-1.5.0/modelscope/models/cv/image_face_fusion/network/ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_human_parsing/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_human_parsing/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_human_parsing/backbone/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_human_parsing/backbone/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_human_parsing/backbone/deeplab_resnet.py` & `modelscope-1.5.0/modelscope/models/cv/image_human_parsing/backbone/deeplab_resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_human_parsing/m2fp/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_human_parsing/m2fp/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_human_parsing/m2fp/m2fp_decoder.py` & `modelscope-1.5.0/modelscope/models/cv/image_human_parsing/m2fp/m2fp_decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_human_parsing/m2fp/m2fp_encoder.py` & `modelscope-1.5.0/modelscope/models/cv/image_human_parsing/m2fp/m2fp_encoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_human_parsing/m2fp_net.py` & `modelscope-1.5.0/modelscope/models/cv/image_human_parsing/m2fp_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_human_parsing/parsing_utils.py` & `modelscope-1.5.0/modelscope/models/cv/image_human_parsing/parsing_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_inpainting/base.py` & `modelscope-1.5.0/modelscope/models/cv/image_inpainting/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_inpainting/default.py` & `modelscope-1.5.0/modelscope/models/cv/image_inpainting/default.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_inpainting/model.py` & `modelscope-1.5.0/modelscope/models/cv/image_inpainting/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_inpainting/modules/ade20k/base.py` & `modelscope-1.5.0/modelscope/models/cv/image_inpainting/modules/ade20k/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_inpainting/modules/ade20k/resnet.py` & `modelscope-1.5.0/modelscope/models/cv/image_inpainting/modules/ade20k/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_inpainting/modules/adversarial.py` & `modelscope-1.5.0/modelscope/models/cv/image_inpainting/modules/adversarial.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_inpainting/modules/feature_matching.py` & `modelscope-1.5.0/modelscope/models/cv/image_inpainting/modules/feature_matching.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_inpainting/modules/ffc.py` & `modelscope-1.5.0/modelscope/models/cv/image_inpainting/modules/ffc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_inpainting/modules/inception.py` & `modelscope-1.5.0/modelscope/models/cv/image_inpainting/modules/inception.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_inpainting/modules/perceptual.py` & `modelscope-1.5.0/modelscope/models/cv/image_inpainting/modules/perceptual.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_inpainting/modules/pix2pixhd.py` & `modelscope-1.5.0/modelscope/models/cv/image_inpainting/modules/pix2pixhd.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_inpainting/refinement.py` & `modelscope-1.5.0/modelscope/models/cv/image_inpainting/refinement.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/backbones/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/backbones/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/backbones/swin_transformer.py` & `modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/backbones/swin_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/cascade_mask_rcnn_swin.py` & `modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/cascade_mask_rcnn_swin.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/datasets/transforms.py` & `modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/datasets/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/maskdino/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/maskdino/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/maskdino/dino_decoder.py` & `modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/maskdino/dino_decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_decoder.py` & `modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_encoder.py` & `modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_encoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/maskdino/ms_deform_attn.py` & `modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/maskdino/ms_deform_attn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/maskdino/position_encoding.py` & `modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/maskdino/position_encoding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/maskdino/utils.py` & `modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/maskdino/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/maskdino_model.py` & `modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/maskdino_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/maskdino_swin.py` & `modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/maskdino_swin.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/model.py` & `modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_instance_segmentation/postprocess_utils.py` & `modelscope-1.5.0/modelscope/models/cv/image_instance_segmentation/postprocess_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_matching/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_matching/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_matching/config/default.py` & `modelscope-1.5.0/modelscope/models/cv/image_matching/config/default.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/backbone/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/backbone/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/backbone/resnet_fpn.py` & `modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/backbone/resnet_fpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr.py` & `modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/fine_preprocess.py` & `modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/fine_preprocess.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/linear_attention.py` & `modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/linear_attention.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/quadtree_attention.py` & `modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/quadtree_attention.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/transformer.py` & `modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/utils/coarse_matching.py` & `modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/coarse_matching.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/utils/fine_matching.py` & `modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/fine_matching.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_matching/loftr_quadtree/utils/position_encoding.py` & `modelscope-1.5.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/position_encoding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_matching/quadtree_attention_model.py` & `modelscope-1.5.0/modelscope/models/cv/image_matching/quadtree_attention_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_mvs_depth_estimation/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_mvs_depth_estimation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_mvs_depth_estimation/cas_mvsnet.py` & `modelscope-1.5.0/modelscope/models/cv/image_mvs_depth_estimation/cas_mvsnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_mvs_depth_estimation/casmvs_model.py` & `modelscope-1.5.0/modelscope/models/cv/image_mvs_depth_estimation/casmvs_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_mvs_depth_estimation/colmap2mvsnet.py` & `modelscope-1.5.0/modelscope/models/cv/image_mvs_depth_estimation/colmap2mvsnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_mvs_depth_estimation/depth_filter.py` & `modelscope-1.5.0/modelscope/models/cv/image_mvs_depth_estimation/depth_filter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_mvs_depth_estimation/general_eval_dataset.py` & `modelscope-1.5.0/modelscope/models/cv/image_mvs_depth_estimation/general_eval_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_mvs_depth_estimation/module.py` & `modelscope-1.5.0/modelscope/models/cv/image_mvs_depth_estimation/module.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_mvs_depth_estimation/utils.py` & `modelscope-1.5.0/modelscope/models/cv/image_mvs_depth_estimation/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_paintbyexample/model.py` & `modelscope-1.5.0/modelscope/models/cv/image_paintbyexample/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_panoptic_segmentation/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_panoptic_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_panoptic_segmentation/panseg_model.py` & `modelscope-1.5.0/modelscope/models/cv/image_panoptic_segmentation/panseg_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_panoptic_segmentation/r50_panseg_model.py` & `modelscope-1.5.0/modelscope/models/cv/image_panoptic_segmentation/r50_panseg_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/align_faces.py` & `modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/align_faces.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/eqface/fqa.py` & `modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/eqface/fqa.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/eqface/model_resnet.py` & `modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/eqface/model_resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/gpen.py` & `modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/gpen.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/image_portrait_enhancement.py` & `modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/image_portrait_enhancement.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/losses/helpers.py` & `modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/losses/helpers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/losses/losses.py` & `modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/losses/losses.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/losses/model_irse.py` & `modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/losses/model_irse.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/retinaface/detection.py` & `modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/retinaface/detection.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/retinaface/models/net.py` & `modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/retinaface/models/retinaface.py` & `modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/retinaface.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_portrait_enhancement/retinaface/utils.py` & `modelscope-1.5.0/modelscope/models/cv/image_portrait_enhancement/retinaface/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_probing_model/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_probing_model/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_probing_model/backbone.py` & `modelscope-1.5.0/modelscope/models/cv/image_probing_model/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_probing_model/model.py` & `modelscope-1.5.0/modelscope/models/cv/image_probing_model/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_probing_model/utils.py` & `modelscope-1.5.0/modelscope/models/cv/image_probing_model/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_degradation/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_degradation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_degradation/degradation_model.py` & `modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_degradation/degradation_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py` & `modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_man/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_man/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_man/image_quality_assessment_man.py` & `modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_man/image_quality_assessment_man.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_man/maniqa.py` & `modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_man/maniqa.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_man/swin.py` & `modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_man/swin.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_mos/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_mos/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_mos/backbones/resnet.py` & `modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_mos/backbones/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_mos/censeo_ivqa_model.py` & `modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_mos/censeo_ivqa_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_mos/heads/simple_head.py` & `modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_mos/heads/simple_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_quality_assessment_mos/image_quality_assessment_mos.py` & `modelscope-1.5.0/modelscope/models/cv/image_quality_assessment_mos/image_quality_assessment_mos.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_reid_person/pass_model.py` & `modelscope-1.5.0/modelscope/models/cv/image_reid_person/pass_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_reid_person/transreid_model.py` & `modelscope-1.5.0/modelscope/models/cv/image_reid_person/transreid_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_restoration/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_restoration/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_restoration/demoire_models/nets.py` & `modelscope-1.5.0/modelscope/models/cv/image_restoration/demoire_models/nets.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_restoration/image_restoration_model.py` & `modelscope-1.5.0/modelscope/models/cv/image_restoration/image_restoration_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/data_util.py` & `modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/data_util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/feature_extractors.py` & `modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/feature_extractors.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/pixel_classifier.py` & `modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/pixel_classifier.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/utils.py` & `modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py` & `modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/pan_merge/base_panoptic_fusion_head.py` & `modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/pan_merge/base_panoptic_fusion_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/pan_merge/maskformer_semantic_head.py` & `modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/pan_merge/maskformer_semantic_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/segformer.py` & `modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/segformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/semantic_seg_model.py` & `modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/semantic_seg_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/adapter_modules.py` & `modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/adapter_modules.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/beit.py` & `modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/beit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/beit_adapter.py` & `modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/beit_adapter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/base_decode_head.py` & `modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/base_decode_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/mask2former_head_from_mmseg.py` & `modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/mask2former_head_from_mmseg.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/base_segmentor.py` & `modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/base_segmentor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/encoder_decoder_mask2former.py` & `modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/encoder_decoder_mask2former.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/data_process_func.py` & `modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/data_process_func.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/seg_func.py` & `modelscope-1.5.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/seg_func.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_skychange/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_skychange/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_skychange/preprocessor.py` & `modelscope-1.5.0/modelscope/models/cv/image_skychange/preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_skychange/ptsemseg/BlockModules.py` & `modelscope-1.5.0/modelscope/models/cv/image_skychange/ptsemseg/BlockModules.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_skychange/ptsemseg/hrnet_backnone.py` & `modelscope-1.5.0/modelscope/models/cv/image_skychange/ptsemseg/hrnet_backnone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_skychange/ptsemseg/hrnet_super_and_ocr.py` & `modelscope-1.5.0/modelscope/models/cv/image_skychange/ptsemseg/hrnet_super_and_ocr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_skychange/ptsemseg/unet.py` & `modelscope-1.5.0/modelscope/models/cv/image_skychange/ptsemseg/unet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_skychange/skychange.py` & `modelscope-1.5.0/modelscope/models/cv/image_skychange/skychange.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_skychange/skychange_model.py` & `modelscope-1.5.0/modelscope/models/cv/image_skychange/skychange_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/data/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/data/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/data/transforms.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/data/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/model.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/models/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/models/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/models/autoencoder.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/models/autoencoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/models/clip.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/models/clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/ops/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/ops/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/ops/diffusion.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/ops/diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_generation/ops/losses.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_generation/ops/losses.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/data/transforms.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/data/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/model_translation.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/model_translation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/models/autoencoder.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/models/autoencoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/models/clip.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/models/clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/ops/degradation.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/ops/degradation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/ops/diffusion.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/ops/diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/ops/losses.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/ops/losses.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/ops/metrics.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/ops/metrics.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/ops/random_color.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/ops/random_color.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/ops/random_mask.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/ops/random_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/ops/svd.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/ops/svd.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/image_to_image_translation/ops/utils.py` & `modelscope-1.5.0/modelscope/models/cv/image_to_image_translation/ops/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/backbone/resnet_DA.py` & `modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/resnet_DA.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/backbone/vit_horizon_pry_image.py` & `modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/vit_horizon_pry_image.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/misc/fourier.py` & `modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/fourier.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/misc/panostretch.py` & `modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/panostretch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/misc/post_proc.py` & `modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/post_proc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/modality/layout.py` & `modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/modality/layout.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/panovit.py` & `modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/panovit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/networks/utils.py` & `modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/networks/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/indoor_layout_estimation/panovit.py` & `modelscope-1.5.0/modelscope/models/cv/indoor_layout_estimation/panovit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/language_guided_video_summarization/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/language_guided_video_summarization/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/language_guided_video_summarization/summarizer.py` & `modelscope-1.5.0/modelscope/models/cv/language_guided_video_summarization/summarizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/language_guided_video_summarization/transformer/layers.py` & `modelscope-1.5.0/modelscope/models/cv/language_guided_video_summarization/transformer/layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/language_guided_video_summarization/transformer/models.py` & `modelscope-1.5.0/modelscope/models/cv/language_guided_video_summarization/transformer/models.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/language_guided_video_summarization/transformer/modules.py` & `modelscope-1.5.0/modelscope/models/cv/language_guided_video_summarization/transformer/modules.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/language_guided_video_summarization/transformer/sub_layers.py` & `modelscope-1.5.0/modelscope/models/cv/language_guided_video_summarization/transformer/sub_layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/motion_generation/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/motion_generation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/motion_generation/model.py` & `modelscope-1.5.0/modelscope/models/cv/motion_generation/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/motion_generation/modules/cfg_sampler.py` & `modelscope-1.5.0/modelscope/models/cv/motion_generation/modules/cfg_sampler.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/motion_generation/modules/gaussian_diffusion.py` & `modelscope-1.5.0/modelscope/models/cv/motion_generation/modules/gaussian_diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/motion_generation/modules/mdm.py` & `modelscope-1.5.0/modelscope/models/cv/motion_generation/modules/mdm.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/motion_generation/modules/respace.py` & `modelscope-1.5.0/modelscope/models/cv/motion_generation/modules/respace.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/motion_generation/modules/rotation2xyz.py` & `modelscope-1.5.0/modelscope/models/cv/motion_generation/modules/rotation2xyz.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/motion_generation/modules/smpl.py` & `modelscope-1.5.0/modelscope/models/cv/motion_generation/modules/smpl.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/movie_scene_segmentation/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/movie_scene_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/movie_scene_segmentation/get_model.py` & `modelscope-1.5.0/modelscope/models/cv/movie_scene_segmentation/get_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/movie_scene_segmentation/model.py` & `modelscope-1.5.0/modelscope/models/cv/movie_scene_segmentation/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/movie_scene_segmentation/utils/head.py` & `modelscope-1.5.0/modelscope/models/cv/movie_scene_segmentation/utils/head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/movie_scene_segmentation/utils/save_op.py` & `modelscope-1.5.0/modelscope/models/cv/movie_scene_segmentation/utils/save_op.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/movie_scene_segmentation/utils/shot_encoder.py` & `modelscope-1.5.0/modelscope/models/cv/movie_scene_segmentation/utils/shot_encoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/movie_scene_segmentation/utils/trn.py` & `modelscope-1.5.0/modelscope/models/cv/movie_scene_segmentation/utils/trn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/nerf_recon_acc/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/nerf_recon_acc/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/nerf_recon_acc/dataloader/nerf_dataset.py` & `modelscope-1.5.0/modelscope/models/cv/nerf_recon_acc/dataloader/nerf_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/nerf_recon_acc/dataloader/read_write_model.py` & `modelscope-1.5.0/modelscope/models/cv/nerf_recon_acc/dataloader/read_write_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/nerf_recon_acc/nerf_preprocess.py` & `modelscope-1.5.0/modelscope/models/cv/nerf_recon_acc/nerf_preprocess.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/nerf_recon_acc/nerf_recon_acc.py` & `modelscope-1.5.0/modelscope/models/cv/nerf_recon_acc/nerf_recon_acc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/nerf_recon_acc/network/nerf.py` & `modelscope-1.5.0/modelscope/models/cv/nerf_recon_acc/network/nerf.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/nerf_recon_acc/network/segmenter.py` & `modelscope-1.5.0/modelscope/models/cv/nerf_recon_acc/network/segmenter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/nerf_recon_acc/network/utils.py` & `modelscope-1.5.0/modelscope/models/cv/nerf_recon_acc/network/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection/dino.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection/dino.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_model.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/backbones/vit.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/backbones/vit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/anchor_head.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/anchor_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/necks/fpn.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/necks/fpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/fcn_mask_head.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/fcn_mask_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/utils/checkpoint.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/utils/checkpoint.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection/mmdet_ms/utils/convModule_norm.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection/mmdet_ms/utils/convModule_norm.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection/yolox_pai.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection/yolox_pai.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/depe_detect.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/depe_detect.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/match_cost.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/match_cost.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/util.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/nuscenes_dataset.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/nuscenes_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/loading.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/loading.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/vovnet.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/vovnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/depth_net.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/depth_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/petrv2_dednhead.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/petrv2_dednhead.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/petr3d.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/petr3d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/cp_fpn.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/cp_fpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/positional_encoding.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/positional_encoding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/object_detection_3d/depe/result_vis.py` & `modelscope-1.5.0/modelscope/models/cv/object_detection_3d/depe/result_vis.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/ocr_detection/model.py` & `modelscope-1.5.0/modelscope/models/cv/ocr_detection/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/ocr_detection/modules/dbnet.py` & `modelscope-1.5.0/modelscope/models/cv/ocr_detection/modules/dbnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/ocr_detection/modules/seg_detector_loss.py` & `modelscope-1.5.0/modelscope/models/cv/ocr_detection/modules/seg_detector_loss.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/ocr_detection/preprocessor.py` & `modelscope-1.5.0/modelscope/models/cv/ocr_detection/preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/ocr_detection/utils.py` & `modelscope-1.5.0/modelscope/models/cv/ocr_detection/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/ocr_recognition/model.py` & `modelscope-1.5.0/modelscope/models/cv/ocr_recognition/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/ocr_recognition/modules/convnext.py` & `modelscope-1.5.0/modelscope/models/cv/ocr_recognition/modules/convnext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/ocr_recognition/modules/convnextvit.py` & `modelscope-1.5.0/modelscope/models/cv/ocr_recognition/modules/convnextvit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/ocr_recognition/modules/crnn.py` & `modelscope-1.5.0/modelscope/models/cv/ocr_recognition/modules/crnn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/ocr_recognition/modules/timm_tinyc.py` & `modelscope-1.5.0/modelscope/models/cv/ocr_recognition/modules/timm_tinyc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/ocr_recognition/modules/vitstr.py` & `modelscope-1.5.0/modelscope/models/cv/ocr_recognition/modules/vitstr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/ocr_recognition/preprocessor.py` & `modelscope-1.5.0/modelscope/models/cv/ocr_recognition/preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/open_vocabulary_detection_vild/vild.py` & `modelscope-1.5.0/modelscope/models/cv/open_vocabulary_detection_vild/vild.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/panorama_depth_estimation/networks/equi.py` & `modelscope-1.5.0/modelscope/models/cv/panorama_depth_estimation/networks/equi.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/panorama_depth_estimation/networks/layers.py` & `modelscope-1.5.0/modelscope/models/cv/panorama_depth_estimation/networks/layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/panorama_depth_estimation/networks/mobilenet.py` & `modelscope-1.5.0/modelscope/models/cv/panorama_depth_estimation/networks/mobilenet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/panorama_depth_estimation/networks/resnet.py` & `modelscope-1.5.0/modelscope/models/cv/panorama_depth_estimation/networks/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/panorama_depth_estimation/networks/unifuse.py` & `modelscope-1.5.0/modelscope/models/cv/panorama_depth_estimation/networks/unifuse.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/panorama_depth_estimation/networks/util.py` & `modelscope-1.5.0/modelscope/models/cv/panorama_depth_estimation/networks/util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/panorama_depth_estimation/unifuse_model.py` & `modelscope-1.5.0/modelscope/models/cv/panorama_depth_estimation/unifuse_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/pointcloud_sceneflow_estimation/common.py` & `modelscope-1.5.0/modelscope/models/cv/pointcloud_sceneflow_estimation/common.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/pointcloud_sceneflow_estimation/pointnet2_utils.py` & `modelscope-1.5.0/modelscope/models/cv/pointcloud_sceneflow_estimation/pointnet2_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/pointcloud_sceneflow_estimation/rcp_model.py` & `modelscope-1.5.0/modelscope/models/cv/pointcloud_sceneflow_estimation/rcp_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/pointcloud_sceneflow_estimation/sf_rcp.py` & `modelscope-1.5.0/modelscope/models/cv/pointcloud_sceneflow_estimation/sf_rcp.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/product_retrieval_embedding/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/product_retrieval_embedding/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/product_retrieval_embedding/item_detection.py` & `modelscope-1.5.0/modelscope/models/cv/product_retrieval_embedding/item_detection.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/product_retrieval_embedding/item_embedding.py` & `modelscope-1.5.0/modelscope/models/cv/product_retrieval_embedding/item_embedding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/product_retrieval_embedding/item_model.py` & `modelscope-1.5.0/modelscope/models/cv/product_retrieval_embedding/item_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/product_segmentation/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/product_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/product_segmentation/net.py` & `modelscope-1.5.0/modelscope/models/cv/product_segmentation/net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/product_segmentation/seg_infer.py` & `modelscope-1.5.0/modelscope/models/cv/product_segmentation/seg_infer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/model.py` & `modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/backbone.py` & `modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/criterion.py` & `modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/criterion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/matcher.py` & `modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/matcher.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/misc.py` & `modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/misc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/mttr.py` & `modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/mttr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/multimodal_transformer.py` & `modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/multimodal_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/position_encoding_2d.py` & `modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/position_encoding_2d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/postprocessing.py` & `modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/postprocessing.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/segmentation.py` & `modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/segmentation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/referring_video_object_segmentation/utils/swin_transformer.py` & `modelscope-1.5.0/modelscope/models/cv/referring_video_object_segmentation/utils/swin_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/robust_image_classification/easyrobust_model.py` & `modelscope-1.5.0/modelscope/models/cv/robust_image_classification/easyrobust_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/salient_detection/models/backbone/Res2Net_v1b.py` & `modelscope-1.5.0/modelscope/models/cv/salient_detection/models/backbone/Res2Net_v1b.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/salient_detection/models/modules.py` & `modelscope-1.5.0/modelscope/models/cv/salient_detection/models/modules.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/salient_detection/models/senet.py` & `modelscope-1.5.0/modelscope/models/cv/salient_detection/models/senet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/salient_detection/models/u2net.py` & `modelscope-1.5.0/modelscope/models/cv/salient_detection/models/u2net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/salient_detection/models/utils.py` & `modelscope-1.5.0/modelscope/models/cv/salient_detection/models/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/salient_detection/salient_model.py` & `modelscope-1.5.0/modelscope/models/cv/salient_detection/salient_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/shop_segmentation/common.py` & `modelscope-1.5.0/modelscope/models/cv/shop_segmentation/common.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/shop_segmentation/head_fpn.py` & `modelscope-1.5.0/modelscope/models/cv/shop_segmentation/head_fpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/shop_segmentation/models.py` & `modelscope-1.5.0/modelscope/models/cv/shop_segmentation/models.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/shop_segmentation/neck_fpn.py` & `modelscope-1.5.0/modelscope/models/cv/shop_segmentation/neck_fpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/shop_segmentation/shop_seg_base.py` & `modelscope-1.5.0/modelscope/models/cv/shop_segmentation/shop_seg_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/shop_segmentation/shop_seg_model.py` & `modelscope-1.5.0/modelscope/models/cv/shop_segmentation/shop_seg_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/shop_segmentation/utils.py` & `modelscope-1.5.0/modelscope/models/cv/shop_segmentation/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/skin_retouching/detection_model/detection_module.py` & `modelscope-1.5.0/modelscope/models/cv/skin_retouching/detection_model/detection_module.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/skin_retouching/detection_model/detection_unet_in.py` & `modelscope-1.5.0/modelscope/models/cv/skin_retouching/detection_model/detection_unet_in.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/skin_retouching/inpainting_model/gconv.py` & `modelscope-1.5.0/modelscope/models/cv/skin_retouching/inpainting_model/gconv.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/skin_retouching/inpainting_model/inpainting_unet.py` & `modelscope-1.5.0/modelscope/models/cv/skin_retouching/inpainting_model/inpainting_unet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/skin_retouching/retinaface/box_utils.py` & `modelscope-1.5.0/modelscope/models/cv/skin_retouching/retinaface/box_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/skin_retouching/retinaface/net.py` & `modelscope-1.5.0/modelscope/models/cv/skin_retouching/retinaface/net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/skin_retouching/retinaface/network.py` & `modelscope-1.5.0/modelscope/models/cv/skin_retouching/retinaface/network.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/skin_retouching/retinaface/predict_single.py` & `modelscope-1.5.0/modelscope/models/cv/skin_retouching/retinaface/predict_single.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/skin_retouching/retinaface/prior_box.py` & `modelscope-1.5.0/modelscope/models/cv/skin_retouching/retinaface/prior_box.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/skin_retouching/retinaface/utils.py` & `modelscope-1.5.0/modelscope/models/cv/skin_retouching/retinaface/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/skin_retouching/unet_deploy.py` & `modelscope-1.5.0/modelscope/models/cv/skin_retouching/unet_deploy.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/skin_retouching/utils.py` & `modelscope-1.5.0/modelscope/models/cv/skin_retouching/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/skin_retouching/weights_init.py` & `modelscope-1.5.0/modelscope/models/cv/skin_retouching/weights_init.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/stream_yolo/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/stream_yolo/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/stream_yolo/data/data_augment.py` & `modelscope-1.5.0/modelscope/models/cv/stream_yolo/data/data_augment.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/stream_yolo/exp/default/streamyolo.py` & `modelscope-1.5.0/modelscope/models/cv/stream_yolo/exp/default/streamyolo.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/stream_yolo/exp/yolox_base.py` & `modelscope-1.5.0/modelscope/models/cv/stream_yolo/exp/yolox_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/stream_yolo/models/darknet.py` & `modelscope-1.5.0/modelscope/models/cv/stream_yolo/models/darknet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/stream_yolo/models/dfp_pafpn.py` & `modelscope-1.5.0/modelscope/models/cv/stream_yolo/models/dfp_pafpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/stream_yolo/models/network_blocks.py` & `modelscope-1.5.0/modelscope/models/cv/stream_yolo/models/network_blocks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/stream_yolo/models/streamyolo.py` & `modelscope-1.5.0/modelscope/models/cv/stream_yolo/models/streamyolo.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/stream_yolo/models/tal_head.py` & `modelscope-1.5.0/modelscope/models/cv/stream_yolo/models/tal_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/stream_yolo/realtime_video_detector.py` & `modelscope-1.5.0/modelscope/models/cv/stream_yolo/realtime_video_detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/stream_yolo/utils/boxes.py` & `modelscope-1.5.0/modelscope/models/cv/stream_yolo/utils/boxes.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/super_resolution/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/super_resolution/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/super_resolution/arch_util.py` & `modelscope-1.5.0/modelscope/models/cv/super_resolution/arch_util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/super_resolution/ecb.py` & `modelscope-1.5.0/modelscope/models/cv/super_resolution/ecb.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/super_resolution/ecbsr_model.py` & `modelscope-1.5.0/modelscope/models/cv/super_resolution/ecbsr_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/super_resolution/rrdbnet_arch.py` & `modelscope-1.5.0/modelscope/models/cv/super_resolution/rrdbnet_arch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/table_recognition/lineless_table_process.py` & `modelscope-1.5.0/modelscope/models/cv/table_recognition/lineless_table_process.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/table_recognition/model_lore.py` & `modelscope-1.5.0/modelscope/models/cv/table_recognition/model_lore.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/table_recognition/modules/lore_detector.py` & `modelscope-1.5.0/modelscope/models/cv/table_recognition/modules/lore_detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/table_recognition/modules/lore_processor.py` & `modelscope-1.5.0/modelscope/models/cv/table_recognition/modules/lore_processor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/text_driven_segmentation/clip.py` & `modelscope-1.5.0/modelscope/models/cv/text_driven_segmentation/clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/text_driven_segmentation/lseg_base.py` & `modelscope-1.5.0/modelscope/models/cv/text_driven_segmentation/lseg_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/text_driven_segmentation/lseg_blocks.py` & `modelscope-1.5.0/modelscope/models/cv/text_driven_segmentation/lseg_blocks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/text_driven_segmentation/lseg_model.py` & `modelscope-1.5.0/modelscope/models/cv/text_driven_segmentation/lseg_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/text_driven_segmentation/lseg_net.py` & `modelscope-1.5.0/modelscope/models/cv/text_driven_segmentation/lseg_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/text_driven_segmentation/lseg_vit.py` & `modelscope-1.5.0/modelscope/models/cv/text_driven_segmentation/lseg_vit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/text_driven_segmentation/model.py` & `modelscope-1.5.0/modelscope/models/cv/text_driven_segmentation/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/text_driven_segmentation/simple_tokenizer.py` & `modelscope-1.5.0/modelscope/models/cv/text_driven_segmentation/simple_tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_classfication/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_classfication/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_classfication/basic_blocks.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_classfication/basic_blocks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_classfication/global_utils.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_classfication/global_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_classfication/master_net.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_classfication/master_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_classfication/model_zoo.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_classfication/model_zoo.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_classfication/plain_net_utils.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_classfication/plain_net_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_classfication/super_blocks.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_classfication/super_blocks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_classfication/super_res_idwexkx.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_classfication/super_res_idwexkx.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_classfication/super_res_k1kxk1.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_classfication/super_res_k1kxk1.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_classfication/super_res_kxkx.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_classfication/super_res_kxkx.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/apis/detector_evaluater.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/apis/detector_evaluater.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/apis/detector_inference.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/apis/detector_inference.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/box_level_augs.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/box_level_augs.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/color_augs.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/color_augs.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/gaussian_maps.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/gaussian_maps.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/geometric_augs.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/geometric_augs.py`

 * *Files 4% similar despite different names*

```diff
@@ -69,22 +69,33 @@
                                                  height).long().tolist() - x2
             if offset_x != 0 or offset_y != 0:
                 offset = [offset_y, offset_x]
                 boxes_new.append(box + torch.Tensor(offset * 2))
                 labels_new.append(labels[i])
         else:
             x_crop = transforms.functional.to_pil_image(x_crop.cpu())
-            x_crop = transforms.functional.affine(
-                x_crop,
-                angle,
-                translate,
-                scale,
-                shear,
-                resample=2,
-                fillcolor=tuple([int(i) for i in pixel_mean]))
+            try:
+                x_crop = transforms.functional.affine(
+                    x_crop,
+                    angle,
+                    translate,
+                    scale,
+                    shear,
+                    resample=2,
+                    fillcolor=tuple([int(i) for i in pixel_mean]))
+            except Exception:
+                x_crop = transforms.functional.affine(
+                    x_crop,
+                    angle,
+                    translate,
+                    scale,
+                    shear,
+                    interpolation=2,
+                    fill=tuple([int(i) for i in pixel_mean]))
+
             x_crop = transforms.functional.to_tensor(x_crop).to(x.device)
         x_crops.append(x_crop)
     y = _transform(x, x_crops, boxes_crops, translate)
 
     if translate[0] + translate[1] != 0 and len(boxes_new) > 0:
         target.bbox = torch.cat((target.bbox, torch.stack(boxes_new)))
         target.extra_fields['labels'] = torch.cat(
```

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/augmentations/scale_aware_aug.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/augmentations/scale_aware_aug.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/darknet.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/darknet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_csp.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_csp.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_res.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_res.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/base_ops.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/base_ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/neck_ops.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/neck_ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/ops.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/ota_assigner.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/ota_assigner.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/repvgg_block.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/repvgg_block.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/utils.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/weight_init.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/weight_init.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/heads/gfocal_v2_tiny.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/gfocal_v2_tiny.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/heads/zero_head.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/zero_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/losses/distill_loss.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/distill_loss.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/losses/gfocal_loss.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/gfocal_loss.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_config.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn_btn.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn_btn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/detectors/detector.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/detectors/detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/structures/bounding_box.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/structures/bounding_box.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/structures/boxlist_ops.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/structures/boxlist_ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/structures/image_list.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/structures/image_list.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/utils/boxes.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/utils/boxes.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/utils/model_utils.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/utils/model_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/damo/utils/scheduler.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/damo/utils/scheduler.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/detector.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/tinynas_damoyolo.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/tinynas_damoyolo.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/tinynas_detector.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/tinynas_detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/tinynas_detection/utils.py` & `modelscope-1.5.0/modelscope/models/cv/tinynas_detection/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_deinterlace/UNet_for_video_deinterlace.py` & `modelscope-1.5.0/modelscope/models/cv/video_deinterlace/UNet_for_video_deinterlace.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_deinterlace/deinterlace_arch.py` & `modelscope-1.5.0/modelscope/models/cv/video_deinterlace/deinterlace_arch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_deinterlace/models/archs.py` & `modelscope-1.5.0/modelscope/models/cv/video_deinterlace/models/archs.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_deinterlace/models/deep_fourier_upsampling.py` & `modelscope-1.5.0/modelscope/models/cv/video_deinterlace/models/deep_fourier_upsampling.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_deinterlace/models/enh.py` & `modelscope-1.5.0/modelscope/models/cv/video_deinterlace/models/enh.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_deinterlace/models/fre.py` & `modelscope-1.5.0/modelscope/models/cv/video_deinterlace/models/fre.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_deinterlace/models/utils.py` & `modelscope-1.5.0/modelscope/models/cv/video_deinterlace/models/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/configs/default_config.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/configs/default_config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/dro_model.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/dro_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/geometry/camera.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/geometry/camera.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/geometry/camera_utils.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/geometry/camera_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/geometry/pose.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/geometry/pose.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/geometry/pose_utils.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/geometry/pose_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/models/model_checkpoint.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/models/model_checkpoint.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/models/model_utils.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/models/model_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/models/model_wrapper.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/models/model_wrapper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/models/sfm_model_mf.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/models/sfm_model_mf.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/models/sup_model_mf.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/models/sup_model_mf.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/depth_pose/depth_pose_net.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/depth_pose/depth_pose_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/depth_decoder.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/depth_decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/layers.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/pose_decoder.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/pose_decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/resnet_encoder.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/resnet_encoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/optim/extractor.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/optim/extractor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/networks/optim/update.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/networks/optim/update.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/utils/augmentations.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/utils/augmentations.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/utils/config.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/utils/config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/utils/depth.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/utils/depth.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/utils/horovod.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/utils/horovod.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/utils/image.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/utils/image.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/utils/image_gt.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/utils/image_gt.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/utils/load.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/utils/load.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/utils/misc.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/utils/misc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_depth_estimation/utils/types.py` & `modelscope-1.5.0/modelscope/models/cv/video_depth_estimation/utils/types.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/VFINet_arch.py` & `modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/VFINet_arch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/VFINet_for_video_frame_interpolation.py` & `modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/VFINet_for_video_frame_interpolation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/flow_model/corr.py` & `modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/flow_model/corr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/flow_model/extractor.py` & `modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/flow_model/extractor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/flow_model/raft.py` & `modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/flow_model/raft.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/flow_model/update.py` & `modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/flow_model/update.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/interp_model/IFNet_swin.py` & `modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/interp_model/IFNet_swin.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/interp_model/UNet.py` & `modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/interp_model/UNet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/interp_model/flow_reversal.py` & `modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/interp_model/flow_reversal.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/interp_model/refinenet_arch.py` & `modelscope-1.5.0/modelscope/models/cv/video_object_segmentation/modules.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,488 +1,523 @@
-# Part of the implementation is borrowed and modified from QVI, publicly available at https://github.com/xuxy09/QVI
+# Adopted from https://github.com/Limingxing00/RDE-VOS-CVPR2022
+# under MIT License
 
-import numpy as np
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
+from torchvision import models
 
-from modelscope.models.cv.video_frame_interpolation.interp_model.flow_reversal import \
-    FlowReversal
-from modelscope.models.cv.video_frame_interpolation.interp_model.IFNet_swin import \
-    IFNet
-from modelscope.models.cv.video_frame_interpolation.interp_model.UNet import \
-    Small_UNet_Ds
+from modelscope.models.cv.video_object_segmentation import cbam, mod_resnet
 
 
-class AcFusionLayer(nn.Module):
-
-    def __init__(self, ):
-        super(AcFusionLayer, self).__init__()
-
-    def forward(self, flo10, flo12, flo21, flo23, t=0.5):
-        return 0.5 * ((t + t**2) * flo12 - (t - t**2) * flo10), \
-            0.5 * (((1 - t) + (1 - t)**2) * flo21 - ((1 - t) - (1 - t)**2) * flo23)
-        # return 0.375 * flo12 - 0.125 * flo10, 0.375 * flo21 - 0.125 * flo23
+class ResBlock(nn.Module):
 
+    def __init__(self, indim, outdim=None):
+        super(ResBlock, self).__init__()
+        if outdim is None:
+            outdim = indim
+        if indim == outdim:
+            self.downsample = None
+        else:
+            self.downsample = nn.Conv2d(
+                indim, outdim, kernel_size=3, padding=1)
 
-class Get_gradient(nn.Module):
-
-    def __init__(self):
-        super(Get_gradient, self).__init__()
-        kernel_v = [[0, -1, 0], [0, 0, 0], [0, 1, 0]]
-        kernel_h = [[0, 0, 0], [-1, 0, 1], [0, 0, 0]]
-        kernel_h = torch.FloatTensor(kernel_h).unsqueeze(0).unsqueeze(0)
-        kernel_v = torch.FloatTensor(kernel_v).unsqueeze(0).unsqueeze(0)
-        self.weight_h = nn.Parameter(data=kernel_h, requires_grad=False)
-        self.weight_v = nn.Parameter(data=kernel_v, requires_grad=False)
+        self.conv1 = nn.Conv2d(indim, outdim, kernel_size=3, padding=1)
+        self.conv2 = nn.Conv2d(outdim, outdim, kernel_size=3, padding=1)
 
     def forward(self, x):
-        x0 = x[:, 0]  # R
-        x1 = x[:, 1]  # G
-        x2 = x[:, 2]  # B
-        x0_v = F.conv2d(x0.unsqueeze(1), self.weight_v, padding=1)
-        x0_h = F.conv2d(x0.unsqueeze(1), self.weight_h, padding=1)
-
-        x1_v = F.conv2d(x1.unsqueeze(1), self.weight_v, padding=1)
-        x1_h = F.conv2d(x1.unsqueeze(1), self.weight_h, padding=1)
-
-        x2_v = F.conv2d(x2.unsqueeze(1), self.weight_v, padding=1)
-        x2_h = F.conv2d(x2.unsqueeze(1), self.weight_h, padding=1)
-
-        x0 = torch.sqrt(torch.pow(x0_v, 2) + torch.pow(x0_h, 2) + 1e-6)
-        x1 = torch.sqrt(torch.pow(x1_v, 2) + torch.pow(x1_h, 2) + 1e-6)
-        x2 = torch.sqrt(torch.pow(x2_v, 2) + torch.pow(x2_h, 2) + 1e-6)
+        r = self.conv1(F.relu(x))
+        r = self.conv2(F.relu(r))
 
-        x = torch.cat([x0, x1, x2], dim=1)
-        return x
+        if self.downsample is not None:
+            x = self.downsample(x)
 
+        return x + r
 
-class LowPassFilter(nn.Module):
 
-    def __init__(self):
-        super(LowPassFilter, self).__init__()
-        kernel_lpf = [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1],
-                      [1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1],
-                      [1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1],
-                      [1, 1, 1, 1, 1, 1, 1]]
+class FeatureFusionBlock(nn.Module):
 
-        kernel_lpf = torch.FloatTensor(kernel_lpf).unsqueeze(0).unsqueeze(
-            0) / 49
+    def __init__(self, indim, outdim):
+        super().__init__()
 
-        self.weight_lpf = nn.Parameter(data=kernel_lpf, requires_grad=False)
+        self.block1 = ResBlock(indim, outdim)
+        self.attention = cbam.CBAM(outdim)
+        self.block2 = ResBlock(outdim, outdim)
 
-    def forward(self, x):
-        x0 = x[:, 0]
-        x1 = x[:, 1]
-        y0 = F.conv2d(x0.unsqueeze(1), self.weight_lpf, padding=3)
-        y1 = F.conv2d(x1.unsqueeze(1), self.weight_lpf, padding=3)
+    def forward(self, x, f16):
+        x = torch.cat([x, f16], 1)
+        x = self.block1(x)
+        r = self.attention(x)
+        x = self.block2(x + r)
 
-        y = torch.cat([y0, y1], dim=1)
+        return x
 
-        return y
 
+# Single object version, used only in static image pretraining
+# See model.py (load_network) for the modification procedure
+class ValueEncoderSO(nn.Module):
 
-def backwarp(img, flow):
-    _, _, H, W = img.size()
+    def __init__(self):
+        super().__init__()
 
-    u = flow[:, 0, :, :]
-    v = flow[:, 1, :, :]
+        resnet = mod_resnet.resnet18(pretrained=False, extra_chan=1)
+        self.conv1 = resnet.conv1
+        self.bn1 = resnet.bn1
+        self.relu = resnet.relu  # 1/2, 64
+        self.maxpool = resnet.maxpool
+
+        self.layer1 = resnet.layer1  # 1/4, 64
+        self.layer2 = resnet.layer2  # 1/8, 128
+        self.layer3 = resnet.layer3  # 1/16, 256
+
+        self.fuser = FeatureFusionBlock(1024 + 256, 512)
+
+    def forward(self, image, key_f16, mask):
+        # key_f16 is the feature from the key encoder
+
+        f = torch.cat([image, mask], 1)
+
+        x = self.conv1(f)
+        x = self.bn1(x)
+        x = self.relu(x)  # 1/2, 64
+        x = self.maxpool(x)  # 1/4, 64
+        x = self.layer1(x)  # 1/4, 64
+        x = self.layer2(x)  # 1/8, 128
+        x = self.layer3(x)  # 1/16, 256
 
-    gridX, gridY = np.meshgrid(np.arange(W), np.arange(H))
+        x = self.fuser(x, key_f16)
 
-    gridX = torch.tensor(
-        gridX,
-        requires_grad=False,
-    ).cuda()
-    gridY = torch.tensor(
-        gridY,
-        requires_grad=False,
-    ).cuda()
-    x = gridX.unsqueeze(0).expand_as(u).float() + u
-    y = gridY.unsqueeze(0).expand_as(v).float() + v
+        return x
 
-    x = 2 * (x / (W - 1) - 0.5)
-    y = 2 * (y / (H - 1) - 0.5)
 
-    grid = torch.stack((x, y), dim=3)
+# Multiple objects version, used in other times
+class ValueEncoder(nn.Module):
 
-    imgOut = torch.nn.functional.grid_sample(
-        img, grid, padding_mode='border', align_corners=True)
+    def __init__(self):
+        super().__init__()
 
-    return imgOut
+        resnet = mod_resnet.resnet18(pretrained=False, extra_chan=2)
+        self.conv1 = resnet.conv1
+        self.bn1 = resnet.bn1
+        self.relu = resnet.relu  # 1/2, 64
+        self.maxpool = resnet.maxpool
+
+        self.layer1 = resnet.layer1  # 1/4, 64
+        self.layer2 = resnet.layer2  # 1/8, 128
+        self.layer3 = resnet.layer3  # 1/16, 256
+
+        self.fuser = FeatureFusionBlock(1024 + 256, 512)
+
+    def forward(self, image, key_f16, mask, other_masks):
+        # key_f16 is the feature from the key encoder
+
+        f = torch.cat([image, mask, other_masks], 1)
+
+        x = self.conv1(f)
+        x = self.bn1(x)
+        x = self.relu(x)  # 1/2, 64
+        x = self.maxpool(x)  # 1/4, 64
+        x = self.layer1(x)  # 1/4, 64
+        x = self.layer2(x)  # 1/8, 128
+        x = self.layer3(x)  # 1/16, 256
+        # x = torch.cat([x, x], dim=1)
+        x = self.fuser(x, key_f16)
 
+        return x
 
-class SmallMaskNet(nn.Module):
-    """A three-layer network for predicting mask"""
 
-    def __init__(self, input, output):
-        super(SmallMaskNet, self).__init__()
-        self.conv1 = nn.Conv2d(input, 32, 5, padding=2)
-        self.conv2 = nn.Conv2d(32, 16, 3, padding=1)
-        self.conv3 = nn.Conv2d(16, output, 3, padding=1)
+# from retrying import retry
+# @retry(stop_max_attempt_number=5)
+class KeyEncoder(nn.Module):
 
-    def forward(self, x):
-        x = F.leaky_relu(self.conv1(x), negative_slope=0.1)
-        x = F.leaky_relu(self.conv2(x), negative_slope=0.1)
-        x = self.conv3(x)
+    def __init__(self):
+        super().__init__()
+        resnet = models.resnet50(pretrained=False)
+        # if torch.distributed.get_rank() == 0:
+        #     torch.distributed.barrier()
+        self.conv1 = resnet.conv1
+        self.bn1 = resnet.bn1
+        self.relu = resnet.relu  # 1/2, 64
+        self.maxpool = resnet.maxpool
+
+        self.res2 = resnet.layer1  # 1/4, 256
+        self.layer2 = resnet.layer2  # 1/8, 512
+        self.layer3 = resnet.layer3  # 1/16, 1024
+
+    def forward(self, f):
+        x = self.conv1(f)
+        x = self.bn1(x)
+        x = self.relu(x)  # 1/2, 64
+        x = self.maxpool(x)  # 1/4, 64
+        f4 = self.res2(x)  # 1/4, 256
+        f8 = self.layer2(f4)  # 1/8, 512
+        f16 = self.layer3(f8)  # 1/16, 1024
+
+        return f16, f8, f4
+
+
+class UpsampleBlock(nn.Module):
+
+    def __init__(self, skip_c, up_c, out_c, scale_factor=2):
+        super().__init__()
+        self.skip_conv = nn.Conv2d(skip_c, up_c, kernel_size=3, padding=1)
+        self.out_conv = ResBlock(up_c, out_c)
+        self.scale_factor = scale_factor
+
+    def forward(self, skip_f, up_f):
+        x = self.skip_conv(skip_f)
+        x = x + F.interpolate(
+            up_f,
+            scale_factor=self.scale_factor,
+            mode='bilinear',
+            align_corners=False)
+        x = self.out_conv(x)
         return x
 
 
-class StaticMaskNet(nn.Module):
-    """static mask"""
-
-    def __init__(self, input, output):
-        super(StaticMaskNet, self).__init__()
+class KeyProjection(nn.Module):
 
-        modules_body = []
-        modules_body.append(
-            nn.Conv2d(
-                in_channels=input,
-                out_channels=32,
-                kernel_size=3,
-                stride=1,
-                padding=1))
-        modules_body.append(nn.LeakyReLU(inplace=False, negative_slope=0.1))
-        modules_body.append(
-            nn.Conv2d(
-                in_channels=32,
-                out_channels=32,
-                kernel_size=3,
-                stride=1,
-                padding=1))
-        modules_body.append(nn.LeakyReLU(inplace=False, negative_slope=0.1))
-        modules_body.append(
-            nn.Conv2d(
-                in_channels=32,
-                out_channels=16,
-                kernel_size=3,
-                stride=1,
-                padding=1))
-        modules_body.append(nn.LeakyReLU(inplace=False, negative_slope=0.1))
-        modules_body.append(
-            nn.Conv2d(
-                in_channels=16,
-                out_channels=16,
-                kernel_size=3,
-                stride=1,
-                padding=1))
-        modules_body.append(nn.LeakyReLU(inplace=False, negative_slope=0.1))
-        modules_body.append(
-            nn.Conv2d(
-                in_channels=16,
-                out_channels=output,
-                kernel_size=3,
-                stride=1,
-                padding=1))
-        modules_body.append(nn.Sigmoid())
+    def __init__(self, indim, keydim):
+        super().__init__()
+        self.key_proj = nn.Conv2d(indim, keydim, kernel_size=3, padding=1)
 
-        self.body = nn.Sequential(*modules_body)
+        nn.init.orthogonal_(self.key_proj.weight.data)
+        nn.init.zeros_(self.key_proj.bias.data)
 
     def forward(self, x):
-        y = self.body(x)
-        return y
-
-
-def tensor_erode(bin_img, ksize=5):
-    B, C, H, W = bin_img.shape
-    pad = (ksize - 1) // 2
-    bin_img = F.pad(bin_img, [pad, pad, pad, pad], mode='constant', value=0)
+        return self.key_proj(x)
 
-    patches = bin_img.unfold(dimension=2, size=ksize, step=1)
-    patches = patches.unfold(dimension=3, size=ksize, step=1)
 
-    eroded, _ = patches.reshape(B, C, H, W, -1).max(dim=-1)
-    return eroded
+class _NonLocalBlockND(nn.Module):
 
+    def __init__(self,
+                 in_channels,
+                 inter_channels=None,
+                 dimension=3,
+                 sub_sample=True,
+                 bn_layer=False):
+        super(_NonLocalBlockND, self).__init__()
+
+        assert dimension in [1, 2, 3]
+
+        self.dimension = dimension
+        self.sub_sample = sub_sample
+
+        self.in_channels = in_channels
+        self.inter_channels = inter_channels
+
+        if self.inter_channels is None:
+            self.inter_channels = in_channels // 2
+            if self.inter_channels == 0:
+                self.inter_channels = 1
+
+        if dimension == 3:
+            conv_nd = nn.Conv3d
+            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))
+            bn = nn.InstanceNorm3d
+        elif dimension == 2:
+            conv_nd = nn.Conv2d
+            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))
+            bn = nn.BatchNorm2d
+        else:
+            conv_nd = nn.Conv1d
+            max_pool_layer = nn.MaxPool1d(kernel_size=(2))
+            bn = nn.BatchNorm1d
+
+        self.g = conv_nd(
+            in_channels=self.in_channels,
+            out_channels=self.inter_channels,
+            kernel_size=1,
+            stride=1,
+            padding=0)
+
+        if bn_layer:
+            self.W = nn.Sequential(
+                conv_nd(
+                    in_channels=self.inter_channels,
+                    out_channels=self.in_channels,
+                    kernel_size=1,
+                    stride=1,
+                    padding=0), bn(self.in_channels))
+            # nn.init.constant_(self.W[1].weight, 0)
+            # nn.init.constant_(self.W[1].bias, 0)
+        else:
+            self.W = conv_nd(
+                in_channels=self.inter_channels,
+                out_channels=self.in_channels,
+                kernel_size=1,
+                stride=1,
+                padding=0)
+            # nn.init.constant_(self.W.weight, 0)
+            # nn.init.constant_(self.W.bias, 0)
+
+        self.theta = conv_nd(
+            in_channels=self.in_channels,
+            out_channels=self.inter_channels,
+            kernel_size=1,
+            stride=1,
+            padding=0)
+
+        self.phi = conv_nd(
+            in_channels=self.in_channels,
+            out_channels=self.inter_channels,
+            kernel_size=1,
+            stride=1,
+            padding=0)
+
+        if sub_sample:
+            self.g = nn.Sequential(self.g, max_pool_layer)
+            self.phi = nn.Sequential(self.phi, max_pool_layer)
 
-class QVI_inter_Ds(nn.Module):
-    """Given flow, implement Quadratic Video Interpolation"""
+    def forward(self, x):
+        '''
+        :param x: (b, c, t, h, w)
+        :return:
+        '''
+
+        batch_size = x.size(0)
+
+        g_x = self.g(x).view(batch_size, self.inter_channels, -1)
+        g_x = g_x.permute(0, 2, 1)
+
+        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)
+        theta_x = theta_x.permute(0, 2, 1)
+        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)
+        f = torch.matmul(theta_x, phi_x)
+        N = f.size(-1)
+        f_div_C = f / N
+
+        y = torch.matmul(f_div_C, g_x)
+        y = y.permute(0, 2, 1).contiguous()
+        y = y.view(batch_size, self.inter_channels, *x.size()[2:])
+        W_y = self.W(y)
+        z = W_y + x
+
+        return z
+
+
+class NONLocalBlock1D(_NonLocalBlockND):
+
+    def __init__(self,
+                 in_channels,
+                 inter_channels=None,
+                 sub_sample=True,
+                 bn_layer=True):
+        super(NONLocalBlock1D, self).__init__(
+            in_channels,
+            inter_channels=inter_channels,
+            dimension=1,
+            sub_sample=sub_sample,
+            bn_layer=bn_layer)
+
+
+class NONLocalBlock2D(_NonLocalBlockND):
+
+    def __init__(self,
+                 in_channels,
+                 inter_channels=None,
+                 sub_sample=True,
+                 bn_layer=False):
+        super(NONLocalBlock2D, self).__init__(
+            in_channels,
+            inter_channels=inter_channels,
+            dimension=2,
+            sub_sample=sub_sample,
+            bn_layer=bn_layer)
+
+
+class NONLocalBlock3D(_NonLocalBlockND):
+
+    def __init__(self,
+                 in_channels,
+                 inter_channels=None,
+                 sub_sample=True,
+                 bn_layer=True):
+        super(NONLocalBlock3D, self).__init__(
+            in_channels,
+            inter_channels=inter_channels,
+            dimension=3,
+            sub_sample=sub_sample,
+            bn_layer=bn_layer)
+
+
+class _ASPPModule3D(nn.Module):
+
+    def __init__(self, inplanes, planes, kernel_size, padding, dilation):
+        super(_ASPPModule3D, self).__init__()
+        self.atrous_conv = nn.Conv3d(
+            inplanes,
+            planes,
+            kernel_size=kernel_size,
+            stride=1,
+            padding=padding,
+            dilation=dilation,
+            bias=False)
 
-    def __init__(self, debug_en=False, is_training=False):
-        super(QVI_inter_Ds, self).__init__()
-        self.acc = AcFusionLayer()
-        self.fwarp = FlowReversal()
-        self.refinenet = Small_UNet_Ds(20, 8)
-        self.masknet = SmallMaskNet(38, 1)
+    def forward(self, x):
+        x = self.atrous_conv(x)
+        return F.relu(x, inplace=True)
 
-        self.staticnet = StaticMaskNet(56, 1)
-        self.lpfilter = LowPassFilter()
 
-        self.get_grad = Get_gradient()
-        self.debug_en = debug_en
-        self.is_training = is_training
+class ASPP3D(nn.Module):
 
-    def fill_flow_hole(self, ft, norm, ft_fill):
-        (N, C, H, W) = ft.shape
-        ft[norm == 0] = ft_fill[norm == 0]
+    def __init__(self, in_plane, out_plane, reduction=4):
+        super().__init__()
+        dilations = [1, 2, 4, 6]
+        mid_plane = out_plane // reduction
+        self.aspp1 = _ASPPModule3D(
+            in_plane, mid_plane, 1, padding=0, dilation=dilations[0])
+        self.aspp2 = _ASPPModule3D(
+            in_plane,
+            mid_plane, (1, 3, 3),
+            padding=(0, dilations[1], dilations[1]),
+            dilation=(1, dilations[1], dilations[1]))
+        self.aspp3 = _ASPPModule3D(
+            in_plane,
+            mid_plane, (1, 3, 3),
+            padding=(0, dilations[2], dilations[2]),
+            dilation=(1, dilations[2], dilations[2]))
+        self.aspp4 = _ASPPModule3D(
+            in_plane,
+            mid_plane, (1, 3, 3),
+            padding=(0, dilations[3], dilations[3]),
+            dilation=(1, dilations[3], dilations[3]))
+        self.conv1 = nn.Conv3d(
+            mid_plane * 4,
+            out_plane,
+            kernel_size=(1, 3, 3),
+            padding=(0, 1, 1),
+            bias=False)
 
-        ft_1 = self.lpfilter(ft.clone())
-        ft_ds = torch.nn.functional.interpolate(
-            input=ft_1,
-            size=(H // 4, W // 4),
-            mode='bilinear',
-            align_corners=False)
-        ft_up = torch.nn.functional.interpolate(
-            input=ft_ds, size=(H, W), mode='bilinear', align_corners=False)
-
-        ft[norm == 0] = ft_up[norm == 0]
-
-        return ft
+    def forward(self, x):
+        x1 = self.aspp1(x)
+        x2 = self.aspp2(x)
+        x3 = self.aspp3(x)
+        x4 = self.aspp4(x)
+        x = torch.cat((x1, x2, x3, x4), dim=1)
+        x = F.relu(self.conv1(x), inplace=True)
+        return x
 
-    def forward(self, F10_Ds, F12_Ds, F21_Ds, F23_Ds, I1_Ds, I2_Ds, I1, I2, t):
-        if F12_Ds is None or F21_Ds is None:
-            return I1
 
-        if F10_Ds is not None and F23_Ds is not None:
-            F1t_Ds, F2t_Ds = self.acc(F10_Ds, F12_Ds, F21_Ds, F23_Ds, t)
+class SELayerS(nn.Module):
 
-        else:
-            F1t_Ds = t * F12_Ds
-            F2t_Ds = (1 - t) * F21_Ds
+    def __init__(self, channel, reduction=16):
+        super(SELayerS, self).__init__()
+        channel = channel * 2  # 2 is time axis
+        self.avg_pool = nn.AdaptiveAvgPool3d((2, 1, 1))
+        self.fc = nn.Sequential(
+            nn.Linear(channel, channel // reduction, bias=False),
+            nn.ReLU(inplace=True),
+            nn.Linear(channel // reduction, channel, bias=False), nn.Sigmoid())
 
-        # Flow Reversal
-        F1t_Ds2 = F.interpolate(
-            F1t_Ds, scale_factor=1.0 / 3, mode='nearest') / 3
-        F2t_Ds2 = F.interpolate(
-            F2t_Ds, scale_factor=1.0 / 3, mode='nearest') / 3
-        Ft1_Ds2, norm1_Ds2 = self.fwarp(F1t_Ds2, F1t_Ds2)
-        Ft1_Ds2 = -Ft1_Ds2
-        Ft2_Ds2, norm2_Ds2 = self.fwarp(F2t_Ds2, F2t_Ds2)
-        Ft2_Ds2 = -Ft2_Ds2
-
-        Ft1_Ds2[norm1_Ds2 > 0] \
-            = Ft1_Ds2[norm1_Ds2 > 0] / norm1_Ds2[norm1_Ds2 > 0].clone()
-        Ft2_Ds2[norm2_Ds2 > 0] \
-            = Ft2_Ds2[norm2_Ds2 > 0] / norm2_Ds2[norm2_Ds2 > 0].clone()
-        if 1:
-            Ft1_Ds2_fill = -F1t_Ds2
-            Ft2_Ds2_fill = -F2t_Ds2
-            Ft1_Ds2 = self.fill_flow_hole(Ft1_Ds2, norm1_Ds2, Ft1_Ds2_fill)
-            Ft2_Ds2 = self.fill_flow_hole(Ft2_Ds2, norm2_Ds2, Ft2_Ds2_fill)
-
-        Ft1_Ds = F.interpolate(
-            Ft1_Ds2, size=[F1t_Ds.size(2), F1t_Ds.size(3)], mode='nearest') * 3
-        Ft2_Ds = F.interpolate(
-            Ft2_Ds2, size=[F2t_Ds.size(2), F2t_Ds.size(3)], mode='nearest') * 3
-
-        I1t_Ds = backwarp(I1_Ds, Ft1_Ds)
-        I2t_Ds = backwarp(I2_Ds, Ft2_Ds)
-
-        output_Ds, feature_Ds = self.refinenet(
-            torch.cat(
-                [I1_Ds, I2_Ds, I1t_Ds, I2t_Ds, F12_Ds, F21_Ds, Ft1_Ds, Ft2_Ds],
-                dim=1))
-
-        # Adaptive filtering
-        Ft1r_Ds = backwarp(
-            Ft1_Ds, 10 * torch.tanh(output_Ds[:, 4:6])) + output_Ds[:, :2]
-        Ft2r_Ds = backwarp(
-            Ft2_Ds, 10 * torch.tanh(output_Ds[:, 6:8])) + output_Ds[:, 2:4]
-
-        # warping and fusing
-        I1tf_Ds = backwarp(I1_Ds, Ft1r_Ds)
-        I2tf_Ds = backwarp(I2_Ds, Ft2r_Ds)
-
-        G1_Ds = self.get_grad(I1_Ds)
-        G2_Ds = self.get_grad(I2_Ds)
-        G1tf_Ds = backwarp(G1_Ds, Ft1r_Ds)
-        G2tf_Ds = backwarp(G2_Ds, Ft2r_Ds)
-
-        M_Ds = torch.sigmoid(
-            self.masknet(torch.cat([I1tf_Ds, I2tf_Ds, feature_Ds],
-                                   dim=1))).repeat(1, 3, 1, 1)
-
-        Ft1r = F.interpolate(
-            Ft1r_Ds * 2, scale_factor=2, mode='bilinear', align_corners=False)
-        Ft2r = F.interpolate(
-            Ft2r_Ds * 2, scale_factor=2, mode='bilinear', align_corners=False)
-
-        I1tf = backwarp(I1, Ft1r)
-        I2tf = backwarp(I2, Ft2r)
-
-        M = F.interpolate(
-            M_Ds, scale_factor=2, mode='bilinear', align_corners=False)
-
-        # fuse
-        It_warp = ((1 - t) * M * I1tf + t * (1 - M) * I2tf) \
-            / ((1 - t) * M + t * (1 - M)).clone()
-
-        # static blending
-        It_static = (1 - t) * I1 + t * I2
-        tmp = torch.cat((I1tf_Ds, I2tf_Ds, G1tf_Ds, G2tf_Ds, I1_Ds, I2_Ds,
-                         G1_Ds, G2_Ds, feature_Ds),
-                        dim=1)
-        M_static_Ds = self.staticnet(tmp)
-        M_static_dilate = tensor_erode(M_static_Ds)
-        M_static_dilate = tensor_erode(M_static_dilate)
-        M_static = F.interpolate(
-            M_static_dilate,
-            scale_factor=2,
-            mode='bilinear',
-            align_corners=False)
-
-        It_warp = (1 - M_static) * It_warp + M_static * It_static
-
-        if self.is_training:
-            return It_warp, Ft1r, Ft2r
-        else:
-            if self.debug_en:
-                return It_warp, M, M_static, I1tf, I2tf, Ft1r, Ft2r
-            else:
-                return It_warp
-
-
-class QVI_inter(nn.Module):
-    """Given flow, implement Quadratic Video Interpolation"""
-
-    def __init__(self, debug_en=False, is_training=False):
-        super(QVI_inter, self).__init__()
-        self.acc = AcFusionLayer()
-        self.fwarp = FlowReversal()
-        self.refinenet = Small_UNet_Ds(20, 8)
-        self.masknet = SmallMaskNet(38, 1)
-
-        self.staticnet = StaticMaskNet(56, 1)
-        self.lpfilter = LowPassFilter()
-
-        self.get_grad = Get_gradient()
-        self.debug_en = debug_en
-        self.is_training = is_training
-
-    def fill_flow_hole(self, ft, norm, ft_fill):
-        (N, C, H, W) = ft.shape
-        ft[norm == 0] = ft_fill[norm == 0]
-
-        ft_1 = self.lpfilter(ft.clone())
-        ft_ds = torch.nn.functional.interpolate(
-            input=ft_1,
-            size=(H // 4, W // 4),
-            mode='bilinear',
-            align_corners=False)
-        ft_up = torch.nn.functional.interpolate(
-            input=ft_ds, size=(H, W), mode='bilinear', align_corners=False)
+    def forward(self, x):
+        b, c, _, _, _ = x.size()
+        y = self.avg_pool(x).view(b, 2 * c)
+        y = self.fc(y).view(b, c, 2, 1, 1)
+        return x * y.expand_as(x)
+
+
+class SEBasicBlock(nn.Module):
+    expansion = 1
+
+    # https://github.com/moskomule/senet.pytorch
+
+    def __init__(self,
+                 inplanes,
+                 planes,
+                 stride=1,
+                 downsample=None,
+                 reduction=8):
+        super(SEBasicBlock, self).__init__()
+        self.conv1 = nn.Conv3d(
+            inplanes, planes, kernel_size=(1, 3, 3), padding=(0, 1, 1))
+        self.in1 = nn.InstanceNorm3d(planes)
+        self.relu = nn.ReLU(inplace=True)
+        self.conv2 = nn.Conv3d(
+            planes, planes, kernel_size=(1, 3, 3), padding=(0, 1, 1))
+        self.in2 = nn.InstanceNorm3d(planes)
+        self.ses = SELayerS(planes, reduction)
+        self.in3 = nn.InstanceNorm3d(planes)
+        self.downsample = downsample
+        self.stride = stride
 
-        ft[norm == 0] = ft_up[norm == 0]
+    def forward(self, x):
+        residual = x
+        out = self.conv1(x)
+        out = self.in1(out)
+        out = self.relu(out)
+
+        out = self.conv2(out)
+        out = self.in2(out)
+
+        out = self.ses(out)
+        out = self.in3(out)
+
+        if self.downsample is not None:
+            residual = self.downsample(x)
+
+        out += residual
+        out = self.relu(out)
+
+        return out
+
+
+class SAM(nn.Module):
+    """
+    Spatio-temporal aggregation module (SAM)
+    """
+
+    def __init__(self, indim, outdim=None, repeat=0, norm=False):
+        super(SAM, self).__init__()
+        self.indim = indim
+        self.repeat = repeat
+        if outdim is None:
+            outdim = indim
+        if repeat > 0:
+            self.se_block = self.seRepeat(repeat)
+        self.conv1 = ASPP3D(indim, outdim, reduction=4)  # norm is 4
+
+        # self.conv2 = nn.Conv2d(outdim, outdim, kernel_size=3, padding=1)
+        self.non_local = NONLocalBlock3D(indim, bn_layer=False)
+
+    def seRepeat(self, repeat=2):
+        return nn.Sequential(*nn.ModuleList(
+            [SEBasicBlock(self.indim, self.indim) for _ in range(repeat)]))
 
-        return ft
+    def forward(self, x):
+        x = self.non_local(x)
 
-    def forward(self, F10, F12, F21, F23, I1, I2, t):
-        if F12 is None or F21 is None:
-            return I1
+        if self.repeat > 0:
+            x = self.se_block(x)
+        r = x
+        x = self.conv1(x) + r
+        return x
 
-        if F10 is not None and F23 is not None:
-            F1t, F2t = self.acc(F10, F12, F21, F23, t)
 
-        else:
-            F1t = t * F12
-            F2t = (1 - t) * F21
+class MemCrompress(nn.Module):
 
-        # Flow Reversal
-        F1t_Ds = F.interpolate(F1t, scale_factor=1.0 / 3, mode='nearest') / 3
-        F2t_Ds = F.interpolate(F2t, scale_factor=1.0 / 3, mode='nearest') / 3
-        Ft1_Ds, norm1_Ds = self.fwarp(F1t_Ds, F1t_Ds)
-        Ft1_Ds = -Ft1_Ds
-        Ft2_Ds, norm2_Ds = self.fwarp(F2t_Ds, F2t_Ds)
-        Ft2_Ds = -Ft2_Ds
-
-        Ft1_Ds[norm1_Ds > 0] \
-            = Ft1_Ds[norm1_Ds > 0] / norm1_Ds[norm1_Ds > 0].clone()
-        Ft2_Ds[norm2_Ds > 0] \
-            = Ft2_Ds[norm2_Ds > 0] / norm2_Ds[norm2_Ds > 0].clone()
-        if 1:
-            Ft1_fill = -F1t_Ds
-            Ft2_fill = -F2t_Ds
-            Ft1_Ds = self.fill_flow_hole(Ft1_Ds, norm1_Ds, Ft1_fill)
-            Ft2_Ds = self.fill_flow_hole(Ft2_Ds, norm2_Ds, Ft2_fill)
-
-        Ft1 = F.interpolate(
-            Ft1_Ds, size=[F1t.size(2), F1t.size(3)], mode='nearest') * 3
-        Ft2 = F.interpolate(
-            Ft2_Ds, size=[F2t.size(2), F2t.size(3)], mode='nearest') * 3
-
-        I1t = backwarp(I1, Ft1)
-        I2t = backwarp(I2, Ft2)
-
-        output, feature = self.refinenet(
-            torch.cat([I1, I2, I1t, I2t, F12, F21, Ft1, Ft2], dim=1))
-
-        # Adaptive filtering
-        Ft1r = backwarp(Ft1, 10 * torch.tanh(output[:, 4:6])) + output[:, :2]
-        Ft2r = backwarp(Ft2, 10 * torch.tanh(output[:, 6:8])) + output[:, 2:4]
-
-        # warping and fusing
-        I1tf = backwarp(I1, Ft1r)
-        I2tf = backwarp(I2, Ft2r)
-
-        M = torch.sigmoid(
-            self.masknet(torch.cat([I1tf, I2tf, feature],
-                                   dim=1))).repeat(1, 3, 1, 1)
-
-        It_warp = ((1 - t) * M * I1tf + t * (1 - M) * I2tf) \
-            / ((1 - t) * M + t * (1 - M)).clone()
-
-        G1 = self.get_grad(I1)
-        G2 = self.get_grad(I2)
-        G1tf = backwarp(G1, Ft1r)
-        G2tf = backwarp(G2, Ft2r)
-
-        # static blending
-        It_static = (1 - t) * I1 + t * I2
-        M_static = self.staticnet(
-            torch.cat([I1tf, I2tf, G1tf, G2tf, I1, I2, G1, G2, feature],
-                      dim=1))
-        M_static_dilate = tensor_erode(M_static)
-        M_static_dilate = tensor_erode(M_static_dilate)
-        It_warp = (1 - M_static_dilate) * It_warp + M_static_dilate * It_static
+    def __init__(self, repeat=0, norm=True):
+        super().__init__()
 
-        if self.is_training:
-            return It_warp, Ft1r, Ft2r
-        else:
-            if self.debug_en:
-                return It_warp, M, M_static, I1tf, I2tf, Ft1r, Ft2r
-            else:
-                return It_warp
-
-
-class InterpNetDs(nn.Module):
-
-    def __init__(self, debug_en=False, is_training=False):
-        super(InterpNetDs, self).__init__()
-        self.ifnet = IFNet()
-        self.internet = QVI_inter_Ds(
-            debug_en=debug_en, is_training=is_training)
-
-    def forward(self,
-                img1,
-                img2,
-                F10_up,
-                F12_up,
-                F21_up,
-                F23_up,
-                UHD=2,
-                timestep=0.5):
-        F12, F21 = self.ifnet(img1, img2, F12_up, F21_up, UHD)
-        It_warp = self.internet(F10_up, F12, F21, F23_up, img1, img2, timestep)
-
-        return It_warp
-
-
-class InterpNet(nn.Module):
-
-    def __init__(self, debug_en=False, is_training=False):
-        super(InterpNet, self).__init__()
-        self.ifnet = IFNet()
-        self.internet = QVI_inter(debug_en=debug_en, is_training=is_training)
-
-    def forward(self,
-                img1,
-                img2,
-                F10_up,
-                F12_up,
-                F21_up,
-                F23_up,
-                UHD=2,
-                timestep=0.5):
-        F12, F21 = self.ifnet(img1, img2, F12_up, F21_up, UHD)
-        It_warp = self.internet(F10_up, F12, F21, F23_up, img1, img2, timestep)
+        self.key_encoder = SAM(64, 64, repeat=repeat, norm=norm)
+        self.value_encoder = SAM(512, 512, repeat=repeat, norm=norm)
 
-        return It_warp
+        self.compress_key = nn.Conv3d(
+            64, 64, kernel_size=(2, 3, 3), padding=(0, 1, 1))
+        self.compress_value = nn.Conv3d(
+            512, 512, kernel_size=(2, 3, 3), padding=(0, 1, 1))
+        # self.temporal_shuffle = temporal_shuffle
+
+    def forward(self, key, value):
+        # key    N, C, T, H, W      [4, 64, 2, 24, 24]
+        # value  N, O, C, T, H, W
+        # return
+        # key    N, C, 1, H, W
+        # value  N, O, C, 1, H, W
+        N, O, C, T, H, W = value.shape
+        value = value.flatten(
+            start_dim=0, end_dim=1)  # N*O, C, T, H, W [8, 512, 2, 24, 24]
+
+        k = self.compress_key(self.key_encoder(key))
+        v = self.compress_value(self.value_encoder(value))
+        v = v.view(N, O, C, 1, H, W)
+        return k, v
```

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/interp_model/transformer_layers.py` & `modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/interp_model/transformer_layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/utils/scene_change_detection.py` & `modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/utils/scene_change_detection.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_frame_interpolation/utils/utils.py` & `modelscope-1.5.0/modelscope/models/cv/video_frame_interpolation/utils/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_human_matting/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/video_human_matting/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_human_matting/model.py` & `modelscope-1.5.0/modelscope/models/cv/video_human_matting/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_human_matting/models/decoder.py` & `modelscope-1.5.0/modelscope/models/cv/video_human_matting/models/decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_human_matting/models/deep_guided_filter.py` & `modelscope-1.5.0/modelscope/models/cv/video_human_matting/models/deep_guided_filter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_human_matting/models/effv2.py` & `modelscope-1.5.0/modelscope/models/cv/video_human_matting/models/effv2.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_human_matting/models/lraspp.py` & `modelscope-1.5.0/modelscope/models/cv/video_human_matting/models/lraspp.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_human_matting/models/matting.py` & `modelscope-1.5.0/modelscope/models/cv/video_human_matting/models/matting.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_inpainting/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/video_inpainting/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_inpainting/inpainting.py` & `modelscope-1.5.0/modelscope/models/cv/video_inpainting/inpainting.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_inpainting/inpainting_model.py` & `modelscope-1.5.0/modelscope/models/cv/video_inpainting/inpainting_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/head/kernel_frame_iter_head.py` & `modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/head/kernel_frame_iter_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/head/kernel_head.py` & `modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/head/kernel_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/head/kernel_iter_head.py` & `modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/head/kernel_iter_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/head/kernel_update_head.py` & `modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/head/kernel_update_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/head/kernel_updator.py` & `modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/head/kernel_updator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/neck/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/neck/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/neck/msdeformattn_decoder.py` & `modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/neck/msdeformattn_decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/track/kernel_update_head.py` & `modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/track/kernel_update_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py` & `modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/utils.py` & `modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_instance_segmentation/video_knet.py` & `modelscope-1.5.0/modelscope/models/cv/video_instance_segmentation/video_knet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/models/common.py` & `modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/models/common.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/models/decode.py` & `modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/models/decode.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/models/model.py` & `modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/models/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/models/yolo.py` & `modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/models/yolo.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/tracker/basetrack.py` & `modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/tracker/basetrack.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/tracker/matching.py` & `modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/tracker/matching.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/tracker/multitracker.py` & `modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/tracker/multitracker.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/utils/image.py` & `modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/utils/image.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/utils/kalman_filter.py` & `modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/utils/kalman_filter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/utils/utils.py` & `modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/utils/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_multi_object_tracking/utils/visualization.py` & `modelscope-1.5.0/modelscope/models/cv/video_multi_object_tracking/utils/visualization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_object_segmentation/aggregate.py` & `modelscope-1.5.0/modelscope/models/cv/video_object_segmentation/aggregate.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_object_segmentation/cbam.py` & `modelscope-1.5.0/modelscope/models/cv/video_object_segmentation/cbam.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_object_segmentation/eval_network.py` & `modelscope-1.5.0/modelscope/models/cv/video_object_segmentation/eval_network.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_object_segmentation/inference_core.py` & `modelscope-1.5.0/modelscope/models/cv/video_object_segmentation/inference_core.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_object_segmentation/inference_memory_bank.py` & `modelscope-1.5.0/modelscope/models/cv/video_object_segmentation/inference_memory_bank.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_object_segmentation/mod_resnet.py` & `modelscope-1.5.0/modelscope/models/cv/video_object_segmentation/mod_resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_object_segmentation/model.py` & `modelscope-1.5.0/modelscope/models/cv/video_object_segmentation/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_object_segmentation/modules.py` & `modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/model_vlpt.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,523 +1,443 @@
-# Adopted from https://github.com/Limingxing00/RDE-VOS-CVPR2022
-# under MIT License
+# ------------------------------------------------------------------------------
+# Part of implementation is adopted from ViLT,
+# made publicly available under the Apache License 2.0 at https://github.com/dandelin/ViLT.
+# ------------------------------------------------------------------------------
+
+import math
+import os
+import sys
 
 import torch
 import torch.nn as nn
-import torch.nn.functional as F
-from torchvision import models
 
-from modelscope.models.cv.video_object_segmentation import cbam, mod_resnet
+BatchNorm2d = nn.BatchNorm2d
 
 
-class ResBlock(nn.Module):
+def constant_init(module, constant, bias=0):
+    nn.init.constant_(module.weight, constant)
+    if hasattr(module, 'bias'):
+        nn.init.constant_(module.bias, bias)
 
-    def __init__(self, indim, outdim=None):
-        super(ResBlock, self).__init__()
-        if outdim is None:
-            outdim = indim
-        if indim == outdim:
-            self.downsample = None
-        else:
-            self.downsample = nn.Conv2d(
-                indim, outdim, kernel_size=3, padding=1)
-
-        self.conv1 = nn.Conv2d(indim, outdim, kernel_size=3, padding=1)
-        self.conv2 = nn.Conv2d(outdim, outdim, kernel_size=3, padding=1)
-
-    def forward(self, x):
-        r = self.conv1(F.relu(x))
-        r = self.conv2(F.relu(r))
-
-        if self.downsample is not None:
-            x = self.downsample(x)
-
-        return x + r
 
+def conv3x3(in_planes, out_planes, stride=1):
+    """3x3 convolution with padding"""
+    return nn.Conv2d(
+        in_planes,
+        out_planes,
+        kernel_size=3,
+        stride=stride,
+        padding=1,
+        bias=False)
 
-class FeatureFusionBlock(nn.Module):
 
-    def __init__(self, indim, outdim):
-        super().__init__()
-
-        self.block1 = ResBlock(indim, outdim)
-        self.attention = cbam.CBAM(outdim)
-        self.block2 = ResBlock(outdim, outdim)
-
-    def forward(self, x, f16):
-        x = torch.cat([x, f16], 1)
-        x = self.block1(x)
-        r = self.attention(x)
-        x = self.block2(x + r)
+class BasicBlock(nn.Module):
+    expansion = 1
 
-        return x
+    def __init__(self, inplanes, planes, stride=1, downsample=None, dcn=None):
+        super(BasicBlock, self).__init__()
+        self.with_dcn = dcn is not None
+        self.conv1 = conv3x3(inplanes, planes, stride)
+        self.bn1 = BatchNorm2d(planes)
+        self.relu = nn.ReLU(inplace=True)
+        self.with_modulated_dcn = False
+        if self.with_dcn:
+            fallback_on_stride = dcn.get('fallback_on_stride', False)
+            self.with_modulated_dcn = dcn.get('modulated', False)
+        # self.conv2 = conv3x3(planes, planes)
+        if not self.with_dcn or fallback_on_stride:
+            self.conv2 = nn.Conv2d(
+                planes, planes, kernel_size=3, padding=1, bias=False)
+        else:
+            deformable_groups = dcn.get('deformable_groups', 1)
+            if not self.with_modulated_dcn:
+                from assets.ops.dcn import DeformConv
+                conv_op = DeformConv
+                offset_channels = 18
+            else:
+                from assets.ops.dcn import ModulatedDeformConv
+                conv_op = ModulatedDeformConv
+                offset_channels = 27
+            self.conv2_offset = nn.Conv2d(
+                planes,
+                deformable_groups * offset_channels,
+                kernel_size=3,
+                padding=1)
+            self.conv2 = conv_op(
+                planes,
+                planes,
+                kernel_size=3,
+                padding=1,
+                deformable_groups=deformable_groups,
+                bias=False)
+        self.bn2 = BatchNorm2d(planes)
+        self.downsample = downsample
+        self.stride = stride
 
+    def forward(self, x):
+        residual = x
 
-# Single object version, used only in static image pretraining
-# See model.py (load_network) for the modification procedure
-class ValueEncoderSO(nn.Module):
+        out = self.conv1(x)
+        out = self.bn1(out)
+        out = self.relu(out)
 
-    def __init__(self):
-        super().__init__()
+        # out = self.conv2(out)
+        if not self.with_dcn:
+            out = self.conv2(out)
+        elif self.with_modulated_dcn:
+            offset_mask = self.conv2_offset(out)
+            offset = offset_mask[:, :18, :, :]
+            mask = offset_mask[:, -9:, :, :].sigmoid()
+            out = self.conv2(out, offset, mask)
+        else:
+            offset = self.conv2_offset(out)
+            out = self.conv2(out, offset)
+        out = self.bn2(out)
 
-        resnet = mod_resnet.resnet18(pretrained=False, extra_chan=1)
-        self.conv1 = resnet.conv1
-        self.bn1 = resnet.bn1
-        self.relu = resnet.relu  # 1/2, 64
-        self.maxpool = resnet.maxpool
+        if self.downsample is not None:
+            residual = self.downsample(x)
 
-        self.layer1 = resnet.layer1  # 1/4, 64
-        self.layer2 = resnet.layer2  # 1/8, 128
-        self.layer3 = resnet.layer3  # 1/16, 256
+        out += residual
+        out = self.relu(out)
 
-        self.fuser = FeatureFusionBlock(1024 + 256, 512)
+        return out
 
-    def forward(self, image, key_f16, mask):
-        # key_f16 is the feature from the key encoder
 
-        f = torch.cat([image, mask], 1)
+class Bottleneck(nn.Module):
+    expansion = 4
 
-        x = self.conv1(f)
-        x = self.bn1(x)
-        x = self.relu(x)  # 1/2, 64
-        x = self.maxpool(x)  # 1/4, 64
-        x = self.layer1(x)  # 1/4, 64
-        x = self.layer2(x)  # 1/8, 128
-        x = self.layer3(x)  # 1/16, 256
+    def __init__(self, inplanes, planes, stride=1, downsample=None, dcn=None):
+        super(Bottleneck, self).__init__()
+        self.with_dcn = dcn is not None
+        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
+        self.bn1 = BatchNorm2d(planes)
+        fallback_on_stride = False
+        self.with_modulated_dcn = False
+        if self.with_dcn:
+            fallback_on_stride = dcn.get('fallback_on_stride', False)
+            self.with_modulated_dcn = dcn.get('modulated', False)
+        if not self.with_dcn or fallback_on_stride:
+            self.conv2 = nn.Conv2d(
+                planes,
+                planes,
+                kernel_size=3,
+                stride=stride,
+                padding=1,
+                bias=False)
+        else:
+            deformable_groups = dcn.get('deformable_groups', 1)
+            if not self.with_modulated_dcn:
+                from assets.ops.dcn import DeformConv
+                conv_op = DeformConv
+                offset_channels = 18
+            else:
+                from assets.ops.dcn import ModulatedDeformConv
+                conv_op = ModulatedDeformConv
+                offset_channels = 27
+            self.conv2_offset = nn.Conv2d(
+                planes,
+                deformable_groups * offset_channels,
+                kernel_size=3,
+                padding=1)
+            self.conv2 = conv_op(
+                planes,
+                planes,
+                kernel_size=3,
+                padding=1,
+                stride=stride,
+                deformable_groups=deformable_groups,
+                bias=False)
+        self.bn2 = BatchNorm2d(planes)
+        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)
+        self.bn3 = BatchNorm2d(planes * 4)
+        self.relu = nn.ReLU(inplace=True)
+        self.downsample = downsample
+        self.stride = stride
+        self.dcn = dcn
+        self.with_dcn = dcn is not None
 
-        x = self.fuser(x, key_f16)
+    def forward(self, x):
+        residual = x
 
-        return x
+        out = self.conv1(x)
+        out = self.bn1(out)
+        out = self.relu(out)
 
+        # out = self.conv2(out)
+        if not self.with_dcn:
+            out = self.conv2(out)
+        elif self.with_modulated_dcn:
+            offset_mask = self.conv2_offset(out)
+            offset = offset_mask[:, :18, :, :]
+            mask = offset_mask[:, -9:, :, :].sigmoid()
+            out = self.conv2(out, offset, mask)
+        else:
+            offset = self.conv2_offset(out)
+            out = self.conv2(out, offset)
+        out = self.bn2(out)
+        out = self.relu(out)
 
-# Multiple objects version, used in other times
-class ValueEncoder(nn.Module):
+        out = self.conv3(out)
+        out = self.bn3(out)
 
-    def __init__(self):
-        super().__init__()
+        if self.downsample is not None:
+            residual = self.downsample(x)
 
-        resnet = mod_resnet.resnet18(pretrained=False, extra_chan=2)
-        self.conv1 = resnet.conv1
-        self.bn1 = resnet.bn1
-        self.relu = resnet.relu  # 1/2, 64
-        self.maxpool = resnet.maxpool
+        out += residual
+        out = self.relu(out)
 
-        self.layer1 = resnet.layer1  # 1/4, 64
-        self.layer2 = resnet.layer2  # 1/8, 128
-        self.layer3 = resnet.layer3  # 1/16, 256
+        return out
 
-        self.fuser = FeatureFusionBlock(1024 + 256, 512)
 
-    def forward(self, image, key_f16, mask, other_masks):
-        # key_f16 is the feature from the key encoder
+class ResNet(nn.Module):
 
-        f = torch.cat([image, mask, other_masks], 1)
+    def __init__(self,
+                 block,
+                 layers,
+                 num_classes=1000,
+                 dcn=None,
+                 stage_with_dcn=(False, False, False, False)):
+        self.dcn = dcn
+        self.stage_with_dcn = stage_with_dcn
+        self.inplanes = 64
+        super(ResNet, self).__init__()
+        self.conv1 = nn.Conv2d(
+            3, 64, kernel_size=7, stride=2, padding=3, bias=False)
+        self.bn1 = BatchNorm2d(64)
+        self.relu = nn.ReLU(inplace=True)
+        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
+        self.layer1 = self._make_layer(block, 64, layers[0])
+        self.layer2 = self._make_layer(
+            block, 128, layers[1], stride=2, dcn=dcn)
+        self.layer3 = self._make_layer(
+            block, 256, layers[2], stride=2, dcn=dcn)
+        self.layer4 = self._make_layer(
+            block, 512, layers[3], stride=2, dcn=dcn)
+        # self.avgpool = nn.AvgPool2d(7, stride=1)
+        # self.fc = nn.Linear(512 * block.expansion, num_classes)
+
+        # self.smooth = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=1)
+
+        for m in self.modules():
+            if isinstance(m, nn.Conv2d):
+                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
+                m.weight.data.normal_(0, math.sqrt(2. / n))
+            elif isinstance(m, BatchNorm2d):
+                m.weight.data.fill_(1)
+                m.bias.data.zero_()
+        if self.dcn is not None:
+            for m in self.modules():
+                if isinstance(m, Bottleneck) or isinstance(m, BasicBlock):
+                    if hasattr(m, 'conv2_offset'):
+                        constant_init(m.conv2_offset, 0)
+
+    def _make_layer(self, block, planes, blocks, stride=1, dcn=None):
+        downsample = None
+        if stride != 1 or self.inplanes != planes * block.expansion:
+            downsample = nn.Sequential(
+                nn.Conv2d(
+                    self.inplanes,
+                    planes * block.expansion,
+                    kernel_size=1,
+                    stride=stride,
+                    bias=False),
+                BatchNorm2d(planes * block.expansion),
+            )
+
+        layers = []
+        layers.append(
+            block(self.inplanes, planes, stride, downsample, dcn=dcn))
+        self.inplanes = planes * block.expansion
+        for i in range(1, blocks):
+            layers.append(block(self.inplanes, planes, dcn=dcn))
 
-        x = self.conv1(f)
-        x = self.bn1(x)
-        x = self.relu(x)  # 1/2, 64
-        x = self.maxpool(x)  # 1/4, 64
-        x = self.layer1(x)  # 1/4, 64
-        x = self.layer2(x)  # 1/8, 128
-        x = self.layer3(x)  # 1/16, 256
-        # x = torch.cat([x, x], dim=1)
-        x = self.fuser(x, key_f16)
-
-        return x
-
-
-# from retrying import retry
-# @retry(stop_max_attempt_number=5)
-class KeyEncoder(nn.Module):
-
-    def __init__(self):
-        super().__init__()
-        resnet = models.resnet50(pretrained=False)
-        # if torch.distributed.get_rank() == 0:
-        #     torch.distributed.barrier()
-        self.conv1 = resnet.conv1
-        self.bn1 = resnet.bn1
-        self.relu = resnet.relu  # 1/2, 64
-        self.maxpool = resnet.maxpool
-
-        self.res2 = resnet.layer1  # 1/4, 256
-        self.layer2 = resnet.layer2  # 1/8, 512
-        self.layer3 = resnet.layer3  # 1/16, 1024
+        return nn.Sequential(*layers)
 
-    def forward(self, f):
-        x = self.conv1(f)
+    def forward(self, x):
+        x = self.conv1(x)
         x = self.bn1(x)
-        x = self.relu(x)  # 1/2, 64
-        x = self.maxpool(x)  # 1/4, 64
-        f4 = self.res2(x)  # 1/4, 256
-        f8 = self.layer2(f4)  # 1/8, 512
-        f16 = self.layer3(f8)  # 1/16, 1024
-
-        return f16, f8, f4
-
-
-class UpsampleBlock(nn.Module):
-
-    def __init__(self, skip_c, up_c, out_c, scale_factor=2):
-        super().__init__()
-        self.skip_conv = nn.Conv2d(skip_c, up_c, kernel_size=3, padding=1)
-        self.out_conv = ResBlock(up_c, out_c)
-        self.scale_factor = scale_factor
-
-    def forward(self, skip_f, up_f):
-        x = self.skip_conv(skip_f)
-        x = x + F.interpolate(
-            up_f,
-            scale_factor=self.scale_factor,
-            mode='bilinear',
-            align_corners=False)
-        x = self.out_conv(x)
-        return x
-
-
-class KeyProjection(nn.Module):
-
-    def __init__(self, indim, keydim):
-        super().__init__()
-        self.key_proj = nn.Conv2d(indim, keydim, kernel_size=3, padding=1)
+        x = self.relu(x)
+        x = self.maxpool(x)
 
-        nn.init.orthogonal_(self.key_proj.weight.data)
-        nn.init.zeros_(self.key_proj.bias.data)
+        x2 = self.layer1(x)
+        x3 = self.layer2(x2)
+        x4 = self.layer3(x3)
+        x5 = self.layer4(x4)
 
-    def forward(self, x):
-        return self.key_proj(x)
+        return x2, x3, x4, x5
 
 
-class _NonLocalBlockND(nn.Module):
+class SegDetector(nn.Module):
 
     def __init__(self,
-                 in_channels,
-                 inter_channels=None,
-                 dimension=3,
-                 sub_sample=True,
-                 bn_layer=False):
-        super(_NonLocalBlockND, self).__init__()
-
-        assert dimension in [1, 2, 3]
-
-        self.dimension = dimension
-        self.sub_sample = sub_sample
-
-        self.in_channels = in_channels
-        self.inter_channels = inter_channels
-
-        if self.inter_channels is None:
-            self.inter_channels = in_channels // 2
-            if self.inter_channels == 0:
-                self.inter_channels = 1
-
-        if dimension == 3:
-            conv_nd = nn.Conv3d
-            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))
-            bn = nn.InstanceNorm3d
-        elif dimension == 2:
-            conv_nd = nn.Conv2d
-            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))
-            bn = nn.BatchNorm2d
-        else:
-            conv_nd = nn.Conv1d
-            max_pool_layer = nn.MaxPool1d(kernel_size=(2))
-            bn = nn.BatchNorm1d
-
-        self.g = conv_nd(
-            in_channels=self.in_channels,
-            out_channels=self.inter_channels,
-            kernel_size=1,
-            stride=1,
-            padding=0)
-
-        if bn_layer:
-            self.W = nn.Sequential(
-                conv_nd(
-                    in_channels=self.inter_channels,
-                    out_channels=self.in_channels,
-                    kernel_size=1,
-                    stride=1,
-                    padding=0), bn(self.in_channels))
-            # nn.init.constant_(self.W[1].weight, 0)
-            # nn.init.constant_(self.W[1].bias, 0)
-        else:
-            self.W = conv_nd(
-                in_channels=self.inter_channels,
-                out_channels=self.in_channels,
-                kernel_size=1,
-                stride=1,
-                padding=0)
-            # nn.init.constant_(self.W.weight, 0)
-            # nn.init.constant_(self.W.bias, 0)
-
-        self.theta = conv_nd(
-            in_channels=self.in_channels,
-            out_channels=self.inter_channels,
-            kernel_size=1,
-            stride=1,
-            padding=0)
-
-        self.phi = conv_nd(
-            in_channels=self.in_channels,
-            out_channels=self.inter_channels,
-            kernel_size=1,
-            stride=1,
-            padding=0)
-
-        if sub_sample:
-            self.g = nn.Sequential(self.g, max_pool_layer)
-            self.phi = nn.Sequential(self.phi, max_pool_layer)
-
-    def forward(self, x):
+                 in_channels=[64, 128, 256, 512],
+                 inner_channels=256,
+                 k=10,
+                 bias=False,
+                 adaptive=False,
+                 smooth=False,
+                 serial=False,
+                 *args,
+                 **kwargs):
         '''
-        :param x: (b, c, t, h, w)
-        :return:
+        bias: Whether conv layers have bias or not.
+        adaptive: Whether to use adaptive threshold training or not.
+        smooth: If true, use bilinear instead of deconv.
+        serial: If true, thresh prediction will combine segmentation result as input.
         '''
-
-        batch_size = x.size(0)
-
-        g_x = self.g(x).view(batch_size, self.inter_channels, -1)
-        g_x = g_x.permute(0, 2, 1)
-
-        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)
-        theta_x = theta_x.permute(0, 2, 1)
-        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)
-        f = torch.matmul(theta_x, phi_x)
-        N = f.size(-1)
-        f_div_C = f / N
-
-        y = torch.matmul(f_div_C, g_x)
-        y = y.permute(0, 2, 1).contiguous()
-        y = y.view(batch_size, self.inter_channels, *x.size()[2:])
-        W_y = self.W(y)
-        z = W_y + x
-
-        return z
-
-
-class NONLocalBlock1D(_NonLocalBlockND):
-
-    def __init__(self,
-                 in_channels,
-                 inter_channels=None,
-                 sub_sample=True,
-                 bn_layer=True):
-        super(NONLocalBlock1D, self).__init__(
-            in_channels,
-            inter_channels=inter_channels,
-            dimension=1,
-            sub_sample=sub_sample,
-            bn_layer=bn_layer)
-
-
-class NONLocalBlock2D(_NonLocalBlockND):
-
-    def __init__(self,
-                 in_channels,
-                 inter_channels=None,
-                 sub_sample=True,
-                 bn_layer=False):
-        super(NONLocalBlock2D, self).__init__(
-            in_channels,
-            inter_channels=inter_channels,
-            dimension=2,
-            sub_sample=sub_sample,
-            bn_layer=bn_layer)
-
-
-class NONLocalBlock3D(_NonLocalBlockND):
-
-    def __init__(self,
-                 in_channels,
-                 inter_channels=None,
-                 sub_sample=True,
-                 bn_layer=True):
-        super(NONLocalBlock3D, self).__init__(
-            in_channels,
-            inter_channels=inter_channels,
-            dimension=3,
-            sub_sample=sub_sample,
-            bn_layer=bn_layer)
-
-
-class _ASPPModule3D(nn.Module):
-
-    def __init__(self, inplanes, planes, kernel_size, padding, dilation):
-        super(_ASPPModule3D, self).__init__()
-        self.atrous_conv = nn.Conv3d(
-            inplanes,
-            planes,
-            kernel_size=kernel_size,
-            stride=1,
-            padding=padding,
-            dilation=dilation,
-            bias=False)
-
-    def forward(self, x):
-        x = self.atrous_conv(x)
-        return F.relu(x, inplace=True)
-
-
-class ASPP3D(nn.Module):
-
-    def __init__(self, in_plane, out_plane, reduction=4):
-        super().__init__()
-        dilations = [1, 2, 4, 6]
-        mid_plane = out_plane // reduction
-        self.aspp1 = _ASPPModule3D(
-            in_plane, mid_plane, 1, padding=0, dilation=dilations[0])
-        self.aspp2 = _ASPPModule3D(
-            in_plane,
-            mid_plane, (1, 3, 3),
-            padding=(0, dilations[1], dilations[1]),
-            dilation=(1, dilations[1], dilations[1]))
-        self.aspp3 = _ASPPModule3D(
-            in_plane,
-            mid_plane, (1, 3, 3),
-            padding=(0, dilations[2], dilations[2]),
-            dilation=(1, dilations[2], dilations[2]))
-        self.aspp4 = _ASPPModule3D(
-            in_plane,
-            mid_plane, (1, 3, 3),
-            padding=(0, dilations[3], dilations[3]),
-            dilation=(1, dilations[3], dilations[3]))
-        self.conv1 = nn.Conv3d(
-            mid_plane * 4,
-            out_plane,
-            kernel_size=(1, 3, 3),
-            padding=(0, 1, 1),
-            bias=False)
-
-    def forward(self, x):
-        x1 = self.aspp1(x)
-        x2 = self.aspp2(x)
-        x3 = self.aspp3(x)
-        x4 = self.aspp4(x)
-        x = torch.cat((x1, x2, x3, x4), dim=1)
-        x = F.relu(self.conv1(x), inplace=True)
-        return x
-
-
-class SELayerS(nn.Module):
-
-    def __init__(self, channel, reduction=16):
-        super(SELayerS, self).__init__()
-        channel = channel * 2  # 2 is time axis
-        self.avg_pool = nn.AdaptiveAvgPool3d((2, 1, 1))
-        self.fc = nn.Sequential(
-            nn.Linear(channel, channel // reduction, bias=False),
+        super(SegDetector, self).__init__()
+        self.k = k
+        self.serial = serial
+        self.up5 = nn.Upsample(scale_factor=2, mode='nearest')
+        self.up4 = nn.Upsample(scale_factor=2, mode='nearest')
+        self.up3 = nn.Upsample(scale_factor=2, mode='nearest')
+
+        self.in5 = nn.Conv2d(in_channels[-1], inner_channels, 1, bias=bias)
+        self.in4 = nn.Conv2d(in_channels[-2], inner_channels, 1, bias=bias)
+        self.in3 = nn.Conv2d(in_channels[-3], inner_channels, 1, bias=bias)
+        self.in2 = nn.Conv2d(in_channels[-4], inner_channels, 1, bias=bias)
+
+        self.out5 = nn.Sequential(
+            nn.Conv2d(
+                inner_channels, inner_channels // 4, 3, padding=1, bias=bias),
+            nn.Upsample(scale_factor=8, mode='nearest'))
+        self.out4 = nn.Sequential(
+            nn.Conv2d(
+                inner_channels, inner_channels // 4, 3, padding=1, bias=bias),
+            nn.Upsample(scale_factor=4, mode='nearest'))
+        self.out3 = nn.Sequential(
+            nn.Conv2d(
+                inner_channels, inner_channels // 4, 3, padding=1, bias=bias),
+            nn.Upsample(scale_factor=2, mode='nearest'))
+        self.out2 = nn.Conv2d(
+            inner_channels, inner_channels // 4, 3, padding=1, bias=bias)
+
+        self.binarize = nn.Sequential(
+            nn.Conv2d(
+                inner_channels, inner_channels // 4, 3, padding=1, bias=bias),
+            BatchNorm2d(inner_channels // 4), nn.ReLU(inplace=True),
+            nn.ConvTranspose2d(inner_channels // 4, inner_channels // 4, 2, 2),
+            BatchNorm2d(inner_channels // 4), nn.ReLU(inplace=True),
+            nn.ConvTranspose2d(inner_channels // 4, 1, 2, 2), nn.Sigmoid())
+        self.binarize.apply(self.weights_init)
+
+        self.adaptive = adaptive
+        if adaptive:
+            self.thresh = self._init_thresh(
+                inner_channels, serial=serial, smooth=smooth, bias=bias)
+            self.thresh.apply(self.weights_init)
+
+        self.in5.apply(self.weights_init)
+        self.in4.apply(self.weights_init)
+        self.in3.apply(self.weights_init)
+        self.in2.apply(self.weights_init)
+        self.out5.apply(self.weights_init)
+        self.out4.apply(self.weights_init)
+        self.out3.apply(self.weights_init)
+        self.out2.apply(self.weights_init)
+
+    def weights_init(self, m):
+        classname = m.__class__.__name__
+        if classname.find('Conv') != -1:
+            nn.init.kaiming_normal_(m.weight.data)
+        elif classname.find('BatchNorm') != -1:
+            m.weight.data.fill_(1.)
+            m.bias.data.fill_(1e-4)
+
+    def _init_thresh(self,
+                     inner_channels,
+                     serial=False,
+                     smooth=False,
+                     bias=False):
+        in_channels = inner_channels
+        if serial:
+            in_channels += 1
+        self.thresh = nn.Sequential(
+            nn.Conv2d(
+                in_channels, inner_channels // 4, 3, padding=1, bias=bias),
+            BatchNorm2d(inner_channels // 4), nn.ReLU(inplace=True),
+            self._init_upsample(
+                inner_channels // 4,
+                inner_channels // 4,
+                smooth=smooth,
+                bias=bias), BatchNorm2d(inner_channels // 4),
             nn.ReLU(inplace=True),
-            nn.Linear(channel // reduction, channel, bias=False), nn.Sigmoid())
-
-    def forward(self, x):
-        b, c, _, _, _ = x.size()
-        y = self.avg_pool(x).view(b, 2 * c)
-        y = self.fc(y).view(b, c, 2, 1, 1)
-        return x * y.expand_as(x)
-
-
-class SEBasicBlock(nn.Module):
-    expansion = 1
+            self._init_upsample(
+                inner_channels // 4, 1, smooth=smooth, bias=bias),
+            nn.Sigmoid())
+        return self.thresh
+
+    def _init_upsample(self,
+                       in_channels,
+                       out_channels,
+                       smooth=False,
+                       bias=False):
+        if smooth:
+            inter_out_channels = out_channels
+            if out_channels == 1:
+                inter_out_channels = in_channels
+            module_list = [
+                nn.Upsample(scale_factor=2, mode='nearest'),
+                nn.Conv2d(in_channels, inter_out_channels, 3, 1, 1, bias=bias)
+            ]
+            if out_channels == 1:
+                module_list.append(
+                    nn.Conv2d(
+                        in_channels,
+                        out_channels,
+                        kernel_size=1,
+                        stride=1,
+                        padding=1,
+                        bias=True))
 
-    # https://github.com/moskomule/senet.pytorch
+            return nn.Sequential(module_list)
+        else:
+            return nn.ConvTranspose2d(in_channels, out_channels, 2, 2)
 
-    def __init__(self,
-                 inplanes,
-                 planes,
-                 stride=1,
-                 downsample=None,
-                 reduction=8):
-        super(SEBasicBlock, self).__init__()
-        self.conv1 = nn.Conv3d(
-            inplanes, planes, kernel_size=(1, 3, 3), padding=(0, 1, 1))
-        self.in1 = nn.InstanceNorm3d(planes)
-        self.relu = nn.ReLU(inplace=True)
-        self.conv2 = nn.Conv3d(
-            planes, planes, kernel_size=(1, 3, 3), padding=(0, 1, 1))
-        self.in2 = nn.InstanceNorm3d(planes)
-        self.ses = SELayerS(planes, reduction)
-        self.in3 = nn.InstanceNorm3d(planes)
-        self.downsample = downsample
-        self.stride = stride
+    def forward(self, features, gt=None, masks=None, training=False):
+        c2, c3, c4, c5 = features
+        in5 = self.in5(c5)
+        in4 = self.in4(c4)
+        in3 = self.in3(c3)
+        in2 = self.in2(c2)
+
+        out4 = self.up5(in5) + in4  # 1/16
+        out3 = self.up4(out4) + in3  # 1/8
+        out2 = self.up3(out3) + in2  # 1/4
+
+        p5 = self.out5(in5)
+        p4 = self.out4(out4)
+        p3 = self.out3(out3)
+        p2 = self.out2(out2)
+
+        fuse = torch.cat((p5, p4, p3, p2), 1)
+        # this is the pred module, not binarization module;
+        # We do not correct the name due to the trained model.
+        binary = self.binarize(fuse)
+        return binary
+
+    def step_function(self, x, y):
+        return torch.reciprocal(1 + torch.exp(-self.k * (x - y)))
+
+
+class VLPTModel(nn.Module):
+
+    def __init__(self, *args, **kwargs):
+        super(VLPTModel, self).__init__()
+        self.backbone = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)
+        self.decoder = SegDetector(
+            in_channels=[256, 512, 1024, 2048], adaptive=True, k=50, **kwargs)
 
     def forward(self, x):
-        residual = x
-        out = self.conv1(x)
-        out = self.in1(out)
-        out = self.relu(out)
-
-        out = self.conv2(out)
-        out = self.in2(out)
-
-        out = self.ses(out)
-        out = self.in3(out)
-
-        if self.downsample is not None:
-            residual = self.downsample(x)
-
-        out += residual
-        out = self.relu(out)
+        return self.decoder(self.backbone(x))
 
-        return out
 
+class DBModel(nn.Module):
 
-class SAM(nn.Module):
-    """
-    Spatio-temporal aggregation module (SAM)
-    """
-
-    def __init__(self, indim, outdim=None, repeat=0, norm=False):
-        super(SAM, self).__init__()
-        self.indim = indim
-        self.repeat = repeat
-        if outdim is None:
-            outdim = indim
-        if repeat > 0:
-            self.se_block = self.seRepeat(repeat)
-        self.conv1 = ASPP3D(indim, outdim, reduction=4)  # norm is 4
-
-        # self.conv2 = nn.Conv2d(outdim, outdim, kernel_size=3, padding=1)
-        self.non_local = NONLocalBlock3D(indim, bn_layer=False)
-
-    def seRepeat(self, repeat=2):
-        return nn.Sequential(*nn.ModuleList(
-            [SEBasicBlock(self.indim, self.indim) for _ in range(repeat)]))
+    def __init__(self, *args, **kwargs):
+        super(DBModel, self).__init__()
+        self.backbone = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)
+        self.decoder = SegDetector(
+            in_channels=[64, 128, 256, 512], adaptive=True, k=50, **kwargs)
 
     def forward(self, x):
-        x = self.non_local(x)
-
-        if self.repeat > 0:
-            x = self.se_block(x)
-        r = x
-        x = self.conv1(x) + r
-        return x
-
-
-class MemCrompress(nn.Module):
-
-    def __init__(self, repeat=0, norm=True):
-        super().__init__()
-
-        self.key_encoder = SAM(64, 64, repeat=repeat, norm=norm)
-        self.value_encoder = SAM(512, 512, repeat=repeat, norm=norm)
-
-        self.compress_key = nn.Conv3d(
-            64, 64, kernel_size=(2, 3, 3), padding=(0, 1, 1))
-        self.compress_value = nn.Conv3d(
-            512, 512, kernel_size=(2, 3, 3), padding=(0, 1, 1))
-        # self.temporal_shuffle = temporal_shuffle
-
-    def forward(self, key, value):
-        # key    N, C, T, H, W      [4, 64, 2, 24, 24]
-        # value  N, O, C, T, H, W
-        # return
-        # key    N, C, 1, H, W
-        # value  N, O, C, 1, H, W
-        N, O, C, T, H, W = value.shape
-        value = value.flatten(
-            start_dim=0, end_dim=1)  # N*O, C, T, H, W [8, 512, 2, 24, 24]
-
-        k = self.compress_key(self.key_encoder(key))
-        v = self.compress_value(self.value_encoder(value))
-        v = v.view(N, O, C, 1, H, W)
-        return k, v
+        return self.decoder(self.backbone(x))
```

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_object_segmentation/network.py` & `modelscope-1.5.0/modelscope/models/cv/video_object_segmentation/network.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_checkpoint.py` & `modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_checkpoint.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_transformer.py` & `modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/head/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/head/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/head/kernel_head.py` & `modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/head/kernel_iter_head.py` & `modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_iter_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/head/kernel_update_head.py` & `modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_update_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/head/kernel_updator.py` & `modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_updator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/head/mask.py` & `modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/head/mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/head/semantic_fpn_wrapper.py` & `modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/head/semantic_fpn_wrapper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/head/track_heads.py` & `modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/head/track_heads.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/neck/fpn.py` & `modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/neck/fpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/track/quasi_dense_embed_tracker.py` & `modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/track/quasi_dense_embed_tracker.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/video_k_net.py` & `modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/video_k_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_panoptic_segmentation/visualizer.py` & `modelscope-1.5.0/modelscope/models/cv/video_panoptic_segmentation/visualizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/config/ostrack.py` & `modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/config/ostrack.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/layers/attn.py` & `modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/layers/attn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/layers/attn_blocks.py` & `modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/layers/attn_blocks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/layers/head.py` & `modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/layers/head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/layers/patch_embed.py` & `modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/layers/patch_embed.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/ostrack/base_backbone.py` & `modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/base_backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/ostrack/ostrack.py` & `modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/ostrack.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/ostrack/utils.py` & `modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/ostrack/vit_ce.py` & `modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/vit_ce.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/procontext/procontext.py` & `modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/procontext/procontext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/procontext/utils.py` & `modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/procontext/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/models/procontext/vit_ce.py` & `modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/models/procontext/vit_ce.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/tracker/ostrack.py` & `modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/tracker/ostrack.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/tracker/procontext.py` & `modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/tracker/procontext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_single_object_tracking/utils/utils.py` & `modelscope-1.5.0/modelscope/models/cv/video_single_object_tracking/utils/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/DUT_raft.py` & `modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/DUT_raft.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/MotionPro.py` & `modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/MotionPro.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/RAFT/corr.py` & `modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/RAFT/corr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/RAFT/extractor.py` & `modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/RAFT/extractor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/RAFT/raft.py` & `modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/RAFT/raft.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/RAFT/update.py` & `modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/RAFT/update.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/Smoother.py` & `modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/Smoother.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/config.py` & `modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/rf_det_module.py` & `modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/rf_det_module.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUT/rf_det_so.py` & `modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUT/rf_det_so.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_stabilization/DUTRAFTStabilizer.py` & `modelscope-1.5.0/modelscope/models/cv/video_stabilization/DUTRAFTStabilizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_stabilization/utils/IterativeSmooth.py` & `modelscope-1.5.0/modelscope/models/cv/video_stabilization/utils/IterativeSmooth.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_stabilization/utils/MedianFilter.py` & `modelscope-1.5.0/modelscope/models/cv/video_stabilization/utils/MedianFilter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_stabilization/utils/ProjectionUtils.py` & `modelscope-1.5.0/modelscope/models/cv/video_stabilization/utils/ProjectionUtils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_stabilization/utils/RAFTUtils.py` & `modelscope-1.5.0/modelscope/models/cv/video_stabilization/utils/RAFTUtils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_stabilization/utils/WarpUtils.py` & `modelscope-1.5.0/modelscope/models/cv/video_stabilization/utils/WarpUtils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_stabilization/utils/image_utils.py` & `modelscope-1.5.0/modelscope/models/cv/video_stabilization/utils/image_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_stabilization/utils/math_utils.py` & `modelscope-1.5.0/modelscope/models/cv/video_stabilization/utils/math_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_streaming_perception/longshortnet/exp/longshortnet_base.py` & `modelscope-1.5.0/modelscope/models/cv/video_streaming_perception/longshortnet/exp/longshortnet_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_streaming_perception/longshortnet/longshortnet.py` & `modelscope-1.5.0/modelscope/models/cv/video_streaming_perception/longshortnet/longshortnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_long.py` & `modelscope-1.5.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_long.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_short.py` & `modelscope-1.5.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_short.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort.py` & `modelscope-1.5.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort_backbone_neck.py` & `modelscope-1.5.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort_backbone_neck.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_summarization/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/video_summarization/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_summarization/base_model.py` & `modelscope-1.5.0/modelscope/models/cv/video_summarization/base_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_summarization/kts/cpd_auto.py` & `modelscope-1.5.0/modelscope/models/cv/video_summarization/kts/cpd_auto.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_summarization/kts/cpd_nonlin.py` & `modelscope-1.5.0/modelscope/models/cv/video_summarization/kts/cpd_nonlin.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_summarization/pgl_sum.py` & `modelscope-1.5.0/modelscope/models/cv/video_summarization/pgl_sum.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_summarization/summarizer.py` & `modelscope-1.5.0/modelscope/models/cv/video_summarization/summarizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_super_resolution/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/video_super_resolution/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_super_resolution/basicvsr_net.py` & `modelscope-1.5.0/modelscope/models/cv/video_super_resolution/basicvsr_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_super_resolution/common.py` & `modelscope-1.5.0/modelscope/models/cv/video_super_resolution/common.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_super_resolution/msrresnet_lite_model.py` & `modelscope-1.5.0/modelscope/models/cv/video_super_resolution/msrresnet_lite_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_super_resolution/real_basicvsr_for_video_super_resolution.py` & `modelscope-1.5.0/modelscope/models/cv/video_super_resolution/real_basicvsr_for_video_super_resolution.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/video_super_resolution/real_basicvsr_net.py` & `modelscope-1.5.0/modelscope/models/cv/video_super_resolution/real_basicvsr_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vidt/backbone.py` & `modelscope-1.5.0/modelscope/models/cv/vidt/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vidt/deformable_transformer.py` & `modelscope-1.5.0/modelscope/models/cv/vidt/deformable_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vidt/fpn_fusion.py` & `modelscope-1.5.0/modelscope/models/cv/vidt/fpn_fusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vidt/head.py` & `modelscope-1.5.0/modelscope/models/cv/vidt/head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vidt/model.py` & `modelscope-1.5.0/modelscope/models/cv/vidt/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/virual_tryon/sdafnet.py` & `modelscope-1.5.0/modelscope/models/cv/virual_tryon/sdafnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vision_efficient_tuning/backbone.py` & `modelscope-1.5.0/modelscope/models/cv/vision_efficient_tuning/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vision_efficient_tuning/head.py` & `modelscope-1.5.0/modelscope/models/cv/vision_efficient_tuning/head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vision_efficient_tuning/model.py` & `modelscope-1.5.0/modelscope/models/cv/vision_efficient_tuning/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vision_efficient_tuning/petl.py` & `modelscope-1.5.0/modelscope/models/cv/vision_efficient_tuning/petl.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vision_efficient_tuning/timm_helpers.py` & `modelscope-1.5.0/modelscope/models/cv/vision_efficient_tuning/timm_helpers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vision_efficient_tuning/timm_vision_transformer.py` & `modelscope-1.5.0/modelscope/models/cv/vision_efficient_tuning/timm_vision_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vision_efficient_tuning/timm_weight_init.py` & `modelscope-1.5.0/modelscope/models/cv/vision_efficient_tuning/timm_weight_init.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vision_efficient_tuning/vision_efficient_tuning.py` & `modelscope-1.5.0/modelscope/models/cv/vision_efficient_tuning/vision_efficient_tuning.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vision_middleware/backbone.py` & `modelscope-1.5.0/modelscope/models/cv/vision_middleware/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vision_middleware/head.py` & `modelscope-1.5.0/modelscope/models/cv/vision_middleware/head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vision_middleware/model.py` & `modelscope-1.5.0/modelscope/models/cv/vision_middleware/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vision_middleware/vim.py` & `modelscope-1.5.0/modelscope/models/cv/vision_middleware/vim.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vop_retrieval/__init__.py` & `modelscope-1.5.0/modelscope/models/cv/vop_retrieval/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vop_retrieval/backbone.py` & `modelscope-1.5.0/modelscope/models/cv/vop_retrieval/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vop_retrieval/basic_utils.py` & `modelscope-1.5.0/modelscope/models/cv/vop_retrieval/basic_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vop_retrieval/model.py` & `modelscope-1.5.0/modelscope/models/cv/vop_retrieval/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vop_retrieval/model_se.py` & `modelscope-1.5.0/modelscope/models/cv/vop_retrieval/model_se.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/cv/vop_retrieval/tokenization_clip.py` & `modelscope-1.5.0/modelscope/models/cv/vop_retrieval/tokenization_clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/__init__.py` & `modelscope-1.5.0/modelscope/models/multi_modal/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -15,14 +15,15 @@
     from .ofa_for_all_tasks import OfaForAllTasks
     from .ofa_for_text_to_image_synthesis_model import \
         OfaForTextToImageSynthesis
     from .multi_stage_diffusion import \
         MultiStageDiffusionForTextToImageSynthesis
     from .vldoc import VLDocForDocVLEmbedding
     from .video_synthesis import TextToVideoSynthesis
+    from .efficient_diffusion_tuning import EfficientStableDiffusion
 
 else:
     _import_structure = {
         'clip': ['CLIPForMultiModalEmbedding'],
         'diffusion': ['DiffusionForTextToImageSynthesis'],
         'gemm': ['GEMMForMultiModalEmbedding'],
         'rleg': ['RLEGForMultiModalEmbedding'],
@@ -32,14 +33,15 @@
         'ofa_for_all_tasks': ['OfaForAllTasks'],
         'ofa_for_text_to_image_synthesis_model':
         ['OfaForTextToImageSynthesis'],
         'multi_stage_diffusion':
         ['MultiStageDiffusionForTextToImageSynthesis'],
         'vldoc': ['VLDocForDocVLEmbedding'],
         'video_synthesis': ['TextToVideoSynthesis'],
+        'efficient_diffusion_tuning': ['EfficientStableDiffusion']
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
```

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/clip/bert_tokenizer.py` & `modelscope-1.5.0/modelscope/models/multi_modal/clip/bert_tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/clip/configuration_bert.py` & `modelscope-1.5.0/modelscope/models/multi_modal/clip/configuration_bert.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/clip/model.py` & `modelscope-1.5.0/modelscope/models/multi_modal/clip/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/clip/modeling_bert.py` & `modelscope-1.5.0/modelscope/models/multi_modal/clip/modeling_bert.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/diffusion/diffusion.py` & `modelscope-1.5.0/modelscope/models/multi_modal/diffusion/diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/diffusion/model.py` & `modelscope-1.5.0/modelscope/models/multi_modal/diffusion/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/diffusion/structbert.py` & `modelscope-1.5.0/modelscope/models/multi_modal/diffusion/structbert.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/diffusion/tokenizer.py` & `modelscope-1.5.0/modelscope/models/multi_modal/diffusion/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/diffusion/unet_generator.py` & `modelscope-1.5.0/modelscope/models/multi_modal/diffusion/unet_generator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/diffusion/unet_upsampler_1024.py` & `modelscope-1.5.0/modelscope/models/multi_modal/diffusion/unet_upsampler_1024.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/diffusion/unet_upsampler_256.py` & `modelscope-1.5.0/modelscope/models/multi_modal/diffusion/unet_upsampler_256.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/dpm_solver_pytorch.py` & `modelscope-1.5.0/modelscope/models/multi_modal/dpm_solver_pytorch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/gemm/gemm_base.py` & `modelscope-1.5.0/modelscope/models/multi_modal/gemm/gemm_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/gemm/gemm_model.py` & `modelscope-1.5.0/modelscope/models/multi_modal/gemm/gemm_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/gemm/tokenizer.py` & `modelscope-1.5.0/modelscope/models/multi_modal/gemm/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/guided_diffusion/__init__.py` & `modelscope-1.5.0/modelscope/models/multi_modal/guided_diffusion/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/guided_diffusion/gaussian_diffusion.py` & `modelscope-1.5.0/modelscope/models/multi_modal/guided_diffusion/gaussian_diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/guided_diffusion/respace.py` & `modelscope-1.5.0/modelscope/models/multi_modal/guided_diffusion/respace.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/guided_diffusion/script.py` & `modelscope-1.5.0/modelscope/models/multi_modal/guided_diffusion/script.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/guided_diffusion/unet.py` & `modelscope-1.5.0/modelscope/models/multi_modal/guided_diffusion/unet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/mgeo/__init__.py` & `modelscope-1.5.0/modelscope/models/multi_modal/mgeo/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/mgeo/backbone.py` & `modelscope-1.5.0/modelscope/models/multi_modal/mgeo/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/mgeo/text_classification.py` & `modelscope-1.5.0/modelscope/models/multi_modal/mgeo/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/mgeo/text_ranking.py` & `modelscope-1.5.0/modelscope/models/multi_modal/mgeo/text_ranking.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/mgeo/token_classification.py` & `modelscope-1.5.0/modelscope/models/multi_modal/mgeo/token_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/mmr/dataloaders/rawvideo_util.py` & `modelscope-1.5.0/modelscope/models/multi_modal/mmr/dataloaders/rawvideo_util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/mmr/models/clip_for_mm_video_embedding.py` & `modelscope-1.5.0/modelscope/models/multi_modal/mmr/models/clip_for_mm_video_embedding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/mmr/models/dynamic_inverted_softmax.py` & `modelscope-1.5.0/modelscope/models/multi_modal/mmr/models/dynamic_inverted_softmax.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/mmr/models/modeling.py` & `modelscope-1.5.0/modelscope/models/multi_modal/mmr/models/modeling.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/mmr/models/module_clip.py` & `modelscope-1.5.0/modelscope/models/multi_modal/mmr/models/module_clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/mmr/models/module_cross.py` & `modelscope-1.5.0/modelscope/models/multi_modal/mmr/models/module_cross.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/mmr/models/tokenization_clip.py` & `modelscope-1.5.0/modelscope/models/multi_modal/mmr/models/tokenization_clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/mmr/models/until_module.py` & `modelscope-1.5.0/modelscope/models/multi_modal/mmr/models/until_module.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/mplug/__init__.py` & `modelscope-1.5.0/modelscope/models/multi_modal/mplug/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/mplug/clip/clip.py` & `modelscope-1.5.0/modelscope/models/multi_modal/mplug/clip/clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/mplug/configuration_mplug.py` & `modelscope-1.5.0/modelscope/models/multi_modal/mplug/configuration_mplug.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/mplug/modeling_mplug.py` & `modelscope-1.5.0/modelscope/models/multi_modal/mplug/modeling_mplug.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/mplug/mvit.py` & `modelscope-1.5.0/modelscope/models/multi_modal/mplug/mvit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/mplug/predictor.py` & `modelscope-1.5.0/modelscope/models/multi_modal/mplug/predictor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/mplug_for_all_tasks.py` & `modelscope-1.5.0/modelscope/models/multi_modal/mplug_for_all_tasks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/multi_stage_diffusion/clip.py` & `modelscope-1.5.0/modelscope/models/multi_modal/multi_stage_diffusion/clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/multi_stage_diffusion/decoder.py` & `modelscope-1.5.0/modelscope/models/multi_modal/multi_stage_diffusion/decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/multi_stage_diffusion/gaussian_diffusion.py` & `modelscope-1.5.0/modelscope/models/multi_modal/multi_stage_diffusion/gaussian_diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/multi_stage_diffusion/model.py` & `modelscope-1.5.0/modelscope/models/multi_modal/multi_stage_diffusion/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/multi_stage_diffusion/prior.py` & `modelscope-1.5.0/modelscope/models/multi_modal/multi_stage_diffusion/prior.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/multi_stage_diffusion/tokenizer.py` & `modelscope-1.5.0/modelscope/models/multi_modal/multi_stage_diffusion/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/multi_stage_diffusion/upsampler.py` & `modelscope-1.5.0/modelscope/models/multi_modal/multi_stage_diffusion/upsampler.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/multi_stage_diffusion/xglm.py` & `modelscope-1.5.0/modelscope/models/multi_modal/multi_stage_diffusion/xglm.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/ofa/configuration_mmspeech.py` & `modelscope-1.5.0/modelscope/models/multi_modal/ofa/configuration_mmspeech.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/ofa/configuration_ofa.py` & `modelscope-1.5.0/modelscope/models/multi_modal/ofa/configuration_ofa.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/ofa/generate/incremental_decoding_utils.py` & `modelscope-1.5.0/modelscope/models/multi_modal/ofa/generate/incremental_decoding_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/ofa/generate/multihead_attention.py` & `modelscope-1.5.0/modelscope/models/multi_modal/ofa/generate/multihead_attention.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/ofa/generate/ngram_repeat_block.py` & `modelscope-1.5.0/modelscope/models/multi_modal/ofa/generate/ngram_repeat_block.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/ofa/generate/search.py` & `modelscope-1.5.0/modelscope/models/multi_modal/ofa/generate/search.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/ofa/generate/sequence_generator.py` & `modelscope-1.5.0/modelscope/models/multi_modal/ofa/generate/sequence_generator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/ofa/generate/token_generation_constraints.py` & `modelscope-1.5.0/modelscope/models/multi_modal/ofa/generate/token_generation_constraints.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/ofa/generate/utils.py` & `modelscope-1.5.0/modelscope/models/multi_modal/ofa/generate/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/ofa/modeling_mmspeech.py` & `modelscope-1.5.0/modelscope/models/multi_modal/ofa/modeling_mmspeech.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/ofa/modeling_ofa.py` & `modelscope-1.5.0/modelscope/models/multi_modal/ofa/modeling_ofa.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/ofa/resnet.py` & `modelscope-1.5.0/modelscope/models/multi_modal/ofa/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/ofa/tokenization_ofa.py` & `modelscope-1.5.0/modelscope/models/multi_modal/ofa/tokenization_ofa.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/ofa/tokenization_ofa_fast.py` & `modelscope-1.5.0/modelscope/models/multi_modal/ofa/tokenization_ofa_fast.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/ofa/utils/constant.py` & `modelscope-1.5.0/modelscope/models/multi_modal/ofa/utils/constant.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/ofa/utils/utils.py` & `modelscope-1.5.0/modelscope/models/multi_modal/ofa/utils/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/ofa/vit.py` & `modelscope-1.5.0/modelscope/models/multi_modal/ofa/vit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/ofa_for_all_tasks.py` & `modelscope-1.5.0/modelscope/models/multi_modal/ofa_for_all_tasks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/ofa_for_text_to_image_synthesis_model.py` & `modelscope-1.5.0/modelscope/models/multi_modal/ofa_for_text_to_image_synthesis_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/rleg/__init__.py` & `modelscope-1.5.0/modelscope/models/multi_modal/rleg/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/rleg/model.py` & `modelscope-1.5.0/modelscope/models/multi_modal/rleg/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/rleg/rleg.py` & `modelscope-1.5.0/modelscope/models/multi_modal/rleg/rleg.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/soonet/__init__.py` & `modelscope-1.5.0/modelscope/models/multi_modal/soonet/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/soonet/blocks.py` & `modelscope-1.5.0/modelscope/models/multi_modal/soonet/blocks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/soonet/clip.py` & `modelscope-1.5.0/modelscope/models/multi_modal/soonet/clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/soonet/model.py` & `modelscope-1.5.0/modelscope/models/multi_modal/soonet/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/soonet/swin_transformer.py` & `modelscope-1.5.0/modelscope/models/multi_modal/soonet/swin_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/soonet/tokenizer.py` & `modelscope-1.5.0/modelscope/models/multi_modal/soonet/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/soonet/utils.py` & `modelscope-1.5.0/modelscope/models/multi_modal/soonet/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/team/team_model.py` & `modelscope-1.5.0/modelscope/models/multi_modal/team/team_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/team/utils.py` & `modelscope-1.5.0/modelscope/models/multi_modal/team/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/video_synthesis/__init__.py` & `modelscope-1.5.0/modelscope/models/multi_modal/video_synthesis/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/video_synthesis/autoencoder.py` & `modelscope-1.5.0/modelscope/models/multi_modal/video_synthesis/autoencoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/video_synthesis/diffusion.py` & `modelscope-1.5.0/modelscope/models/multi_modal/video_synthesis/diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py` & `modelscope-1.5.0/modelscope/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/video_synthesis/unet_sd.py` & `modelscope-1.5.0/modelscope/models/multi_modal/video_synthesis/unet_sd.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/vldoc/conv_fpn_trans.py` & `modelscope-1.5.0/modelscope/models/multi_modal/vldoc/conv_fpn_trans.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/vldoc/convnext.py` & `modelscope-1.5.0/modelscope/models/multi_modal/vldoc/convnext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/vldoc/model.py` & `modelscope-1.5.0/modelscope/models/multi_modal/vldoc/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/vldoc/modeling_layout_roberta.py` & `modelscope-1.5.0/modelscope/models/multi_modal/vldoc/modeling_layout_roberta.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/vldoc/processing.py` & `modelscope-1.5.0/modelscope/models/multi_modal/vldoc/processing.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/vldoc/tokenization.py` & `modelscope-1.5.0/modelscope/models/multi_modal/vldoc/tokenization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/multi_modal/vldoc/transformer_local.py` & `modelscope-1.5.0/modelscope/models/multi_modal/vldoc/transformer_local.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/T5/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/T5/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/T5/backbone.py` & `modelscope-1.5.0/modelscope/models/nlp/T5/backbone.py`

 * *Files 0% similar despite different names*

```diff
@@ -20,14 +20,16 @@
 import warnings
 from typing import Optional, Tuple, Union
 
 import torch
 from torch import nn
 from torch.utils.checkpoint import checkpoint
 from transformers.activations import ACT2FN
+from transformers.modeling_outputs import \
+    BaseModelOutputWithPastAndCrossAttentions
 from transformers.modeling_utils import (PreTrainedModel,
                                          find_pruneable_heads_and_indices,
                                          prune_linear_layer)
 from transformers.utils import (DUMMY_INPUTS, DUMMY_MASK, add_start_docstrings,
                                 add_start_docstrings_to_model_forward,
                                 is_torch_fx_proxy, replace_return_docstrings)
 from transformers.utils.model_parallel_utils import (assert_device_map,
@@ -1180,15 +1182,15 @@
             return tuple(v for v in [
                 hidden_states,
                 present_key_value_states,
                 all_hidden_states,
                 all_attentions,
                 all_cross_attentions,
             ] if v is not None)
-        return AttentionBackboneModelOutput(
+        return BaseModelOutputWithPastAndCrossAttentions(
             last_hidden_state=hidden_states,
             past_key_values=present_key_value_states,
             hidden_states=all_hidden_states,
             attentions=all_attentions,
             cross_attentions=all_cross_attentions,
         )
```

### Comparing `modelscope-1.4.2/modelscope/models/nlp/T5/configuration.py` & `modelscope-1.5.0/modelscope/models/nlp/T5/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/T5/text2text_generation.py` & `modelscope-1.5.0/modelscope/models/nlp/T5/text2text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,15 +14,17 @@
         BertForDocumentSegmentation,
         BertModel,
         BertConfig,
         SiameseUieModel,
     )
     from .bloom import BloomModel
     from .codegeex import CodeGeeXForCodeTranslation, CodeGeeXForCodeGeneration
+    from .glm_130b import GLM130bForTextGeneration
     from .csanmt import CsanmtForTranslation
+    from .canmt import CanmtForTranslation
     from .deberta_v2 import DebertaV2ForMaskedLM, DebertaV2Model
     from .gpt_neo import GPTNeoModel
     from .gpt2 import GPT2Model
     from .gpt3 import GPT3ForTextGeneration, DistributedGPT3
     from .gpt_moe import GPTMoEForTextGeneration, DistributedGPTMoE
     from .heads import TextClassificationHead
     from .hf_transformers import TransformersModel
@@ -66,14 +68,15 @@
     from .veco import (VecoConfig, VecoForMaskedLM,
                        VecoForSequenceClassification,
                        VecoForTokenClassification, VecoModel)
     from .dgds import (DocumentGroundedDialogGenerateModel,
                        DocumentGroundedDialogRetrievalModel,
                        DocumentGroundedDialogRerankModel)
     from .xlm_roberta import XLMRobertaConfig, XLMRobertaModel
+    from .llama import LlamaForTextGeneration, LlamaConfig, LlamaModel, LlamaTokenizer, LlamaTokenizerFast
 
 else:
     _import_structure = {
         'bart': ['BartForTextErrorCorrection'],
         'bert': [
             'BertForMaskedLM',
             'BertForTextRanking',
@@ -83,16 +86,18 @@
             'BertForDocumentSegmentation',
             'BertModel',
             'BertConfig',
             'SiameseUieModel',
         ],
         'bloom': ['BloomModel'],
         'csanmt': ['CsanmtForTranslation'],
+        'canmt': ['CanmtForTranslation'],
         'codegeex':
         ['CodeGeeXForCodeTranslation', 'CodeGeeXForCodeGeneration'],
+        'glm_130b': ['GLM130bForTextGeneration'],
         'deberta_v2': ['DebertaV2ForMaskedLM', 'DebertaV2Model'],
         'heads': ['TextClassificationHead'],
         'hf_transformers': ['TransformersModel'],
         'gpt2': ['GPT2Model'],
         'gpt3': ['GPT3ForTextGeneration', 'DistributedGPT3'],
         'gpt_moe': ['GPTMoEForTextGeneration', 'DistributedGPTMoE'],
         'gpt_neo': ['GPTNeoModel'],
@@ -147,14 +152,18 @@
         'use': ['UserSatisfactionEstimation'],
         'dgds': [
             'DocumentGroundedDialogGenerateModel',
             'DocumentGroundedDialogRetrievalModel',
             'DocumentGroundedDialogRerankModel'
         ],
         'xlm_roberta': ['XLMRobertaConfig', 'XLMRobertaModel'],
+        'llama': [
+            'LlamaForTextGeneration', 'LlamaConfig', 'LlamaModel',
+            'LlamaTokenizer', 'LlamaTokenizerFast'
+        ],
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
```

### Comparing `modelscope-1.4.2/modelscope/models/nlp/bart/text_error_correction.py` & `modelscope-1.5.0/modelscope/models/nlp/bart/text_error_correction.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/bert/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/bert/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/bert/backbone.py` & `modelscope-1.5.0/modelscope/models/nlp/bert/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/bert/configuration.py` & `modelscope-1.5.0/modelscope/models/nlp/bert/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/bert/document_segmentation.py` & `modelscope-1.5.0/modelscope/models/nlp/bert/document_segmentation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/bert/fill_mask.py` & `modelscope-1.5.0/modelscope/models/nlp/bert/fill_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/bert/sentence_embedding.py` & `modelscope-1.5.0/modelscope/models/nlp/bert/sentence_embedding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/bert/siamese_uie.py` & `modelscope-1.5.0/modelscope/models/nlp/bert/siamese_uie.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/bert/text_classification.py` & `modelscope-1.5.0/modelscope/models/nlp/bert/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/bert/text_ranking.py` & `modelscope-1.5.0/modelscope/models/nlp/bert/text_ranking.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/bert/token_classification.py` & `modelscope-1.5.0/modelscope/models/nlp/bert/token_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/bert/word_alignment.py` & `modelscope-1.5.0/modelscope/models/nlp/bert/word_alignment.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/codegeex/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/codegeex/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/codegeex/codegeex.py` & `modelscope-1.5.0/modelscope/models/nlp/codegeex/codegeex.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/codegeex/codegeex_for_code_generation.py` & `modelscope-1.5.0/modelscope/models/nlp/codegeex/codegeex_for_code_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/codegeex/codegeex_for_code_translation.py` & `modelscope-1.5.0/modelscope/models/nlp/codegeex/codegeex_for_code_translation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/codegeex/inference.py` & `modelscope-1.5.0/modelscope/models/nlp/codegeex/inference.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/codegeex/tokenizer.py` & `modelscope-1.5.0/modelscope/models/nlp/codegeex/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/csanmt/translation.py` & `modelscope-1.5.0/modelscope/models/nlp/csanmt/translation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/deberta_v2/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/deberta_v2/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/deberta_v2/backbone.py` & `modelscope-1.5.0/modelscope/models/nlp/deberta_v2/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/deberta_v2/configuration.py` & `modelscope-1.5.0/modelscope/models/nlp/deberta_v2/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/deberta_v2/fill_mask.py` & `modelscope-1.5.0/modelscope/models/nlp/deberta_v2/fill_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/deberta_v2/tokenization.py` & `modelscope-1.5.0/modelscope/models/nlp/deberta_v2/tokenization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/deberta_v2/tokenization_fast.py` & `modelscope-1.5.0/modelscope/models/nlp/deberta_v2/tokenization_fast.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/dgds/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/dgds/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import TYPE_CHECKING
 
 from modelscope.utils.import_utils import LazyImportModule
 
 if TYPE_CHECKING:
     from .document_grounded_dialog_generate import DocumentGroundedDialogGenerateModel
-    from .document_grounded_dialog_retrieval import DocumentGroundedDialogRerankModel
+    from .document_grounded_dialog_rerank import DocumentGroundedDialogRerankModel
     from .document_grounded_dialog_retrieval import DocumentGroundedDialogRetrievalModel
 else:
     _import_structure = {
         'document_grounded_dialog_generate':
         ['DocumentGroundedDialogGenerateModel'],
         'document_grounded_dialog_rerank':
         ['DocumentGroundedDialogRerankModel'],
```

### Comparing `modelscope-1.4.2/modelscope/models/nlp/dgds/backbone.py` & `modelscope-1.5.0/modelscope/models/nlp/dgds/backbone.py`

 * *Files 1% similar despite different names*

```diff
@@ -9,15 +9,14 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""PyTorch BERT model."""
 
 from __future__ import absolute_import, division, print_function
 import os.path
 
 import torch
 import torch.nn.functional as F
 from torch import nn
```

### Comparing `modelscope-1.4.2/modelscope/models/nlp/dgds/document_grounded_dialog_generate.py` & `modelscope-1.5.0/modelscope/models/nlp/dgds/document_grounded_dialog_generate.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/dgds/document_grounded_dialog_rerank.py` & `modelscope-1.5.0/modelscope/models/nlp/dgds/document_grounded_dialog_rerank.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/dgds/document_grounded_dialog_retrieval.py` & `modelscope-1.5.0/modelscope/models/nlp/dgds/document_grounded_dialog_retrieval.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/fid_plug/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/fid_plug/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/fid_plug/backbone.py` & `modelscope-1.5.0/modelscope/models/nlp/fid_plug/backbone.py`

 * *Files 1% similar despite different names*

```diff
@@ -848,14 +848,18 @@
                               min_length: int = 10,
                               bad_words_ids=None,
                               early_stopping=True,
                               num_beams=3,
                               length_penalty=1.2,
                               repetition_penalty=1.2,
                               no_repeat_ngram_size=4,
+                              do_sample=False,
+                              temperature=1.0,
+                              top_k=0,
+                              top_p=1.0,
                               *args,
                               **kwargs):
         # TODO: faster code path for beam_size == 1.
         # TODO: support these blacklisted features.
 
         num_beams = num_beams
         batch_size = batch.batch_size
@@ -908,15 +912,15 @@
         results = {}
         results['predictions'] = [[] for _ in range(batch_size)]  # noqa: F812
         results['scores'] = [[] for _ in range(batch_size)]  # noqa: F812
         results['gold_score'] = [0] * batch_size
         results['batch'] = batch
 
         for step in range(max_length):
-            self.logger.info(f'step: {step + 1} / {max_length}')
+            # self.logger.info(f'step: {step + 1} / {max_length}')
             decoder_input = alive_seq[:, -1].view(1, -1)
 
             # Decoder forward.
             decoder_input = decoder_input.transpose(0, 1)
             dec_out, attns, state = self.plug.decoder(
                 state, decoder_input, src_features, step=step)
 
@@ -962,22 +966,18 @@
                         else:
                             log_probs[i, previous_token] /= repetition_penalty
 
             # Multiply probs by the beam probability.
 
             curr_length_penalty = (step + 1)**length_penalty
             # '''
-            if self.config.sample_topk:
-                temperature = self.config.temperature
+            if do_sample:
                 _scores = log_probs / temperature
                 _scores = self._top_k_top_p_filtering(
-                    _scores,
-                    top_k=self.config.top_k,
-                    top_p=self.config.top_p,
-                    min_tokens_to_keep=1
+                    _scores, top_k=top_k, top_p=top_p, min_tokens_to_keep=1
                 )  # (batch_size * num_beams, vocab_size)
                 # Sample 2 next words for each beam (so we have some spare tokens
                 # and match output of greedy beam search)
                 topk_ids = torch.multinomial(
                     F.softmax(_scores, dim=-1),
                     num_samples=1)  # (batch_size * num_beams, 2)
                 # Compute next scores
```

### Comparing `modelscope-1.4.2/modelscope/models/nlp/fid_plug/configuration.py` & `modelscope-1.5.0/modelscope/models/nlp/fid_plug/configuration.py`

 * *Files 2% similar despite different names*

```diff
@@ -79,15 +79,14 @@
                  dec_layers=12,
                  dec_hidden_size=768,
                  dec_heads=8,
                  dec_ff_size=3072,
                  dec_dropout=0.2,
                  use_bert_emb=True,
                  label_smoothing=0.1,
-                 sample_topk=False,
                  block_trigram=False,
                  **kwargs):
         super().__init__(**kwargs)
         self.encoder = encoder
         self.encoder_pth = encoder_pth
         self.max_pos = max_pos
         self.share_emb = share_emb
@@ -95,9 +94,8 @@
         self.dec_hidden_size = dec_hidden_size
         self.dec_heads = dec_heads
         self.dec_ff_size = dec_ff_size
         self.dec_dropout = dec_dropout
         self.use_bert_emb = use_bert_emb
         self.label_smoothing = label_smoothing
         # Translator
-        self.sample_topk = sample_topk
         self.block_trigram = block_trigram
```

### Comparing `modelscope-1.4.2/modelscope/models/nlp/fid_plug/text_generation.py` & `modelscope-1.5.0/modelscope/models/nlp/fid_plug/text_generation.py`

 * *Files 1% similar despite different names*

```diff
@@ -137,27 +137,22 @@
              from_tf=False):  # only invoked when model is not onnx format
         self.unwrap_encoder()
         super().load(pretrained_model_path)
         self.wrap_encoder()
 
     def generate(self, inputs, *args, **kwargs):
         input_ids = inputs.get('input_ids')
-        attention_mask = inputs.get('attention_mask', None)
         token_type_ids = inputs.get('token_type_ids', None)
         n_passages = input_ids.size(1)
         self.backbone.plug.bert.set_n_passages(n_passages)
         input_ids = input_ids.view(input_ids.size(0), -1)
         if token_type_ids is not None:
             token_type_ids = token_type_ids.view(token_type_ids.size(0), -1)
         response = super().generate(
-            input_ids,
-            attention_mask=attention_mask,
-            token_type_ids=token_type_ids,
-            *args,
-            **kwargs)
+            input_ids, token_type_ids=token_type_ids, *args, **kwargs)
         return TokenGeneratorOutput(sequences=response)
 
     def forward(self,
                 input_ids,
                 decoder_input_ids,
                 token_type_ids=None,
                 *args,
```

### Comparing `modelscope-1.4.2/modelscope/models/nlp/gpt3/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/gpt3/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/gpt3/backbone.py` & `modelscope-1.5.0/modelscope/models/nlp/gpt3/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/gpt3/configuration.py` & `modelscope-1.5.0/modelscope/models/nlp/gpt3/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/gpt3/distributed_gpt3.py` & `modelscope-1.5.0/modelscope/models/nlp/gpt3/distributed_gpt3.py`

 * *Files 0% similar despite different names*

```diff
@@ -951,15 +951,14 @@
                  model_dir,
                  rank,
                  path_load_tag='model',
                  *args,
                  megatron_cfg=None,
                  **kwargs):
         super().__init__(model_dir, *args, **kwargs)
-
         init_megatron_util(megatron_cfg, model_dir, rank=rank)
 
         self.config = GPT3Config.from_pretrained(model_dir)
         # Build model.
         model = GPT3Model(self.config)
 
         for param in model.parameters():
@@ -977,15 +976,16 @@
         tensor_ws = mpu.get_tensor_model_parallel_world_size()
         ckpt_ws = get_args().get('checkpoint_tensor_model_parallel_size', None)
         ckpt_ws = tensor_ws if ckpt_ws is None else ckpt_ws
         ckpt_rank = mpu.get_tensor_model_parallel_rank() * ckpt_ws // tensor_ws
         load_model = pre_load(ckpt_rank, model_dir, tag=path_load_tag)
         load_model = split_state_dict(load_model, model, tensor_ws // ckpt_ws)
 
-        self.dist_model.load_state_dict(load_model)
+        self.dist_model.load_state_dict(
+            load_model, strict=kwargs.get('strict', True))
 
         self.inference_params = None
 
     def train(self, mode: bool = True):
         if mode:
             self.inference_params = None
         return super().train(mode)
@@ -1265,14 +1265,16 @@
                         save_function: Callable = None,
                         config: Optional[dict] = None,
                         **kwargs):
         # DistributedPipeline type is different from task name
         config['pipeline']['type'] = 'gpt3-generation'
 
         config['model'].pop('rank', None)
+        config['model'].pop('megatron_cfg', None)
+        config['megatron'].pop('rank', None)
         config['megatron'].pop('checkpoint_tensor_model_parallel_size', None)
         tp_size = get_args().tensor_model_parallel_size
         pp_size = get_args().pipeline_model_parallel_size
         config['megatron']['world_size'] = tp_size * pp_size
 
         return super().save_pretrained(target_folder, save_checkpoint_names,
                                        save_function, config, **kwargs)
```

### Comparing `modelscope-1.4.2/modelscope/models/nlp/gpt3/text_generation.py` & `modelscope-1.5.0/modelscope/models/nlp/gpt3/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/gpt3/tokenizer.py` & `modelscope-1.5.0/modelscope/models/nlp/gpt3/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/gpt_moe/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/gpt_moe/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/gpt_moe/backbone.py` & `modelscope-1.5.0/modelscope/models/nlp/gpt_moe/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/gpt_moe/checkpointing.py` & `modelscope-1.5.0/modelscope/models/nlp/gpt_moe/checkpointing.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/gpt_moe/configuration.py` & `modelscope-1.5.0/modelscope/models/nlp/gpt_moe/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/gpt_moe/distributed_gpt_moe.py` & `modelscope-1.5.0/modelscope/models/nlp/gpt_moe/distributed_gpt_moe.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/gpt_moe/moe/experts.py` & `modelscope-1.5.0/modelscope/models/nlp/gpt_moe/moe/experts.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/gpt_moe/moe/layer.py` & `modelscope-1.5.0/modelscope/models/nlp/gpt_moe/moe/layer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/gpt_moe/moe/mappings.py` & `modelscope-1.5.0/modelscope/models/nlp/gpt_moe/moe/mappings.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/gpt_moe/moe/sharded_moe.py` & `modelscope-1.5.0/modelscope/models/nlp/gpt_moe/moe/sharded_moe.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/gpt_moe/moe/utils.py` & `modelscope-1.5.0/modelscope/models/nlp/gpt_moe/moe/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/gpt_moe/text_generation.py` & `modelscope-1.5.0/modelscope/models/nlp/gpt_moe/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/gpt_moe/tokenizer.py` & `modelscope-1.5.0/modelscope/models/nlp/gpt_moe/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/gpt_neo/backbone.py` & `modelscope-1.5.0/modelscope/models/nlp/gpt_neo/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/heads/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/heads/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/heads/crf_head.py` & `modelscope-1.5.0/modelscope/models/nlp/heads/crf_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/heads/fill_mask_head.py` & `modelscope-1.5.0/modelscope/models/nlp/heads/fill_mask_head.py`

 * *Files 21% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 
 from typing import Dict
 
 import torch
 import torch.nn.functional as F
 from torch import nn
 from torch.nn import CrossEntropyLoss
-from transformers.activations import ACT2FN
+from transformers.activations import ACT2FN, gelu
 
 from modelscope.metainfo import Heads
 from modelscope.models.base import TorchHead
 from modelscope.models.builder import HEADS
 from modelscope.outputs import (AttentionFillMaskModelOutput, ModelOutputBase,
                                 OutputKeys)
 from modelscope.utils.constant import Tasks
@@ -67,14 +67,59 @@
     def compute_loss(self, logits: torch.Tensor, labels) -> torch.Tensor:
         loss_fct = CrossEntropyLoss()  # -100 index = padding token
         masked_lm_loss = loss_fct(
             logits.view(-1, self.config.vocab_size), labels.view(-1))
         return masked_lm_loss
 
 
+@HEADS.register_module(Tasks.fill_mask, module_name=Heads.xlm_roberta_mlm)
+class XlmRobertaMaskHead(TorchHead):
+    _keys_to_ignore_on_load_missing = [
+        r'lm_head.decoder.weight', 'lm_head.decoder.bias'
+    ]
+
+    def __init__(self,
+                 hidden_size=1024,
+                 hidden_act='gelu',
+                 layer_norm_eps=1e-05,
+                 vocab_size=274701,
+                 **kwargs):
+        super().__init__(
+            hidden_size=hidden_size,
+            hidden_act=hidden_act,
+            layer_norm_eps=layer_norm_eps,
+            vocab_size=vocab_size)
+        self.lm_head = XLMRobertaLMHead(self.config)
+
+    def forward(self,
+                inputs: ModelOutputBase,
+                attention_mask=None,
+                labels=None,
+                **kwargs):
+        logits = self.lm_head(inputs.last_hidden_state)
+        loss = None
+        if labels is not None:
+            loss = self.compute_loss(logits, labels)
+        return AttentionFillMaskModelOutput(
+            loss=loss,
+            logits=logits,
+            hidden_states=inputs.hidden_states,
+            attentions=inputs.attentions,
+        )
+
+    def compute_loss(self, logits: torch.Tensor, labels) -> torch.Tensor:
+        loss_fct = CrossEntropyLoss()
+        masked_lm_loss = loss_fct(
+            logits.view(-1, self.config.vocab_size), labels.view(-1))
+        return masked_lm_loss
+
+    def get_output_embeddings(self):
+        return self.lm_head.decoder
+
+
 class BertPredictionHeadTransform(nn.Module):
 
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
         if isinstance(config.hidden_act, str):
             self.transform_act_fn = ACT2FN[config.hidden_act]
@@ -117,7 +162,39 @@
     def __init__(self, config):
         super().__init__()
         self.predictions = BertLMPredictionHead(config)
 
     def forward(self, sequence_output: torch.Tensor) -> torch.Tensor:
         prediction_scores = self.predictions(sequence_output)
         return prediction_scores
+
+
+class XLMRobertaLMHead(nn.Module):
+    """Roberta Head for masked language modeling."""
+
+    def __init__(self, config):
+        super().__init__()
+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
+        self.layer_norm = nn.LayerNorm(
+            config.hidden_size, eps=config.layer_norm_eps)
+
+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)
+        self.bias = nn.Parameter(torch.zeros(config.vocab_size))
+        self.decoder.bias = self.bias
+
+    def forward(self, features, **kwargs):
+        x = self.dense(features)
+        x = gelu(x)
+        x = self.layer_norm(x)
+
+        # project back to size of vocabulary with bias
+        x = self.decoder(x)
+
+        return x
+
+    def _tie_weights(self):
+        # To tie those two weights if they get disconnected (on TPU or when the bias is resized)
+        # For accelerate compatibility and to not break backward compatibility
+        if self.decoder.bias.device.type == 'meta':
+            self.decoder.bias = self.bias
+        else:
+            self.bias = self.decoder.bias
```

### Comparing `modelscope-1.4.2/modelscope/models/nlp/heads/infromation_extraction_head.py` & `modelscope-1.5.0/modelscope/models/nlp/heads/infromation_extraction_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/heads/text_classification_head.py` & `modelscope-1.5.0/modelscope/models/nlp/heads/text_classification_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/heads/text_generation_head.py` & `modelscope-1.5.0/modelscope/models/nlp/heads/text_generation_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/heads/text_ranking_head.py` & `modelscope-1.5.0/modelscope/models/nlp/heads/text_ranking_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/heads/token_classification_head.py` & `modelscope-1.5.0/modelscope/models/nlp/heads/token_classification_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/heads/torch_pretrain_head.py` & `modelscope-1.5.0/modelscope/models/nlp/heads/torch_pretrain_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/hf_transformers/backbone.py` & `modelscope-1.5.0/modelscope/models/nlp/hf_transformers/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/lstm/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/lstm/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/lstm/backbone.py` & `modelscope-1.5.0/modelscope/models/nlp/lstm/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/lstm/token_classification.py` & `modelscope-1.5.0/modelscope/models/nlp/lstm/token_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/megatron_bert/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/megatron_bert/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/megatron_bert/backbone.py` & `modelscope-1.5.0/modelscope/models/nlp/megatron_bert/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/megatron_bert/configuration.py` & `modelscope-1.5.0/modelscope/models/nlp/megatron_bert/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/megatron_bert/fill_mask.py` & `modelscope-1.5.0/modelscope/models/nlp/megatron_bert/fill_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/arguments.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/arguments.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/blocklm_utils.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/blocklm_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/configure_data.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/configure_data.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/corpora.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/corpora.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/datasets.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/datasets.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/extraction.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/extraction.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/file_utils.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/file_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/lazy_loader.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/lazy_loader.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/samplers.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/samplers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/sp_tokenizer.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/sp_tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/tokenization.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/tokenization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/tokenization_gpt2.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/tokenization_gpt2.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/data_utils/wordpiece.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/data_utils/wordpiece.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/generation_utils.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/generation_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/mglm_for_text_summarization.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/mglm_for_text_summarization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/model/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/model/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/model/distributed.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/model/distributed.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/model/downstream.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/model/downstream.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/model/modeling_bert.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/model/modeling_bert.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/model/modeling_glm.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/model/modeling_glm.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/model/prompt.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/model/prompt.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/model/transformer.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/model/transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/process_grid.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/process_grid.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/test/test_block.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/test/test_block.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/test/test_rel_shift.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/test/test_rel_shift.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/train_utils.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/train_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/mglm/utils.py` & `modelscope-1.5.0/modelscope/models/nlp/mglm/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/palm_v2/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/palm_v2/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/palm_v2/configuration.py` & `modelscope-1.5.0/modelscope/models/nlp/palm_v2/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/palm_v2/dureader_eval.py` & `modelscope-1.5.0/modelscope/models/nlp/palm_v2/dureader_eval.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/palm_v2/text_generation.py` & `modelscope-1.5.0/modelscope/models/nlp/palm_v2/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/peer/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/peer/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/peer/backbone.py` & `modelscope-1.5.0/modelscope/models/nlp/peer/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/peer/configuration.py` & `modelscope-1.5.0/modelscope/models/nlp/peer/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/peer/sas_utils.py` & `modelscope-1.5.0/modelscope/models/nlp/peer/sas_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/peer/text_classification.py` & `modelscope-1.5.0/modelscope/models/nlp/peer/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/plug/AnnealingLR.py` & `modelscope-1.5.0/modelscope/models/nlp/plug/AnnealingLR.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/plug/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/plug/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/plug/backbone.py` & `modelscope-1.5.0/modelscope/models/nlp/plug/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/plug/configuration.py` & `modelscope-1.5.0/modelscope/models/nlp/plug/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/plug/distributed_plug.py` & `modelscope-1.5.0/modelscope/models/nlp/plug/distributed_plug.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/plug/generator.py` & `modelscope-1.5.0/modelscope/models/nlp/plug/generator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/plug_mental/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/plug_mental/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/plug_mental/adv_utils.py` & `modelscope-1.5.0/modelscope/models/nlp/plug_mental/adv_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/plug_mental/backbone.py` & `modelscope-1.5.0/modelscope/models/nlp/plug_mental/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/plug_mental/configuration.py` & `modelscope-1.5.0/modelscope/models/nlp/plug_mental/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/plug_mental/text_classification.py` & `modelscope-1.5.0/modelscope/models/nlp/plug_mental/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/ponet/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/ponet/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/ponet/backbone.py` & `modelscope-1.5.0/modelscope/models/nlp/ponet/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/ponet/configuration.py` & `modelscope-1.5.0/modelscope/models/nlp/ponet/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/ponet/document_segmentation.py` & `modelscope-1.5.0/modelscope/models/nlp/ponet/document_segmentation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/ponet/fill_mask.py` & `modelscope-1.5.0/modelscope/models/nlp/ponet/fill_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/ponet/tokenization.py` & `modelscope-1.5.0/modelscope/models/nlp/ponet/tokenization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/space/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/space/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/space/configuration.py` & `modelscope-1.5.0/modelscope/models/nlp/space/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/space/dialog_intent_prediction.py` & `modelscope-1.5.0/modelscope/models/nlp/space/dialog_intent_prediction.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/space/dialog_modeling.py` & `modelscope-1.5.0/modelscope/models/nlp/space/dialog_modeling.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/space/dialog_state_tracking.py` & `modelscope-1.5.0/modelscope/models/nlp/space/dialog_state_tracking.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/space/model/gen_unified_transformer.py` & `modelscope-1.5.0/modelscope/models/nlp/space/model/gen_unified_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/space/model/generator.py` & `modelscope-1.5.0/modelscope/models/nlp/space/model/generator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/space/model/intent_unified_transformer.py` & `modelscope-1.5.0/modelscope/models/nlp/space/model/intent_unified_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/space/model/model_base.py` & `modelscope-1.5.0/modelscope/models/nlp/space/model/model_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/space/model/tokenization_space.py` & `modelscope-1.5.0/modelscope/models/nlp/space/model/tokenization_space.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/space/model/unified_transformer.py` & `modelscope-1.5.0/modelscope/models/nlp/space/model/unified_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/space/modules/embedder.py` & `modelscope-1.5.0/modelscope/models/nlp/space/modules/embedder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/space/modules/feedforward.py` & `modelscope-1.5.0/modelscope/models/nlp/space/modules/feedforward.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/space/modules/functions.py` & `modelscope-1.5.0/modelscope/models/nlp/space/modules/functions.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/space/modules/multihead_attention.py` & `modelscope-1.5.0/modelscope/models/nlp/space/modules/multihead_attention.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/space/modules/transformer_block.py` & `modelscope-1.5.0/modelscope/models/nlp/space/modules/transformer_block.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/space_T_cn/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/space_T_cn/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/space_T_cn/backbone.py` & `modelscope-1.5.0/modelscope/models/nlp/space_T_cn/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/space_T_cn/configuration.py` & `modelscope-1.5.0/modelscope/models/nlp/space_T_cn/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/space_T_cn/table_question_answering.py` & `modelscope-1.5.0/modelscope/models/nlp/space_T_cn/table_question_answering.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/space_T_en/text_to_sql.py` & `modelscope-1.5.0/modelscope/models/nlp/space_T_en/text_to_sql.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/structbert/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/structbert/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/structbert/adv_utils.py` & `modelscope-1.5.0/modelscope/models/nlp/structbert/adv_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/structbert/backbone.py` & `modelscope-1.5.0/modelscope/models/nlp/structbert/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/structbert/configuration.py` & `modelscope-1.5.0/modelscope/models/nlp/structbert/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/structbert/faq_question_answering.py` & `modelscope-1.5.0/modelscope/models/nlp/structbert/faq_question_answering.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/structbert/fill_mask.py` & `modelscope-1.5.0/modelscope/models/nlp/structbert/fill_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/structbert/text_classification.py` & `modelscope-1.5.0/modelscope/models/nlp/structbert/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/structbert/token_classification.py` & `modelscope-1.5.0/modelscope/models/nlp/structbert/token_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/task_models/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/task_models/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/task_models/feature_extraction.py` & `modelscope-1.5.0/modelscope/models/nlp/task_models/feature_extraction.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/task_models/fill_mask.py` & `modelscope-1.5.0/modelscope/models/nlp/task_models/fill_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/task_models/information_extraction.py` & `modelscope-1.5.0/modelscope/models/nlp/task_models/information_extraction.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/task_models/task_model.py` & `modelscope-1.5.0/modelscope/models/nlp/task_models/task_model.py`

 * *Files 2% similar despite different names*

```diff
@@ -480,14 +480,24 @@
         super().__init__(model_dir, *args, **kwargs)
         self.config = ConfigDict(kwargs)
         backbone_cfg = self.parse_encoder_cfg()
         head_cfg = self.parse_head_cfg()
         self.build_encoder(backbone_cfg)
         if head_cfg.type is not None:
             self.build_head(head_cfg)
+        self.post_init()
+
+    def post_init(self):
+        try:
+            head_keys_to_ignore_on_load_missing = getattr(
+                self.head, '_keys_to_ignore_on_load_missing')
+            for i in head_keys_to_ignore_on_load_missing:
+                self._keys_to_ignore_on_load_missing.append('head.' + i)
+        except Exception:
+            logger.info('head has no _keys_to_ignore_on_load_missing')
 
     def __repr__(self):
         # only log backbone and head name
         depth = 1
         return _repr(self, depth)
 
     def _get_transformer_config(self):
```

### Comparing `modelscope-1.4.2/modelscope/models/nlp/task_models/text_classification.py` & `modelscope-1.5.0/modelscope/models/nlp/task_models/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/task_models/text_ranking.py` & `modelscope-1.5.0/modelscope/models/nlp/task_models/text_ranking.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/task_models/token_classification.py` & `modelscope-1.5.0/modelscope/models/nlp/task_models/token_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/unite/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/unite/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/unite/modeling_unite.py` & `modelscope-1.5.0/modelscope/models/nlp/unite/modeling_unite.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/use/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/use/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/use/transformer.py` & `modelscope-1.5.0/modelscope/models/nlp/use/transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/use/user_satisfaction_estimation.py` & `modelscope-1.5.0/modelscope/models/nlp/use/user_satisfaction_estimation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/veco/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/veco/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/veco/backbone.py` & `modelscope-1.5.0/modelscope/models/nlp/veco/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/veco/configuration.py` & `modelscope-1.5.0/modelscope/models/nlp/veco/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/veco/fill_mask.py` & `modelscope-1.5.0/modelscope/models/nlp/veco/fill_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/veco/text_classification.py` & `modelscope-1.5.0/modelscope/models/nlp/veco/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/veco/token_classification.py` & `modelscope-1.5.0/modelscope/models/nlp/veco/token_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/xlm_roberta/__init__.py` & `modelscope-1.5.0/modelscope/models/nlp/xlm_roberta/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/xlm_roberta/backbone.py` & `modelscope-1.5.0/modelscope/models/nlp/xlm_roberta/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/nlp/xlm_roberta/configuration.py` & `modelscope-1.5.0/modelscope/models/nlp/xlm_roberta/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/config.py` & `modelscope-1.5.0/modelscope/models/science/unifold/config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/data/__init__.py` & `modelscope-1.5.0/modelscope/models/science/unifold/data/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/data/data_ops.py` & `modelscope-1.5.0/modelscope/models/science/unifold/data/data_ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/data/msa_pairing.py` & `modelscope-1.5.0/modelscope/models/science/unifold/data/msa_pairing.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/data/process.py` & `modelscope-1.5.0/modelscope/models/science/unifold/data/process.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/data/process_multimer.py` & `modelscope-1.5.0/modelscope/models/science/unifold/data/process_multimer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/data/protein.py` & `modelscope-1.5.0/modelscope/models/science/unifold/data/protein.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/data/residue_constants.py` & `modelscope-1.5.0/modelscope/models/science/unifold/data/residue_constants.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/data/utils.py` & `modelscope-1.5.0/modelscope/models/science/unifold/data/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/dataset.py` & `modelscope-1.5.0/modelscope/models/science/unifold/dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/model.py` & `modelscope-1.5.0/modelscope/models/science/unifold/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/modules/alphafold.py` & `modelscope-1.5.0/modelscope/models/science/unifold/modules/alphafold.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/modules/attentions.py` & `modelscope-1.5.0/modelscope/models/science/unifold/modules/attentions.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/modules/auxillary_heads.py` & `modelscope-1.5.0/modelscope/models/science/unifold/modules/auxillary_heads.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/modules/common.py` & `modelscope-1.5.0/modelscope/models/science/unifold/modules/common.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/modules/confidence.py` & `modelscope-1.5.0/modelscope/models/science/unifold/modules/confidence.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/modules/embedders.py` & `modelscope-1.5.0/modelscope/models/science/unifold/modules/embedders.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/modules/evoformer.py` & `modelscope-1.5.0/modelscope/models/science/unifold/modules/evoformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/modules/featurization.py` & `modelscope-1.5.0/modelscope/models/science/unifold/modules/featurization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/modules/frame.py` & `modelscope-1.5.0/modelscope/models/science/unifold/modules/frame.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/modules/structure_module.py` & `modelscope-1.5.0/modelscope/models/science/unifold/modules/structure_module.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/modules/template.py` & `modelscope-1.5.0/modelscope/models/science/unifold/modules/template.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/modules/triangle_multiplication.py` & `modelscope-1.5.0/modelscope/models/science/unifold/modules/triangle_multiplication.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/msa/mmcif.py` & `modelscope-1.5.0/modelscope/models/science/unifold/msa/mmcif.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/msa/msa_identifiers.py` & `modelscope-1.5.0/modelscope/models/science/unifold/msa/msa_identifiers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/msa/parsers.py` & `modelscope-1.5.0/modelscope/models/science/unifold/msa/parsers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/msa/pipeline.py` & `modelscope-1.5.0/modelscope/models/science/unifold/msa/pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/msa/templates.py` & `modelscope-1.5.0/modelscope/models/science/unifold/msa/templates.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/msa/tools/__init__.py` & `modelscope-1.5.0/modelscope/models/science/unifold/msa/tools/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/msa/tools/hhblits.py` & `modelscope-1.5.0/modelscope/models/science/unifold/msa/tools/hhblits.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/msa/tools/hhsearch.py` & `modelscope-1.5.0/modelscope/models/science/unifold/msa/tools/hhsearch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/msa/tools/hmmbuild.py` & `modelscope-1.5.0/modelscope/models/science/unifold/msa/tools/hmmbuild.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/msa/tools/hmmsearch.py` & `modelscope-1.5.0/modelscope/models/science/unifold/msa/tools/hmmsearch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/msa/tools/jackhmmer.py` & `modelscope-1.5.0/modelscope/models/science/unifold/msa/tools/jackhmmer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/msa/tools/kalign.py` & `modelscope-1.5.0/modelscope/models/science/unifold/msa/tools/kalign.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/msa/tools/utils.py` & `modelscope-1.5.0/modelscope/models/science/unifold/msa/tools/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/models/science/unifold/msa/utils.py` & `modelscope-1.5.0/modelscope/models/science/unifold/msa/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/auth/auth_config.py` & `modelscope-1.5.0/modelscope/msdatasets/auth/auth_config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/context/dataset_context_config.py` & `modelscope-1.5.0/modelscope/msdatasets/context/dataset_context_config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/data_files/data_files_manager.py` & `modelscope-1.5.0/modelscope/msdatasets/data_files/data_files_manager.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/data_loader/data_loader.py` & `modelscope-1.5.0/modelscope/msdatasets/data_loader/data_loader.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/data_loader/data_loader_manager.py` & `modelscope-1.5.0/modelscope/msdatasets/data_loader/data_loader_manager.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/__init__.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/__init__.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/asr_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/asr_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_farfield_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_farfield_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_dataset.py`

 * *Files 17% similar despite different names*

```diff
@@ -16,15 +16,16 @@
 
 import torch
 import torch.distributed as dist
 from torch.utils.data import IterableDataset
 
 import modelscope.msdatasets.dataset_cls.custom_datasets.audio.kws_nearfield_processor as processor
 from modelscope.trainers.audio.kws_utils.file_utils import (make_pair,
-                                                            read_lists)
+                                                            read_lists,
+                                                            tokenize)
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 
 class Processor(IterableDataset):
 
@@ -115,49 +116,80 @@
         indexes = self.sampler.sample(self.lists)
         for index in indexes:
             # yield dict(src=src)
             data = dict(src=self.lists[index])
             data.update(sampler_info)
             yield data
 
+    def dump(self, dump_file):
+        with open(dump_file, 'w', encoding='utf8') as fout:
+            for obj in self.lists:
+                if hasattr(obj, 'get') and obj.get('tokens', None) is not None:
+                    assert 'key' in obj
+                    assert 'wav' in obj
+                    assert 'txt' in obj
+                    assert len(obj['tokens']) == len(obj['txt'])
+                    dump_line = obj['key'] + ':\n'
+                    dump_line += '\t' + obj['wav'] + '\n'
+                    dump_line += '\t'
+                    for token, idx in zip(obj['tokens'], obj['txt']):
+                        dump_line += '%s(%d) ' % (token, idx)
+                    dump_line += '\n\n'
+                    fout.write(dump_line)
+                else:
+                    infos = json.loads(obj)
+                    assert 'key' in infos
+                    assert 'wav' in infos
+                    assert 'txt' in infos
+                    dump_line = infos['key'] + ':\n'
+                    dump_line += '\t' + infos['wav'] + '\n'
+                    dump_line += '\t'
+                    dump_line += '%d' % infos['txt']
+                    dump_line += '\n\n'
+                    fout.write(dump_line)
+
 
 def kws_nearfield_dataset(data_file,
                           trans_file,
                           conf,
                           symbol_table,
                           lexicon_table,
+                          need_dump=False,
+                          dump_file='',
                           partition=True):
     """ Construct dataset from arguments
 
         We have two shuffle stage in the Dataset. The first is global
         shuffle at shards tar/raw file level. The second is global shuffle
         at training samples level.
 
         Args:
             data_file (str): wave list with kaldi style
             trans_file (str): transcription list with kaldi style
             symbol_table (Dict): token list, [token_str, token_id]
             lexicon_table (Dict): words list defined with basic tokens
+            need_dump (bool): whether to dump data with mapping tokens or not
+            dump_file (str): dumping file path
             partition (bool): whether to do data partition in terms of rank
     """
 
     lists = []
     filter_conf = conf.get('filter_conf', {})
 
     wav_lists = read_lists(data_file)
     trans_lists = read_lists(trans_file)
     lists = make_pair(wav_lists, trans_lists)
+    lists = tokenize(lists, symbol_table, lexicon_table)
 
     shuffle = conf.get('shuffle', True)
     dataset = DataList(lists, shuffle=shuffle, partition=partition)
+    if need_dump:
+        dataset.dump(dump_file)
 
     dataset = Processor(dataset, processor.parse_wav)
-    dataset = Processor(dataset, processor.tokenize, symbol_table,
-                        lexicon_table, conf.get('split_with_space', False))
-
     dataset = Processor(dataset, processor.filter, **filter_conf)
 
     feature_extraction_conf = conf.get('feature_extraction_conf', {})
     if feature_extraction_conf['feature_type'] == 'mfcc':
         dataset = Processor(dataset, processor.compute_mfcc,
                             **feature_extraction_conf)
     elif feature_extraction_conf['feature_type'] == 'fbank':
```

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_processor.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_processor.py`

 * *Files 10% similar despite different names*

```diff
@@ -8,26 +8,28 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import logging
 import random
 
 import json
 import kaldiio
 import numpy as np
 import torch
 import torchaudio
 import torchaudio.compliance.kaldi as kaldi
 from torch.nn.utils.rnn import pad_sequence
 
 # torch.set_printoptions(profile="full")
+from modelscope.utils.logger import get_logger
+
+logger = get_logger()
 
 
 def parse_wav(data):
     """ Parse key/wav/txt from dict line
 
         Args:
             data: Iterable[dict()], dict has key/wav/txt/sample_rate keys
@@ -49,62 +51,15 @@
             sample_rate, kaldi_waveform = kaldiio.load_mat(wav_file)
             waveform = torch.tensor(kaldi_waveform, dtype=torch.float32)
             waveform = waveform.unsqueeze(0)
             example = dict(
                 key=key, label=txt, wav=waveform, sample_rate=sample_rate)
             yield example
         except Exception:
-            logging.warning('Failed to read {}'.format(wav_file))
-
-
-def tokenize(data, token_table, lexicon_table, split_with_space=False):
-    """ Decode text to chars
-        Inplace operation
-
-        Args:
-            data: Iterable[{key, wav, txt, sample_rate}]
-            token_table (Dict): token list, [token_str, token_id]
-            lexicon_table (Dict): words list defined with basic tokens
-            split_with_space (bool): if transciption split with space or not
-
-        Returns:
-            Iterable[{key, wav, txt, tokens, label, sample_rate}]
-    """
-    for sample in data:
-        assert 'label' in sample
-        txt = sample['label'].strip()
-
-        if token_table is None or lexicon_table is None:
-            # to compatible with hard token map for max-pooling loss
-            label = int(txt)
-        else:
-            parts = [txt]
-            tokens = []
-            for part in parts:
-                if split_with_space:
-                    part = part.split(' ')
-                for ch in part:
-                    if ch == ' ':
-                        ch = '▁'
-                    tokens.append(ch)
-
-            label = []
-            for ch in tokens:
-                if ch in lexicon_table:
-                    for sub_ch in lexicon_table[ch]:
-                        if sub_ch in token_table:
-                            label.append(token_table[sub_ch])
-                        else:
-                            label.append(token_table['<blk>'])
-                else:
-                    label.append(token_table['<blk>'])
-
-            sample['tokens'] = tokens
-        sample['label'] = label
-        yield sample
+            logger.warning('Failed to read {}'.format(wav_file))
 
 
 def filter(data, max_length=10240, min_length=10):
     """ Filter sample according to feature and label length
         Inplace operation.
 
         Args::
@@ -124,19 +79,19 @@
             num_frames = int(sample['wav'].size(1) / sample['sample_rate']
                              * 100)
         elif 'feat' in sample:
             num_frames = sample['feat'].size(0)
 
         # print("{} num frames is {}".format(sample['key'], num_frames))
         if num_frames < min_length:
-            logging.warning('{} is discard for too short: {} frames'.format(
+            logger.warning('{} is discard for too short: {} frames'.format(
                 sample['key'], num_frames))
             continue
         if num_frames > max_length:
-            logging.warning('{} is discard for too long: {} frames'.format(
+            logger.warning('{} is discard for too long: {} frames'.format(
                 sample['key'], num_frames))
             continue
         yield sample
 
 
 def resample(data, resample_rate=16000):
     """ Resample data.
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/__init__.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/bad_image_detecting_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/bad_image_detecting_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/builder.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/build.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/build.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/collate_batch.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/collate_batch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/coco.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/coco.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/mosaic_wrapper.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/mosaic_wrapper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/__init__.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/__init__.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/coco_eval.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/coco_eval.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/distributed.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/distributed.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/grouped_batch_sampler.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/grouped_batch_sampler.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/iteration_based_batch_sampler.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/iteration_based_batch_sampler.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/build.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/build.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/transforms.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/easycv_base.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/easycv_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/face_2d_keypoins/__init__.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/face_2d_keypoins/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/face_2d_keypoins/face_2d_keypoints_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/face_2d_keypoins/face_2d_keypoints_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/gopro_image_deblurring_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/gopro_image_deblurring_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/hand_2d_keypoints/__init__.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/hand_2d_keypoints/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/hand_2d_keypoints/hand_2d_keypoints_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/hand_2d_keypoints/hand_2d_keypoints_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/human_wholebody_keypoint/__init__.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/human_wholebody_keypoint/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/human_wholebody_keypoint/human_wholebody_keypoint_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/human_wholebody_keypoint/human_wholebody_keypoint_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_classification/classification_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_classification/classification_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/__init__.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/image_colorization_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/image_colorization_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/__init__.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/aug.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/aug.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_instance_segmentation_coco_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_instance_segmentation_coco_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/__init__.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/data_utils.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/data_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/image_portrait_enhancement_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/image_portrait_enhancement_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/__init__.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/image_quality_assessment_degradation_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/image_quality_assessment_degradation_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/__init__.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/image_quality_assessment_mos_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/image_quality_assessment_mos_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_semantic_segmentation/segmentation_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_semantic_segmentation/segmentation_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/language_guided_video_summarization_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/language_guided_video_summarization_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/mgeo_ranking_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/mgeo_ranking_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/__init__.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/sampler.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/sampler.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/object_detection/__init__.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/object_detection/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/object_detection/detection_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/object_detection/detection_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/augmenter.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/augmenter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/data_loader.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/data_loader.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/image_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/image_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/iou_evaluator.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/iou_evaluator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/quad_measurer.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/quad_measurer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/augment_data.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/augment_data.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/data_process.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/data_process.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_border_map.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_border_map.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_icdar_data.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_icdar_data.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_seg_detection_data.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_seg_detection_data.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/normalize_image.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/normalize_image.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/random_crop_data.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/random_crop_data.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_recognition_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_recognition_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/reds_image_deblurring_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/reds_image_deblurring_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/__init__.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/referring_video_object_segmentation_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/referring_video_object_segmentation_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/transformers.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/transformers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/__init__.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/data_utils.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/data_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/sidd_image_denoising_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/sidd_image_denoising_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/transforms.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/torch_custom_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/torch_custom_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/veco_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/veco_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/__init__.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/data_utils.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/data_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/video_frame_interpolation_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/video_frame_interpolation_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/__init__.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/video_stabilization_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/video_stabilization_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_summarization_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_summarization_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/__init__.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/video_super_resolution_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/video_super_resolution_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/dataset_cls/dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/dataset_cls/dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/download/dataset_builder.py` & `modelscope-1.5.0/modelscope/msdatasets/download/dataset_builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/download/download_config.py` & `modelscope-1.5.0/modelscope/msdatasets/download/download_config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/download/download_manager.py` & `modelscope-1.5.0/modelscope/msdatasets/download/download_manager.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/meta/data_meta_config.py` & `modelscope-1.5.0/modelscope/msdatasets/meta/data_meta_config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/meta/data_meta_manager.py` & `modelscope-1.5.0/modelscope/msdatasets/meta/data_meta_manager.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/ms_dataset.py` & `modelscope-1.5.0/modelscope/msdatasets/ms_dataset.py`

 * *Files 1% similar despite different names*

```diff
@@ -28,19 +28,14 @@
 from modelscope.utils.constant import (DEFAULT_DATASET_NAMESPACE,
                                        DEFAULT_DATASET_REVISION, ConfigFields,
                                        DownloadMode, Hubs, ModeKeys, Tasks,
                                        UploadMode)
 from modelscope.utils.import_utils import is_tf_available, is_torch_available
 from modelscope.utils.logger import get_logger
 
-try:
-    from tensorflow.data import Dataset as TfDataset
-except Exception as e:
-    print(e)
-
 logger = get_logger()
 
 
 def format_list(para) -> List:
     if para is None:
         para = []
     elif isinstance(para, str):
@@ -590,15 +585,16 @@
             preprocessors, list) else [preprocessors]
 
         columns = format_list(columns)
 
         columns = [
             key for key in self._hf_ds.features.keys() if key in columns
         ]
-        retained_columns = []
+        retained_numeric_columns = []
+        retained_unumeric_columns = []
         if to_tensor:
             sample = next(iter(self._hf_ds))
 
             sample_res = {k: np.array(sample[k]) for k in columns}
             for processor in preprocessor_list:
                 sample_res.update(
                     {k: np.array(v)
@@ -608,28 +604,31 @@
                 return np.issubdtype(value.dtype, np.integer) or np.issubdtype(
                     value.dtype, np.floating)
 
             for k in sample_res.keys():
                 if not is_numpy_number(sample_res[k]):
                     logger.warning(
                         f'Data of column {k} is non-numeric, will be removed')
+                    retained_unumeric_columns.append(k)
                     continue
-                retained_columns.append(k)
+                retained_numeric_columns.append(k)
 
         import torch
 
         class MsMapDataset(torch.utils.data.Dataset):
 
             def __init__(self, dataset: Iterable, preprocessor_list,
-                         retained_columns, columns, to_tensor):
+                         retained_numeric_columns, retained_unumeric_columns,
+                         columns, to_tensor):
                 super(MsDataset).__init__()
                 self.dataset = dataset
                 self.preprocessor_list = preprocessor_list
                 self.to_tensor = to_tensor
-                self.retained_columns = retained_columns
+                self.retained_numeric_columns = retained_numeric_columns
+                self.retained_unumeric_columns = retained_unumeric_columns
                 self.columns = columns
 
             def __len__(self):
                 return len(self.dataset)
 
             def type_converter(self, x):
                 if self.to_tensor:
@@ -637,27 +636,29 @@
                 else:
                     return x
 
             def __getitem__(self, index):
                 item_dict = self.dataset[index]
                 res = {
                     k: self.type_converter(item_dict[k])
-                    for k in self.columns
-                    if (not self.to_tensor) or k in self.retained_columns
+                    for k in self.columns if (not self.to_tensor)
+                    or k in self.retained_numeric_columns
                 }
                 for preprocessor in self.preprocessor_list:
-                    res.update({
-                        k: self.type_converter(v)
-                        for k, v in preprocessor(item_dict).items()
-                        if (not self.to_tensor) or k in self.retained_columns
-                    })
+                    for k, v in preprocessor(item_dict).items():
+                        if (not self.to_tensor) or \
+                                k in self.retained_numeric_columns:
+                            res[k] = self.type_converter(v)
+                        elif k in self.retained_unumeric_columns:
+                            res[k] = v
                 return res
 
-        return MsMapDataset(self._hf_ds, preprocessor_list, retained_columns,
-                            columns, to_tensor)
+        return MsMapDataset(self._hf_ds, preprocessor_list,
+                            retained_numeric_columns,
+                            retained_unumeric_columns, columns, to_tensor)
 
     def _to_tf_dataset_with_processors(
         self,
         batch_size: int,
         shuffle: bool,
         preprocessors: Union[Callable, List[Callable]],
         drop_remainder: bool = None,
```

### Comparing `modelscope-1.4.2/modelscope/msdatasets/task_datasets/__init__.py` & `modelscope-1.5.0/modelscope/msdatasets/task_datasets/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/utils/dataset_utils.py` & `modelscope-1.5.0/modelscope/msdatasets/utils/dataset_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/utils/delete_utils.py` & `modelscope-1.5.0/modelscope/msdatasets/utils/delete_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/utils/oss_utils.py` & `modelscope-1.5.0/modelscope/msdatasets/utils/oss_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/msdatasets/utils/upload_utils.py` & `modelscope-1.5.0/modelscope/msdatasets/utils/upload_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/ops/ailut/pyinterfaces.py` & `modelscope-1.5.0/modelscope/ops/ailut/pyinterfaces.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/ops/quadtree_attention/functions/quadtree_attention.py` & `modelscope-1.5.0/modelscope/ops/quadtree_attention/functions/quadtree_attention.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/ops/quadtree_attention/modules/quadtree_attention.py` & `modelscope-1.5.0/modelscope/ops/quadtree_attention/modules/quadtree_attention.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/outputs/cv_outputs.py` & `modelscope-1.5.0/modelscope/outputs/cv_outputs.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/outputs/nlp_outputs.py` & `modelscope-1.5.0/modelscope/outputs/nlp_outputs.py`

 * *Files 2% similar despite different names*

```diff
@@ -314,14 +314,15 @@
     Args:
         attentions (`tuple(Tensor)`, *optional* Attentions weights after the
         attention softmax, used to compute the weighted average in the
         self-attention heads.
     """
     attentions: Tensor = None
     hidden_states: Tensor = None
+    past_key_values: Tensor = None
 
 
 @dataclass
 class TextErrorCorrectionOutput(ModelOutputBase):
     """The output class for information extraction models.
     """
 
@@ -348,14 +349,29 @@
     """
 
     logits: Tensor = None
     loss: Tensor = None
 
 
 @dataclass
+class AttentionTextGenerationModelOutput(TextGenerationModelOutput):
+    """The output class for text generation of attention based models.
+
+    Args:
+        logits (`Tensor`): The logits output of the model. loss (`Tensor`,
+        *optional*) The loss of the model, available when training.
+        hidden_states (`Tensor`, *optional*) Hidden-states of the model at the
+        output of each layer plus the optional initial embedding outputs.
+    """
+    attentions: Tensor = None
+    hidden_states: Tensor = None
+    past_key_values: Tensor = None
+
+
+@dataclass
 class TokenGeneratorOutput(ModelOutputBase):
     """
     The output class for generate method of text generation models.
 
 
     Args:
         sequences (`torch.LongTensor` of shape `(batch_size*num_return_sequences, sequence_length)`):
```

### Comparing `modelscope-1.4.2/modelscope/outputs/outputs.py` & `modelscope-1.5.0/modelscope/outputs/outputs.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,14 +27,16 @@
     OUTPUT = 'output'
     OUTPUT_IMG = 'output_img'
     OUTPUT_IMGS = 'output_imgs'
     OUTPUT_VIDEO = 'output_video'
     OUTPUT_PCM = 'output_pcm'
     OUTPUT_PCM_LIST = 'output_pcm_list'
     OUTPUT_WAV = 'output_wav'
+    OUTPUT_OBJ = 'output_obj'
+    OUTPUT_MESH = 'output_mesh'
     IMG_EMBEDDING = 'img_embedding'
     SPK_EMBEDDING = 'spk_embedding'
     SPO_LIST = 'spo_list'
     TEXT_EMBEDDING = 'text_embedding'
     TRANSLATION = 'translation'
     RESPONSE = 'response'
     PREDICTION = 'prediction'
@@ -439,25 +441,49 @@
     #   "output_video": "path_to_rendered_video" , this is optional
     # and is only available when the "render" option is enabled.
     # }
     Tasks.body_3d_keypoints: [
         OutputKeys.KEYPOINTS, OutputKeys.TIMESTAMPS, OutputKeys.OUTPUT_VIDEO
     ],
 
+    # pedestrain attribute recognition result for single sample
+    # {
+    #   "boxes": [
+    #               [x1, y1, x2, y2],
+    #               [x1, y1, x2, y2],
+    #               [x1, y1, x2, y2],
+    #             ]
+    #   "labels": [
+    #                    ['Female', 'AgeOver60', 'Front', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes',
+    #                        'LongSleeve', 'Black', 'Trousers', 'Black' ],
+    #                    ['Female', 'AgeOver60', 'Front', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes',
+    #                        'LongSleeve', 'Black', 'Trousers', 'Black' ],
+    #                    ['Female', 'AgeOver60', 'Front', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes',
+    #                        'LongSleeve', 'Black', 'Trousers', 'Black' ],
+    #           ]
+    # }
+    Tasks.pedestrian_attribute_recognition: [
+        OutputKeys.BOXES, OutputKeys.LABELS
+    ],
+
     # 3D face reconstruction result for single sample
     # {
+    #     "output_obj": io.BytesIO,
+    #     "output_img": np.array with shape(h, w, 3),
     #     "output": {
-    #         "vertices": np.array with shape(n, 3),
-    #         "faces": np.array with shape(n, 3),
-    #         "faces_uv": np.array with shape(n, 3),
-    #         "faces_normal": np.array with shape(n, 3),
-    #         "colors": np.array with shape(n, 3),
-    #         "UVs": np.array with shape(n, 2),
-    #         "normals": np.array with shape(n, 3),
-    #         "texture_map": np.array with shape(h, w, 3),
+    #         "mesh": {
+    #             "vertices": np.array with shape(n, 3),
+    #             "faces": np.array with shape(n, 3),
+    #             "faces_uv": np.array with shape(n, 3),
+    #             "faces_normal": np.array with shape(n, 3),
+    #             "UVs": np.array with shape(n, 2),
+    #             "normals": np.array with shape(n, 3),
+    #         },
+    #         "vis_image": np.array with shape(h, w, 3),
+    #         "frame_list", [np.array with shape(h, w, 3), ...],
     #     }
     # }
     Tasks.face_reconstruction: [OutputKeys.OUTPUT],
 
     # 3D human reconstruction result for single sample
     # {
     #     "output": {
```

### Comparing `modelscope-1.4.2/modelscope/pipeline_inputs.py` & `modelscope-1.5.0/modelscope/pipeline_inputs.py`

 * *Files 1% similar despite different names*

```diff
@@ -147,14 +147,16 @@
     InputType.VIDEO,
     Tasks.body_2d_keypoints:
     InputType.IMAGE,
     Tasks.body_3d_keypoints:
     InputType.VIDEO,
     Tasks.hand_2d_keypoints:
     InputType.IMAGE,
+    Tasks.pedestrian_attribute_recognition:
+    InputType.IMAGE,
     Tasks.video_single_object_tracking: (InputType.VIDEO, InputType.BOX),
     Tasks.video_multi_object_tracking:
     InputType.VIDEO,
     Tasks.video_category:
     InputType.VIDEO,
     Tasks.product_retrieval_embedding:
     InputType.IMAGE,
@@ -191,14 +193,16 @@
     InputType.TEXT,
     Tasks.zero_shot_classification:
     InputType.TEXT,
     Tasks.relation_extraction:
     InputType.TEXT,
     Tasks.translation:
     InputType.TEXT,
+    Tasks.competency_aware_translation:
+    InputType.TEXT,
     Tasks.word_segmentation: [InputType.TEXT, {
         'text': InputType.TEXT,
     }],
     Tasks.part_of_speech:
     InputType.TEXT,
     Tasks.named_entity_recognition:
     InputType.TEXT,
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/audio/__init__.py` & `modelscope-1.5.0/modelscope/pipelines/audio/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -17,14 +17,15 @@
         'ans_dfsmn_pipeline': ['ANSDFSMNPipeline'],
         'ans_pipeline': ['ANSPipeline'],
         'asr_inference_pipeline': ['AutomaticSpeechRecognitionPipeline'],
         'kws_farfield_pipeline': ['KWSFarfieldPipeline'],
         'kws_kwsbp_pipeline': ['KeyWordSpottingKwsbpPipeline'],
         'linear_aec_pipeline': ['LinearAECPipeline'],
         'text_to_speech_pipeline': ['TextToSpeechSambertHifiganPipeline'],
+        'itn_inference_pipeline': ['InverseTextProcessingPipeline'],
         'inverse_text_processing_pipeline': ['InverseTextProcessingPipeline'],
         'speaker_verification_pipeline': ['SpeakerVerificationPipeline']
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/audio/ans_dfsmn_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/audio/ans_dfsmn_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/audio/ans_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/audio/ans_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/audio/asr_inference_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/audio/asr_inference_pipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -9,15 +9,16 @@
 from modelscope.models import Model
 from modelscope.outputs import OutputKeys
 from modelscope.pipelines.base import Pipeline
 from modelscope.pipelines.builder import PIPELINES
 from modelscope.preprocessors import WavToScp
 from modelscope.utils.audio.audio_utils import (extract_pcm_from_wav,
                                                 generate_scp_from_url,
-                                                load_bytes_from_url)
+                                                load_bytes_from_url,
+                                                update_local_model)
 from modelscope.utils.constant import Frameworks, ModelFile, Tasks
 from modelscope.utils.hub import snapshot_download
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 __all__ = ['AutomaticSpeechRecognitionPipeline']
@@ -113,15 +114,15 @@
         self.punc_model_revision = punc_model_revision
         self.lm_model = lm_model
         self.lm_model_revision = lm_model_revision
         self.timestamp_model = timestamp_model
         self.timestamp_model_revision = timestamp_model_revision
         self.model_cfg = self.model.forward()
 
-        self.cmd = self.get_cmd(kwargs)
+        self.cmd = self.get_cmd(kwargs, model)
         if self.cmd['code_base'] == 'funasr':
             from funasr.bin import asr_inference_launch
             self.funasr_infer_modelscope = asr_inference_launch.inference_launch(
                 mode=self.cmd['mode'],
                 maxlenratio=self.cmd['maxlenratio'],
                 minlenratio=self.cmd['minlenratio'],
                 batch_size=self.cmd['batch_size'],
@@ -149,14 +150,15 @@
                 vad_infer_config=self.cmd['vad_infer_config'],
                 vad_model_file=self.cmd['vad_model_file'],
                 vad_cmvn_file=self.cmd['vad_cmvn_file'],
                 punc_model_file=self.cmd['punc_model_file'],
                 punc_infer_config=self.cmd['punc_infer_config'],
                 timestamp_model_file=self.cmd['timestamp_model_file'],
                 timestamp_infer_config=self.cmd['timestamp_infer_config'],
+                timestamp_cmvn_file=self.cmd['timestamp_cmvn_file'],
                 outputs_dict=self.cmd['outputs_dict'],
                 param_dict=self.cmd['param_dict'],
                 token_num_relax=self.cmd['token_num_relax'],
                 decoding_ind=self.cmd['decoding_ind'],
                 decoding_mode=self.cmd['decoding_mode'],
             )
 
@@ -255,15 +257,15 @@
         output = self.preprocessor.forward(self.model_cfg, self.recog_type,
                                            self.audio_format, self.audio_in,
                                            self.audio_fs, self.cmd)
         output = self.forward(output, **kwargs)
         rst = self.postprocess(output)
         return rst
 
-    def get_cmd(self, extra_args) -> Dict[str, Any]:
+    def get_cmd(self, extra_args, model_path) -> Dict[str, Any]:
         if self.preprocessor is None:
             self.preprocessor = WavToScp()
 
         outputs = self.preprocessor.config_checking(self.model_cfg)
         # generate asr inference command
         cmd = {
             'maxlenratio': 0.0,
@@ -295,14 +297,15 @@
             'vad_model_file': None,
             'vad_cmvn_file': None,
             'time_stamp_writer': True,
             'punc_infer_config': None,
             'punc_model_file': None,
             'timestamp_infer_config': None,
             'timestamp_model_file': None,
+            'timestamp_cmvn_file': None,
             'outputs_dict': True,
             'param_dict': None,
             'model_type': outputs['model_type'],
             'idx_text': '',
             'sampled_ids': 'seq2seq/sampled_ids',
             'sampled_lengths': 'seq2seq/sampled_lengths',
             'lang': 'zh-cn',
@@ -368,14 +371,15 @@
                 self.punc_model_revision = model_config['punc_model_revision']
             if model_config.__contains__(
                     'timestamp_model') and self.timestamp_model != '':
                 self.timestamp_model = model_config['timestamp_model']
             if model_config.__contains__('timestamp_model_revision'):
                 self.timestamp_model_revision = model_config[
                     'timestamp_model_revision']
+            update_local_model(model_config, model_path, extra_args)
             self.load_vad_model(cmd)
             self.load_punc_model(cmd)
             self.load_lm_model(cmd)
             self.load_timestamp_model(cmd)
 
             user_args_dict = [
                 'output_dir',
@@ -491,18 +495,21 @@
                 'loading timestamp model from {0} ...'.format(timestamp_model))
             config_path = os.path.join(timestamp_model,
                                        ModelFile.CONFIGURATION)
             model_cfg = json.loads(open(config_path).read())
             model_dir = os.path.dirname(config_path)
             cmd['timestamp_model_file'] = os.path.join(
                 model_dir,
-                model_cfg['model']['model_config']['timestamp_model_name'])
+                model_cfg['model']['model_config']['timestamp_model_file'])
             cmd['timestamp_infer_config'] = os.path.join(
                 model_dir,
-                model_cfg['model']['model_config']['timestamp_model_config'])
+                model_cfg['model']['model_config']['timestamp_infer_config'])
+            cmd['timestamp_cmvn_file'] = os.path.join(
+                model_dir,
+                model_cfg['model']['model_config']['timestamp_cmvn_file'])
 
     def forward(self, inputs: Dict[str, Any], **kwargs) -> Dict[str, Any]:
         """Decoding
         """
 
         logger.info(f"Decoding with {inputs['audio_format']} files ...")
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/audio/asr_wenet_inference_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/audio/asr_wenet_inference_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/audio/inverse_text_processing_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/audio/inverse_text_processing_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/audio/kws_farfield_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/audio/kws_farfield_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/audio/kws_kwsbp_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/audio/kws_kwsbp_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/audio/linear_aec_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/audio/linear_aec_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/audio/lm_infer_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/audio/lm_infer_pipeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,15 +3,16 @@
 from typing import Any, Dict, Union
 
 from modelscope.metainfo import Pipelines
 from modelscope.models import Model
 from modelscope.outputs import OutputKeys
 from modelscope.pipelines.base import Pipeline
 from modelscope.pipelines.builder import PIPELINES
-from modelscope.utils.audio.audio_utils import generate_text_from_url
+from modelscope.utils.audio.audio_utils import (generate_text_from_url,
+                                                update_local_model)
 from modelscope.utils.config import Config
 from modelscope.utils.constant import Frameworks, ModelFile, Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 __all__ = ['LanguageModelPipeline']
@@ -65,15 +66,15 @@
             seg_dict_file('str'):
                 seg dict file
             param_dict('dict'):
                 extra kwargs
         """
         super().__init__(model=model, **kwargs)
         config_path = os.path.join(model, ModelFile.CONFIGURATION)
-        self.cmd = self.get_cmd(config_path, kwargs)
+        self.cmd = self.get_cmd(config_path, kwargs, model)
 
         from funasr.bin import lm_inference_launch
         self.funasr_infer_modelscope = lm_inference_launch.inference_launch(
             mode=self.cmd['mode'],
             batch_size=self.cmd['batch_size'],
             dtype=self.cmd['dtype'],
             ngpu=self.cmd['ngpu'],
@@ -132,27 +133,29 @@
                 text = inputs[0]['value']
                 if len(text) > 0:
                     rst[OutputKeys.TEXT] = text
             else:
                 rst[inputs[i]['key']] = inputs[i]['value']
         return rst
 
-    def get_cmd(self, config_path, extra_args) -> Dict[str, Any]:
+    def get_cmd(self, config_path, extra_args, model_path) -> Dict[str, Any]:
         # generate inference command
         model_cfg = Config.from_file(config_path)
         model_dir = os.path.dirname(config_path)
         mode = model_cfg.model['model_config']['mode']
         lm_model_path = os.path.join(
             model_dir, model_cfg.model['model_config']['lm_model_name'])
         lm_model_config = os.path.join(
             model_dir, model_cfg.model['model_config']['lm_model_config'])
         seg_dict_file = None
         if 'seg_dict_file' in model_cfg.model['model_config']:
             seg_dict_file = os.path.join(
                 model_dir, model_cfg.model['model_config']['seg_dict_file'])
+        update_local_model(model_cfg.model['model_config'], model_path,
+                           extra_args)
 
         cmd = {
             'mode': mode,
             'batch_size': 1,
             'dtype': 'float32',
             'ngpu': 1,  # 0: only CPU, ngpu>=1: gpu number if cuda is available
             'seed': 0,
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/audio/punctuation_processing_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/audio/punctuation_processing_pipeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,15 +6,16 @@
 import yaml
 
 from modelscope.metainfo import Pipelines
 from modelscope.models import Model
 from modelscope.outputs import OutputKeys
 from modelscope.pipelines.base import Pipeline
 from modelscope.pipelines.builder import PIPELINES
-from modelscope.utils.audio.audio_utils import generate_text_from_url
+from modelscope.utils.audio.audio_utils import (generate_text_from_url,
+                                                update_local_model)
 from modelscope.utils.constant import Frameworks, Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 __all__ = ['PunctuationProcessingPipeline']
 
@@ -39,15 +40,15 @@
     """
 
     def __init__(self, model: Union[Model, str] = None, **kwargs):
         """use `model` to create an asr pipeline for prediction
         """
         super().__init__(model=model, **kwargs)
         self.model_cfg = self.model.forward()
-        self.cmd = self.get_cmd(kwargs)
+        self.cmd = self.get_cmd(kwargs, model)
 
         from funasr.bin import punc_inference_launch
         self.funasr_infer_modelscope = punc_inference_launch.inference_launch(
             mode=self.cmd['mode'],
             batch_size=self.cmd['batch_size'],
             dtype=self.cmd['dtype'],
             ngpu=self.cmd['ngpu'],
@@ -92,22 +93,24 @@
                             rst[OutputKeys.TEXT] = value
                     elif key != 'key':
                         rst[key] = value
             else:
                 rst[inputs[i]['key']] = inputs[i]['value']
         return rst
 
-    def get_cmd(self, extra_args) -> Dict[str, Any]:
+    def get_cmd(self, extra_args, model_path) -> Dict[str, Any]:
         # generate inference command
         lang = self.model_cfg['model_config']['lang']
         punc_model_path = self.model_cfg['punc_model_path']
         punc_model_config = os.path.join(
             self.model_cfg['model_workspace'],
             self.model_cfg['model_config']['punc_config'])
         mode = self.model_cfg['model_config']['mode']
+        update_local_model(self.model_cfg['model_config'], model_path,
+                           extra_args)
         cmd = {
             'mode': mode,
             'batch_size': 1,
             'dtype': 'float32',
             'ngpu': 1,  # 0: only CPU, ngpu>=1: gpu number if cuda is available
             'seed': 0,
             'num_workers': 0,
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/audio/separation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/audio/separation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/audio/speaker_diarization_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/audio/speaker_diarization_pipeline.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,15 +9,16 @@
 
 from modelscope.metainfo import Pipelines
 from modelscope.models import Model
 from modelscope.outputs import OutputKeys
 from modelscope.pipelines.base import Pipeline
 from modelscope.pipelines.builder import PIPELINES
 from modelscope.utils.audio.audio_utils import (generate_scp_for_sv,
-                                                generate_sd_scp_from_url)
+                                                generate_sd_scp_from_url,
+                                                update_local_model)
 from modelscope.utils.constant import Frameworks, ModelFile, Tasks
 from modelscope.utils.hub import snapshot_download
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 __all__ = ['SpeakerDiarizationPipeline']
@@ -59,18 +60,19 @@
             sv_model (Optional: 'Model' or 'str'):
                 speaker verification model from model hub or local
                 example: 'damo/speech_xvector_sv-zh-cn-cnceleb-16k-spk3465-pytorch'
             sv_model_revision (Optional: 'str'):
                 speaker verfication model revision from model hub
         """
         super().__init__(model=model, **kwargs)
+        self.model_cfg = None
         config_path = os.path.join(model, ModelFile.CONFIGURATION)
         self.sv_model = sv_model
         self.sv_model_revision = sv_model_revision
-        self.cmd = self.get_cmd(config_path, kwargs)
+        self.cmd = self.get_cmd(config_path, kwargs, model)
 
         from funasr.bin import diar_inference_launch
         self.funasr_infer_modelscope = diar_inference_launch.inference_launch(
             mode=self.cmd['mode'],
             output_dir=self.cmd['output_dir'],
             batch_size=self.cmd['batch_size'],
             dtype=self.cmd['dtype'],
@@ -132,23 +134,27 @@
             # for demo service
             if i == 0 and len(inputs) == 1:
                 rst[OutputKeys.TEXT] = inputs[0]['value']
             else:
                 rst[inputs[i]['key']] = inputs[i]['value']
         return rst
 
-    def get_cmd(self, config_path, extra_args) -> Dict[str, Any]:
-        model_cfg = json.loads(open(config_path).read())
+    def get_cmd(self, config_path, extra_args, model_path) -> Dict[str, Any]:
+        self.model_cfg = json.loads(open(config_path).read())
         model_dir = os.path.dirname(config_path)
         # generate sd inference command
-        mode = model_cfg['model']['model_config']['mode']
+        mode = self.model_cfg['model']['model_config']['mode']
         diar_model_path = os.path.join(
-            model_dir, model_cfg['model']['model_config']['diar_model_name'])
+            model_dir,
+            self.model_cfg['model']['model_config']['diar_model_name'])
         diar_model_config = os.path.join(
-            model_dir, model_cfg['model']['model_config']['diar_model_config'])
+            model_dir,
+            self.model_cfg['model']['model_config']['diar_model_config'])
+        update_local_model(self.model_cfg['model']['model_config'], model_path,
+                           extra_args)
         cmd = {
             'mode': mode,
             'output_dir': None,
             'batch_size': 1,
             'dtype': 'float32',
             'ngpu': 1,  # 0: only CPU, ngpu>=1: gpu number if cuda is available
             'seed': 0,
@@ -178,32 +184,21 @@
             'streaming',
             'num_workers',
             'smooth_size',
             'dur_threshold',
             'out_format',
             'param_dict',
         ]
-        model_config = model_cfg['model']['model_config']
+        model_config = self.model_cfg['model']['model_config']
         if model_config.__contains__('sv_model') and self.sv_model != '':
             self.sv_model = model_config['sv_model']
         if model_config.__contains__('sv_model_revision'):
             self.sv_model_revision = model_config['sv_model_revision']
         self.load_sv_model(cmd)
 
-        # re-write the config with configure.json
-        for user_args in user_args_dict:
-            if (user_args in self.model_cfg['model_config']
-                    and self.model_cfg['model_config'][user_args] is not None):
-                if isinstance(cmd[user_args], dict) and isinstance(
-                        self.model_cfg['model_config'][user_args], dict):
-                    cmd[user_args].update(
-                        self.model_cfg['model_config'][user_args])
-                else:
-                    cmd[user_args] = self.model_cfg['model_config'][user_args]
-
         # rewrite the config with user args
         for user_args in user_args_dict:
             if user_args in extra_args and extra_args[user_args] is not None:
                 if isinstance(cmd[user_args], dict) and isinstance(
                         extra_args[user_args], dict):
                     cmd[user_args].update(extra_args[user_args])
                 else:
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/audio/speaker_verification_light_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/audio/speaker_verification_light_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/audio/speaker_verification_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/audio/speaker_verification_pipeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,15 +7,16 @@
 
 from modelscope.metainfo import Pipelines
 from modelscope.models import Model
 from modelscope.outputs import OutputKeys
 from modelscope.pipelines.base import Pipeline
 from modelscope.pipelines.builder import PIPELINES
 from modelscope.utils.audio.audio_utils import (generate_scp_for_sv,
-                                                generate_sv_scp_from_url)
+                                                generate_sv_scp_from_url,
+                                                update_local_model)
 from modelscope.utils.constant import Frameworks, Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 __all__ = ['SpeakerVerificationPipeline']
 
@@ -41,15 +42,15 @@
     """
 
     def __init__(self, model: Union[Model, str] = None, **kwargs):
         """use `model` to create an asr pipeline for prediction
         """
         super().__init__(model=model, **kwargs)
         self.model_cfg = self.model.forward()
-        self.cmd = self.get_cmd(kwargs)
+        self.cmd = self.get_cmd(kwargs, model)
 
         from funasr.bin import sv_inference_launch
         self.funasr_infer_modelscope = sv_inference_launch.inference_launch(
             mode=self.cmd['mode'],
             output_dir=self.cmd['output_dir'],
             batch_size=self.cmd['batch_size'],
             dtype=self.cmd['dtype'],
@@ -103,21 +104,23 @@
                     embedding = inputs[0]['value']
                     rst[OutputKeys.SPK_EMBEDDING] = embedding
             else:
                 # for multiple inputs
                 rst[inputs[i]['key']] = inputs[i]['value']
         return rst
 
-    def get_cmd(self, extra_args) -> Dict[str, Any]:
+    def get_cmd(self, extra_args, model_path) -> Dict[str, Any]:
         # generate asr inference command
         mode = self.model_cfg['model_config']['mode']
         sv_model_path = self.model_cfg['model_path']
         sv_model_config = os.path.join(
             self.model_cfg['model_workspace'],
             self.model_cfg['model_config']['sv_model_config'])
+        update_local_model(self.model_cfg['model_config'], model_path,
+                           extra_args)
         cmd = {
             'mode': mode,
             'output_dir': None,
             'batch_size': 1,
             'dtype': 'float32',
             'ngpu': 1,  # 0: only CPU, ngpu>=1: gpu number if cuda is available
             'seed': 0,
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/audio/text_to_speech_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/audio/text_to_speech_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/audio/timestamp_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/audio/timestamp_pipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -7,15 +7,16 @@
 from funasr.utils import asr_utils
 
 from modelscope.metainfo import Pipelines
 from modelscope.models import Model
 from modelscope.outputs import OutputKeys
 from modelscope.pipelines.base import Pipeline
 from modelscope.pipelines.builder import PIPELINES
-from modelscope.utils.audio.audio_utils import generate_scp_from_url
+from modelscope.utils.audio.audio_utils import (generate_scp_from_url,
+                                                update_local_model)
 from modelscope.utils.constant import Frameworks, ModelFile, Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 __all__ = ['TimestampPipeline']
 
@@ -60,15 +61,15 @@
             seg_dict_file('str'):
                 seg dict file
             param_dict('dict'):
                 extra kwargs
         """
         super().__init__(model=model, **kwargs)
         config_path = os.path.join(model, ModelFile.CONFIGURATION)
-        self.cmd = self.get_cmd(config_path, kwargs)
+        self.cmd = self.get_cmd(config_path, kwargs, model)
 
         from funasr.bin import tp_inference_launch
         self.funasr_infer_modelscope = tp_inference_launch.inference_launch(
             mode=self.cmd['mode'],
             batch_size=self.cmd['batch_size'],
             dtype=self.cmd['dtype'],
             ngpu=self.cmd['ngpu'],
@@ -191,15 +192,15 @@
                             rst[OutputKeys.TEXT] = value
                     elif key != 'key':
                         rst[key] = value
             else:
                 rst[inputs[i]['key']] = inputs[i]['value']
         return rst
 
-    def get_cmd(self, config_path, extra_args) -> Dict[str, Any]:
+    def get_cmd(self, config_path, extra_args, model_path) -> Dict[str, Any]:
         model_cfg = json.loads(open(config_path).read())
         model_dir = os.path.dirname(config_path)
         # generate inference command
         timestamp_model_file = os.path.join(
             model_dir,
             model_cfg['model']['model_config']['timestamp_model_file'])
         timestamp_infer_config = os.path.join(
@@ -216,14 +217,16 @@
             config_file.close()
             if 'frontend_conf' in root:
                 frontend_conf = root['frontend_conf']
         seg_dict_file = None
         if 'seg_dict_file' in model_cfg['model']['model_config']:
             seg_dict_file = os.path.join(
                 model_dir, model_cfg['model']['model_config']['seg_dict_file'])
+        update_local_model(model_cfg['model']['model_config'], model_path,
+                           extra_args)
 
         cmd = {
             'mode': mode,
             'batch_size': 1,
             'dtype': 'float32',
             'ngpu': 0,  # 0: only CPU, ngpu>=1: gpu number if cuda is available
             'seed': 0,
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/audio/voice_activity_detection_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/audio/voice_activity_detection_pipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -7,15 +7,16 @@
 from funasr.utils import asr_utils
 
 from modelscope.metainfo import Pipelines
 from modelscope.models import Model
 from modelscope.outputs import OutputKeys
 from modelscope.pipelines.base import Pipeline
 from modelscope.pipelines.builder import PIPELINES
-from modelscope.utils.audio.audio_utils import generate_scp_from_url
+from modelscope.utils.audio.audio_utils import (generate_scp_from_url,
+                                                update_local_model)
 from modelscope.utils.constant import Frameworks, ModelFile, Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 __all__ = ['VoiceActivityDetectionPipeline']
 
@@ -41,15 +42,15 @@
     """
 
     def __init__(self, model: Union[Model, str] = None, **kwargs):
         """use `model` to create an vad pipeline for prediction
         """
         super().__init__(model=model, **kwargs)
         config_path = os.path.join(model, ModelFile.CONFIGURATION)
-        self.cmd = self.get_cmd(config_path, kwargs)
+        self.cmd = self.get_cmd(config_path, kwargs, model)
 
         from funasr.bin import vad_inference_launch
         self.funasr_infer_modelscope = vad_inference_launch.inference_launch(
             mode=self.cmd['mode'],
             batch_size=self.cmd['batch_size'],
             dtype=self.cmd['dtype'],
             ngpu=self.cmd['ngpu'],
@@ -153,15 +154,15 @@
                 text = inputs[0]['value']
                 if len(text) > 0:
                     rst[OutputKeys.TEXT] = text
             else:
                 rst[inputs[i]['key']] = inputs[i]['value']
         return rst
 
-    def get_cmd(self, config_path, extra_args) -> Dict[str, Any]:
+    def get_cmd(self, config_path, extra_args, model_path) -> Dict[str, Any]:
         model_cfg = json.loads(open(config_path).read())
         model_dir = os.path.dirname(config_path)
         # generate inference command
         vad_model_path = os.path.join(
             model_dir, model_cfg['model']['model_config']['vad_model_name'])
         vad_model_config = os.path.join(
             model_dir, model_cfg['model']['model_config']['vad_model_config'])
@@ -171,14 +172,17 @@
         frontend_conf = None
         if os.path.exists(vad_model_config):
             config_file = open(vad_model_config, encoding='utf-8')
             root = yaml.full_load(config_file)
             config_file.close()
             if 'frontend_conf' in root:
                 frontend_conf = root['frontend_conf']
+        update_local_model(model_cfg['model']['model_config'], model_path,
+                           extra_args)
+
         cmd = {
             'mode': mode,
             'batch_size': 1,
             'dtype': 'float32',
             'ngpu': 1,  # 0: only CPU, ngpu>=1: gpu number if cuda is available
             'seed': 0,
             'num_workers': 0,
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/base.py` & `modelscope-1.5.0/modelscope/pipelines/base.py`

 * *Files 1% similar despite different names*

```diff
@@ -335,23 +335,26 @@
             elif isinstance(input_type, dict):
                 for k in input_type.keys():
                     # allow single input for multi-modal models
                     if k in input:
                         check_input_type(input_type[k], input[k])
             else:
                 raise ValueError(f'invalid input_type definition {input_type}')
-        else:
+        elif not getattr(self, '_input_has_warned', False):
             logger.warning(f'task {task_name} input definition is missing')
+            self._input_has_warned = True
 
     def _check_output(self, input):
         # this attribute is dynamically attached by registry
         # when cls is registered in registry using task name
         task_name = self.group_key
         if task_name not in TASK_OUTPUTS:
-            logger.warning(f'task {task_name} output keys are missing')
+            if not getattr(self, '_output_has_warned', False):
+                logger.warning(f'task {task_name} output keys are missing')
+                self._output_has_warned = True
             return
         output_keys = TASK_OUTPUTS[task_name]
         missing_keys = []
         input = input.keys() if isinstance(input,
                                            (dict, ModelOutputBase)) else input
         for k in output_keys:
             if k not in input:
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/builder.py` & `modelscope-1.5.0/modelscope/pipelines/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/__init__.py` & `modelscope-1.5.0/modelscope/pipelines/cv/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -110,14 +110,15 @@
     from .image_quality_assessment_man_pipeline import ImageQualityAssessmentMANPipeline
     from .bad_image_detecting_pipeline import BadImageDetecingPipeline
     from .mobile_image_super_resolution_pipeline import MobileImageSuperResolutionPipeline
     from .image_human_parsing_pipeline import ImageHumanParsingPipeline
     from .nerf_recon_acc_pipeline import NeRFReconAccPipeline
     from .controllable_image_generation_pipeline import ControllableImageGenerationPipeline
     from .image_bts_depth_estimation_pipeline import ImageBTSDepthEstimationPipeline
+    from .pedestrian_attribute_recognition_pipeline import PedestrainAttributeRecognitionPipeline
 
 else:
     _import_structure = {
         'action_recognition_pipeline': ['ActionRecognitionPipeline'],
         'action_detection_pipeline': ['ActionDetectionPipeline'],
         'animal_recognition_pipeline': ['AnimalRecognitionPipeline'],
         'body_2d_keypoints_pipeline': ['Body2DKeypointsPipeline'],
@@ -276,14 +277,17 @@
         'image_human_parsing_pipeline': ['ImageHumanParsingPipeline'],
         'nerf_recon_acc_pipeline': ['NeRFReconAccPipeline'],
         'controllable_image_generation_pipeline': [
             'ControllableImageGenerationPipeline'
         ],
         'image_bts_depth_estimation_pipeline': [
             'ImageBTSDepthEstimationPipeline'
+        ],
+        'pedestrian_attribute_recognition_pipeline': [
+            'PedestrainAttributeRecognitionPipeline'
         ]
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/action_detection_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/action_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/action_recognition_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/action_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/animal_recognition_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/animal_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/arc_face_recognition_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/face_recognition_pipeline.py`

 * *Files 5% similar despite different names*

```diff
@@ -5,47 +5,49 @@
 import cv2
 import numpy as np
 import PIL
 import torch
 
 from modelscope.metainfo import Pipelines
 from modelscope.models.cv.face_recognition.align_face import align_face
-from modelscope.models.cv.face_recognition.torchkit.backbone.arcface_backbone import \
-    _iresnet
+from modelscope.models.cv.face_recognition.torchkit.backbone import get_model
 from modelscope.outputs import OutputKeys
 from modelscope.pipelines import pipeline
 from modelscope.pipelines.base import Input, Pipeline
 from modelscope.pipelines.builder import PIPELINES
 from modelscope.preprocessors import LoadImage
 from modelscope.utils.constant import ModelFile, Tasks
 from modelscope.utils.logger import get_logger
 from . import FaceProcessingBasePipeline
 
 logger = get_logger()
 
 
 @PIPELINES.register_module(
-    Tasks.face_recognition, module_name=Pipelines.arc_face_recognition)
-class ArcFaceRecognitionPipeline(FaceProcessingBasePipeline):
+    Tasks.face_recognition, module_name=Pipelines.face_recognition)
+class FaceRecognitionPipeline(FaceProcessingBasePipeline):
 
     def __init__(self, model: str, **kwargs):
         """
         use `model` to create a face recognition pipeline for prediction
         Args:
             model: model id on modelscope hub.
         """
 
         # face recong model
         super().__init__(model=model, **kwargs)
-        face_model = _iresnet('arcface_i50', [3, 4, 14, 3])
+        device = torch.device(
+            f'cuda:{0}' if torch.cuda.is_available() else 'cpu')
+        self.device = device
+        face_model = get_model('IR_101')([112, 112])
         face_model.load_state_dict(
             torch.load(
-                osp.join(model, ModelFile.TORCH_MODEL_FILE),
-                map_location=self.device))
-        face_model = face_model.to(self.device)
+                osp.join(model, ModelFile.TORCH_MODEL_BIN_FILE),
+                map_location=device))
+        face_model = face_model.to(device)
         face_model.eval()
         self.face_model = face_model
         logger.info('face recognition model loaded!')
 
     def preprocess(self, input: Input) -> Dict[str, Any]:
         result = super().preprocess(input)
         align_img = result['img']
@@ -53,14 +55,15 @@
         face_img = np.transpose(face_img, axes=(2, 0, 1))
         face_img = (face_img / 255. - 0.5) / 0.5
         face_img = face_img.astype(np.float32)
         result['img'] = face_img
         return result
 
     def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
+        assert input['img'] is not None
         img = input['img'].unsqueeze(0)
         emb = self.face_model(img).detach().cpu().numpy()
         emb /= np.sqrt(np.sum(emb**2, -1, keepdims=True))  # l2 norm
         return {OutputKeys.IMG_EMBEDDING: emb}
 
     def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
         return inputs
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/bad_image_detecting_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/bad_image_detecting_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/body_2d_keypoints_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/body_2d_keypoints_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/body_3d_keypoints_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/body_3d_keypoints_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/card_detection_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/card_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/cmdssl_video_embedding_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/cmdssl_video_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/content_check_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/content_check_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/controllable_image_generation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/controllable_image_generation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/crowd_counting_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/crowd_counting_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/ddcolor_image_colorization_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/ddcolor_image_colorization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/ddpm_semantic_segmentation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/ddpm_semantic_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/easycv_pipelines/__init__.py` & `modelscope-1.5.0/modelscope/pipelines/cv/easycv_pipelines/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/easycv_pipelines/base.py` & `modelscope-1.5.0/modelscope/pipelines/cv/easycv_pipelines/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/easycv_pipelines/detection_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/easycv_pipelines/detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/easycv_pipelines/face_2d_keypoints_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/easycv_pipelines/face_2d_keypoints_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/easycv_pipelines/human_wholebody_keypoint_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/easycv_pipelines/human_wholebody_keypoint_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/easycv_pipelines/segmentation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/easycv_pipelines/segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/face_attribute_recognition_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/face_attribute_recognition_pipeline.py`

 * *Files 6% similar despite different names*

```diff
@@ -50,16 +50,22 @@
             '0-2', '3-9', '10-19', '20-29', '30-39', '40-49', '50-59', '60-69',
             '70+'
         ]
         self.map_list = [male_list, age_list]
 
     def preprocess(self, input: Input) -> Dict[str, Any]:
         result = super().preprocess(input)
+        if result is None:
+            rtn_dict = {}
+            rtn_dict['img'] = None
+            return rtn_dict
         return result
 
     def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
+        if input['img'] is None:
+            return {OutputKeys.SCORES: None, OutputKeys.LABELS: None}
         scores = self.fairface(input['img'])
         assert scores is not None
         return {OutputKeys.SCORES: scores, OutputKeys.LABELS: self.map_list}
 
     def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
         return inputs
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/face_detection_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/face_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/face_emotion_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/face_emotion_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/face_human_hand_detection_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/face_human_hand_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/face_image_generation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/face_image_generation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/face_liveness_ir_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/face_liveness_ir_pipeline.py`

 * *Files 15% similar despite different names*

```diff
@@ -53,27 +53,33 @@
             input_node_name.append(node.name)
 
         return sess, input_node_name, out_node_name
 
     def preprocess(self, input: Input) -> Dict[str, Any]:
 
         result = super().preprocess(input)
+        if result is None:
+            rtn_dict = {}
+            rtn_dict['input_tensor'] = None
+            return rtn_dict
         orig_img = LoadImage.convert_to_ndarray(input)
         orig_img = orig_img[:, :, ::-1]
         img = super(FaceLivenessIrPipeline,
                     self).align_face_padding(orig_img, result['bbox'], 16)
         if img.shape[0] != 112:
             img = img[8:120, 8:120, :]
         img = (img - 127.5) * 0.0078125
         input_tensor = img.astype('float32').transpose(
             (2, 0, 1))[np.newaxis, :]
         result['input_tensor'] = input_tensor
         return result
 
     def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
+        if input['input_tensor'] is None:
+            return {OutputKeys.SCORES: None, OutputKeys.BOXES: None}
         input_feed = {}
         input_feed[
             self.input_node_name[0]] = input['input_tensor'].cpu().numpy()
         result = self.sess.run(self.out_node_name, input_feed=input_feed)
         out = F.softmax(torch.FloatTensor(result), dim=-1)[0][0]
         assert result is not None
         scores = [1 - out[1].tolist()]
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/face_liveness_xc_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/face_liveness_xc_pipeline.py`

 * *Files 4% similar despite different names*

```diff
@@ -60,24 +60,30 @@
         for node in sess.get_inputs():
             input_node_name.append(node.name)
 
         return sess, input_node_name, out_node_name
 
     def preprocess(self, input: Input) -> Dict[str, Any]:
         result = super().preprocess(input)
+        if result is None:
+            rtn_dict = {}
+            rtn_dict['input_tensor'] = None
+            return rtn_dict
         img = result['img']
         img = (img - 127.5) * 0.0078125
         img = np.expand_dims(img, 0).copy()
         input_tensor = np.concatenate([img, img, img, img], axis=3)
         input_tensor = np.transpose(
             input_tensor, axes=(0, 3, 1, 2)).astype(np.float32)
         result['input_tensor'] = input_tensor
         return result
 
     def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
+        if input['input_tensor'] is None:
+            return {OutputKeys.SCORES: None, OutputKeys.BOXES: None}
         input_feed = {}
         input_feed[
             self.input_node_name[0]] = input['input_tensor'].cpu().numpy()
         result = self.sess.run(self.out_node_name, input_feed=input_feed)
         scores = [result[0][0][0].tolist()]
 
         boxes = input['bbox'].cpu().numpy()[np.newaxis, :].tolist()
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/face_processing_base_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/face_processing_base_pipeline.py`

 * *Files 0% similar despite different names*

```diff
@@ -28,15 +28,15 @@
 
         Args:
             model: model id on modelscope hub.
 
         """
         super().__init__(model=model, **kwargs)
         # face detect pipeline
-        det_model_id = 'damo/cv_resnet50_face-detection_retinaface'
+        det_model_id = 'damo/cv_ddsar_face-detection_iclr23-damofd'
         self.face_detection = pipeline(
             Tasks.face_detection, model=det_model_id)
 
     def _choose_face(self,
                      det_result,
                      min_face=10,
                      top_face=1,
@@ -97,20 +97,22 @@
         det_result = self.face_detection(img.copy())
         rtn = self._choose_face(det_result, img_shape=img.shape)
         if rtn is not None:
             scores, bboxes, face_lmks = rtn
             face_lmks = face_lmks.reshape(5, 2)
             align_img, _ = align_face(img, (112, 112), face_lmks)
 
-        result = {}
-        result['img'] = np.ascontiguousarray(align_img)
-        result['scores'] = [scores]
-        result['bbox'] = bboxes
-        result['lmks'] = face_lmks
-        return result
+            result = {}
+            result['img'] = np.ascontiguousarray(align_img)
+            result['scores'] = [scores]
+            result['bbox'] = bboxes
+            result['lmks'] = face_lmks
+            return result
+        else:
+            return None
 
     def align_face_padding(self, img, rect, padding_size=16, pad_pixel=127):
         rect = np.reshape(rect, (-1, 4))
         if img is None:
             return None
         if img.ndim == 2:
             w, h = img.shape
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/face_quality_assessment_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/face_quality_assessment_pipeline.py`

 * *Files 8% similar despite different names*

```diff
@@ -69,28 +69,34 @@
         for node in sess.get_inputs():
             input_node_name.append(node.name)
 
         return sess, input_node_name, out_node_name
 
     def preprocess(self, input: Input) -> Dict[str, Any]:
         result = super().preprocess(input)
+        if result is None:
+            rtn_dict = {}
+            rtn_dict['input_tensor'] = None
+            return rtn_dict
         align_img = result['img']
         face_img = align_img[:, :, ::-1]  # to rgb
         face_img = (face_img / 255. - 0.5) / 0.5
         face_img = np.expand_dims(face_img, 0).copy()
         face_img = np.transpose(face_img, axes=(0, 3, 1, 2))
         face_img = face_img.astype(np.float32)
         result['input_tensor'] = face_img
         return result
 
     def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
+        if input['input_tensor'] is None:
+            return {OutputKeys.SCORES: None, OutputKeys.BOXES: None}
         input_feed = {}
         input_feed[
             self.input_node_name[0]] = input['input_tensor'].cpu().numpy()
         result = self.sess.run(self.out_node_name, input_feed=input_feed)
         assert result is not None
-        scores = [result[0][0][0]]
+        scores = [np.mean(result[0][0])]
         boxes = input['bbox'].cpu().numpy()[np.newaxis, :].tolist()
         return {OutputKeys.SCORES: scores, OutputKeys.BOXES: boxes}
 
     def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
         return inputs
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/face_recognition_onnx_fm_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/face_recognition_onnx_fm_pipeline.py`

 * *Files 8% similar despite different names*

```diff
@@ -62,24 +62,30 @@
         for node in sess.get_inputs():
             input_node_name.append(node.name)
 
         return sess, input_node_name, out_node_name
 
     def preprocess(self, input: Input) -> Dict[str, Any]:
         result = super().preprocess(input)
+        if result is None:
+            rtn_dict = {}
+            rtn_dict['input_tensor'] = None
+            return rtn_dict
         align_img = result['img']
         face_img = align_img[:, :, ::-1]  # to rgb
         face_img = (face_img / 255. - 0.5) / 0.5
         face_img = np.expand_dims(face_img, 0).copy()
         face_img = np.transpose(face_img, axes=(0, 3, 1, 2))
         face_img = face_img.astype(np.float32)
         result['input_tensor'] = face_img
         return result
 
     def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
+        if input['input_tensor'] is None:
+            return {OutputKeys.IMG_EMBEDDING: None}
         input_feed = {}
         input_feed[
             self.input_node_name[0]] = input['input_tensor'].cpu().numpy()
         emb = self.sess.run(self.out_node_name, input_feed=input_feed)[0]
         emb /= np.sqrt(np.sum(emb**2, -1, keepdims=True))  # l2 norm
         return {OutputKeys.IMG_EMBEDDING: emb}
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/face_recognition_onnx_ir_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/face_recognition_onnx_ir_pipeline.py`

 * *Files 8% similar despite different names*

```diff
@@ -59,24 +59,30 @@
         for node in sess.get_inputs():
             input_node_name.append(node.name)
 
         return sess, input_node_name, out_node_name
 
     def preprocess(self, input: Input) -> Dict[str, Any]:
         result = super().preprocess(input)
+        if result is None:
+            rtn_dict = {}
+            rtn_dict['input_tensor'] = None
+            return rtn_dict
         align_img = result['img']
         face_img = align_img[:, :, ::-1]  # to rgb
         face_img = (face_img / 255. - 0.5) / 0.5
         face_img = np.expand_dims(face_img, 0).copy()
         face_img = np.transpose(face_img, axes=(0, 3, 1, 2))
         face_img = face_img.astype(np.float32)
         result['input_tensor'] = face_img
         return result
 
     def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
+        if input['input_tensor'] is None:
+            return {OutputKeys.IMG_EMBEDDING: None}
         input_feed = {}
         input_feed[
             self.input_node_name[0]] = input['input_tensor'].cpu().numpy()
         emb = self.sess.run(self.out_node_name, input_feed=input_feed)[0]
         emb /= np.sqrt(np.sum(emb**2, -1, keepdims=True))  # l2 norm
         return {OutputKeys.IMG_EMBEDDING: emb}
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/face_recognition_ood_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/face_recognition_ood_pipeline.py`

 * *Files 14% similar despite different names*

```diff
@@ -47,24 +47,29 @@
         face_model = face_model.to(self.device)
         face_model.eval()
         self.face_model = face_model
         logger.info('face recognition model loaded!')
 
     def preprocess(self, input: Input) -> Dict[str, Any]:
         result = super().preprocess(input)
+        if result is None:
+            rtn_dict = {}
+            rtn_dict['img'] = None
+            return rtn_dict
         align_img = result['img']
         face_img = align_img[:, :, ::-1]  # to rgb
         face_img = np.transpose(face_img, axes=(2, 0, 1))
         face_img = (face_img / 255. - 0.5) / 0.5
         face_img = face_img.astype(np.float32)
         result['img'] = face_img
         return result
 
     def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
-        assert input['img'] is not None
+        if input['img'] is None:
+            return {OutputKeys.IMG_EMBEDDING: None, OutputKeys.SCORES: None}
         img = input['img'].unsqueeze(0)
         output = self.face_model(img)
         emb = output[0].detach().cpu().numpy()
         emb /= np.sqrt(np.sum(emb**2, -1, keepdims=True))  # l2 norm
         scores = output[1].exp().detach().cpu().numpy().tolist()
         return {OutputKeys.IMG_EMBEDDING: emb, OutputKeys.SCORES: scores}
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/face_recognition_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/arc_face_recognition_pipeline.py`

 * *Files 12% similar despite different names*

```diff
@@ -5,65 +5,68 @@
 import cv2
 import numpy as np
 import PIL
 import torch
 
 from modelscope.metainfo import Pipelines
 from modelscope.models.cv.face_recognition.align_face import align_face
-from modelscope.models.cv.face_recognition.torchkit.backbone import get_model
+from modelscope.models.cv.face_recognition.torchkit.backbone.arcface_backbone import \
+    _iresnet
 from modelscope.outputs import OutputKeys
 from modelscope.pipelines import pipeline
 from modelscope.pipelines.base import Input, Pipeline
 from modelscope.pipelines.builder import PIPELINES
 from modelscope.preprocessors import LoadImage
 from modelscope.utils.constant import ModelFile, Tasks
 from modelscope.utils.logger import get_logger
 from . import FaceProcessingBasePipeline
 
 logger = get_logger()
 
 
 @PIPELINES.register_module(
-    Tasks.face_recognition, module_name=Pipelines.face_recognition)
-class FaceRecognitionPipeline(FaceProcessingBasePipeline):
+    Tasks.face_recognition, module_name=Pipelines.arc_face_recognition)
+class ArcFaceRecognitionPipeline(FaceProcessingBasePipeline):
 
     def __init__(self, model: str, **kwargs):
         """
         use `model` to create a face recognition pipeline for prediction
         Args:
             model: model id on modelscope hub.
         """
 
         # face recong model
         super().__init__(model=model, **kwargs)
-        device = torch.device(
-            f'cuda:{0}' if torch.cuda.is_available() else 'cpu')
-        self.device = device
-        face_model = get_model('IR_101')([112, 112])
+        face_model = _iresnet('arcface_i50', [3, 4, 14, 3])
         face_model.load_state_dict(
             torch.load(
-                osp.join(model, ModelFile.TORCH_MODEL_BIN_FILE),
-                map_location=device))
-        face_model = face_model.to(device)
+                osp.join(model, ModelFile.TORCH_MODEL_FILE),
+                map_location=self.device))
+        face_model = face_model.to(self.device)
         face_model.eval()
         self.face_model = face_model
         logger.info('face recognition model loaded!')
 
     def preprocess(self, input: Input) -> Dict[str, Any]:
         result = super().preprocess(input)
+        if result is None:
+            rtn_dict = {}
+            rtn_dict['img'] = None
+            return rtn_dict
         align_img = result['img']
         face_img = align_img[:, :, ::-1]  # to rgb
         face_img = np.transpose(face_img, axes=(2, 0, 1))
         face_img = (face_img / 255. - 0.5) / 0.5
         face_img = face_img.astype(np.float32)
         result['img'] = face_img
         return result
 
     def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
-        assert input['img'] is not None
+        if input['img'] is None:
+            return {OutputKeys.IMG_EMBEDDING: None}
         img = input['img'].unsqueeze(0)
         emb = self.face_model(img).detach().cpu().numpy()
         emb /= np.sqrt(np.sum(emb**2, -1, keepdims=True))  # l2 norm
         return {OutputKeys.IMG_EMBEDDING: emb}
 
     def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
         return inputs
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/facial_expression_recognition_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/facial_expression_recognition_pipeline.py`

 * *Files 16% similar despite different names*

```diff
@@ -45,19 +45,24 @@
         logger.info('load model done')
 
         self.map_list = [
             'Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral'
         ]
 
     def preprocess(self, input: Input) -> Dict[str, Any]:
-        result = super(FacialExpressionRecognitionPipeline,
-                       self).preprocess(input)
+        result = super().preprocess(input)
+        if result is None:
+            rtn_dict = {}
+            rtn_dict['img'] = None
+            return rtn_dict
         return result
 
     def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
+        if input['img'] is None:
+            return {OutputKeys.SCORES: None, OutputKeys.LABELS: None}
         result = self.fer(input)
         assert result is not None
         scores = result[0].tolist()
         return {OutputKeys.SCORES: scores, OutputKeys.LABELS: self.map_list}
 
     def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
         return inputs
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/facial_landmark_confidence_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/mog_face_detection_pipeline.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,67 +1,55 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import os.path as osp
 from typing import Any, Dict
 
-import cv2
 import numpy as np
-import PIL
-import torch
 
 from modelscope.metainfo import Pipelines
-from modelscope.models.cv.face_recognition.align_face import align_face
-from modelscope.models.cv.facial_landmark_confidence import \
-    FacialLandmarkConfidence
+from modelscope.models.cv.face_detection import MogFaceDetector
 from modelscope.outputs import OutputKeys
-from modelscope.pipelines import pipeline
 from modelscope.pipelines.base import Input, Pipeline
 from modelscope.pipelines.builder import PIPELINES
 from modelscope.preprocessors import LoadImage
 from modelscope.utils.constant import ModelFile, Tasks
 from modelscope.utils.logger import get_logger
-from . import FaceProcessingBasePipeline
 
 logger = get_logger()
 
 
 @PIPELINES.register_module(
-    Tasks.face_2d_keypoints, module_name=Pipelines.facial_landmark_confidence)
-class FacialLandmarkConfidencePipeline(FaceProcessingBasePipeline):
+    Tasks.face_detection, module_name=Pipelines.mog_face_detection)
+class MogFaceDetectionPipeline(Pipeline):
 
     def __init__(self, model: str, **kwargs):
         """
-        use `model` to create a facial landmrk confidence pipeline for prediction
+        use `model` to create a face detection pipeline for prediction
         Args:
             model: model id on modelscope hub.
         """
         super().__init__(model=model, **kwargs)
         ckpt_path = osp.join(model, ModelFile.TORCH_MODEL_FILE)
         logger.info(f'loading model from {ckpt_path}')
-        flcm = FacialLandmarkConfidence(
-            model_path=ckpt_path, device=self.device)
-        self.flcm = flcm
+        detector = MogFaceDetector(model_path=ckpt_path, device=self.device)
+        self.detector = detector
         logger.info('load model done')
 
     def preprocess(self, input: Input) -> Dict[str, Any]:
-
-        result = super().preprocess(input)
         img = LoadImage.convert_to_ndarray(input)
-        img = img[:, :, ::-1]
-        result['orig_img'] = img.astype(np.float32)
+        img = img.astype(np.float32)
+        result = {'img': img}
         return result
 
     def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
-        result = self.flcm(input)
+
+        result = self.detector(input)
         assert result is not None
-        lms = result[0].reshape(-1, 10).tolist()
-        scores = [1 - result[1].tolist()]
-        boxes = input['bbox'].cpu().numpy()[np.newaxis, :].tolist()
-        output_poses = []
+        bboxes = result[:, :4].tolist()
+        scores = result[:, 4].tolist()
         return {
             OutputKeys.SCORES: scores,
-            OutputKeys.POSES: output_poses,
-            OutputKeys.KEYPOINTS: lms,
-            OutputKeys.BOXES: boxes
+            OutputKeys.BOXES: bboxes,
+            OutputKeys.KEYPOINTS: None,
         }
 
     def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
         return inputs
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/general_recognition_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/general_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/hand_2d_keypoints_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/hand_2d_keypoints_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/hand_static_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/hand_static_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/hicossl_video_embedding_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/hicossl_video_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/human_reconstruction_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/human_reconstruction_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_body_reshaping_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_body_reshaping_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_bts_depth_estimation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_bts_depth_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_cartoon_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_cartoon_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_classification_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_classification_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_color_enhance_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_color_enhance_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_colorization_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_colorization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_debanding_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_debanding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_deblur_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_deblur_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_defrcn_fewshot_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_defrcn_fewshot_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_denoise_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_denoise_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_depth_estimation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_depth_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_detection_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_driving_perception_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_driving_perception_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_face_fusion_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_face_fusion_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_human_parsing_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_human_parsing_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_inpainting_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_inpainting_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_inpainting_sdv2_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_inpainting_sdv2_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_instance_segmentation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_instance_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_matching_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_matching_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_matting_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_matting_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_mvs_depth_estimation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_mvs_depth_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_open_vocabulary_detection_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_open_vocabulary_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_paintbyexample_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_paintbyexample_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_panoptic_segmentation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_panoptic_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_portrait_enhancement_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_portrait_enhancement_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_quality_assessment_degradation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_quality_assessment_degradation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_quality_assessment_man_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_quality_assessment_man_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_quality_assessment_mos_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_quality_assessment_mos_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_reid_person_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_reid_person_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_restoration_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_restoration_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_salient_detection_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_salient_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_semantic_segmentation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_semantic_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_skychange_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_skychange_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_structured_model_probing_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_structured_model_probing_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_style_transfer_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_style_transfer_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_super_resolution_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_super_resolution_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_to_image_generate_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_to_image_generate_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/image_to_image_translation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/image_to_image_translation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/indoor_layout_estimation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/indoor_layout_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/language_guided_video_summarization_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/language_guided_video_summarization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/license_plate_detection_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/license_plate_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/lineless_table_recognition_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/lineless_table_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/live_category_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/live_category_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/mask_face_recognition_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/mask_face_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/maskdino_instance_segmentation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/maskdino_instance_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/mobile_image_super_resolution_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/mobile_image_super_resolution_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/mog_face_detection_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/ulfd_face_detection_pipeline.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,54 +1,56 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import os.path as osp
 from typing import Any, Dict
 
+import cv2
 import numpy as np
+import PIL
+import torch
 
 from modelscope.metainfo import Pipelines
-from modelscope.models.cv.face_detection import MogFaceDetector
+from modelscope.models.cv.face_detection import UlfdFaceDetector
 from modelscope.outputs import OutputKeys
 from modelscope.pipelines.base import Input, Pipeline
 from modelscope.pipelines.builder import PIPELINES
 from modelscope.preprocessors import LoadImage
 from modelscope.utils.constant import ModelFile, Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 
 @PIPELINES.register_module(
-    Tasks.face_detection, module_name=Pipelines.mog_face_detection)
-class MogFaceDetectionPipeline(Pipeline):
+    Tasks.face_detection, module_name=Pipelines.ulfd_face_detection)
+class UlfdFaceDetectionPipeline(Pipeline):
 
     def __init__(self, model: str, **kwargs):
         """
         use `model` to create a face detection pipeline for prediction
         Args:
             model: model id on modelscope hub.
         """
         super().__init__(model=model, **kwargs)
         ckpt_path = osp.join(model, ModelFile.TORCH_MODEL_FILE)
         logger.info(f'loading model from {ckpt_path}')
-        detector = MogFaceDetector(model_path=ckpt_path, device=self.device)
+        detector = UlfdFaceDetector(model_path=ckpt_path, device=self.device)
         self.detector = detector
         logger.info('load model done')
 
     def preprocess(self, input: Input) -> Dict[str, Any]:
         img = LoadImage.convert_to_ndarray(input)
         img = img.astype(np.float32)
         result = {'img': img}
         return result
 
     def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
-
         result = self.detector(input)
         assert result is not None
-        bboxes = result[:, :4].tolist()
-        scores = result[:, 4].tolist()
+        bboxes = result[0].tolist()
+        scores = result[1].tolist()
         return {
             OutputKeys.SCORES: scores,
             OutputKeys.BOXES: bboxes,
             OutputKeys.KEYPOINTS: None,
         }
 
     def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/motion_generation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/motion_generation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/movie_scene_segmentation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/movie_scene_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/mtcnn_face_detection_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/mtcnn_face_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/nerf_recon_acc_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/nerf_recon_acc_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/object_detection_3d_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/object_detection_3d_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/ocr_detection_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/ocr_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/ocr_recognition_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/ocr_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/__init__.py` & `modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/model_convnext_transformer.py` & `modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/model_convnext_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/model_dla34.py` & `modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/model_dla34.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/model_resnet18_half.py` & `modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/model_resnet18_half.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/model_resnet_mutex_v4_linewithchar.py` & `modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/model_resnet_mutex_v4_linewithchar.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/model_vlpt.py` & `modelscope-1.5.0/modelscope/trainers/nlp/csanmt_translation_trainer.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,443 +1,334 @@
-# ------------------------------------------------------------------------------
-# Part of implementation is adopted from ViLT,
-# made publicly available under the Apache License 2.0 at https://github.com/dandelin/ViLT.
-# ------------------------------------------------------------------------------
-
-import math
-import os
-import sys
-
-import torch
-import torch.nn as nn
-
-BatchNorm2d = nn.BatchNorm2d
-
-
-def constant_init(module, constant, bias=0):
-    nn.init.constant_(module.weight, constant)
-    if hasattr(module, 'bias'):
-        nn.init.constant_(module.bias, bias)
-
-
-def conv3x3(in_planes, out_planes, stride=1):
-    """3x3 convolution with padding"""
-    return nn.Conv2d(
-        in_planes,
-        out_planes,
-        kernel_size=3,
-        stride=stride,
-        padding=1,
-        bias=False)
-
-
-class BasicBlock(nn.Module):
-    expansion = 1
-
-    def __init__(self, inplanes, planes, stride=1, downsample=None, dcn=None):
-        super(BasicBlock, self).__init__()
-        self.with_dcn = dcn is not None
-        self.conv1 = conv3x3(inplanes, planes, stride)
-        self.bn1 = BatchNorm2d(planes)
-        self.relu = nn.ReLU(inplace=True)
-        self.with_modulated_dcn = False
-        if self.with_dcn:
-            fallback_on_stride = dcn.get('fallback_on_stride', False)
-            self.with_modulated_dcn = dcn.get('modulated', False)
-        # self.conv2 = conv3x3(planes, planes)
-        if not self.with_dcn or fallback_on_stride:
-            self.conv2 = nn.Conv2d(
-                planes, planes, kernel_size=3, padding=1, bias=False)
-        else:
-            deformable_groups = dcn.get('deformable_groups', 1)
-            if not self.with_modulated_dcn:
-                from assets.ops.dcn import DeformConv
-                conv_op = DeformConv
-                offset_channels = 18
-            else:
-                from assets.ops.dcn import ModulatedDeformConv
-                conv_op = ModulatedDeformConv
-                offset_channels = 27
-            self.conv2_offset = nn.Conv2d(
-                planes,
-                deformable_groups * offset_channels,
-                kernel_size=3,
-                padding=1)
-            self.conv2 = conv_op(
-                planes,
-                planes,
-                kernel_size=3,
-                padding=1,
-                deformable_groups=deformable_groups,
-                bias=False)
-        self.bn2 = BatchNorm2d(planes)
-        self.downsample = downsample
-        self.stride = stride
-
-    def forward(self, x):
-        residual = x
-
-        out = self.conv1(x)
-        out = self.bn1(out)
-        out = self.relu(out)
-
-        # out = self.conv2(out)
-        if not self.with_dcn:
-            out = self.conv2(out)
-        elif self.with_modulated_dcn:
-            offset_mask = self.conv2_offset(out)
-            offset = offset_mask[:, :18, :, :]
-            mask = offset_mask[:, -9:, :, :].sigmoid()
-            out = self.conv2(out, offset, mask)
-        else:
-            offset = self.conv2_offset(out)
-            out = self.conv2(out, offset)
-        out = self.bn2(out)
-
-        if self.downsample is not None:
-            residual = self.downsample(x)
-
-        out += residual
-        out = self.relu(out)
-
-        return out
-
-
-class Bottleneck(nn.Module):
-    expansion = 4
-
-    def __init__(self, inplanes, planes, stride=1, downsample=None, dcn=None):
-        super(Bottleneck, self).__init__()
-        self.with_dcn = dcn is not None
-        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
-        self.bn1 = BatchNorm2d(planes)
-        fallback_on_stride = False
-        self.with_modulated_dcn = False
-        if self.with_dcn:
-            fallback_on_stride = dcn.get('fallback_on_stride', False)
-            self.with_modulated_dcn = dcn.get('modulated', False)
-        if not self.with_dcn or fallback_on_stride:
-            self.conv2 = nn.Conv2d(
-                planes,
-                planes,
-                kernel_size=3,
-                stride=stride,
-                padding=1,
-                bias=False)
-        else:
-            deformable_groups = dcn.get('deformable_groups', 1)
-            if not self.with_modulated_dcn:
-                from assets.ops.dcn import DeformConv
-                conv_op = DeformConv
-                offset_channels = 18
-            else:
-                from assets.ops.dcn import ModulatedDeformConv
-                conv_op = ModulatedDeformConv
-                offset_channels = 27
-            self.conv2_offset = nn.Conv2d(
-                planes,
-                deformable_groups * offset_channels,
-                kernel_size=3,
-                padding=1)
-            self.conv2 = conv_op(
-                planes,
-                planes,
-                kernel_size=3,
-                padding=1,
-                stride=stride,
-                deformable_groups=deformable_groups,
-                bias=False)
-        self.bn2 = BatchNorm2d(planes)
-        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)
-        self.bn3 = BatchNorm2d(planes * 4)
-        self.relu = nn.ReLU(inplace=True)
-        self.downsample = downsample
-        self.stride = stride
-        self.dcn = dcn
-        self.with_dcn = dcn is not None
-
-    def forward(self, x):
-        residual = x
-
-        out = self.conv1(x)
-        out = self.bn1(out)
-        out = self.relu(out)
-
-        # out = self.conv2(out)
-        if not self.with_dcn:
-            out = self.conv2(out)
-        elif self.with_modulated_dcn:
-            offset_mask = self.conv2_offset(out)
-            offset = offset_mask[:, :18, :, :]
-            mask = offset_mask[:, -9:, :, :].sigmoid()
-            out = self.conv2(out, offset, mask)
-        else:
-            offset = self.conv2_offset(out)
-            out = self.conv2(out, offset)
-        out = self.bn2(out)
-        out = self.relu(out)
-
-        out = self.conv3(out)
-        out = self.bn3(out)
-
-        if self.downsample is not None:
-            residual = self.downsample(x)
-
-        out += residual
-        out = self.relu(out)
-
-        return out
-
-
-class ResNet(nn.Module):
-
-    def __init__(self,
-                 block,
-                 layers,
-                 num_classes=1000,
-                 dcn=None,
-                 stage_with_dcn=(False, False, False, False)):
-        self.dcn = dcn
-        self.stage_with_dcn = stage_with_dcn
-        self.inplanes = 64
-        super(ResNet, self).__init__()
-        self.conv1 = nn.Conv2d(
-            3, 64, kernel_size=7, stride=2, padding=3, bias=False)
-        self.bn1 = BatchNorm2d(64)
-        self.relu = nn.ReLU(inplace=True)
-        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
-        self.layer1 = self._make_layer(block, 64, layers[0])
-        self.layer2 = self._make_layer(
-            block, 128, layers[1], stride=2, dcn=dcn)
-        self.layer3 = self._make_layer(
-            block, 256, layers[2], stride=2, dcn=dcn)
-        self.layer4 = self._make_layer(
-            block, 512, layers[3], stride=2, dcn=dcn)
-        # self.avgpool = nn.AvgPool2d(7, stride=1)
-        # self.fc = nn.Linear(512 * block.expansion, num_classes)
-
-        # self.smooth = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=1)
-
-        for m in self.modules():
-            if isinstance(m, nn.Conv2d):
-                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
-                m.weight.data.normal_(0, math.sqrt(2. / n))
-            elif isinstance(m, BatchNorm2d):
-                m.weight.data.fill_(1)
-                m.bias.data.zero_()
-        if self.dcn is not None:
-            for m in self.modules():
-                if isinstance(m, Bottleneck) or isinstance(m, BasicBlock):
-                    if hasattr(m, 'conv2_offset'):
-                        constant_init(m.conv2_offset, 0)
-
-    def _make_layer(self, block, planes, blocks, stride=1, dcn=None):
-        downsample = None
-        if stride != 1 or self.inplanes != planes * block.expansion:
-            downsample = nn.Sequential(
-                nn.Conv2d(
-                    self.inplanes,
-                    planes * block.expansion,
-                    kernel_size=1,
-                    stride=stride,
-                    bias=False),
-                BatchNorm2d(planes * block.expansion),
-            )
-
-        layers = []
-        layers.append(
-            block(self.inplanes, planes, stride, downsample, dcn=dcn))
-        self.inplanes = planes * block.expansion
-        for i in range(1, blocks):
-            layers.append(block(self.inplanes, planes, dcn=dcn))
-
-        return nn.Sequential(*layers)
-
-    def forward(self, x):
-        x = self.conv1(x)
-        x = self.bn1(x)
-        x = self.relu(x)
-        x = self.maxpool(x)
-
-        x2 = self.layer1(x)
-        x3 = self.layer2(x2)
-        x4 = self.layer3(x3)
-        x5 = self.layer4(x4)
-
-        return x2, x3, x4, x5
-
-
-class SegDetector(nn.Module):
-
-    def __init__(self,
-                 in_channels=[64, 128, 256, 512],
-                 inner_channels=256,
-                 k=10,
-                 bias=False,
-                 adaptive=False,
-                 smooth=False,
-                 serial=False,
+# Copyright (c) Alibaba, Inc. and its affiliates.
+
+import os.path as osp
+import time
+from typing import Dict, Optional
+
+import tensorflow as tf
+
+from modelscope.hub.snapshot_download import snapshot_download
+from modelscope.models.nlp import CsanmtForTranslation
+from modelscope.trainers.base import BaseTrainer
+from modelscope.trainers.builder import TRAINERS
+from modelscope.utils.constant import ModelFile
+from modelscope.utils.logger import get_logger
+
+if tf.__version__ >= '2.0':
+    tf = tf.compat.v1
+    tf.disable_eager_execution()
+
+logger = get_logger()
+
+
+@TRAINERS.register_module(module_name=r'csanmt-translation')
+class CsanmtTranslationTrainer(BaseTrainer):
+
+    def __init__(self, model: str, cfg_file: str = None, *args, **kwargs):
+        model = self.get_or_download_model_dir(model)
+        tf.reset_default_graph()
+
+        self.model_dir = model
+        self.model_path = osp.join(model, ModelFile.TF_CHECKPOINT_FOLDER)
+        if cfg_file is None:
+            cfg_file = osp.join(model, ModelFile.CONFIGURATION)
+
+        super().__init__(cfg_file)
+
+        self.params = {}
+        self._override_params_from_file()
+
+        tf_config = tf.ConfigProto(allow_soft_placement=True)
+        tf_config.gpu_options.allow_growth = True
+        self._session = tf.Session(config=tf_config)
+
+        self.source_wids = tf.placeholder(
+            dtype=tf.int64, shape=[None, None], name='source_wids')
+        self.target_wids = tf.placeholder(
+            dtype=tf.int64, shape=[None, None], name='target_wids')
+        self.output = {}
+
+        self.global_step = tf.train.create_global_step()
+
+        self.model = CsanmtForTranslation(self.model_path, **self.params)
+        output = self.model(input=self.source_wids, label=self.target_wids)
+        self.output.update(output)
+
+        self.model_saver = tf.train.Saver(
+            tf.global_variables(),
+            max_to_keep=self.params['keep_checkpoint_max'])
+        with self._session.as_default() as sess:
+            logger.info(f'loading model from {self.model_path}')
+
+            pretrained_variables_map = get_pretrained_variables_map(
+                self.model_path)
+
+            tf.train.init_from_checkpoint(self.model_path,
+                                          pretrained_variables_map)
+            sess.run(tf.global_variables_initializer())
+
+    def _override_params_from_file(self):
+
+        self.params['hidden_size'] = self.cfg['model']['hidden_size']
+        self.params['filter_size'] = self.cfg['model']['filter_size']
+        self.params['num_heads'] = self.cfg['model']['num_heads']
+        self.params['num_encoder_layers'] = self.cfg['model'][
+            'num_encoder_layers']
+        self.params['num_decoder_layers'] = self.cfg['model'][
+            'num_decoder_layers']
+        self.params['layer_preproc'] = self.cfg['model']['layer_preproc']
+        self.params['layer_postproc'] = self.cfg['model']['layer_postproc']
+        self.params['shared_embedding_and_softmax_weights'] = self.cfg[
+            'model']['shared_embedding_and_softmax_weights']
+        self.params['shared_source_target_embedding'] = self.cfg['model'][
+            'shared_source_target_embedding']
+        self.params['initializer_scale'] = self.cfg['model'][
+            'initializer_scale']
+        self.params['position_info_type'] = self.cfg['model'][
+            'position_info_type']
+        self.params['max_relative_dis'] = self.cfg['model']['max_relative_dis']
+        self.params['num_semantic_encoder_layers'] = self.cfg['model'][
+            'num_semantic_encoder_layers']
+        self.params['src_vocab_size'] = self.cfg['model']['src_vocab_size']
+        self.params['trg_vocab_size'] = self.cfg['model']['trg_vocab_size']
+        self.params['attention_dropout'] = 0.0
+        self.params['residual_dropout'] = 0.0
+        self.params['relu_dropout'] = 0.0
+
+        self.params['train_src'] = self.cfg['dataset']['train_src']
+        self.params['train_trg'] = self.cfg['dataset']['train_trg']
+        self.params['vocab_src'] = self.cfg['dataset']['src_vocab']['file']
+        self.params['vocab_trg'] = self.cfg['dataset']['trg_vocab']['file']
+
+        self.params['num_gpus'] = self.cfg['train']['num_gpus']
+        self.params['warmup_steps'] = self.cfg['train']['warmup_steps']
+        self.params['update_cycle'] = self.cfg['train']['update_cycle']
+        self.params['keep_checkpoint_max'] = self.cfg['train'][
+            'keep_checkpoint_max']
+        self.params['confidence'] = self.cfg['train']['confidence']
+        self.params['optimizer'] = self.cfg['train']['optimizer']
+        self.params['adam_beta1'] = self.cfg['train']['adam_beta1']
+        self.params['adam_beta2'] = self.cfg['train']['adam_beta2']
+        self.params['adam_epsilon'] = self.cfg['train']['adam_epsilon']
+        self.params['gradient_clip_norm'] = self.cfg['train'][
+            'gradient_clip_norm']
+        self.params['learning_rate_decay'] = self.cfg['train'][
+            'learning_rate_decay']
+        self.params['initializer'] = self.cfg['train']['initializer']
+        self.params['initializer_scale'] = self.cfg['train'][
+            'initializer_scale']
+        self.params['learning_rate'] = self.cfg['train']['learning_rate']
+        self.params['train_batch_size_words'] = self.cfg['train'][
+            'train_batch_size_words']
+        self.params['scale_l1'] = self.cfg['train']['scale_l1']
+        self.params['scale_l2'] = self.cfg['train']['scale_l2']
+        self.params['train_max_len'] = self.cfg['train']['train_max_len']
+        self.params['num_of_epochs'] = self.cfg['train']['num_of_epochs']
+        self.params['save_checkpoints_steps'] = self.cfg['train'][
+            'save_checkpoints_steps']
+        self.params['num_of_samples'] = self.cfg['train']['num_of_samples']
+        self.params['eta'] = self.cfg['train']['eta']
+
+        self.params['beam_size'] = self.cfg['evaluation']['beam_size']
+        self.params['lp_rate'] = self.cfg['evaluation']['lp_rate']
+        self.params['max_decoded_trg_len'] = self.cfg['evaluation'][
+            'max_decoded_trg_len']
+
+        self.params['seed'] = self.cfg['model']['seed']
+
+    def train(self, *args, **kwargs):
+        logger.info('Begin csanmt training')
+
+        train_src = osp.join(self.model_dir, self.params['train_src'])
+        train_trg = osp.join(self.model_dir, self.params['train_trg'])
+        vocab_src = osp.join(self.model_dir, self.params['vocab_src'])
+        vocab_trg = osp.join(self.model_dir, self.params['vocab_trg'])
+
+        epoch = 0
+        iteration = 0
+
+        with self._session.as_default() as tf_session:
+            while True:
+                epoch += 1
+                if epoch >= self.params['num_of_epochs']:
+                    break
+                tf.logging.info('%s: Epoch %i' % (__name__, epoch))
+                train_input_fn = input_fn(
+                    train_src,
+                    train_trg,
+                    vocab_src,
+                    vocab_trg,
+                    batch_size_words=self.params['train_batch_size_words'],
+                    max_len=self.params['train_max_len'],
+                    num_gpus=self.params['num_gpus']
+                    if self.params['num_gpus'] > 1 else 1,
+                    is_train=True,
+                    session=tf_session,
+                    epoch=epoch)
+
+                features, labels = train_input_fn
+
+                try:
+                    while True:
+                        features_batch, labels_batch = tf_session.run(
+                            [features, labels])
+                        iteration += 1
+                        feed_dict = {
+                            self.source_wids: features_batch,
+                            self.target_wids: labels_batch
+                        }
+                        sess_outputs = self._session.run(
+                            self.output, feed_dict=feed_dict)
+                        loss_step = sess_outputs['loss']
+                        logger.info('Iteration: {}, step loss: {:.6f}'.format(
+                            iteration, loss_step))
+
+                        if iteration % self.params[
+                                'save_checkpoints_steps'] == 0:
+                            tf.logging.info('%s: Saving model on step: %d.' %
+                                            (__name__, iteration))
+                            ck_path = self.model_dir + 'model.ckpt'
+                            self.model_saver.save(
+                                tf_session,
+                                ck_path,
+                                global_step=tf.train.get_global_step())
+
+                except tf.errors.OutOfRangeError:
+                    tf.logging.info('epoch %d end!' % (epoch))
+
+            tf.logging.info(
+                '%s: NMT training completed at time: %s.' %
+                (__name__, time.asctime(time.localtime(time.time()))))
+
+    def evaluate(self,
+                 checkpoint_path: Optional[str] = None,
                  *args,
-                 **kwargs):
-        '''
-        bias: Whether conv layers have bias or not.
-        adaptive: Whether to use adaptive threshold training or not.
-        smooth: If true, use bilinear instead of deconv.
-        serial: If true, thresh prediction will combine segmentation result as input.
-        '''
-        super(SegDetector, self).__init__()
-        self.k = k
-        self.serial = serial
-        self.up5 = nn.Upsample(scale_factor=2, mode='nearest')
-        self.up4 = nn.Upsample(scale_factor=2, mode='nearest')
-        self.up3 = nn.Upsample(scale_factor=2, mode='nearest')
-
-        self.in5 = nn.Conv2d(in_channels[-1], inner_channels, 1, bias=bias)
-        self.in4 = nn.Conv2d(in_channels[-2], inner_channels, 1, bias=bias)
-        self.in3 = nn.Conv2d(in_channels[-3], inner_channels, 1, bias=bias)
-        self.in2 = nn.Conv2d(in_channels[-4], inner_channels, 1, bias=bias)
-
-        self.out5 = nn.Sequential(
-            nn.Conv2d(
-                inner_channels, inner_channels // 4, 3, padding=1, bias=bias),
-            nn.Upsample(scale_factor=8, mode='nearest'))
-        self.out4 = nn.Sequential(
-            nn.Conv2d(
-                inner_channels, inner_channels // 4, 3, padding=1, bias=bias),
-            nn.Upsample(scale_factor=4, mode='nearest'))
-        self.out3 = nn.Sequential(
-            nn.Conv2d(
-                inner_channels, inner_channels // 4, 3, padding=1, bias=bias),
-            nn.Upsample(scale_factor=2, mode='nearest'))
-        self.out2 = nn.Conv2d(
-            inner_channels, inner_channels // 4, 3, padding=1, bias=bias)
-
-        self.binarize = nn.Sequential(
-            nn.Conv2d(
-                inner_channels, inner_channels // 4, 3, padding=1, bias=bias),
-            BatchNorm2d(inner_channels // 4), nn.ReLU(inplace=True),
-            nn.ConvTranspose2d(inner_channels // 4, inner_channels // 4, 2, 2),
-            BatchNorm2d(inner_channels // 4), nn.ReLU(inplace=True),
-            nn.ConvTranspose2d(inner_channels // 4, 1, 2, 2), nn.Sigmoid())
-        self.binarize.apply(self.weights_init)
-
-        self.adaptive = adaptive
-        if adaptive:
-            self.thresh = self._init_thresh(
-                inner_channels, serial=serial, smooth=smooth, bias=bias)
-            self.thresh.apply(self.weights_init)
-
-        self.in5.apply(self.weights_init)
-        self.in4.apply(self.weights_init)
-        self.in3.apply(self.weights_init)
-        self.in2.apply(self.weights_init)
-        self.out5.apply(self.weights_init)
-        self.out4.apply(self.weights_init)
-        self.out3.apply(self.weights_init)
-        self.out2.apply(self.weights_init)
-
-    def weights_init(self, m):
-        classname = m.__class__.__name__
-        if classname.find('Conv') != -1:
-            nn.init.kaiming_normal_(m.weight.data)
-        elif classname.find('BatchNorm') != -1:
-            m.weight.data.fill_(1.)
-            m.bias.data.fill_(1e-4)
-
-    def _init_thresh(self,
-                     inner_channels,
-                     serial=False,
-                     smooth=False,
-                     bias=False):
-        in_channels = inner_channels
-        if serial:
-            in_channels += 1
-        self.thresh = nn.Sequential(
-            nn.Conv2d(
-                in_channels, inner_channels // 4, 3, padding=1, bias=bias),
-            BatchNorm2d(inner_channels // 4), nn.ReLU(inplace=True),
-            self._init_upsample(
-                inner_channels // 4,
-                inner_channels // 4,
-                smooth=smooth,
-                bias=bias), BatchNorm2d(inner_channels // 4),
-            nn.ReLU(inplace=True),
-            self._init_upsample(
-                inner_channels // 4, 1, smooth=smooth, bias=bias),
-            nn.Sigmoid())
-        return self.thresh
-
-    def _init_upsample(self,
-                       in_channels,
-                       out_channels,
-                       smooth=False,
-                       bias=False):
-        if smooth:
-            inter_out_channels = out_channels
-            if out_channels == 1:
-                inter_out_channels = in_channels
-            module_list = [
-                nn.Upsample(scale_factor=2, mode='nearest'),
-                nn.Conv2d(in_channels, inter_out_channels, 3, 1, 1, bias=bias)
-            ]
-            if out_channels == 1:
-                module_list.append(
-                    nn.Conv2d(
-                        in_channels,
-                        out_channels,
-                        kernel_size=1,
-                        stride=1,
-                        padding=1,
-                        bias=True))
-
-            return nn.Sequential(module_list)
-        else:
-            return nn.ConvTranspose2d(in_channels, out_channels, 2, 2)
-
-    def forward(self, features, gt=None, masks=None, training=False):
-        c2, c3, c4, c5 = features
-        in5 = self.in5(c5)
-        in4 = self.in4(c4)
-        in3 = self.in3(c3)
-        in2 = self.in2(c2)
-
-        out4 = self.up5(in5) + in4  # 1/16
-        out3 = self.up4(out4) + in3  # 1/8
-        out2 = self.up3(out3) + in2  # 1/4
-
-        p5 = self.out5(in5)
-        p4 = self.out4(out4)
-        p3 = self.out3(out3)
-        p2 = self.out2(out2)
-
-        fuse = torch.cat((p5, p4, p3, p2), 1)
-        # this is the pred module, not binarization module;
-        # We do not correct the name due to the trained model.
-        binary = self.binarize(fuse)
-        return binary
-
-    def step_function(self, x, y):
-        return torch.reciprocal(1 + torch.exp(-self.k * (x - y)))
-
-
-class VLPTModel(nn.Module):
-
-    def __init__(self, *args, **kwargs):
-        super(VLPTModel, self).__init__()
-        self.backbone = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)
-        self.decoder = SegDetector(
-            in_channels=[256, 512, 1024, 2048], adaptive=True, k=50, **kwargs)
-
-    def forward(self, x):
-        return self.decoder(self.backbone(x))
-
-
-class DBModel(nn.Module):
-
-    def __init__(self, *args, **kwargs):
-        super(DBModel, self).__init__()
-        self.backbone = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)
-        self.decoder = SegDetector(
-            in_channels=[64, 128, 256, 512], adaptive=True, k=50, **kwargs)
+                 **kwargs) -> Dict[str, float]:
+        """evaluate a dataset
+
+        evaluate a dataset via a specific model from the `checkpoint_path` path, if the `checkpoint_path`
+        does not exist, read from the config file.
+
+        Args:
+            checkpoint_path (Optional[str], optional): the model path. Defaults to None.
 
-    def forward(self, x):
-        return self.decoder(self.backbone(x))
+        Returns:
+            Dict[str, float]: the results about the evaluation
+            Example:
+            {"accuracy": 0.5091743119266054, "f1": 0.673780487804878}
+        """
+        pass
+
+
+def input_fn(src_file,
+             trg_file,
+             src_vocab_file,
+             trg_vocab_file,
+             num_buckets=20,
+             max_len=100,
+             batch_size=200,
+             batch_size_words=4096,
+             num_gpus=1,
+             is_train=True,
+             session=None,
+             epoch=None):
+    src_vocab = tf.lookup.StaticVocabularyTable(
+        tf.lookup.TextFileInitializer(
+            src_vocab_file,
+            key_dtype=tf.string,
+            key_index=tf.lookup.TextFileIndex.WHOLE_LINE,
+            value_dtype=tf.int64,
+            value_index=tf.lookup.TextFileIndex.LINE_NUMBER),
+        num_oov_buckets=1)  # NOTE unk-> vocab_size
+    trg_vocab = tf.lookup.StaticVocabularyTable(
+        tf.lookup.TextFileInitializer(
+            trg_vocab_file,
+            key_dtype=tf.string,
+            key_index=tf.lookup.TextFileIndex.WHOLE_LINE,
+            value_dtype=tf.int64,
+            value_index=tf.lookup.TextFileIndex.LINE_NUMBER),
+        num_oov_buckets=1)  # NOTE unk-> vocab_size
+    src_dataset = tf.data.TextLineDataset(src_file)
+    trg_dataset = tf.data.TextLineDataset(trg_file)
+    src_trg_dataset = tf.data.Dataset.zip((src_dataset, trg_dataset))
+    src_trg_dataset = src_trg_dataset.map(
+        lambda src, trg: (tf.string_split([src]), tf.string_split([trg])),
+        num_parallel_calls=10).prefetch(1000000)
+    src_trg_dataset = src_trg_dataset.map(
+        lambda src, trg: (src.values, trg.values),
+        num_parallel_calls=10).prefetch(1000000)
+    src_trg_dataset = src_trg_dataset.map(
+        lambda src, trg: (src_vocab.lookup(src), trg_vocab.lookup(trg)),
+        num_parallel_calls=10).prefetch(1000000)
+
+    if is_train:
+
+        def key_func(src_data, trg_data):
+            bucket_width = (max_len + num_buckets - 1) // num_buckets
+            bucket_id = tf.maximum(
+                tf.size(input=src_data) // bucket_width,
+                tf.size(input=trg_data) // bucket_width)
+            return tf.cast(tf.minimum(num_buckets, bucket_id), dtype=tf.int64)
+
+        def reduce_func(unused_key, windowed_data):
+            return windowed_data.padded_batch(
+                batch_size_words, padded_shapes=([None], [None]))
+
+        def window_size_func(key):
+            bucket_width = (max_len + num_buckets - 1) // num_buckets
+            key += 1
+            size = (num_gpus * batch_size_words // (key * bucket_width))
+            return tf.cast(size, dtype=tf.int64)
+
+        src_trg_dataset = src_trg_dataset.filter(
+            lambda src, trg: tf.logical_and(
+                tf.size(input=src) <= max_len,
+                tf.size(input=trg) <= max_len))
+        src_trg_dataset = src_trg_dataset.apply(
+            tf.data.experimental.group_by_window(
+                key_func=key_func,
+                reduce_func=reduce_func,
+                window_size_func=window_size_func))
+
+    else:
+        src_trg_dataset = src_trg_dataset.padded_batch(
+            batch_size * num_gpus, padded_shapes=([None], [None]))
+
+    iterator = tf.data.make_initializable_iterator(src_trg_dataset)
+    tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)
+    features, labels = iterator.get_next()
+
+    if is_train:
+        session.run(iterator.initializer)
+        if epoch == 1:
+            session.run(tf.tables_initializer())
+    return features, labels
+
+
+def get_pretrained_variables_map(checkpoint_file_path, ignore_scope=None):
+    reader = tf.train.NewCheckpointReader(
+        tf.train.latest_checkpoint(checkpoint_file_path))
+    saved_shapes = reader.get_variable_to_shape_map()
+    if ignore_scope is None:
+        var_names = sorted([(var.name, var.name.split(':')[0])
+                            for var in tf.global_variables()
+                            if var.name.split(':')[0] in saved_shapes])
+    else:
+        var_names = sorted([(var.name, var.name.split(':')[0])
+                            for var in tf.global_variables()
+                            if var.name.split(':')[0] in saved_shapes and all(
+                                scope not in var.name
+                                for scope in ignore_scope)])
+    restore_vars = []
+    name2var = dict(
+        zip(
+            map(lambda x: x.name.split(':')[0], tf.global_variables()),
+            tf.global_variables()))
+    restore_map = {}
+    with tf.variable_scope('', reuse=True):
+        for var_name, saved_var_name in var_names:
+            curr_var = name2var[saved_var_name]
+            var_shape = curr_var.get_shape().as_list()
+            if var_shape == saved_shapes[saved_var_name]:
+                restore_vars.append(curr_var)
+                restore_map[saved_var_name] = curr_var
+    return restore_map
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/ocr_modules/__init__.py` & `modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/ocr_modules/convnext.py` & `modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/convnext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/ocr_modules/timm_tinyc.py` & `modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/timm_tinyc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/ocr_modules/vitstr.py` & `modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/vitstr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/ops.py` & `modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/resnet18_v1.py` & `modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/resnet18_v1.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/resnet_utils.py` & `modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/resnet_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/table_process.py` & `modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/table_process.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/ocr_utils/utils.py` & `modelscope-1.5.0/modelscope/pipelines/cv/ocr_utils/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/panorama_depth_estimation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/panorama_depth_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/product_retrieval_embedding_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/product_retrieval_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/product_segmentation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/product_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/realtime_video_object_detection_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/realtime_video_object_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/referring_video_object_segmentation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/referring_video_object_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/retina_face_detection_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/retina_face_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/shop_segmentation_pipleline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/shop_segmentation_pipleline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/skin_retouching_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/skin_retouching_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/table_recognition_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/table_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/tbs_detection_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/tbs_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/tbs_detection_utils/utils.py` & `modelscope-1.5.0/modelscope/pipelines/cv/tbs_detection_utils/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/text_driven_segmentation_pipleline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/text_driven_segmentation_pipleline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/tinynas_classification_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/tinynas_classification_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/tinynas_detection_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/tinynas_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/ulfd_face_detection_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/facial_landmark_confidence_pipeline.py`

 * *Files 20% similar despite different names*

```diff
@@ -4,54 +4,75 @@
 
 import cv2
 import numpy as np
 import PIL
 import torch
 
 from modelscope.metainfo import Pipelines
-from modelscope.models.cv.face_detection import UlfdFaceDetector
+from modelscope.models.cv.face_recognition.align_face import align_face
+from modelscope.models.cv.facial_landmark_confidence import \
+    FacialLandmarkConfidence
 from modelscope.outputs import OutputKeys
+from modelscope.pipelines import pipeline
 from modelscope.pipelines.base import Input, Pipeline
 from modelscope.pipelines.builder import PIPELINES
 from modelscope.preprocessors import LoadImage
 from modelscope.utils.constant import ModelFile, Tasks
 from modelscope.utils.logger import get_logger
+from . import FaceProcessingBasePipeline
 
 logger = get_logger()
 
 
 @PIPELINES.register_module(
-    Tasks.face_detection, module_name=Pipelines.ulfd_face_detection)
-class UlfdFaceDetectionPipeline(Pipeline):
+    Tasks.face_2d_keypoints, module_name=Pipelines.facial_landmark_confidence)
+class FacialLandmarkConfidencePipeline(FaceProcessingBasePipeline):
 
     def __init__(self, model: str, **kwargs):
         """
-        use `model` to create a face detection pipeline for prediction
+        use `model` to create a facial landmrk confidence pipeline for prediction
         Args:
             model: model id on modelscope hub.
         """
         super().__init__(model=model, **kwargs)
         ckpt_path = osp.join(model, ModelFile.TORCH_MODEL_FILE)
         logger.info(f'loading model from {ckpt_path}')
-        detector = UlfdFaceDetector(model_path=ckpt_path, device=self.device)
-        self.detector = detector
+        flcm = FacialLandmarkConfidence(
+            model_path=ckpt_path, device=self.device)
+        self.flcm = flcm
         logger.info('load model done')
 
     def preprocess(self, input: Input) -> Dict[str, Any]:
+
+        result = super().preprocess(input)
+        if result is None:
+            rtn_dict = {}
+            rtn_dict['img'] = None
+            return rtn_dict
         img = LoadImage.convert_to_ndarray(input)
-        img = img.astype(np.float32)
-        result = {'img': img}
+        img = img[:, :, ::-1]
+        result['orig_img'] = img.astype(np.float32)
         return result
 
     def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
-        result = self.detector(input)
+        if input['img'] is None:
+            return {
+                OutputKeys.SCORES: None,
+                OutputKeys.POSES: None,
+                OutputKeys.KEYPOINTS: None,
+                OutputKeys.BOXES: None
+            }
+        result = self.flcm(input)
         assert result is not None
-        bboxes = result[0].tolist()
-        scores = result[1].tolist()
+        lms = result[0].reshape(-1, 10).tolist()
+        scores = [1 - result[1].tolist()]
+        boxes = input['bbox'].cpu().numpy()[np.newaxis, :].tolist()
+        output_poses = []
         return {
             OutputKeys.SCORES: scores,
-            OutputKeys.BOXES: bboxes,
-            OutputKeys.KEYPOINTS: None,
+            OutputKeys.POSES: output_poses,
+            OutputKeys.KEYPOINTS: lms,
+            OutputKeys.BOXES: boxes
         }
 
     def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
         return inputs
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/video_category_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/video_category_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/video_colorization_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/video_colorization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/video_deinterlace_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/video_deinterlace_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/video_depth_estimation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/video_depth_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/video_frame_interpolation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/video_frame_interpolation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/video_human_matting_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/video_human_matting_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/video_inpainting_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/video_inpainting_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/video_instance_segmentation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/video_instance_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/video_multi_object_tracking_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/video_multi_object_tracking_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/video_object_segmentation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/video_object_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/video_panoptic_segmentation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/video_panoptic_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/video_single_object_tracking_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/video_single_object_tracking_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/video_stabilization_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/video_stabilization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/video_summarization_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/video_summarization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/video_super_resolution_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/video_super_resolution_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/vidt_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/vidt_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/virtual_try_on_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/virtual_try_on_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/vision_efficient_tuning_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/vision_efficient_tuning_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/vision_middleware_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/vision_middleware_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/vop_retrieval_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/vop_retrieval_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/cv/vop_retrieval_se_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/cv/vop_retrieval_se_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/__init__.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/asr_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/asr_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/diffusers_wrapped/__init__.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/diffusers_wrapped/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/diffusers_wrapped/diffusers_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/diffusers_wrapped/diffusers_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/__init__.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/__init__.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/utils.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/document_vl_embedding_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/document_vl_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/gridvlp_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/gridvlp_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/image_captioning_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/image_captioning_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/image_text_retrieval_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/image_text_retrieval_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/mgeo_ranking_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/mgeo_ranking_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/multi_modal_embedding_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/multi_modal_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/ocr_recognition_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/ocr_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/sudoku_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/sudoku_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/team_multi_modal_similarity_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/team_multi_modal_similarity_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/text2sql_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/text2sql_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/text_to_image_synthesis_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/text_to_image_synthesis_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/text_to_video_synthesis_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/text_to_video_synthesis_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/video_captioning_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/video_captioning_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/video_multi_modal_embedding_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/video_multi_modal_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/video_question_answering_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/video_question_answering_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/visual_entailment_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/visual_entailment_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/visual_grounding_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/visual_grounding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/multi_modal/visual_question_answering_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/multi_modal/visual_question_answering_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/__init__.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -26,19 +26,21 @@
     from .translation_quality_estimation_pipeline import TranslationQualityEstimationPipeline
     from .text_error_correction_pipeline import TextErrorCorrectionPipeline
     from .word_alignment_pipeline import WordAlignmentPipeline
     from .text_generation_pipeline import TextGenerationPipeline, TextGenerationT5Pipeline
     from .fid_dialogue_pipeline import FidDialoguePipeline
     from .token_classification_pipeline import TokenClassificationPipeline
     from .translation_pipeline import TranslationPipeline
+    from .canmt_translation_pipeline import CanmtTranslationPipeline
     from .word_segmentation_pipeline import WordSegmentationPipeline, WordSegmentationThaiPipeline
     from .zero_shot_classification_pipeline import ZeroShotClassificationPipeline
     from .mglm_text_summarization_pipeline import MGLMTextSummarizationPipeline
     from .codegeex_code_translation_pipeline import CodeGeeXCodeTranslationPipeline
     from .codegeex_code_generation_pipeline import CodeGeeXCodeGenerationPipeline
+    from .glm130b_text_generation_pipeline import GLM130bTextGenerationPipeline
     from .translation_evaluation_pipeline import TranslationEvaluationPipeline
     from .user_satisfaction_estimation_pipeline import UserSatisfactionEstimationPipeline
     from .siamese_uie_pipeline import SiameseUiePipeline
     from .document_grounded_dialog_generate_pipeline import DocumentGroundedDialogGeneratePipeline
     from .document_grounded_dialog_retrieval_pipeline import DocumentGroundedDialogRetrievalPipeline
     from .document_grounded_dialog_rerank_pipeline import DocumentGroundedDialogRerankPipeline
     from .language_identification_pipline import LanguageIdentificationPipeline
@@ -74,25 +76,27 @@
         'text_error_correction_pipeline': ['TextErrorCorrectionPipeline'],
         'word_alignment_pipeline': ['WordAlignmentPipeline'],
         'text_generation_pipeline':
         ['TextGenerationPipeline', 'TextGenerationT5Pipeline'],
         'fid_dialogue_pipeline': ['FidDialoguePipeline'],
         'token_classification_pipeline': ['TokenClassificationPipeline'],
         'translation_pipeline': ['TranslationPipeline'],
+        'canmt_translation_pipeline': ['CanmtTranslationPipeline'],
         'translation_quality_estimation_pipeline':
         ['TranslationQualityEstimationPipeline'],
         'word_segmentation_pipeline':
         ['WordSegmentationPipeline', 'WordSegmentationThaiPipeline'],
         'zero_shot_classification_pipeline':
         ['ZeroShotClassificationPipeline'],
         'mglm_text_summarization_pipeline': ['MGLMTextSummarizationPipeline'],
         'codegeex_code_translation_pipeline':
         ['CodeGeeXCodeTranslationPipeline'],
         'codegeex_code_generation_pipeline':
         ['CodeGeeXCodeGenerationPipeline'],
+        'glm130b_text_generation_pipeline': ['GLM130bTextGenerationPipeline'],
         'translation_evaluation_pipeline': ['TranslationEvaluationPipeline'],
         'user_satisfaction_estimation_pipeline':
         ['UserSatisfactionEstimationPipeline'],
         'siamese_uie_pipeline': ['SiameseUiePipeline'],
         'document_grounded_dialog_generate_pipeline': [
             'DocumentGroundedDialogGeneratePipeline'
         ],
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/automatic_post_editing_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/automatic_post_editing_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/codegeex_code_generation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/codegeex_code_generation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/codegeex_code_translation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/codegeex_code_translation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/conversational_text_to_sql_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/conversational_text_to_sql_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/dialog_intent_prediction_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/dialog_intent_prediction_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/dialog_modeling_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/dialog_modeling_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/dialog_state_tracking_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/dialog_state_tracking_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/distributed_gpt3_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/distributed_gpt3_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/distributed_gpt_moe_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/distributed_gpt_moe_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/distributed_plug_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/distributed_plug_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/document_grounded_dialog_generate_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/document_grounded_dialog_generate_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/document_grounded_dialog_rerank_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/document_grounded_dialog_rerank_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/document_grounded_dialog_retrieval_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/document_grounded_dialog_retrieval_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/document_segmentation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/document_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/extractive_summarization_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/extractive_summarization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/faq_question_answering_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/faq_question_answering_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/fasttext_text_classification_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/fasttext_text_classification_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/feature_extraction_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/feature_extraction_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/fid_dialogue_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/fid_dialogue_pipeline.py`

 * *Files 14% similar despite different names*

```diff
@@ -3,18 +3,20 @@
 import re
 from typing import Any, Dict, Optional, Union
 
 import torch
 
 from modelscope.metainfo import Pipelines
 from modelscope.models.base import Model
+from modelscope.models.nlp.fid_T5.text_generation import T5Chat
 from modelscope.outputs import OutputKeys, TokenGeneratorOutput
 from modelscope.pipelines.base import Pipeline
 from modelscope.pipelines.builder import PIPELINES
 from modelscope.preprocessors import Preprocessor
+from modelscope.utils.chinese_utils import remove_space_between_chinese_chars
 from modelscope.utils.constant import ModelFile, Tasks
 
 context_template = '假设我和你正在进行对话，请你给我得体、准确、友好的回复。以下是我们的对话内容。{context}'
 history_template = '假设我和你正在进行对话，请你给我得体、准确、友好的回复。以下是我们的对话内容。{context}' \
                    '#以下是在此之前我们的对话内容，可作为回复时的参考。{history}'
 knowledge_template = '假设我和你正在进行对话，请你给我得体、准确、友好的回复。以下是我们的对话内容。{context}' \
                      '#以下是和对话相关的知识，请你参考该知识进行回复。{knowledge}'
@@ -63,48 +65,66 @@
             model=model,
             preprocessor=preprocessor,
             config_file=config_file,
             device=device,
             auto_collate=auto_collate,
             **kwargs)
 
+        self.is_t5 = isinstance(self.model, T5Chat)
+
         if preprocessor is None:
             self.preprocessor_tokenizer = Preprocessor.from_pretrained(
                 self.model.model_dir, **kwargs)
+        if not self.is_t5:
+            unused_list = []
+            for i in range(1, 100):
+                unused_list.append(f'[unused{i}]')
+            self.preprocessor_tokenizer.nlp_tokenizer.tokenizer.add_special_tokens(
+                {'additional_special_tokens': unused_list})
 
         assert isinstance(self.model, Model), \
             f'please check whether model config exists in {ModelFile.CONFIGURATION}'
         self.model = self.model.to(self.device)
         self.model.eval()
 
         self.SEP = '[SEP]'
 
+    def _sanitize_parameters(self, **pipeline_parameters):
+        preprocess_params = pipeline_parameters.get('preprocess_params', {})
+        forward_params = pipeline_parameters.get('forward_params', {})
+        postprocess_params = pipeline_parameters.get('postprocess_params', {})
+        return preprocess_params, forward_params, postprocess_params
+
     def forward(self, inputs: Dict[str, Any], **forward_params):
         with torch.no_grad():
             return self.model.generate(inputs, **forward_params)
 
     def preprocess(self, inputs: Dict[str, Any],
                    **preprocess_params) -> Dict[str, Any]:
         # init params
         max_encoder_length = 300
+        context_turn = 3
         if 'max_encoder_length' in preprocess_params:
             max_encoder_length = preprocess_params.pop('max_encoder_length')
+        if 'context_turn' in preprocess_params:
+            context_turn = preprocess_params.pop('context_turn')
+
         # get raw data
         history = inputs['history'] if 'history' in inputs else ''
         if len(history) <= 0:
             raise Exception('history is necessary!')
         knowledge = inputs['knowledge'] if 'knowledge' in inputs else ''
         user_profile = inputs[
             'user_profile'] if 'user_profile' in inputs else ''
         bot_profile = inputs['bot_profile'] if 'bot_profile' in inputs else ''
         # parse raw data
         history = history.split(self.SEP)
-        context = history[-3:]
+        context = history[-context_turn:]
         context = self.process_context(context)
-        history = history[:-3]
+        history = history[:-context_turn]
         history = self.process_history(history)
         knowledge = knowledge.split(self.SEP)
 
         model_input = []
         if history and len(history) > 0:
             model_input.append(
                 history_template.format(context=context, history=history))
@@ -121,29 +141,33 @@
                 bot_profile_template.format(
                     context=context, bot_profile=bot_profile))
 
         if not model_input:
             model_input.append(context_template.format(context=context))
 
         for i in range(len(model_input)):
-            model_input[i] = re.sub('[ \t]+', '▂', model_input[i])
+            if self.is_t5:
+                model_input[i] = model_input[i].replace(
+                    '\n', '▁<extra_id_22>').replace('\t',
+                                                    '▁<extra_id_33>').replace(
+                                                        '  ', '▁<extra_id_23>')
+            else:
+                model_input[i] = model_input[i].replace(
+                    '\n', '[unused22]').replace('\t', '[unused33]').replace(
+                        '  ', '[unused23]')
 
         # tokenization
         input_ids = self.preprocessor_tokenizer(
             {'src_txt': model_input},
             padding=True,
             truncation=True,
             max_length=max_encoder_length,
             return_tensors='pt')['input_ids'].unsqueeze(0).to(self.device)
         input_dict = {
-            'input_ids':
-            input_ids.to(torch.int64).to(self.device),
-            'attention_mask': (input_ids != 0).to(torch.int64).to(self.device),
-            'token_type_ids':
-            torch.zeros(input_ids.shape).to(torch.int64).to(self.device)
+            'input_ids': input_ids.to(torch.int64).to(self.device),
         }
 
         return input_dict
 
     def process_context(self, context_list):
         subject = '我'
         for i in range(len(context_list) - 1, -1, -1):
@@ -167,10 +191,28 @@
     def postprocess(self, inputs: TokenGeneratorOutput,
                     **postprocess_params) -> Dict[str, Any]:
 
         if torch.cuda.is_available():
             hypotheses = inputs.sequences.detach().cpu().tolist()
 
         response = self.preprocessor_tokenizer.decode(
-            hypotheses[0], skip_special_tokens=True)
-        response = response.replace(' ', '')
+            hypotheses[0], skip_special_tokens=self.is_t5)
+
+        token_mapping = {
+            '<extra_id_22>': '\n',
+            '<extra_id_33>': '\t',
+            '<extra_id_23>': '  ',
+            '[unused22]': '\n',
+            '[unused33]': '\t',
+            '[unused23]': '  ',
+            '[SEP]': '',
+            '[CLS]': '',
+            '[PAD]': '',
+            '[UNK]': ''
+        }
+        for s, t in token_mapping.items():
+            response = response.replace(s, t)
+
+        if not self.is_t5:
+            response = remove_space_between_chinese_chars(response)
+
         return {OutputKeys.TEXT: response}
```

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/fill_mask_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/fill_mask_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/information_extraction_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/information_extraction_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/interactive_translation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/interactive_translation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/language_identification_pipline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/language_identification_pipline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/mglm_text_summarization_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/mglm_text_summarization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/named_entity_recognition_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/named_entity_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/sentence_embedding_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/sentence_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/siamese_uie_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/siamese_uie_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/summarization_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/summarization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/table_question_answering_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/table_question_answering_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/text_classification_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/text_classification_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/text_error_correction_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/text_error_correction_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/text_generation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/text_generation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/text_ranking_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/text_ranking_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/token_classification_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/token_classification_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/translation_evaluation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/translation_evaluation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/translation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/translation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/translation_quality_estimation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/translation_quality_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/user_satisfaction_estimation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/user_satisfaction_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/word_alignment_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/word_alignment_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/word_segmentation_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/word_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/nlp/zero_shot_classification_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/nlp/zero_shot_classification_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/science/__init__.py` & `modelscope-1.5.0/modelscope/pipelines/science/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/science/protein_structure_pipeline.py` & `modelscope-1.5.0/modelscope/pipelines/science/protein_structure_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/pipelines/util.py` & `modelscope-1.5.0/modelscope/pipelines/util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/__init__.py` & `modelscope-1.5.0/modelscope/preprocessors/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -14,15 +14,16 @@
                         ImageInstanceSegmentationPreprocessor,
                         ImageDenoisePreprocessor, ImageDeblurPreprocessor)
     from .cv import (ImageClassificationMmcvPreprocessor,
                      ImageRestorationPreprocessor,
                      ControllableImageGenerationPreprocessor)
     from .kws import WavToLists
     from .tts import KanttsDataPreprocessor
-    from .multi_modal import (OfaPreprocessor, MPlugPreprocessor,
+    from .multi_modal import (DiffusionImageGenerationPreprocessor,
+                              OfaPreprocessor, MPlugPreprocessor,
                               HiTeAPreprocessor)
     from .nlp import (
         DocumentSegmentationTransformersPreprocessor,
         FaqQuestionAnsweringTransformersPreprocessor,
         FillMaskPoNetPreprocessor, FillMaskTransformersPreprocessor,
         TextRankingTransformersPreprocessor,
         RelationExtractionTransformersPreprocessor,
@@ -36,15 +37,15 @@
         CodeGeeXPreprocessor, MGLMSummarizationPreprocessor,
         ZeroShotClassificationTransformersPreprocessor,
         TextGenerationJiebaPreprocessor, SentencePiecePreprocessor,
         DialogIntentPredictionPreprocessor, DialogModelingPreprocessor,
         DialogStateTrackingPreprocessor, ConversationalTextToSqlPreprocessor,
         TableQuestionAnsweringPreprocessor, NERPreprocessorViet,
         NERPreprocessorThai, WordSegmentationPreprocessorThai,
-        TranslationEvaluationPreprocessor,
+        TranslationEvaluationPreprocessor, CanmtTranslationPreprocessor,
         DialogueClassificationUsePreprocessor, SiameseUiePreprocessor,
         DocumentGroundedDialogGeneratePreprocessor,
         DocumentGroundedDialogRetrievalPreprocessor,
         DocumentGroundedDialogRerankPreprocessor)
     from .video import ReadVideoData, MovieSceneSegmentationPreprocessor
 
 else:
@@ -63,16 +64,18 @@
         'cv': [
             'ImageClassificationMmcvPreprocessor',
             'ImageRestorationPreprocessor',
             'ControllableImageGenerationPreprocessor'
         ],
         'kws': ['WavToLists'],
         'tts': ['KanttsDataPreprocessor'],
-        'multi_modal':
-        ['OfaPreprocessor', 'MPlugPreprocessor', 'HiTeAPreprocessor'],
+        'multi_modal': [
+            'DiffusionImageGenerationPreprocessor', 'OfaPreprocessor',
+            'MPlugPreprocessor', 'HiTeAPreprocessor'
+        ],
         'nlp': [
             'DocumentSegmentationTransformersPreprocessor',
             'FaqQuestionAnsweringTransformersPreprocessor',
             'FillMaskPoNetPreprocessor', 'FillMaskTransformersPreprocessor',
             'NLPTokenizerPreprocessorBase',
             'TextRankingTransformersPreprocessor',
             'RelationExtractionTransformersPreprocessor',
@@ -90,14 +93,15 @@
             'NERPreprocessorViet', 'NERPreprocessorThai',
             'WordSegmentationPreprocessorThai',
             'DialogIntentPredictionPreprocessor', 'DialogModelingPreprocessor',
             'DialogStateTrackingPreprocessor',
             'ConversationalTextToSqlPreprocessor',
             'TableQuestionAnsweringPreprocessor',
             'TranslationEvaluationPreprocessor',
+            'CanmtTranslationPreprocessor',
             'DialogueClassificationUsePreprocessor', 'SiameseUiePreprocessor',
             'DialogueClassificationUsePreprocessor',
             'DocumentGroundedDialogGeneratePreprocessor',
             'DocumentGroundedDialogRetrievalPreprocessor',
             'DocumentGroundedDialogRerankPreprocessor'
         ],
     }
```

### Comparing `modelscope-1.4.2/modelscope/preprocessors/asr.py` & `modelscope-1.5.0/modelscope/preprocessors/asr.py`

 * *Files 0% similar despite different names*

```diff
@@ -237,24 +237,24 @@
             inputs['model_lang'] = 'zh-cn'
 
         return inputs
 
     def scp_generation_from_wav(self, inputs: Dict[str, Any]) -> List[Any]:
         """scp generation from waveform files
         """
-        from easyasr.common import asr_utils
 
         # find all waveform files
         wav_list = []
         if inputs['recog_type'] == 'wav':
             file_path = inputs['wav_path']
             if os.path.isfile(file_path):
                 if file_path.endswith('.wav') or file_path.endswith('.WAV'):
                     wav_list.append(file_path)
         else:
+            from easyasr.common import asr_utils
             wav_dir: str = inputs['wav_path']
             wav_list = asr_utils.recursion_dir_all_wav(wav_list, wav_dir)
 
         list_count: int = len(wav_list)
         inputs['wav_count'] = list_count
 
         # store all wav into audio list
```

### Comparing `modelscope-1.4.2/modelscope/preprocessors/audio.py` & `modelscope-1.5.0/modelscope/preprocessors/audio.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/base.py` & `modelscope-1.5.0/modelscope/preprocessors/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -11,14 +11,16 @@
 from modelscope.utils.logger import get_logger
 from .builder import build_preprocessor
 
 logger = get_logger()
 
 PREPROCESSOR_MAP = {
     # nlp
+    (Models.canmt, Tasks.competency_aware_translation):
+    Preprocessors.canmt_translation,
     # bart
     (Models.bart, Tasks.text_error_correction):
     Preprocessors.text_error_correction,
 
     # bert
     (Models.bert, Tasks.backbone):
     Preprocessors.sen_cls_tokenizer,
```

### Comparing `modelscope-1.4.2/modelscope/preprocessors/builder.py` & `modelscope-1.5.0/modelscope/preprocessors/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/common.py` & `modelscope-1.5.0/modelscope/preprocessors/common.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/cv/__init__.py` & `modelscope-1.5.0/modelscope/preprocessors/cv/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -25,15 +25,17 @@
         'image_quality_assessment_man':
         ['ImageQualityAssessmentMANPreprocessor'],
         'image_restoration_preprocessor': ['ImageRestorationPreprocessor'],
         'bad_image_detecting_preprocessor': ['BadImageDetectingPreprocessor'],
         'controllable_image_generation':
         ['ControllableImageGenerationPreprocessor'],
         'image_classification_preprocessor':
-        ['ImageClassificationPreprocessor']
+        ['ImageClassificationPreprocessor'],
+        'diffusion_image_generation_preprocessor':
+        ['DiffusionImageGenerationPreprocessor']
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
```

### Comparing `modelscope-1.4.2/modelscope/preprocessors/cv/action_detection_mapper.py` & `modelscope-1.5.0/modelscope/preprocessors/cv/action_detection_mapper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/cv/bad_image_detecting_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/cv/bad_image_detecting_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/cv/controllable_image_generation.py` & `modelscope-1.5.0/modelscope/preprocessors/cv/controllable_image_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/cv/cv2_transforms.py` & `modelscope-1.5.0/modelscope/preprocessors/cv/cv2_transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/cv/image_classification_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/cv/image_classification_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/cv/image_quality_assessment_man.py` & `modelscope-1.5.0/modelscope/preprocessors/cv/image_quality_assessment_man.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/cv/image_quality_assessment_mos.py` & `modelscope-1.5.0/modelscope/preprocessors/cv/image_quality_assessment_mos.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/cv/image_restoration_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/cv/image_restoration_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/cv/mmcls_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/cv/mmcls_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/cv/timer.py` & `modelscope-1.5.0/modelscope/preprocessors/cv/timer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/cv/util.py` & `modelscope-1.5.0/modelscope/preprocessors/cv/util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/cv/video_stabilization.py` & `modelscope-1.5.0/modelscope/preprocessors/cv/video_stabilization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/cv/video_super_resolution.py` & `modelscope-1.5.0/modelscope/preprocessors/cv/video_super_resolution.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/image.py` & `modelscope-1.5.0/modelscope/preprocessors/image.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/kws.py` & `modelscope-1.5.0/modelscope/preprocessors/kws.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/movie_scene_segmentation/transforms.py` & `modelscope-1.5.0/modelscope/preprocessors/movie_scene_segmentation/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/multi_modal.py` & `modelscope-1.5.0/modelscope/preprocessors/multi_modal.py`

 * *Files 3% similar despite different names*

```diff
@@ -5,14 +5,15 @@
 
 import decord
 import json
 import numpy as np
 import torch
 from PIL import Image
 from timm.data import create_transform
+from torchvision import transforms
 from torchvision.transforms import Compose, Normalize, Resize, ToTensor
 
 from modelscope.hub.snapshot_download import snapshot_download
 from modelscope.metainfo import Preprocessors
 from modelscope.pipelines.base import Input
 from modelscope.pipelines.cv.cmdssl_video_embedding_pipeline import (
     VCenterCrop, VCompose, VNormalize, VRescale, VToTensor)
@@ -22,15 +23,56 @@
                                        Tasks)
 from .base import Preprocessor
 from .builder import PREPROCESSORS
 from .ofa import *  # noqa
 from .ofa.utils.collate import collate_fn
 from .ofa.utils.constant import OFA_TASK_KEY_MAPPING
 
-__all__ = ['OfaPreprocessor', 'MPlugPreprocessor', 'HiTeAPreprocessor']
+__all__ = [
+    'DiffusionImageGenerationPreprocessor', 'OfaPreprocessor',
+    'MPlugPreprocessor', 'HiTeAPreprocessor'
+]
+
+
+@PREPROCESSORS.register_module(
+    Fields.multi_modal,
+    module_name=Preprocessors.diffusion_image_generation_preprocessor)
+class DiffusionImageGenerationPreprocessor(Preprocessor):
+    """ Preprocessor the data with the combination of image and text.
+        Args:
+            data: process the value as an image for keys ending with 'FILE'
+                or existing in preprocessor_image_keys and pass-through the values of other keys.
+
+    """
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.preprocessor_resolution = kwargs.pop('resolution', 512)
+        self.preprocessor_mean = kwargs.pop('mean', [0.5, 0.5, 0.5])
+        self.preprocessor_std = kwargs.pop('std', [0.5, 0.5, 0.5])
+        self.preprocessor_image_keys = set(kwargs.pop('image_keys', []))
+        self.transform_input = transforms.Compose([
+            transforms.Resize(
+                self.preprocessor_resolution,
+                interpolation=transforms.InterpolationMode.BILINEAR),
+            transforms.ToTensor(),
+            transforms.Normalize(self.preprocessor_mean,
+                                 self.preprocessor_std),
+        ])
+
+    def __call__(self, data) -> Dict[str, Any]:
+        results = {}
+        for key, value in data.items():
+            if key.endswith(':FILE') or key in self.preprocessor_image_keys:
+                image = load_image(value)
+                img = self.transform_input(image)
+                results[key.replace(':FILE', '').lower()] = img
+            else:
+                results[key.lower()] = value
+        return results
 
 
 @PREPROCESSORS.register_module(
     Fields.multi_modal, module_name=Preprocessors.ofa_tasks_preprocessor)
 class OfaPreprocessor(Preprocessor):
 
     def __init__(self,
```

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/__init__.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -26,19 +26,20 @@
                         DialogModelingPreprocessor,
                         DialogStateTrackingPreprocessor, InputFeatures,
                         MultiWOZBPETextField, IntentBPETextField)
     from .space_T_en import ConversationalTextToSqlPreprocessor
     from .space_T_cn import TableQuestionAnsweringPreprocessor
     from .mglm_summarization_preprocessor import MGLMSummarizationPreprocessor
     from .translation_evaluation_preprocessor import TranslationEvaluationPreprocessor
+    from .canmt_translation import CanmtTranslationPreprocessor
     from .dialog_classification_use_preprocessor import DialogueClassificationUsePreprocessor
     from .siamese_uie_preprocessor import SiameseUiePreprocessor
     from .document_grounded_dialog_generate_preprocessor import DocumentGroundedDialogGeneratePreprocessor
     from .document_grounded_dialog_retrieval_preprocessor import DocumentGroundedDialogRetrievalPreprocessor
-    from .document_grounded_dialog_retrieval_preprocessor import DocumentGroundedDialogRerankPreprocessor
+    from .document_grounded_dialog_rerank_preprocessor import DocumentGroundedDialogRerankPreprocessor
 else:
     _import_structure = {
         'bert_seq_cls_tokenizer': ['Tokenize'],
         'document_segmentation_preprocessor':
         ['DocumentSegmentationTransformersPreprocessor'],
         'faq_question_answering_preprocessor':
         ['FaqQuestionAnsweringTransformersPreprocessor'],
@@ -86,14 +87,17 @@
             'MultiWOZBPETextField',
             'IntentBPETextField',
         ],
         'space_T_en': ['ConversationalTextToSqlPreprocessor'],
         'space_T_cn': ['TableQuestionAnsweringPreprocessor'],
         'translation_evaluation_preprocessor':
         ['TranslationEvaluationPreprocessor'],
+        'canmt_translation': [
+            'CanmtTranslationPreprocessor',
+        ],
         'dialog_classification_use_preprocessor':
         ['DialogueClassificationUsePreprocessor'],
         'siamese_uie_preprocessor': ['SiameseUiePreprocessor'],
         'document_grounded_dialog_generate_preprocessor':
         ['DocumentGroundedDialogGeneratePreprocessor'],
         'document_grounded_dialog_retrieval_preprocessor':
         ['DocumentGroundedDialogRetrievalPreprocessor'],
```

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/bert_seq_cls_tokenizer.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/bert_seq_cls_tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/dialog_classification_use_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/dialog_classification_use_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/document_segmentation_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/document_segmentation_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/faq_question_answering_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/faq_question_answering_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/feature_extraction_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/feature_extraction_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/fill_mask_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/fill_mask_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/mgeo_ranking_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/mgeo_ranking_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/mglm_summarization_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/mglm_summarization_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/relation_extraction_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/relation_extraction_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/sentence_embedding_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/sentence_embedding_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/siamese_uie_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/siamese_uie_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space/__init__.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space/args.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space/args.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space/batch.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space/batch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space/data_loader.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space/data_loader.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space/dialog_intent_prediction_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space/dialog_intent_prediction_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space/dialog_modeling_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space/dialog_modeling_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space/dialog_state_tracking_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space/dialog_state_tracking_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space/dst_processors.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space/dst_processors.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space/fields/__init__.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space/fields/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space/fields/gen_field.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space/fields/gen_field.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space/fields/intent_field.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space/fields/intent_field.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space/lazy_dataset.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space/lazy_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space/preprocess.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space/preprocess.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space/sampler.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space/sampler.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space/tensorlistdataset.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space/tensorlistdataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space/tokenizer.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_cn/__init__.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_cn/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_cn/fields/database.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_cn/fields/database.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_cn/fields/schema_link.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_cn/fields/schema_link.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_cn/fields/struct.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_cn/fields/struct.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_cn/table_question_answering_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_cn/table_question_answering_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_en/__init__.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_en/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_en/conversational_text_to_sql_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_en/conversational_text_to_sql_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_en/fields/__init__.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_en/fields/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_en/fields/common_utils.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_en/fields/common_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_en/fields/parse.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_en/fields/parse.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_en/fields/preprocess_dataset.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_en/fields/preprocess_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/space_T_en/fields/process_dataset.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/space_T_en/fields/process_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/text_classification_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/text_classification_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/text_error_correction.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/text_error_correction.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/text_generation_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/text_generation_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/text_ranking_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/text_ranking_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/token_classification_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/token_classification_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/token_classification_thai_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/token_classification_thai_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/token_classification_viet_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/token_classification_viet_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/transformers_tokenizer.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/transformers_tokenizer.py`

 * *Files 4% similar despite different names*

```diff
@@ -84,14 +84,19 @@
             return tokenizer.from_pretrained(
                 model_dir) if model_dir is not None else tokenizer()
         elif model_type == Models.veco:
             from transformers import XLMRobertaTokenizer, XLMRobertaTokenizerFast
             tokenizer = XLMRobertaTokenizerFast if self.use_fast else XLMRobertaTokenizer
             return tokenizer.from_pretrained(
                 model_dir) if model_dir is not None else tokenizer()
+        elif model_type == Models.llama:
+            from modelscope.models.nlp import LlamaTokenizer, LlamaTokenizerFast
+            tokenizer = LlamaTokenizerFast if self.use_fast else LlamaTokenizer
+            return tokenizer.from_pretrained(
+                model_dir) if model_dir is not None else tokenizer()
 
         assert model_dir is not None
         return AutoTokenizer.from_pretrained(model_dir, use_fast=self.use_fast)
 
     def __call__(self, text, text_pair=None, **kwargs):
         kwargs['max_length'] = kwargs.get('max_length',
                                           kwargs.pop('sequence_length', None))
```

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/translation_evaluation_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/translation_evaluation_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/utils.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/word_alignment_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/word_alignment_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/nlp/zero_shot_classification_preprocessor.py` & `modelscope-1.5.0/modelscope/preprocessors/nlp/zero_shot_classification_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/__init__.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/asr.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/asr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/base.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/image_captioning.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/image_captioning.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/image_classification.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/image_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/ocr_recognition.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/ocr_recognition.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/sudoku.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/sudoku.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/summarization.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/summarization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/text2sql.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/text2sql.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/text_classification.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/text_to_image_synthesis.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/text_to_image_synthesis.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/utils/audio_helper.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/utils/audio_helper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/utils/bridge_content_encoder.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/utils/bridge_content_encoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/utils/collate.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/utils/collate.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/utils/constant.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/utils/constant.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/utils/get_tables.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/utils/get_tables.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/utils/random_help.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/utils/random_help.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/utils/text2phone.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/utils/text2phone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/utils/transforms.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/utils/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/utils/vision_helper.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/utils/vision_helper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/visual_entailment.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/visual_entailment.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/visual_grounding.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/visual_grounding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/ofa/visual_question_answering.py` & `modelscope-1.5.0/modelscope/preprocessors/ofa/visual_question_answering.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/science/uni_fold.py` & `modelscope-1.5.0/modelscope/preprocessors/science/uni_fold.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/tts.py` & `modelscope-1.5.0/modelscope/preprocessors/tts.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/preprocessors/video.py` & `modelscope-1.5.0/modelscope/preprocessors/video.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/tools/eval.py` & `modelscope-1.5.0/modelscope/tools/eval.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/tools/speech_tts_autolabel.py` & `modelscope-1.5.0/modelscope/tools/speech_tts_autolabel.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 import argparse
 import os
 import sys
 import zipfile
 
+from modelscope.hub.check_model import check_local_model_is_latest
 from modelscope.hub.snapshot_download import snapshot_download
 from modelscope.utils.constant import ThirdParty
 from modelscope.utils.logger import get_logger
 
 try:
     from tts_autolabel import AutoLabeling
 except ImportError:
```

### Comparing `modelscope-1.4.2/modelscope/tools/train.py` & `modelscope-1.5.0/modelscope/tools/train.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/__init__.py` & `modelscope-1.5.0/modelscope/trainers/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/audio/__init__.py` & `modelscope-1.5.0/modelscope/trainers/audio/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/audio/ans_trainer.py` & `modelscope-1.5.0/modelscope/trainers/audio/ans_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/audio/asr_trainer.py` & `modelscope-1.5.0/modelscope/trainers/audio/asr_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/audio/kws_farfield_trainer.py` & `modelscope-1.5.0/modelscope/trainers/audio/kws_farfield_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/audio/kws_nearfield_trainer.py` & `modelscope-1.5.0/modelscope/trainers/audio/kws_nearfield_trainer.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,14 +2,15 @@
 import copy
 import datetime
 import os
 import re
 from typing import Callable, Dict, Optional
 
 import torch
+import torch.distributed as dist
 import yaml
 from tensorboardX import SummaryWriter
 from torch import nn as nn
 from torch import optim as optim
 from torch.utils.data import DataLoader
 
 from modelscope.metainfo import Trainers
@@ -19,19 +20,18 @@
 from modelscope.trainers.base import BaseTrainer
 from modelscope.trainers.builder import TRAINERS
 from modelscope.utils.checkpoint import load_checkpoint, save_checkpoint
 from modelscope.utils.config import Config
 from modelscope.utils.constant import DEFAULT_MODEL_REVISION, ModelFile
 from modelscope.utils.device import create_device
 from modelscope.utils.logger import get_logger
-from modelscope.utils.torch_utils import (get_dist_info, get_local_rank,
-                                          init_dist, set_random_seed)
+from modelscope.utils.torch_utils import set_random_seed
 from .kws_utils.batch_utils import executor_cv, executor_test, executor_train
 from .kws_utils.det_utils import compute_det
-from .kws_utils.file_utils import query_tokens_id, read_lexicon, read_token
+from .kws_utils.file_utils import query_token_set, read_lexicon, read_token
 from .kws_utils.model_utils import (average_model, convert_to_kaldi,
                                     count_parameters)
 
 logger = get_logger()
 
 
 @TRAINERS.register_module(
@@ -43,50 +43,39 @@
                  work_dir: str,
                  cfg_file: Optional[str] = None,
                  arg_parse_fn: Optional[Callable] = None,
                  model_revision: Optional[str] = DEFAULT_MODEL_REVISION,
                  **kwargs):
         '''
         Args:
-            work_dir (str): main directory for training
+            model (str): model id in modelscope
+            work_dir (str): main directory for training and evaluating
+            cfg_file (str): config file for training and evaluating
             kwargs:
-                checkpoint (str): basemodel checkpoint, if None, default to use base.pt in model path
-                train_data (int): wave list with kaldi style for training
-                cv_data (int): wave list with kaldi style for cross validation
-                trans_data (str): transcription list with kaldi style, merge train and cv
-                tensorboard_dir (str): path to save tensorboard results,
-                                       create 'tensorboard_dir' in work_dir by default
+                seed (int): random seed
         '''
         if isinstance(model, str):
             self.model_dir = self.get_or_download_model_dir(
                 model, model_revision)
             if cfg_file is None:
                 cfg_file = os.path.join(self.model_dir,
                                         ModelFile.CONFIGURATION)
         else:
             assert cfg_file is not None, 'Config file should not be None if model is not from pretrained!'
             self.model_dir = os.path.dirname(cfg_file)
 
         super().__init__(cfg_file, arg_parse_fn)
         configs = Config.from_file(cfg_file)
 
-        print(kwargs)
         self.launcher = 'pytorch'
         self.dist_backend = configs.train.get('dist_backend', 'nccl')
-        self.tensorboard_dir = kwargs.get('tensorboard_dir', 'tensorboard')
-        self.checkpoint = kwargs.get(
-            'checkpoint', os.path.join(self.model_dir, 'train/base.pt'))
-        self.avg_checkpoint = None
 
         # 1. get rank info
         set_random_seed(kwargs.get('seed', 666))
-        self.get_dist_info()
-        logger.info('RANK {}/{}/{}, Master addr:{}, Master port:{}'.format(
-            self.world_size, self.rank, self.local_rank, self.master_addr,
-            self.master_port))
+        self.init_dist()
 
         self.work_dir = work_dir
         if self.rank == 0:
             if not os.path.exists(self.work_dir):
                 os.makedirs(self.work_dir)
             logger.info(f'Current working dir is {work_dir}')
 
@@ -112,48 +101,70 @@
                 os.makedirs(self.work_dir)
             saved_config_path = os.path.join(self.work_dir, 'config.yaml')
             with open(saved_config_path, 'w') as fout:
                 data = yaml.dump(configs.to_dict())
                 fout.write(data)
 
     def train(self, *args, **kwargs):
+        '''
+        Args:
+            kwargs:
+                train_data (int): wave list with kaldi style for training
+                cv_data (int): wave list with kaldi style for cross validation
+                trans_data (str): transcription list with kaldi style, merge train and cv
+                checkpoint (str): basemodel checkpoint, if None, default to use base.pt in model path
+                tensorboard_dir (str): path to save tensorboard results,
+                                       create 'tensorboard_dir' in work_dir by default
+                need_dump (bool): wether to dump data with mapping tokens or not
+        '''
+        train_checkpoint = kwargs.get('checkpoint', None)
+        if train_checkpoint is not None and os.path.exists(train_checkpoint):
+            self.checkpoint = train_checkpoint
+        else:
+            self.checkpoint = os.path.join(self.model_dir, 'train/base.pt')
+        self.tensorboard_dir = kwargs.get('tensorboard_dir', 'tensorboard')
+
         # 1. prepare dataset and dataloader
         assert kwargs['train_data'], 'please config train data in dict kwargs'
         assert kwargs['cv_data'], 'please config cv data in dict kwargs'
         assert kwargs[
             'trans_data'], 'please config transcription data in dict kwargs'
         self.train_data = kwargs['train_data']
         self.cv_data = kwargs['cv_data']
         self.trans_data = kwargs['trans_data']
+        self.need_dump = kwargs.get(
+            'need_dump', False) and (True if self.rank == 0 else False)
 
         train_conf = self.configs['preprocessor']
         cv_conf = copy.deepcopy(train_conf)
         cv_conf['speed_perturb'] = False
         cv_conf['spec_aug'] = False
         cv_conf['shuffle'] = False
-        self.train_dataset = kws_nearfield_dataset(self.train_data,
-                                                   self.trans_data, train_conf,
-                                                   self.token_table,
-                                                   self.lexicon_table, True)
+
+        dump_train_file = os.path.join(self.work_dir, 'dump_train.txt')
+        dump_cv_file = os.path.join(self.work_dir, 'dump_cv.txt')
+        self.train_dataset = kws_nearfield_dataset(
+            self.train_data, self.trans_data, train_conf, self.token_table,
+            self.lexicon_table, self.need_dump, dump_train_file, True)
         self.cv_dataset = kws_nearfield_dataset(self.cv_data, self.trans_data,
                                                 cv_conf, self.token_table,
-                                                self.lexicon_table, True)
+                                                self.lexicon_table,
+                                                self.need_dump, dump_cv_file,
+                                                True)
 
         self.train_dataloader = DataLoader(
             self.train_dataset,
             batch_size=None,
             pin_memory=kwargs.get('pin_memory', False),
-            persistent_workers=True,
             num_workers=self.configs.train.dataloader.workers_per_gpu,
             prefetch_factor=self.configs.train.dataloader.get('prefetch', 2))
         self.cv_dataloader = DataLoader(
             self.cv_dataset,
             batch_size=None,
             pin_memory=kwargs.get('pin_memory', False),
-            persistent_workers=True,
             num_workers=self.configs.evaluation.dataloader.workers_per_gpu,
             prefetch_factor=self.configs.evaluation.dataloader.get(
                 'prefetch', 2))
 
         # 2. Init kws model from configs
         self.model = self.build_model(self.configs)
         num_params = count_parameters(self.model)
@@ -176,26 +187,26 @@
         self.configs['train']['start_epoch'] = self.start_epoch
 
         lr_last_epoch = infos.get('lr',
                                   self.configs['train']['optimizer']['lr'])
         self.configs['train']['optimizer']['lr'] = lr_last_epoch
 
         # 4. model placement
-        self.device_name = kwargs.get('device', 'gpu')
+        device_name = kwargs.get('device', 'gpu')
         if self.world_size > 1:
-            self.device_name = f'cuda:{self.local_rank}'
-        self.device = create_device(self.device_name)
+            device_name = f'cuda:{self.local_rank}'
+        self.train_device = create_device(device_name)
 
         if self.world_size > 1:
             assert (torch.cuda.is_available())
             # cuda model is required for nn.parallel.DistributedDataParallel
-            self.model.cuda()
+            self.model = self.model.to(self.train_device)
             self.model = torch.nn.parallel.DistributedDataParallel(self.model)
         else:
-            self.model = self.model.to(self.device)
+            self.model = self.model.to(self.train_device)
 
         # 5. update training config file
         if self.rank == 0:
             if not os.path.exists(self.work_dir):
                 os.makedirs(self.work_dir)
             saved_config_path = os.path.join(self.work_dir, 'config.yaml')
             with open(saved_config_path, 'w') as fout:
@@ -226,58 +237,61 @@
             threshold=0.01,
         )
 
         final_epoch = None
         if self.start_epoch == 0 and self.rank == 0:
             save_model_path = os.path.join(self.work_dir, 'init.pt')
             save_checkpoint(self.model, save_model_path, None, None, None,
-                            False)
+                            False, True)
 
         # Start training loop
         logger.info('Start training...')
         training_config = {}
         training_config['grad_clip'] = optim_conf['grad_clip']
+        training_config['grad_accum'] = optim_conf.get('grad_accum', 1)
         training_config['log_interval'] = log_interval
         training_config['world_size'] = self.world_size
         training_config['rank'] = self.rank
         training_config['local_rank'] = self.local_rank
 
         max_epoch = self.configs['train']['max_epochs']
         totaltime = datetime.datetime.now()
         for epoch in range(self.start_epoch, max_epoch):
             self.train_dataset.set_epoch(epoch)
             training_config['epoch'] = epoch
 
             lr = optimizer.param_groups[0]['lr']
             logger.info('Epoch {} TRAIN info lr {}'.format(epoch, lr))
             executor_train(self.model, optimizer, self.train_dataloader,
-                           self.device, writer, training_config)
-            cv_loss = executor_cv(self.model, self.cv_dataloader, self.device,
-                                  training_config)
-            logger.info('Epoch {} EVAL info cv_loss {:.6f}'.format(
-                epoch, cv_loss))
+                           self.train_device, writer, training_config)
+            cv_loss, cv_acc = executor_cv(self.model, self.cv_dataloader,
+                                          self.train_device, training_config)
+            logger.info(
+                'Epoch {} EVAL info cv_loss {:.6f}, cv_acc {:.2f}'.format(
+                    epoch, cv_loss, cv_acc))
 
             if self.rank == 0:
                 save_model_path = os.path.join(self.work_dir,
                                                '{}.pt'.format(epoch))
                 save_checkpoint(self.model, save_model_path, None, None, None,
-                                False)
+                                False, True)
 
                 info_path = re.sub('.pt$', '.yaml', save_model_path)
                 info_dict = dict(
                     epoch=epoch,
                     lr=lr,
                     cv_loss=cv_loss,
                 )
                 with open(info_path, 'w') as fout:
                     data = yaml.dump(info_dict)
                     fout.write(data)
 
                 writer.add_scalar('epoch/cv_loss', cv_loss, epoch)
                 writer.add_scalar('epoch/lr', lr, epoch)
+
             final_epoch = epoch
             lr_scheduler.step(cv_loss)
 
         if final_epoch is not None and self.rank == 0:
             writer.close()
 
         totaltime = datetime.datetime.now() - totaltime
@@ -292,25 +306,26 @@
             kwargs:
                 test_dir (str): local path for saving test results
                 test_data (str): wave list with kaldi style
                 trans_data (str): transcription list with kaldi style
                 average_num (int): the NO. to do model averaging(checkpoint_path==None)
                 batch_size (int): batch size during evaluating
                 keywords (str): keyword string, split with ','
-                gpu (int): evaluating with cpu/gpu: -1 for cpu; >=0 for gpu,
-                           os.environ['CUDA_VISIBLE_DEVICES'] will be setted
+                gpu (int): evaluating with cpu/gpu: -1 for cpu; >=0 for gpu
         '''
+
         # 1. get checkpoint
+        self.avg_checkpoint = None
         if checkpoint_path is not None and os.path.exists(checkpoint_path):
             logger.warning(
                 f'evaluating with specific model: {checkpoint_path}')
             eval_checkpoint = checkpoint_path
         else:
             if self.avg_checkpoint is None:
-                avg_num = kwargs.get('average_num', 5)
+                avg_num = kwargs.get('average_num', 10)
                 self.avg_checkpoint = os.path.join(self.work_dir,
                                                    f'avg_{avg_num}.pt')
                 logger.warning(
                     f'default average model not exist: {self.avg_checkpoint}')
                 avg_kwargs = dict(
                     dst_model=self.avg_checkpoint,
                     src_path=self.work_dir,
@@ -349,21 +364,21 @@
         # 3. prepare dataset and dataloader
         test_conf = copy.deepcopy(self.configs['preprocessor'])
         test_conf['filter_conf']['max_length'] = 102400
         test_conf['filter_conf']['min_length'] = 0
         test_conf['speed_perturb'] = False
         test_conf['spec_aug'] = False
         test_conf['shuffle'] = False
-        test_conf['feature_extraction_conf']['dither'] = 0.0
         if kwargs.get('batch_size', None) is not None:
             test_conf['batch_conf']['batch_size'] = kwargs['batch_size']
 
         test_dataset = kws_nearfield_dataset(test_data, trans_data, test_conf,
                                              self.token_table,
-                                             self.lexicon_table, False)
+                                             self.lexicon_table, False, '',
+                                             False)
         test_dataloader = DataLoader(
             test_dataset,
             batch_size=None,
             pin_memory=kwargs.get('pin_memory', False),
             persistent_workers=True,
             num_workers=self.configs.evaluation.dataloader.workers_per_gpu,
             prefetch_factor=self.configs.evaluation.dataloader.get(
@@ -371,41 +386,47 @@
 
         # 4. parse keywords tokens
         assert kwargs.get('keywords',
                           None) is not None, 'at least one keyword is needed'
         keywords_str = kwargs['keywords']
         keywords_list = keywords_str.strip().replace(' ', '').split(',')
         keywords_token = {}
-        keywords_tokenset = {0}
+        keywords_idxset = {0}
+        keywords_strset = {'<blk>'}
+        keywords_tokenmap = {'<blk>': 0}
         for keyword in keywords_list:
-            ids = query_tokens_id(keyword, self.token_table,
-                                  self.lexicon_table)
+            strs, indexes = query_token_set(keyword, self.token_table,
+                                            self.lexicon_table)
             keywords_token[keyword] = {}
-            keywords_token[keyword]['token_id'] = ids
+            keywords_token[keyword]['token_id'] = indexes
             keywords_token[keyword]['token_str'] = ''.join('%s ' % str(i)
-                                                           for i in ids)
-            [keywords_tokenset.add(i) for i in ids]
-        logger.warning(f'Token set is: {keywords_tokenset}')
+                                                           for i in indexes)
+            [keywords_strset.add(i) for i in strs]
+            [keywords_idxset.add(i) for i in indexes]
+            for txt, idx in zip(strs, indexes):
+                if keywords_tokenmap.get(txt, None) is None:
+                    keywords_tokenmap[txt] = idx
+
+        token_print = ''
+        for txt, idx in keywords_tokenmap.items():
+            token_print += f'{txt}({idx}) '
+        logger.warning(f'Token set is: {token_print}')
 
         # 5. build model and load checkpoint
         # support assign specific gpu device
-        os.environ['CUDA_VISIBLE_DEVICES'] = str(kwargs.get('gpu', -1))
+        # Init kws model from configs
         use_cuda = kwargs.get('gpu', -1) >= 0 and torch.cuda.is_available()
-
-        if kwargs.get('jit_model', None):
-            model = torch.jit.load(eval_checkpoint)
-            # For script model, only cpu is supported.
-            device = torch.device('cpu')
-        else:
-            # Init kws model from configs
-            model = self.build_model(self.configs)
-            load_checkpoint(eval_checkpoint, model)
-            device = torch.device('cuda' if use_cuda else 'cpu')
-        model = model.to(device)
-        model.eval()
+        device_name = kwargs.get('device', 'cpu')
+        if self.world_size > 1 and use_cuda:
+            device_name = f'cuda:{self.local_rank}'
+        self.test_device = create_device(device_name)
+
+        self.test_model = self.build_model(self.configs)
+        load_checkpoint(eval_checkpoint, self.test_model)
+        self.test_model = self.test_model.to(self.test_device)
 
         testing_config = {}
         if kwargs.get('test_dir', None) is not None:
             testing_config['test_dir'] = kwargs['test_dir']
         else:
             base_name = os.path.basename(eval_checkpoint)
             testing_config['test_dir'] = os.path.join(self.work_dir,
@@ -413,17 +434,17 @@
         self.test_dir = testing_config['test_dir']
         if not os.path.exists(self.test_dir):
             os.makedirs(self.test_dir)
 
         # 6. executing evaluation and get score file
         logger.info('Start evaluating...')
         totaltime = datetime.datetime.now()
-        score_file = executor_test(model, test_dataloader, device,
-                                   keywords_token, keywords_tokenset,
-                                   testing_config)
+        score_file = executor_test(self.test_model, test_dataloader,
+                                   self.test_device, keywords_token,
+                                   keywords_idxset, testing_config)
         totaltime = datetime.datetime.now() - totaltime
         logger.info('Total time spent: {:.2f} hours'.format(
             totaltime.total_seconds() / 3600.0))
 
         # 7. compute det statistic file with score file
         det_kwargs = dict(
             keywords=keywords_str,
@@ -444,15 +465,15 @@
         model = Model.from_pretrained(
             self.model_dir, cfg_dict=configs, training=True)
         if isinstance(model, TorchModel) and hasattr(model, 'model'):
             return model.model
         elif isinstance(model, nn.Module):
             return model
 
-    def get_dist_info(self):
+    def init_dist(self, train_nodes=1):
         if os.getenv('RANK', None) is None:
             os.environ['RANK'] = '0'
         if os.getenv('LOCAL_RANK', None) is None:
             os.environ['LOCAL_RANK'] = '0'
         if os.getenv('WORLD_SIZE', None) is None:
             os.environ['WORLD_SIZE'] = '1'
         if os.getenv('MASTER_ADDR', None) is None:
@@ -462,10 +483,32 @@
 
         self.rank = int(os.environ['RANK'])
         self.local_rank = int(os.environ['LOCAL_RANK'])
         self.world_size = int(os.environ['WORLD_SIZE'])
         self.master_addr = os.environ['MASTER_ADDR']
         self.master_port = os.environ['MASTER_PORT']
 
-        init_dist(self.launcher, self.dist_backend)
-        self.rank, self.world_size = get_dist_info()
-        self.local_rank = get_local_rank()
+        if train_nodes == 1:
+            if self.world_size > 1:
+                logger.info('init dist on multiple gpus, this gpu {}'.format(
+                    self.local_rank))
+                dist.init_process_group(
+                    backend=self.dist_backend, init_method='env://')
+        elif train_nodes > 1:
+            dist.init_process_group(
+                backend=self.dist_backend, init_method='env://')
+            dist.barrier()
+
+        logger.info('RANK {}/{}/{}, Master addr:{}, Master port:{}'.format(
+            self.world_size, self.rank, self.local_rank, self.master_addr,
+            self.master_port))
+
+    def uninit_dist(self, train_nodes=1):
+        if train_nodes == 1:
+            if self.world_size > 1:
+                logger.info(
+                    'destory dist on multiple gpus, this gpu {}'.format(
+                        self.local_rank))
+                dist.destroy_process_group()
+        elif train_nodes > 1:
+            dist.barrier()
+            dist.destroy_process_group()
```

### Comparing `modelscope-1.4.2/modelscope/trainers/audio/kws_utils/__init__.py` & `modelscope-1.5.0/modelscope/trainers/audio/kws_utils/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/audio/kws_utils/det_utils.py` & `modelscope-1.5.0/modelscope/trainers/audio/kws_utils/det_utils.py`

 * *Files 12% similar despite different names*

```diff
@@ -21,15 +21,15 @@
 import kaldiio
 import matplotlib.font_manager as fm
 import matplotlib.pyplot as plt
 import numpy as np
 import torch
 
 from modelscope.utils.logger import get_logger
-from .file_utils import make_pair, read_lists
+from .file_utils import make_pair, read_lists, space_mixed_label
 
 logger = get_logger()
 
 font = fm.FontProperties(size=15)
 
 
 class thread_wrapper(threading.Thread):
@@ -64,15 +64,15 @@
         try:
             rate, waveform = kaldiio.load_mat(wav_file)
             waveform = torch.tensor(waveform, dtype=torch.float32)
             waveform = waveform.unsqueeze(0)
             frames = len(waveform[0])
             duration = frames / float(rate)
         except Exception:
-            logging.info(f'load file failed: {wav_file}')
+            logger.info(f'load file failed: {wav_file}')
             duration = 0.0
 
         obj['duration'] = duration
         results.append(obj)
 
     return results
 
@@ -84,50 +84,54 @@
         # read score file and store in table
         for line in fin:
             arr = line.strip().split()
             key = arr[0]
             is_detected = arr[1]
             if is_detected == 'detected':
                 if key not in score_table:
-                    score_table.update(
-                        {key: {
-                            'kw': arr[2],
+                    score_table.update({
+                        key: {
+                            'kw': space_mixed_label(arr[2]),
                             'confi': float(arr[3])
-                        }})
+                        }
+                    })
             else:
                 if key not in score_table:
                     score_table.update({key: {'kw': 'unknown', 'confi': -1.0}})
 
     wav_lists = read_lists(data_file)
     trans_lists = read_lists(trans_file)
     data_lists = make_pair(wav_lists, trans_lists)
+    logger.info(f'origin list samples: {len(data_lists)}')
 
     # count duration for each wave use multi-thread
     num_workers = 8
     start = 0
     step = int(len(data_lists) / num_workers)
     tasks = []
-    for idx in range(8):
+    for idx in range(num_workers):
         if idx != num_workers - 1:
             task = thread_wrapper(count_duration,
                                   (idx, data_lists[start:start + step]))
         else:
             task = thread_wrapper(count_duration, (idx, data_lists[start:]))
         task.start()
         tasks.append(task)
         start += step
 
     duration_lists = []
     for task in tasks:
         task.join()
         duration_lists += task.get_result()
+    logger.info(f'after list samples: {len(duration_lists)}')
 
     # build empty structure for keyword-filler infos
     keyword_filler_table = {}
     for keyword in keywords_list:
+        keyword = space_mixed_label(keyword)
         keyword_filler_table[keyword] = {}
         keyword_filler_table[keyword]['keyword_table'] = {}
         keyword_filler_table[keyword]['keyword_duration'] = 0.0
         keyword_filler_table[keyword]['filler_table'] = {}
         keyword_filler_table[keyword]['filler_duration'] = 0.0
 
     for obj in duration_lists:
@@ -135,19 +139,23 @@
         assert 'wav' in obj
         assert 'txt' in obj
         assert 'duration' in obj
 
         key = obj['key']
         # wav_file = obj['wav']
         txt = obj['txt']
+        txt = space_mixed_label(txt)
+        txt_regstr_lrblk = ' ' + txt + ' '
         duration = obj['duration']
         assert key in score_table
 
         for keyword in keywords_list:
-            if txt.find(keyword) != -1:
+            keyword = space_mixed_label(keyword)
+            keyword_regstr_lrblk = ' ' + keyword + ' '
+            if txt_regstr_lrblk.find(keyword_regstr_lrblk) != -1:
                 if keyword == score_table[key]['kw']:
                     keyword_filler_table[keyword]['keyword_table'].update(
                         {key: score_table[key]['confi']})
                 else:
                     # uttrance detected but not match this keyword
                     keyword_filler_table[keyword]['keyword_table'].update(
                         {key: -1.0})
@@ -199,33 +207,35 @@
         stats_dir = os.path.dirname(score_file)
     logger.info(f'store all keyword\'s stats file in {stats_dir}')
     if not os.path.exists(stats_dir):
         os.makedirs(stats_dir)
 
     score_step = kwargs.get('score_step', 0.001)
 
-    keywords_list = keywords.replace(' ', '').strip().split(',')
+    keywords_list = keywords.strip().split(',')
     keyword_filler_table = load_data_and_score(keywords_list, test_data,
                                                trans_data, score_file)
 
     stats_files = {}
     for keyword in keywords_list:
+        keyword = space_mixed_label(keyword)
         keyword_dur = keyword_filler_table[keyword]['keyword_duration']
         keyword_num = len(keyword_filler_table[keyword]['keyword_table'])
         filler_dur = keyword_filler_table[keyword]['filler_duration']
         filler_num = len(keyword_filler_table[keyword]['filler_table'])
         assert keyword_num > 0, 'Can\'t compute det for {} without positive sample'
         assert filler_num > 0, 'Can\'t compute det for {} without negative sample'
 
         logger.info('Computing det for {}'.format(keyword))
         logger.info('  Keyword duration: {} Hours, wave number: {}'.format(
             keyword_dur / 3600.0, keyword_num))
         logger.info('  Filler duration: {} Hours'.format(filler_dur / 3600.0))
 
-        stats_file = os.path.join(stats_dir, 'stats_' + keyword + '.txt')
+        stats_file = os.path.join(
+            stats_dir, 'stats.' + keyword.replace(' ', '_') + '.txt')
         with open(stats_file, 'w', encoding='utf8') as fout:
             threshold = 0.0
             while threshold <= 1.0:
                 num_false_reject = 0
                 num_true_detect = 0
                 # transverse the all keyword_table
                 for key, confi in keyword_filler_table[keyword][
```

### Comparing `modelscope-1.4.2/modelscope/trainers/audio/kws_utils/model_utils.py` & `modelscope-1.5.0/modelscope/trainers/audio/kws_utils/model_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/audio/kws_utils/runtime_utils.py` & `modelscope-1.5.0/modelscope/trainers/audio/kws_utils/runtime_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/audio/separation_trainer.py` & `modelscope-1.5.0/modelscope/trainers/audio/separation_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/audio/tts_trainer.py` & `modelscope-1.5.0/modelscope/trainers/audio/tts_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/base.py` & `modelscope-1.5.0/modelscope/trainers/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/cv/__init__.py` & `modelscope-1.5.0/modelscope/trainers/cv/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/cv/action_detection_trainer.py` & `modelscope-1.5.0/modelscope/trainers/cv/action_detection_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/cv/card_detection_scrfd_trainer.py` & `modelscope-1.5.0/modelscope/trainers/cv/card_detection_scrfd_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/cv/cartoon_translation_trainer.py` & `modelscope-1.5.0/modelscope/trainers/cv/cartoon_translation_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/cv/face_detection_scrfd_trainer.py` & `modelscope-1.5.0/modelscope/trainers/cv/face_detection_scrfd_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/cv/image_classifition_trainer.py` & `modelscope-1.5.0/modelscope/trainers/cv/image_classifition_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/cv/image_defrcn_fewshot_detection_trainer.py` & `modelscope-1.5.0/modelscope/trainers/cv/image_defrcn_fewshot_detection_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/cv/image_detection_damoyolo_trainer.py` & `modelscope-1.5.0/modelscope/trainers/cv/image_detection_damoyolo_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/cv/image_inpainting_trainer.py` & `modelscope-1.5.0/modelscope/trainers/cv/image_inpainting_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/cv/image_instance_segmentation_trainer.py` & `modelscope-1.5.0/modelscope/trainers/cv/image_instance_segmentation_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/cv/image_portrait_enhancement_trainer.py` & `modelscope-1.5.0/modelscope/trainers/cv/image_portrait_enhancement_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/cv/movie_scene_segmentation_trainer.py` & `modelscope-1.5.0/modelscope/trainers/cv/movie_scene_segmentation_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/cv/nerf_recon_acc_trainer.py` & `modelscope-1.5.0/modelscope/trainers/cv/nerf_recon_acc_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/cv/ocr_detection_db_trainer.py` & `modelscope-1.5.0/modelscope/trainers/cv/ocr_detection_db_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/cv/ocr_recognition_trainer.py` & `modelscope-1.5.0/modelscope/trainers/cv/ocr_recognition_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/cv/referring_video_object_segmentation_trainer.py` & `modelscope-1.5.0/modelscope/trainers/cv/referring_video_object_segmentation_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/cv/vision_efficient_tuning_trainer.py` & `modelscope-1.5.0/modelscope/trainers/cv/vision_efficient_tuning_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/default_config.py` & `modelscope-1.5.0/modelscope/trainers/default_config.py`

 * *Files 1% similar despite different names*

```diff
@@ -33,25 +33,23 @@
             'workers_per_gpu': 0,
             'shuffle': False
         },
     }
 })
 
 DEFAULT_HOOKS_CONFIG = {
-    'train': {
-        'hooks': [{
-            'type': 'CheckpointHook',
-            'interval': 1
-        }, {
-            'type': 'TextLoggerHook',
-            'interval': 10
-        }, {
-            'type': 'IterTimerHook'
-        }]
-    }
+    'train.hooks': [{
+        'type': 'CheckpointHook',
+        'interval': 1
+    }, {
+        'type': 'TextLoggerHook',
+        'interval': 10
+    }, {
+        'type': 'IterTimerHook'
+    }]
 }
 
 _HOOK_KEY_CHAIN_MAP = {
     'TextLoggerHook': 'train.logging',
     'CheckpointHook': 'train.checkpoint.period',
     'BestCkptSaverHook': 'train.checkpoint.best',
     'EvaluationHook': 'evaluation.period',
```

### Comparing `modelscope-1.4.2/modelscope/trainers/easycv/trainer.py` & `modelscope-1.5.0/modelscope/trainers/easycv/trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/easycv/utils/__init__.py` & `modelscope-1.5.0/modelscope/trainers/easycv/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/easycv/utils/hooks.py` & `modelscope-1.5.0/modelscope/trainers/easycv/utils/hooks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/easycv/utils/metric.py` & `modelscope-1.5.0/modelscope/trainers/easycv/utils/metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/easycv/utils/register_util.py` & `modelscope-1.5.0/modelscope/trainers/easycv/utils/register_util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/__init__.py` & `modelscope-1.5.0/modelscope/trainers/hooks/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/checkpoint_hook.py` & `modelscope-1.5.0/modelscope/trainers/hooks/checkpoint_hook.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,16 +1,19 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import os
 import random
 import re
+import time
 
 import numpy as np
 import torch
 from packaging import version
 
+from modelscope.hub.check_model import check_model_is_id
+from modelscope.hub.push_to_hub import push_to_hub_async
 from modelscope.metainfo import Hooks, Pipelines
 from modelscope.utils.checkpoint import (load_checkpoint, save_checkpoint,
                                          save_configuration)
 from modelscope.utils.constant import LogKeys, ModelFile
 from modelscope.utils.logger import get_logger
 from modelscope.utils.torch_utils import is_master
 from .builder import HOOKS
@@ -45,23 +48,33 @@
                  interval=0,
                  by_epoch=True,
                  save_optimizer=True,
                  save_dir=None,
                  output_sub_dir=ModelFile.TRAIN_OUTPUT_DIR,
                  save_last=True,
                  max_checkpoint_num=None,
+                 push_to_hub=False,
+                 model_id_with_org=None,
+                 hub_token=None,
+                 private_hub=True,
                  **kwargs):
         self.interval = interval
         self.by_epoch = by_epoch
         self.save_optimizer = save_optimizer
         self.save_dir = save_dir
         self.output_sub_dir = output_sub_dir
         self.save_last = save_last
         self.rng_state = None
         self.max_checkpoint_num = None
+        self.push_to_hub = push_to_hub
+        self.model_id_with_org = model_id_with_org
+        self.hub_token = hub_token
+        self.private_hub = private_hub
+        self.is_model_id = None
+        self.push_to_hub_future = None
         if max_checkpoint_num is not None:
             self.max_checkpoint_num = max(int(max_checkpoint_num), 1)
         self.history_checkpoints = []
 
     def before_run(self, trainer):
         if not self.save_dir:
             self.save_dir = trainer.work_dir
@@ -76,50 +89,95 @@
 
         if is_master():
             output_dir = os.path.join(self.save_dir, self.output_sub_dir)
             # only global master prepares the output folder
             self.prepare_output(trainer, output_dir)
             self.logger.info(f'Checkpoints will be saved to {self.save_dir}')
 
+    def generate_prefix(self, trainer):
+        if self.by_epoch:
+            return f'{LogKeys.EPOCH}_{trainer.epoch + 1}'
+        else:
+            return f'{LogKeys.ITER}_{trainer.iter + 1}'
+
     def after_train_epoch(self, trainer):
         if not self.by_epoch:
             return
 
-        if self._should_save(trainer) and self.should_save_on_rank(trainer):
-            if is_master():
-                self.logger.info(
-                    f'Saving checkpoint at {trainer.epoch + 1} epoch')
-            self._save_checkpoint(trainer)
+        if self._should_save(trainer):
+            # prefix like 'epoch-1' or 'iter-1'
+            prefix = self.generate_prefix(trainer)
+            if self.should_save_on_rank(trainer):
+                if is_master():
+                    self.logger.info(
+                        f'Saving checkpoint at {trainer.epoch + 1} epoch')
+                self._save_checkpoint(trainer, prefix)
+            if is_master() and self.push_to_hub:
+                if self.push_to_hub_future is not None and not self.push_to_hub_future.done(
+                ):
+                    self.logger.error(
+                        f'Another uploading is running, '
+                        f'this uploading with message {prefix} will be canceled.'
+                    )
+                    return
+                self.push_to_hub_future = self._push_to_hub(trainer, prefix)
 
     def after_train_iter(self, trainer):
         if self.by_epoch:
             return
 
-        if self._should_save(trainer) and self.should_save_on_rank(trainer):
-            if is_master():
-                self.logger.info(
-                    f'Saving checkpoint at {trainer.iter + 1} epoch')
-            self._save_checkpoint(trainer)
+        if self._should_save(trainer):
+            # prefix like 'epoch-1' or 'iter-1'
+            prefix = self.generate_prefix(trainer)
+            if self.should_save_on_rank(trainer):
+                if is_master():
+                    self.logger.info(
+                        f'Saving checkpoint at {trainer.iter + 1} iter')
+                self._save_checkpoint(trainer, prefix)
+            if is_master() and self.push_to_hub:
+                if self.push_to_hub_future is not None and not self.push_to_hub_future.done(
+                ):
+                    self.logger.error(
+                        f'Another uploading is running, '
+                        f'this uploading with message {prefix} will be canceled.'
+                    )
+                    return
+                self.push_to_hub_future = self._push_to_hub(trainer, prefix)
+
+    def after_run(self, trainer):
+        if self.push_to_hub_future is not None and not self.push_to_hub_future.done(
+        ):
+            self.logger.info('Train finished. Uploading models, waiting...')
+            while not self.push_to_hub_future.done():
+                time.sleep(1)
+            self.logger.info('Uploading models done.')
+
+    def _push_to_hub(self, trainer, prefix):
+        if self.is_model_id is None:
+            self.is_model_id = check_model_is_id(trainer.input_model_id,
+                                                 self.hub_token)
+
+        return push_to_hub_async(
+            self.model_id_with_org,
+            os.path.join(self.save_dir, self.output_sub_dir),
+            token=self.hub_token,
+            private=self.private_hub,
+            commit_message=prefix,
+            source_repo=trainer.input_model_id if self.is_model_id else '')
 
-    def _save_checkpoint(self, trainer):
+    def _save_checkpoint(self, trainer, prefix):
         """Save checkpoint files and remove obsolete ones
         """
-
-        if self.by_epoch:
-            checkpoint_path_prefix = os.path.join(
-                self.save_dir, f'{LogKeys.EPOCH}_{trainer.epoch + 1}')
-        else:
-            checkpoint_path_prefix = os.path.join(
-                self.save_dir, f'{LogKeys.ITER}_{trainer.iter + 1}')
-
+        checkpoint_path_prefix = os.path.join(self.save_dir, prefix)
         meta = self._create_training_state(trainer)
         self.save_checkpoints(trainer, checkpoint_path_prefix,
                               self.output_sub_dir, meta)
         self.history_checkpoints.append(checkpoint_path_prefix)
         self._remove_obsolete_checkpoints(trainer)
+        return prefix
 
     def _remove_obsolete_checkpoints(self, trainer):
         if self.max_checkpoint_num is not None and \
                 len(self.history_checkpoints) > self.max_checkpoint_num:
             history_checkpoints = [ckpt for ckpt in self.history_checkpoints]
             self.history_checkpoints.clear()
             for i, checkpoint_path_prefix in enumerate(history_checkpoints):
@@ -185,14 +243,23 @@
 
             def __call__(self, _output_dir, _config):
                 self.config = _config
 
             def save_config(self):
                 save_configuration(self.output_dir, self.config)
 
+        for pop_key in [
+                'push_to_hub', 'model_id_with_org', 'hub_token', 'private_hub'
+        ]:
+            if config.safe_get('train.checkpoint.period.'
+                               + pop_key) is not None:
+                config.safe_get('train.checkpoint.period').pop(pop_key)
+            if config.safe_get('train.checkpoint.best.' + pop_key) is not None:
+                config.safe_get('train.checkpoint.best').pop(pop_key)
+
         save_config_fn = SaveConfig(output_dir, config)
 
         if hasattr(model, 'save_pretrained'):
             # Save pretrained of model, skip saving checkpoint
             model.save_pretrained(
                 output_dir,
                 bin_file,
@@ -230,15 +297,15 @@
         This is a strategic function which can be registered by other hook's function.
 
         Args:
             trainer: The trainer instance.
             output_dir: The target folder used in inference.
         """
         model = trainer.unwrap_module(trainer.model)
-        config = trainer.cfg.to_dict()
+        config = trainer.cfg
 
         # override pipeline by tasks name after finetune done,
         # avoid case like fill mask pipeline with a text cls task
         if config['task'] in [
                 getattr(Pipelines, attr) for attr in dir(Pipelines)
                 if not attr.startswith('__')
         ]:
@@ -420,37 +487,35 @@
         else:
             compare_fn = self.rule_map[self.rule]
             if compare_fn(metric_values[self.metric_key], self._best_metric):
                 self._best_metric = metric_values[self.metric_key]
                 return True
         return False
 
-    def _save_checkpoint(self, trainer):
+    def generate_prefix(self, trainer):
+        if self.by_epoch:
+            return f'best_{LogKeys.EPOCH}{trainer.epoch + 1}_{self.metric_key}{self._best_metric}'
+        else:
+            return f'best_{LogKeys.ITER}{trainer.iter + 1}_{self.metric_key}{self._best_metric}'
+
+    def _save_checkpoint(self, trainer, prefix):
         checkpoint_path_prefix = self.save_file_name
         if checkpoint_path_prefix is None:
-            if self.by_epoch:
-                checkpoint_path_prefix = os.path.join(
-                    self.save_dir,
-                    f'best_{LogKeys.EPOCH}{trainer.epoch + 1}_{self.metric_key}{self._best_metric}'
-                )
-            else:
-                checkpoint_path_prefix = os.path.join(
-                    self.save_dir,
-                    f'best_{LogKeys.ITER}{trainer.iter + 1}_{self.metric_key}{self._best_metric}'
-                )
+            checkpoint_path_prefix = os.path.join(self.save_dir, prefix)
         else:
             checkpoint_path_prefix = os.path.join(self.save_dir,
                                                   checkpoint_path_prefix)
 
         self._best_ckpt_file = checkpoint_path_prefix
         meta = self._create_training_state(trainer)
         self.save_checkpoints(trainer, checkpoint_path_prefix,
                               self.output_sub_dir, meta)
         self.history_checkpoints.add(checkpoint_path_prefix)
         self._remove_obsolete_checkpoints(trainer)
+        return prefix
 
     def _remove_obsolete_checkpoints(self, trainer):
 
         def extract_metric_from_filename(name1):
             metric1 = float(name1.split(self.metric_key)[1])
             if self.rule == 'max':
                 return -metric1
```

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/clip_clamp_logit_scale_hook.py` & `modelscope-1.5.0/modelscope/trainers/hooks/clip_clamp_logit_scale_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/compression/__init__.py` & `modelscope-1.5.0/modelscope/trainers/hooks/compression/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/compression/sparsity_hook.py` & `modelscope-1.5.0/modelscope/trainers/hooks/compression/sparsity_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/compression/utils.py` & `modelscope-1.5.0/modelscope/trainers/hooks/compression/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/ddp_hook.py` & `modelscope-1.5.0/modelscope/trainers/hooks/ddp_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/deepspeed_hook.py` & `modelscope-1.5.0/modelscope/trainers/hooks/deepspeed_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/early_stop_hook.py` & `modelscope-1.5.0/modelscope/trainers/hooks/early_stop_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/evaluation_hook.py` & `modelscope-1.5.0/modelscope/trainers/hooks/evaluation_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/hook.py` & `modelscope-1.5.0/modelscope/trainers/hooks/hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/iter_timer_hook.py` & `modelscope-1.5.0/modelscope/trainers/hooks/iter_timer_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/logger/__init__.py` & `modelscope-1.5.0/modelscope/trainers/hooks/logger/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/logger/base.py` & `modelscope-1.5.0/modelscope/trainers/hooks/logger/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/logger/tensorboard_hook.py` & `modelscope-1.5.0/modelscope/trainers/hooks/logger/tensorboard_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/logger/text_logger_hook.py` & `modelscope-1.5.0/modelscope/trainers/hooks/logger/text_logger_hook.py`

 * *Files 1% similar despite different names*

```diff
@@ -71,15 +71,15 @@
                                       '{}.log.json'.format(trainer.timestamp))
         if hasattr(trainer, 'meta') and trainer.meta is not None:
             self._dump_log(trainer.meta)
 
     def _get_max_memory(self, trainer):
         device = torch.cuda.current_device()
         mem = torch.cuda.max_memory_allocated(device=device)
-        mem_mb = torch.tensor([mem / (1024 * 1024)],
+        mem_mb = torch.tensor([int(mem / (1024 * 1024))],
                               dtype=torch.int,
                               device=device)
         if trainer._dist:
             dist.reduce(mem_mb, 0, op=dist.ReduceOp.MAX)
         return mem_mb.item()
 
     def _log_info(self, log_dict, trainer):
```

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/lr_scheduler_hook.py` & `modelscope-1.5.0/modelscope/trainers/hooks/lr_scheduler_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/megatron_hook.py` & `modelscope-1.5.0/modelscope/trainers/hooks/megatron_hook.py`

 * *Files 2% similar despite different names*

```diff
@@ -148,12 +148,12 @@
             bin_file = self.get_bin_file()
 
             model_file = os.path.join(save_dir, prefix + '_' + bin_file)
             load_checkpoint(model_file, model, None, None)
             return meta
 
     def prepare_output(self, trainer, output_dir):
-        config = trainer.cfg.to_dict()
+        config = trainer.cfg
         CheckpointHook.copy_files_and_dump_config(trainer, output_dir, config,
                                                   self._BIN_FILE_DIR)
         os.makedirs(
             os.path.join(output_dir, self._BIN_FILE_DIR), exist_ok=True)
```

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/optimizer/__init__.py` & `modelscope-1.5.0/modelscope/trainers/hooks/optimizer/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/optimizer/apex_optimizer_hook.py` & `modelscope-1.5.0/modelscope/trainers/hooks/optimizer/apex_optimizer_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/optimizer/base.py` & `modelscope-1.5.0/modelscope/trainers/hooks/optimizer/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/optimizer/torch_optimizer_hook.py` & `modelscope-1.5.0/modelscope/trainers/hooks/optimizer/torch_optimizer_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/hooks/priority.py` & `modelscope-1.5.0/modelscope/trainers/hooks/priority.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/lrscheduler/__init__.py` & `modelscope-1.5.0/modelscope/trainers/lrscheduler/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/lrscheduler/builder.py` & `modelscope-1.5.0/modelscope/trainers/lrscheduler/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/lrscheduler/warmup/__init__.py` & `modelscope-1.5.0/modelscope/trainers/lrscheduler/warmup/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/lrscheduler/warmup/base.py` & `modelscope-1.5.0/modelscope/trainers/lrscheduler/warmup/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/lrscheduler/warmup/warmup.py` & `modelscope-1.5.0/modelscope/trainers/lrscheduler/warmup/warmup.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/multi_modal/__init__.py` & `modelscope-1.5.0/modelscope/trainers/multi_modal/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/multi_modal/clip/clip_trainer.py` & `modelscope-1.5.0/modelscope/trainers/multi_modal/clip/clip_trainer.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,18 +13,19 @@
 from modelscope.models.base import Model, TorchModel
 from modelscope.models.multi_modal.clip.model import convert_models_to_fp32
 from modelscope.msdatasets.ms_dataset import MsDataset
 from modelscope.preprocessors.base import Preprocessor
 from modelscope.preprocessors.multi_modal import CLIPPreprocessor
 from modelscope.trainers import EpochBasedTrainer
 from modelscope.trainers.builder import TRAINERS
+from modelscope.trainers.default_config import merge_cfg, update_cfg
 from modelscope.trainers.optimizer.builder import build_optimizer
 from modelscope.utils.config import Config
 from modelscope.utils.constant import (DEFAULT_MODEL_REVISION, ConfigKeys,
-                                       Invoke, ModeKeys)
+                                       Invoke, ModeKeys, ModelFile, ThirdParty)
 from .clip_trainer_utils import get_loss, get_optimizer_params, get_schedule
 
 
 def exclude(n):
     return 'bn' in n or 'ln' in n or 'bias' in n or 'logit_scale' in n
 
 
@@ -35,33 +36,58 @@
 @TRAINERS.register_module(module_name=Trainers.clip_multi_modal_embedding)
 class CLIPTrainer(EpochBasedTrainer):
 
     def __init__(
             self,
             model: Optional[Union[TorchModel, nn.Module, str]] = None,
             cfg_file: Optional[str] = None,
+            cfg_modify_fn: Optional[Callable] = None,
             arg_parse_fn: Optional[Callable] = None,
             data_collator: Optional[Union[Callable, Dict[str,
                                                          Callable]]] = None,
             train_dataset: Optional[Union[MsDataset, Dataset]] = None,
             eval_dataset: Optional[Union[MsDataset, Dataset]] = None,
             preprocessor: Optional[Union[Preprocessor,
                                          Dict[str, Preprocessor]]] = None,
             optimizers: Tuple[torch.optim.Optimizer,
                               torch.optim.lr_scheduler._LRScheduler] = (None,
                                                                         None),
             model_revision: Optional[str] = DEFAULT_MODEL_REVISION,
             seed: int = 42,
             **kwargs):
+        if isinstance(model, str):
+            third_party = kwargs.get(ThirdParty.KEY, None)
+            if third_party is not None:
+                kwargs.pop(ThirdParty.KEY)
+
+            self.model_dir = self.get_or_download_model_dir(
+                model, model_revision, third_party)
+            if cfg_file is None:
+                cfg_file = os.path.join(self.model_dir,
+                                        ModelFile.CONFIGURATION)
+        else:
+            assert cfg_file is not None, 'Config file should not be None if model is not from pretrained!'
+            self.model_dir = os.path.dirname(cfg_file)
+        self.cfg = Config.from_file(cfg_file)
+
+        self.cfg_modify_fn = cfg_modify_fn
+        # add default config
+        merge_cfg(self.cfg)
+        self.cfg = self.rebuild_config(self.cfg)
+        if 'cfg_options' in kwargs:
+            self.cfg.merge_from_dict(kwargs['cfg_options'])
+        self.cfg = update_cfg(self.cfg)
+        cfg = self.cfg
+
         model = Model.from_pretrained(
             model, revision=model_revision, invoked_by=Invoke.TRAINER)
         # for training & eval, we convert the model from FP16 back to FP32
         # to compatible with modelscope amp training
         convert_models_to_fp32(model)
-        cfg = Config.from_file(cfg_file)
+
         if 'work_dir' not in kwargs or len(kwargs['work_dir']) == 0:
             work_dir = cfg.train.work_dir
         else:
             work_dir = kwargs['work_dir']
 
         # fetch the model name of CLIP model (base, large or large-336)
         model_name = cfg.pretrained_model.model_name
@@ -158,14 +184,15 @@
             preprocessor[ConfigKeys.train].set_input_text_key(text_key_name)
             preprocessor[ConfigKeys.val].set_input_text_key(text_key_name)
         self.global_batch_size = cfg.train.dataloader.batch_size_per_gpu * world_size
 
         super().__init__(
             model=model,
             cfg_file=cfg_file,
+            cfg_modify_fn=cfg_modify_fn,
             arg_parse_fn=arg_parse_fn,
             data_collator=data_collator,
             train_dataset=train_dataset,
             eval_dataset=eval_dataset,
             preprocessor=preprocessor,
             optimizers=optimizers,
             seed=seed,
```

### Comparing `modelscope-1.4.2/modelscope/trainers/multi_modal/clip/clip_trainer_utils.py` & `modelscope-1.5.0/modelscope/trainers/multi_modal/clip/clip_trainer_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/multi_modal/mgeo_ranking_trainer.py` & `modelscope-1.5.0/modelscope/trainers/multi_modal/mgeo_ranking_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/multi_modal/mplug/mplug_trainer.py` & `modelscope-1.5.0/modelscope/trainers/multi_modal/mplug/mplug_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/multi_modal/ofa/ofa_trainer.py` & `modelscope-1.5.0/modelscope/trainers/multi_modal/ofa/ofa_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/multi_modal/ofa/ofa_trainer_utils.py` & `modelscope-1.5.0/modelscope/trainers/multi_modal/ofa/ofa_trainer_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/multi_modal/team/team_trainer.py` & `modelscope-1.5.0/modelscope/trainers/multi_modal/team/team_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/multi_modal/team/team_trainer_utils.py` & `modelscope-1.5.0/modelscope/trainers/multi_modal/team/team_trainer_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/nlp/__init__.py` & `modelscope-1.5.0/modelscope/trainers/nlp/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/nlp/document_grounded_dialog_generate_trainer.py` & `modelscope-1.5.0/modelscope/trainers/nlp/document_grounded_dialog_generate_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/nlp/document_grounded_dialog_rerank_trainer.py` & `modelscope-1.5.0/modelscope/trainers/nlp/document_grounded_dialog_rerank_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/nlp/document_grounded_dialog_retrieval_trainer.py` & `modelscope-1.5.0/modelscope/trainers/nlp/document_grounded_dialog_retrieval_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/nlp/faq_question_answering_trainer.py` & `modelscope-1.5.0/modelscope/trainers/nlp/faq_question_answering_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/nlp/gpt3_trainer.py` & `modelscope-1.5.0/modelscope/trainers/nlp/gpt3_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/nlp/gpt_moe_trainer.py` & `modelscope-1.5.0/modelscope/trainers/nlp/gpt_moe_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/nlp/plug_trainer.py` & `modelscope-1.5.0/modelscope/trainers/nlp/plug_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/nlp/sentence_embedding_trainer.py` & `modelscope-1.5.0/modelscope/trainers/nlp/sentence_embedding_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/nlp/sequence_classification_trainer.py` & `modelscope-1.5.0/modelscope/trainers/nlp/sequence_classification_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/nlp/siamese_uie_trainer.py` & `modelscope-1.5.0/modelscope/trainers/nlp/siamese_uie_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/nlp/space/dialog_intent_trainer.py` & `modelscope-1.5.0/modelscope/trainers/nlp/space/dialog_intent_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/nlp/space/dialog_modeling_trainer.py` & `modelscope-1.5.0/modelscope/trainers/nlp/space/dialog_modeling_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/nlp/space/eval.py` & `modelscope-1.5.0/modelscope/trainers/nlp/space/eval.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/nlp/space/metrics/metrics_tracker.py` & `modelscope-1.5.0/modelscope/trainers/nlp/space/metrics/metrics_tracker.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/nlp/space/trainer/gen_trainer.py` & `modelscope-1.5.0/modelscope/trainers/nlp/space/trainer/gen_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/nlp/space/trainer/intent_trainer.py` & `modelscope-1.5.0/modelscope/trainers/nlp/space/trainer/intent_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/nlp/table_question_answering_trainer.py` & `modelscope-1.5.0/modelscope/trainers/nlp/table_question_answering_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/nlp/text_generation_trainer.py` & `modelscope-1.5.0/modelscope/trainers/nlp/text_generation_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/nlp/text_ranking_trainer.py` & `modelscope-1.5.0/modelscope/trainers/nlp/text_ranking_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/nlp_trainer.py` & `modelscope-1.5.0/modelscope/trainers/nlp_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/optimizer/builder.py` & `modelscope-1.5.0/modelscope/trainers/optimizer/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/optimizer/child_tuning_adamw_optimizer.py` & `modelscope-1.5.0/modelscope/trainers/optimizer/child_tuning_adamw_optimizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/parallel/builder.py` & `modelscope-1.5.0/modelscope/trainers/parallel/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/parallel/utils.py` & `modelscope-1.5.0/modelscope/trainers/parallel/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/trainer.py` & `modelscope-1.5.0/modelscope/trainers/trainer.py`

 * *Files 1% similar despite different names*

```diff
@@ -83,14 +83,15 @@
         cfg_modify_fn: An input fn which is used to modify the cfg read out of the file.
         remove_unused_data: Automatically remove unused data keys in mini-batches.
             The remove action based on the `inspect` on the model's forward method, the removed columns will be
             moved to the mini-batch's attributes.
         compile (bool, optional): Compile the model with torch 2.0, default False
         compile_options (dict, optional): The compile options if compile=True,
             default None to use the default params of 'TorchModel.compile'.
+        efficient_tuners (dict, optional): The tuners to use to train the model
 
         Examples of cfg_modify_fn:
             >>> def cfg_modify_fn(cfg):
             >>>     cfg.preprocessor.first_sequence= 'text1'
             >>>     cfg.preprocessor.second_sequence='text2'
             >>>     return cfg
     """
@@ -109,14 +110,15 @@
                                          Dict[str, Preprocessor]]] = None,
             optimizers: Tuple[torch.optim.Optimizer,
                               torch.optim.lr_scheduler._LRScheduler] = (None,
                                                                         None),
             model_revision: Optional[str] = DEFAULT_MODEL_REVISION,
             seed: int = 42,
             callbacks: Optional[List[Hook]] = None,
+            efficient_tuners: List[Dict] = None,
             **kwargs):
 
         self._seed = seed
         set_random_seed(self._seed)
         self._metric_values = None
         self.optimizers = optimizers
         self._mode = ModeKeys.TRAIN
@@ -137,17 +139,19 @@
                 kwargs.pop(ThirdParty.KEY)
 
             self.model_dir = self.get_or_download_model_dir(
                 model, model_revision, third_party)
             if cfg_file is None:
                 cfg_file = os.path.join(self.model_dir,
                                         ModelFile.CONFIGURATION)
+            self.input_model_id = model
         else:
             assert cfg_file is not None, 'Config file should not be None if model is not from pretrained!'
             self.model_dir = os.path.dirname(cfg_file)
+            self.input_model_id = None
 
         super().__init__(cfg_file, arg_parse_fn)
         self.cfg_modify_fn = cfg_modify_fn
         # add default config
         merge_cfg(self.cfg)
         self.cfg = self.rebuild_config(self.cfg)
         if 'cfg_options' in kwargs:
@@ -210,14 +214,15 @@
             self.cfg.safe_get('train.train_iters_per_epoch'))
         self._eval_iters_per_epoch = kwargs.get(
             'val_iters_per_epoch',
             self.cfg.safe_get('evaluation.val_iters_per_epoch'))
         self.use_fp16 = kwargs.get('use_fp16', False)
         self.launcher = kwargs.get('launcher')
         self.device = kwargs.get('device')
+        self.tune_module(efficient_tuners)
 
         # The parallel_groups field will be initialized in the hooks' after_init stage.
         # Please check the DDPHook and MegatronHook for details.
         self.parallel_groups = {}
 
         # Clear the Hook overload functions to avoid duplication.
         Hook.clear_strategies()
@@ -253,14 +258,22 @@
             device_name = self.device if self.device is not None else 'gpu'
             self.device = create_device(device_name)
             if self.device.type == 'cuda':
                 self.model.to(self.device)
 
         self.print_cfg()
 
+    def tune_module(self, efficient_tuners):
+        if efficient_tuners is not None:
+            for tuner in efficient_tuners:
+                type = tuner.pop('type')
+                if type == 'lora':
+                    from modelscope.tuners.lora import LoRATuner
+                    LoRATuner.tune(self.model, **tuner)
+
     def place_model(self):
         """Place model to device, or to DDP
         """
         if self.device.type == 'cuda':
             self.model.to(self.device)
             if not is_parallel(self.model) and self._dist:
                 self.model = self.to_parallel(self.model)
```

### Comparing `modelscope-1.4.2/modelscope/trainers/training_args.py` & `modelscope-1.5.0/modelscope/trainers/training_args.py`

 * *Files 12% similar despite different names*

```diff
@@ -171,48 +171,65 @@
         default=None,
         metadata={
             'cfg_node': 'train.dataloader.batch_size_per_gpu',
             'help': 'The training batch size per GPU',
         })
 
     train_data_worker: int = field(
-        default=None,
+        default=0,
         metadata={
             'cfg_node': 'train.dataloader.workers_per_gpu',
             'help': 'The number of data workers for train dataloader',
         })
 
     train_shuffle: bool = field(
         default=None,
         metadata={
             'cfg_node': 'train.dataloader.shuffle',
             'help': 'Shuffle the train dataset or not',
         })
 
+    train_drop_last: bool = field(
+        default=None,
+        metadata={
+            'cfg_node':
+            'train.dataloader.drop_last',
+            'help':
+            'Whether to drop out the last set of data in the train_dataset',
+        })
+
     per_device_eval_batch_size: int = field(
         default=None,
         metadata={
             'cfg_node': 'evaluation.dataloader.batch_size_per_gpu',
             'help': 'The eval batch size per GPU',
         })
 
     eval_data_worker: int = field(
-        default=None,
+        default=0,
         metadata={
             'cfg_node': 'evaluation.dataloader.workers_per_gpu',
             'help': 'The number of data workers for eval dataloader',
         })
 
     eval_shuffle: bool = field(
         default=None,
         metadata={
             'cfg_node': 'evaluation.dataloader.shuffle',
             'help': 'Shuffle the eval dataset or not',
         })
 
+    eval_drop_last: bool = field(
+        default=None,
+        metadata={
+            'cfg_node': 'evaluation.dataloader.drop_last',
+            'help':
+            'Whether to drop out the last set of data in the eval_dataset',
+        })
+
     max_epochs: int = field(
         default=None,
         metadata={
             'cfg_node': 'train.max_epochs',
             'help': 'The training epochs',
         })
 
@@ -325,14 +342,108 @@
             'hook_type': 'BestCkptSaverHook',
             'key': 'by_epoch',
             'choices': ['by_epoch', 'by_step'],
             'cfg_getter': get_strategy,
             'cfg_setter': set_strategy,
         })
 
+    push_to_hub: bool = field(
+        default=None,
+        metadata={
+            'help':
+            'Push to hub after one checkpoint saved by CheckpointHook in the local disk',
+            'cfg_node': 'train.checkpoint.period.push_to_hub',
+            'hook_type': 'CheckpointHook',
+            'key': 'push_to_hub',
+            'cfg_getter': get_base_hook_args,
+            'cfg_setter': set_base_hook_args,
+        })
+
+    model_id_with_org: str = field(
+        default=None,
+        metadata={
+            'help':
+            'The repo id in modelhub, usually it\'s like "group/model"',
+            'cfg_node': 'train.checkpoint.period.model_id_with_org',
+            'hook_type': 'CheckpointHook',
+            'key': 'model_id_with_org',
+            'cfg_getter': get_base_hook_args,
+            'cfg_setter': set_base_hook_args,
+        })
+
+    hub_token: str = field(
+        default=None,
+        metadata={
+            'help':
+            'The token to push to hub, you can also set the token to the env variable `MODELSCOPE_API_TOKEN`',
+            'cfg_node': 'train.checkpoint.period.hub_token',
+            'hook_type': 'CheckpointHook',
+            'key': 'hub_token',
+            'cfg_getter': get_base_hook_args,
+            'cfg_setter': set_base_hook_args,
+        })
+
+    private_hub: bool = field(
+        default=None,
+        metadata={
+            'help': 'Upload to a private hub',
+            'cfg_node': 'train.checkpoint.period.private_hub',
+            'hook_type': 'CheckpointHook',
+            'key': 'private_hub',
+            'cfg_getter': get_base_hook_args,
+            'cfg_setter': set_base_hook_args,
+        })
+
+    push_to_hub_best_model: bool = field(
+        default=None,
+        metadata={
+            'help':
+            'Push to hub after one checkpoint saved by BestCkptSaverHook in the local disk',
+            'cfg_node': 'train.checkpoint.best.push_to_hub',
+            'hook_type': 'BestCkptSaverHook',
+            'key': 'push_to_hub',
+            'cfg_getter': get_base_hook_args,
+            'cfg_setter': set_base_hook_args,
+        })
+
+    model_id_with_org_best_model: str = field(
+        default=None,
+        metadata={
+            'help':
+            'The repo id in modelhub, usually it\'s like "group/model"',
+            'cfg_node': 'train.checkpoint.best.model_id_with_org',
+            'hook_type': 'BestCkptSaverHook',
+            'key': 'model_id_with_org',
+            'cfg_getter': get_base_hook_args,
+            'cfg_setter': set_base_hook_args,
+        })
+
+    hub_token_best_model: str = field(
+        default=None,
+        metadata={
+            'help':
+            'The token to push to hub, you can also set the token to the env variable `MODELSCOPE_API_TOKEN`',
+            'cfg_node': 'train.checkpoint.best.hub_token',
+            'hook_type': 'BestCkptSaverHook',
+            'key': 'hub_token',
+            'cfg_getter': get_base_hook_args,
+            'cfg_setter': set_base_hook_args,
+        })
+
+    private_hub_best_model: bool = field(
+        default=None,
+        metadata={
+            'help': 'Upload to a private hub',
+            'cfg_node': 'train.checkpoint.best.private_hub',
+            'hook_type': 'BestCkptSaverHook',
+            'key': 'private_hub',
+            'cfg_getter': get_base_hook_args,
+            'cfg_setter': set_base_hook_args,
+        })
+
     ckpt_period_interval: int = field(
         default=1,
         metadata={
             'help':
             'The interval of epoch or iter of saving checkpoint period',
             'cfg_node': 'train.checkpoint.period.interval',
             'hook_type': 'CheckpointHook',
@@ -433,15 +544,15 @@
             'key': 'by_epoch',
             'choices': ['by_epoch', 'by_step'],
             'cfg_getter': get_strategy,
             'cfg_setter': set_strategy,
         })
 
     eval_interval: int = field(
-        default=1,
+        default=None,
         metadata={
             'help': 'Evaluation interval by epoch or iter',
             'cfg_node': 'evaluation.period.interval',
             'hook_type': 'EvaluationHook',
             'key': 'interval',
             'cfg_getter': get_base_hook_args,
             'cfg_setter': set_base_hook_args,
@@ -475,20 +586,20 @@
 
         if args.model is not None:
             try:
                 cfg = read_config(args.model)
             except Exception as e:
                 print('Read config failed with error:', e)
             else:
-                cfg.merge_from_dict(_unknown)
                 self = cls.from_config(cfg, **extra_kwargs)
         for key, value in cfg_dict.items():
             if key is not None and hasattr(self,
                                            key) and key in parser.manual_args:
                 setattr(self, key, value)
+        self.extra_args = _unknown
         return self
 
     def to_args(self):
         """Convert the TrainingArg class to key-value pairs.
 
         Returns: The key-value pair.
 
@@ -543,14 +654,16 @@
         for f in fields(self):
             if 'cfg_node' not in f.metadata:
                 continue
 
             value = getattr(self, f.name)
             if value is not None:
                 self._to_config(f, cfg)
+                if hasattr(self, 'extra_args'):
+                    cfg.merge_from_dict(self.extra_args)
             else:
                 self._to_field(f, cfg)
         return cfg
 
 
 class CliArgumentParser(ArgumentParser):
     """ Argument Parser to define and parse command-line args for training.
```

### Comparing `modelscope-1.4.2/modelscope/trainers/utils/inference.py` & `modelscope-1.5.0/modelscope/trainers/utils/inference.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/trainers/utils/log_buffer.py` & `modelscope-1.5.0/modelscope/trainers/utils/log_buffer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/ast_index_file.py` & `modelscope-1.5.0/modelscope/utils/ast_index_file.py`

 * *Files 10% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.7746532775489384%*

 * *Differences: {"'files_mtime'": "{'TEMPLATE_PATH/models/builder.py': 1681699605.6126351, "*

 * *                  "'TEMPLATE_PATH/models/audio/itn/generic_inverse_text_processing.py': "*

 * *                  '1681699605.608635, '*

 * *                  "'TEMPLATE_PATH/models/audio/sv/generic_speaker_verification.py': "*

 * *                  "1681699605.6126351, 'TEMPLATE_PATH/models/audio/sv/ecapa_tdnn.py': "*

 * *                  "1681699605.6126351, 'TEMPLATE_PATH/models/audio/ans/denoise_net.py': "*

 * *                  "1681699605.608635, 'TE […]*

```diff
@@ -1,1638 +1,1671 @@
 {
     "files_mtime": {
-        "TEMPLATE_PATH/exporters/base.py": 1679382846.439972,
-        "TEMPLATE_PATH/exporters/builder.py": 1679382846.439972,
-        "TEMPLATE_PATH/exporters/cv/cartoon_translation_exporter.py": 1679382846.439972,
-        "TEMPLATE_PATH/exporters/cv/face_detection_scrfd_exporter.py": 1679382846.439972,
-        "TEMPLATE_PATH/exporters/cv/object_detection_damoyolo_exporter.py": 1679382846.439972,
-        "TEMPLATE_PATH/exporters/nlp/csanmt_for_translation_exporter.py": 1679382846.439972,
-        "TEMPLATE_PATH/exporters/nlp/model_for_token_classification_exporter.py": 1679382846.439972,
-        "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py": 1679382846.439972,
-        "TEMPLATE_PATH/exporters/nlp/sbert_for_zero_shot_classification_exporter.py": 1679382846.439972,
-        "TEMPLATE_PATH/exporters/tf_model_exporter.py": 1679382846.439972,
-        "TEMPLATE_PATH/exporters/torch_model_exporter.py": 1679382846.439972,
-        "TEMPLATE_PATH/metrics/accuracy_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/action_detection_evaluator.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/audio_noise_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/base.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/bleu_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/builder.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/ciderD/ciderD.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/ciderD/ciderD_scorer.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/image_color_enhance_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/image_colorization_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/image_denoise_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/image_inpainting_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/image_instance_segmentation_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/image_portrait_enhancement_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/image_quality_assessment_degradation_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/image_quality_assessment_mos_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/inbatch_recall_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/loss_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/map_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/movie_scene_segmentation_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/ned_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/ocr_recognition_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/ppl_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/prediction_saving_wrapper.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/referring_video_object_segmentation_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/sequence_classification_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/text_generation_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/text_ranking_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/token_classification_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/video_frame_interpolation_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/video_stabilization_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/video_summarization_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/video_super_resolution_metric/matlab_functions.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/video_super_resolution_metric/metric_util.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/video_super_resolution_metric/niqe.py": 1679382846.4439719,
-        "TEMPLATE_PATH/metrics/video_super_resolution_metric/video_super_resolution_metric.py": 1679382846.4439719,
-        "TEMPLATE_PATH/models/audio/aec/layers/activations.py": 1679382846.4439719,
-        "TEMPLATE_PATH/models/audio/aec/layers/affine_transform.py": 1679382846.4439719,
-        "TEMPLATE_PATH/models/audio/aec/layers/deep_fsmn.py": 1679382846.4439719,
-        "TEMPLATE_PATH/models/audio/aec/layers/layer_base.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/aec/layers/uni_deep_fsmn.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/aec/network/loss.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/aec/network/modulation_loss.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/aec/network/se_net.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/ans/complex_nn.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/ans/conv_stft.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/ans/denoise_net.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/ans/frcrn.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/ans/layers/activations.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/ans/layers/affine_transform.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/ans/layers/layer_base.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/ans/layers/uni_deep_fsmn.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/ans/se_module_complex.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/ans/unet.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/asr/generic_automatic_speech_recognition.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/asr/wenet_automatic_speech_recognition.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/itn/generic_inverse_text_processing.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/kws/farfield/fsmn.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/kws/farfield/fsmn_sele_v2.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/kws/farfield/model.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/kws/farfield/model_def.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/kws/generic_key_word_spotting.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/kws/nearfield/cmvn.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/kws/nearfield/fsmn.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/kws/nearfield/model.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/punc/generic_punctuation.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/separation/layer_norm.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/separation/mossformer.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/separation/mossformer_block.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/separation/mossformer_conv_module.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/sv/ecapa_tdnn.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/sv/generic_speaker_verification.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/tts/sambert_hifi.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/audio/tts/voice.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/base/base_head.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/base/base_model.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/base/base_torch_head.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/base/base_torch_model.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/builder.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_model.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/cv/action_detection/action_detection_onnx.py": 1679382846.447972,
-        "TEMPLATE_PATH/models/cv/action_detection/modules/action_detection_pytorch.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/action_detection/modules/resnet.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/action_recognition/models.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/action_recognition/s3dg.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/action_recognition/tada_convnext.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/action_recognition/temporal_patch_shift_transformer.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/animal_recognition/resnet.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/animal_recognition/splat.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/bad_image_detecting/bad_image_detecting.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/body_2d_keypoints/hrnet_basic_modules.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/body_2d_keypoints/hrnet_v2.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/body_2d_keypoints/w48.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/cannonical_pose/body_3d_pose.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/cannonical_pose/canonical_pose_modules.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/backbone.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/block.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/directed_graph.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/hdformer.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/hdformer_detector.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/skeleton.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/cartoon/facelib/LK/lk.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/cartoon/facelib/config.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/cartoon/facelib/face_detector.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/cartoon/facelib/face_landmark.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/cartoon/facelib/facer.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/cartoon/loss.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/cartoon/model_tf.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/cartoon/mtcnn_pytorch/src/align_trans.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/cartoon/mtcnn_pytorch/src/matlab_cp2tform.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/cartoon/network.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/cartoon/utils.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/cmdssl_video_embedding/c3d.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/cmdssl_video_embedding/resnet2p1d.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/cmdssl_video_embedding/resnet3d.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/annotator.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/api.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/base_model.py": 1679382846.451972,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/blocks.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/dpt_depth.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/midas_net.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/midas_net_custom.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/transforms.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/vit.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/utils.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/mlsd/mbv2_mlsd_large.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/mlsd/utils.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/body.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/hand.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/model.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/util.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/controlnet.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/crowd_counting/cc_model.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/crowd_counting/hrnet_aspp_relu.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/easycv_base.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_2d_keypoints/face_2d_keypoints_align.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/detectors.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/mogface.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/mogprednet.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/resnet.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/utils.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/box_utils.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/detector.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/first_stage.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/get_nets.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/LK/lk.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/face_detector.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/face_landmark.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/facer.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/retinaface/detection.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/retinaface/models/net.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/retinaface/models/retinaface.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/retinaface/utils.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/transforms.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/bbox_nms.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py": 1679382846.455972,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/retinaface.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/base.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/single_stage.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/tinymog.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/preprocessor.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/scrfd_detect.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/tinymog_detect.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/detection.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/box_utils.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/mb_tiny.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/data_preprocessing.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/fd_config.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/mb_tiny_fd.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/predictor.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/ssd.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/transforms.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_emotion/efficient/model.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_emotion/efficient/utils.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_emotion/emotion_infer.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_emotion/emotion_model.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_emotion/face_alignment/face.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_emotion/face_alignment/face_align.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_generation/op/conv2d_gradfix.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_generation/op/fused_act.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_generation/op/upfirdn2d.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_generation/stylegan2.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_human_hand_detection/det_infer.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_human_hand_detection/ghost_pan.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_human_hand_detection/nanodet_plus_head.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_human_hand_detection/one_stage_detector.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_human_hand_detection/shufflenetv2.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_human_hand_detection/utils.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_recognition/align_face.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/arcface_backbone.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/common.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/facemask_backbone.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/model_irse.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/model_resnet.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/rts_backbone.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/bfm.py": 1679382846.4599721,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facelandmark/large_base_lmks_infer.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facelandmark/large_model_infer.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facelandmark/nets/large_base_lmks_net.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facelandmark/nets/large_eyeball_net.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facerecon_model.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/losses.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/networks.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/nv_diffrast.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/opt.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/utils.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/facial_expression_recognition/fer/transforms.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/facial_expression_recognition/fer/vgg.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/facial_landmark_confidence/flc/manual_landmark_net.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/hand_2d_keypoints/hand_2d_keypoints.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/hand_static/hand_model.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/hand_static/networks.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/Reconstruction.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/Embedding.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/PixToMesh.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/Res_backbone.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/Surface_head.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/detectors.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/geometry.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/human_segmenter.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/networks.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/utils.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/human_wholebody_keypoint/human_wholebody_keypoint.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/image_binary_quant_classification/binary_quant_model.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/image_binary_quant_classification/bnext.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/image_body_reshaping/image_body_reshaping.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/image_body_reshaping/model.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/image_body_reshaping/person_info.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/image_body_reshaping/pose_estimator/body.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/image_body_reshaping/pose_estimator/model.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/image_body_reshaping/pose_estimator/util.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/image_body_reshaping/slim_utils.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/image_classification/backbones/beit_v2.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/image_classification/backbones/nextvit.py": 1679382846.463972,
-        "TEMPLATE_PATH/models/cv/image_classification/mmcls_model.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_classification/resnet50_cc.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_classification/utils.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_color_enhance/adaint/adaint.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_color_enhance/csrnet.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_color_enhance/deeplpf/deeplpf_image_color_enhance.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_color_enhance/deeplpf/deeplpfnet.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_color_enhance/image_color_enhance.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/ddcolor.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/ddcolor_for_image_colorization.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/loss.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/convnext.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/position_encoding.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/transformer_utils.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/unet.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/vgg.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_colorization/unet/unet.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_colorization/unet/utils.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_debanding/rrdb/rrdb_image_debanding.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_deblur/nafnet_for_image_deblur.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/defrcn_for_fewshot.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/evaluation/coco_evaluation.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/evaluation/evaluator.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/evaluation/pascal_voc_evaluation.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/calibration_layer.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/defrcn.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/fast_rcnn.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/gdl.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/resnet.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/roi_heads.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/coco_register.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/configuration_mapper.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/model_surgery_op.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/register_data.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/requirements_check.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/voc_register.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_denoise/nafnet/NAFNet_arch.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_denoise/nafnet/arch_util.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_denoise/nafnet_for_image_denoise.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/newcrf_depth.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/newcrf_layers.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/newcrf_utils.py": 1679382846.467972,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/swin_transformer.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/uper_crf_head.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation/newcrfs_model.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/depth_estimation_bts_model.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/bts_model.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/decoder.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/encoder.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/utils.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_driving_perception/image_driving_percetion_model.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_driving_perception/preprocessor.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_driving_perception/utils.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/gan_wrap.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/model.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/op/conv2d_gradfix.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/op/fused_act.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/op/upfirdn2d.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/facelib/align_trans.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/facelib/matlab_cp2tform.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/image_face_fusion.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/network/aad_layer.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/network/aei_flow_net.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/network/bfm.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/network/dense_motion.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/network/facerecon_model.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/network/model_irse.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/network/ops.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_human_parsing/backbone/deeplab_resnet.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_human_parsing/m2fp/m2fp_decoder.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_human_parsing/m2fp/m2fp_encoder.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_human_parsing/m2fp_net.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_human_parsing/parsing_utils.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_inpainting/base.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_inpainting/default.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_inpainting/model.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/ade20k/base.py": 1679382846.4719722,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/ade20k/resnet.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/adversarial.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/feature_matching.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/ffc.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/inception.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/perceptual.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/pix2pixhd.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_inpainting/refinement.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/backbones/swin_transformer.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/cascade_mask_rcnn_swin.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/datasets/transforms.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/dino_decoder.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/maskdino_decoder.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/maskdino_encoder.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/ms_deform_attn.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/position_encoding.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/utils.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino_model.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino_swin.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/model.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/postprocess_utils.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_matching/config/default.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/backbone/resnet_fpn.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/fine_preprocess.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/linear_attention.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/quadtree_attention.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/transformer.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/utils/coarse_matching.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/utils/fine_matching.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/utils/position_encoding.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_matching/quadtree_attention_model.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_matching/utils/misc.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/cas_mvsnet.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/casmvs_model.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/colmap2mvsnet.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/depth_filter.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/general_eval_dataset.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/module.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/utils.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_paintbyexample/model.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_panoptic_segmentation/panseg_model.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_panoptic_segmentation/r50_panseg_model.py": 1679382846.4759722,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/align_faces.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/eqface/fqa.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/eqface/model_resnet.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/gpen.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/image_portrait_enhancement.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/losses/helpers.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/losses/losses.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/losses/model_irse.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/detection.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/models/net.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/models/retinaface.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/utils.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_probing_model/backbone.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_probing_model/model.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_probing_model/utils.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_degradation/degradation_model.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_man/image_quality_assessment_man.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_man/maniqa.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_man/swin.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/backbones/resnet.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/censeo_ivqa_model.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/heads/simple_head.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/image_quality_assessment_mos.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_reid_person/pass_model.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_reid_person/transreid_model.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_restoration/demoire_models/nets.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_restoration/image_restoration_model.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/data_util.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/feature_extractors.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/pixel_classifier.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/utils.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/pan_merge/base_panoptic_fusion_head.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/pan_merge/maskformer_semantic_head.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/segformer.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/semantic_seg_model.py": 1679382846.4799721,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/adapter_modules.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/beit.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/beit_adapter.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/base_decode_head.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/mask2former_head_from_mmseg.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/base_segmentor.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/encoder_decoder_mask2former.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/utils/builder.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/utils/data_process_func.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/utils/seg_func.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_skychange/preprocessor.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/BlockModules.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/hrnet_backnone.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/hrnet_super_and_ocr.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/unet.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_skychange/skychange.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_skychange/skychange_model.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_to_image_generation/data/transforms.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_to_image_generation/model.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_to_image_generation/models/autoencoder.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_to_image_generation/models/clip.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_to_image_generation/ops/diffusion.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_to_image_generation/ops/losses.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/data/transforms.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/model_translation.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/models/autoencoder.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/models/clip.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/apps.py": 1679382846.483972,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/degradation.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/diffusion.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/losses.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/metrics.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/random_color.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/random_mask.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/svd.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/utils.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/backbone/resnet_DA.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/backbone/vit_horizon_pry_image.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/misc/fourier.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/misc/panostretch.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/misc/post_proc.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/modality/layout.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/panovit.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/utils.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/panovit.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/summarizer.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/layers.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/models.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/modules.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/sub_layers.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/motion_generation/model.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/motion_generation/modules/cfg_sampler.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/motion_generation/modules/gaussian_diffusion.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/motion_generation/modules/mdm.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/motion_generation/modules/respace.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/motion_generation/modules/rotation2xyz.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/motion_generation/modules/smpl.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/get_model.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/model.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/head.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/save_op.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/shot_encoder.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/trn.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/nerf_recon_acc/dataloader/nerf_dataset.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/nerf_recon_acc/dataloader/read_write_model.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/nerf_recon_acc/nerf_preprocess.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/nerf_recon_acc/nerf_recon_acc.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/nerf_recon_acc/network/nerf.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/nerf_recon_acc/network/segmenter.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/nerf_recon_acc/network/utils.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/object_detection/dino.py": 1679382846.4879723,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_model.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/backbones/vit.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/dense_heads/anchor_head.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/necks/fpn.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/fcn_mask_head.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/utils/checkpoint.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/utils/convModule_norm.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection/yolox_pai.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/depe_detect.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/match_cost.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/util.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/nuscenes_dataset.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/loading.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/vovnet.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/depth_net.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/petrv2_dednhead.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/petr3d.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/cp_fpn.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/positional_encoding.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/result_vis.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/ocr_detection/model.py": 1679382846.4919722,
-        "TEMPLATE_PATH/models/cv/ocr_detection/modules/dbnet.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/ocr_detection/modules/seg_detector_loss.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/ocr_detection/preprocessor.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/ocr_detection/utils.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/model.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/convnext.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/convnextvit.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/crnn.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/timm_tinyc.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/vitstr.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/preprocessor.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/open_vocabulary_detection_vild/vild.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/equi.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/layers.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/mobilenet.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/resnet.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/unifuse.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/util.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/unifuse_model.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/common.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/pointnet2_utils.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/rcp_model.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/sf_rcp.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/product_retrieval_embedding/item_detection.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/product_retrieval_embedding/item_embedding.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/product_retrieval_embedding/item_model.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/product_segmentation/net.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/product_segmentation/seg_infer.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/model.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/backbone.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/criterion.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/matcher.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/misc.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/mttr.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/multimodal_transformer.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/position_encoding_2d.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/postprocessing.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/segmentation.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/swin_transformer.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/robust_image_classification/easyrobust_model.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/salient_detection/models/backbone/Res2Net_v1b.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/salient_detection/models/modules.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/salient_detection/models/senet.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/salient_detection/models/u2net.py": 1679382846.4959722,
-        "TEMPLATE_PATH/models/cv/salient_detection/models/utils.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/salient_detection/salient_model.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/shop_segmentation/common.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/shop_segmentation/head_fpn.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/shop_segmentation/models.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/shop_segmentation/neck_fpn.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/shop_segmentation/shop_seg_base.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/shop_segmentation/shop_seg_model.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/shop_segmentation/utils.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/skin_retouching/detection_model/detection_module.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/skin_retouching/detection_model/detection_unet_in.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/skin_retouching/inpainting_model/gconv.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/skin_retouching/inpainting_model/inpainting_unet.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/box_utils.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/net.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/network.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/predict_single.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/prior_box.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/utils.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/skin_retouching/unet_deploy.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/skin_retouching/utils.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/skin_retouching/weights_init.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/stream_yolo/data/data_augment.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/stream_yolo/exp/base_exp.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/stream_yolo/exp/build.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/stream_yolo/exp/default/streamyolo.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/stream_yolo/exp/yolox_base.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/stream_yolo/models/darknet.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/stream_yolo/models/dfp_pafpn.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/stream_yolo/models/network_blocks.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/stream_yolo/models/streamyolo.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/stream_yolo/models/tal_head.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/stream_yolo/realtime_video_detector.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/stream_yolo/utils/boxes.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/stream_yolo/utils/format.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/super_resolution/arch_util.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/super_resolution/ecb.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/super_resolution/ecbsr_model.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/super_resolution/rrdbnet_arch.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/table_recognition/lineless_table_process.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/table_recognition/model_lore.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/table_recognition/modules/lore_detector.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/table_recognition/modules/lore_processor.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/clip.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_base.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_blocks.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_model.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_net.py": 1679382846.4999723,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_vit.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/model.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/simple_tokenizer.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/basic_blocks.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/global_utils.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/master_net.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/model_zoo.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/plain_net_utils.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_blocks.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_res_idwexkx.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_res_k1kxk1.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_res_kxkx.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/apis/detector_evaluater.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/apis/detector_inference.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/box_level_augs.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/color_augs.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/gaussian_maps.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/geometric_augs.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/scale_aware_aug.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/backbones/darknet.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_csp.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_res.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/base_ops.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/neck_ops.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/ops.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/ota_assigner.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/repvgg_block.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/utils.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/weight_init.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/heads/gfocal_v2_tiny.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/heads/zero_head.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/losses/distill_loss.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/losses/gfocal_loss.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/necks/giraffe_config.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn_btn.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/detectors/detector.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/structures/bounding_box.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/structures/boxlist_ops.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/structures/image_list.py": 1679382846.5039723,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/utils/boxes.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/utils/model_utils.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/utils/scheduler.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/detector.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/tinynas_damoyolo.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/tinynas_detector.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/utils.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_deinterlace/UNet_for_video_deinterlace.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_deinterlace/deinterlace_arch.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_deinterlace/models/archs.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_deinterlace/models/deep_fourier_upsampling.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_deinterlace/models/enh.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_deinterlace/models/fre.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_deinterlace/models/utils.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/configs/default_config.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/dro_model.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/camera.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/camera_utils.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/pose.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/pose_utils.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/model_checkpoint.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/model_utils.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/model_wrapper.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/sfm_model_mf.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/sup_model_mf.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/depth_pose/depth_pose_net.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/depth_decoder.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/layers.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/pose_decoder.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/resnet_encoder.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/optim/extractor.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/optim/update.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/augmentations.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/config.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/depth.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/horovod.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/image.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/image_gt.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/load.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/misc.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/types.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/VFINet_arch.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/VFINet_for_video_frame_interpolation.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/corr.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/extractor.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/raft.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/update.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/IFNet_swin.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/UNet.py": 1679382846.5079722,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/flow_reversal.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/refinenet_arch.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/transformer_layers.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/utils/scene_change_detection.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/utils/utils.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_human_matting/model.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_human_matting/models/decoder.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_human_matting/models/deep_guided_filter.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_human_matting/models/effv2.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_human_matting/models/lraspp.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_human_matting/models/matting.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_inpainting/inpainting.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_inpainting/inpainting_model.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_frame_iter_head.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_head.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_iter_head.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_update_head.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_updator.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/neck/msdeformattn_decoder.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/track/kernel_update_head.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/utils.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/video_knet.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/common.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/decode.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/model.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/yolo.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/tracker/basetrack.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/tracker/matching.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/tracker/multitracker.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/image.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/kalman_filter.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/utils.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/visualization.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/aggregate.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/cbam.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/eval_network.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/inference_core.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/inference_memory_bank.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/mod_resnet.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/model.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/modules.py": 1679382846.5119724,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/network.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/backbone/swin_checkpoint.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/backbone/swin_transformer.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_head.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_iter_head.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_update_head.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_updator.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/mask.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/semantic_fpn_wrapper.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/track_heads.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/neck/fpn.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/track/quasi_dense_embed_tracker.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/video_k_net.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/visualizer.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/config/ostrack.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/attn.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/attn_blocks.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/head.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/patch_embed.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/base_backbone.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/ostrack.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/utils.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/vit_ce.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/procontext/procontext.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/procontext/utils.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/procontext/vit_ce.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/tracker/ostrack.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/tracker/procontext.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/utils/utils.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/DUT_raft.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/MotionPro.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/corr.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/extractor.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/raft.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/update.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/Smoother.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/config.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/rf_det_module.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/rf_det_so.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUTRAFTStabilizer.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_stabilization/utils/IterativeSmooth.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_stabilization/utils/MedianFilter.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_stabilization/utils/ProjectionUtils.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_stabilization/utils/RAFTUtils.py": 1679382846.5159724,
-        "TEMPLATE_PATH/models/cv/video_stabilization/utils/WarpUtils.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/video_stabilization/utils/image_utils.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/video_stabilization/utils/math_utils.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/exp/longshortnet_base.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/longshortnet.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_long.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_short.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/longshort.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/longshort_backbone_neck.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/video_summarization/base_model.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/video_summarization/kts/cpd_auto.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/video_summarization/kts/cpd_nonlin.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/video_summarization/pgl_sum.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/video_summarization/summarizer.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/video_super_resolution/basicvsr_net.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/video_super_resolution/common.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/video_super_resolution/msrresnet_lite_model.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/video_super_resolution/real_basicvsr_for_video_super_resolution.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/video_super_resolution/real_basicvsr_net.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/vidt/backbone.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/vidt/deformable_transformer.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/vidt/fpn_fusion.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/vidt/head.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/vidt/model.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/virual_tryon/sdafnet.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/backbone.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/head.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/model.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/petl.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/timm_helpers.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/timm_vision_transformer.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/timm_weight_init.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/vision_efficient_tuning.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/vision_middleware/backbone.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/vision_middleware/head.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/vision_middleware/model.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/vision_middleware/vim.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/vop_retrieval/backbone.py": 1679382846.5199723,
-        "TEMPLATE_PATH/models/cv/vop_retrieval/basic_utils.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/cv/vop_retrieval/model.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/cv/vop_retrieval/model_se.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/cv/vop_retrieval/tokenization_clip.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/clip/bert_tokenizer.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/clip/configuration_bert.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/clip/model.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/clip/modeling_bert.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/diffusion/diffusion.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/diffusion/model.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/diffusion/structbert.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/diffusion/tokenizer.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/diffusion/unet_generator.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/diffusion/unet_upsampler_1024.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/diffusion/unet_upsampler_256.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/dpm_solver_pytorch.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/gemm/gemm_base.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/gemm/gemm_model.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/gemm/tokenizer.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/gaussian_diffusion.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/respace.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/script.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/unet.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/mgeo/backbone.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/mgeo/text_classification.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/mgeo/text_ranking.py": 1679382846.5239725,
-        "TEMPLATE_PATH/models/multi_modal/mgeo/token_classification.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/mmr/dataloaders/rawvideo_util.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/mmr/models/clip_for_mm_video_embedding.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/mmr/models/dynamic_inverted_softmax.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/mmr/models/modeling.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/mmr/models/module_clip.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/mmr/models/module_cross.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/mmr/models/tokenization_clip.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/mmr/models/until_module.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/mplug/clip/clip.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/mplug/configuration_mplug.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/mplug/modeling_mplug.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/mplug/mvit.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/mplug/predictor.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/mplug_for_all_tasks.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/clip.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/decoder.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/gaussian_diffusion.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/model.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/prior.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/tokenizer.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/upsampler.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/xglm.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/ofa/configuration_mmspeech.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/ofa/configuration_ofa.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/ofa/generate/incremental_decoding_utils.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/ofa/generate/multihead_attention.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/ofa/generate/ngram_repeat_block.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/ofa/generate/search.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/ofa/generate/sequence_generator.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/ofa/generate/token_generation_constraints.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/ofa/generate/utils.py": 1679382846.5279725,
-        "TEMPLATE_PATH/models/multi_modal/ofa/modeling_mmspeech.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/ofa/modeling_ofa.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/ofa/resnet.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/ofa/tokenization_ofa.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/ofa/tokenization_ofa_fast.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/ofa/utils/constant.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/ofa/utils/utils.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/ofa/vit.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/ofa_for_text_to_image_synthesis_model.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/rleg/model.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/rleg/rleg.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/soonet/blocks.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/soonet/clip.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/soonet/model.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/soonet/swin_transformer.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/soonet/tokenizer.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/soonet/utils.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/team/team_model.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/team/utils.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/video_synthesis/autoencoder.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/video_synthesis/diffusion.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/video_synthesis/unet_sd.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/vldoc/conv_fpn_trans.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/vldoc/convnext.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/vldoc/model.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/vldoc/modeling_layout_roberta.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/vldoc/processing.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/vldoc/tokenization.py": 1679382846.5319724,
-        "TEMPLATE_PATH/models/multi_modal/vldoc/transformer_local.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/T5/backbone.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/T5/configuration.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/T5/text2text_generation.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/bart/text_error_correction.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/bert/backbone.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/bert/configuration.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/bert/document_segmentation.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/bert/fill_mask.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/bert/sentence_embedding.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/bert/siamese_uie.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/bert/text_classification.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/bert/text_ranking.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/bert/token_classification.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/bert/word_alignment.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/bloom/backbone.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/codegeex/codegeex.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/codegeex/codegeex_for_code_generation.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/codegeex/codegeex_for_code_translation.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/codegeex/inference.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/codegeex/tokenizer.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/csanmt/translation.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/deberta_v2/backbone.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/deberta_v2/configuration.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/deberta_v2/fill_mask.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/deberta_v2/tokenization.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/deberta_v2/tokenization_fast.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/dgds/backbone.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_generate.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_rerank.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_retrieval.py": 1679382846.5359724,
-        "TEMPLATE_PATH/models/nlp/fid_plug/backbone.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/fid_plug/configuration.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/fid_plug/text_generation.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/gpt2/backbone.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/gpt3/backbone.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/gpt3/configuration.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/gpt3/distributed_gpt3.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/gpt3/text_generation.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/gpt3/tokenizer.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/backbone.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/checkpointing.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/configuration.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/distributed_gpt_moe.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/experts.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/layer.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/mappings.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/sharded_moe.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/utils.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/text_generation.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/tokenizer.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/gpt_neo/backbone.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/heads/crf_head.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/heads/fill_mask_head.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/heads/infromation_extraction_head.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/heads/text_classification_head.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/heads/text_generation_head.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/heads/text_ranking_head.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/heads/token_classification_head.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/heads/torch_pretrain_head.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/hf_transformers/backbone.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/lstm/backbone.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/lstm/token_classification.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/megatron_bert/backbone.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/megatron_bert/configuration.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/megatron_bert/fill_mask.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/mglm/arguments.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/mglm/blocklm_utils.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/mglm/configure_data.py": 1679382846.5399725,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/corpora.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/datasets.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/extraction.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/file_utils.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/lazy_loader.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/samplers.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/sp_tokenizer.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/tokenization.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/tokenization_gpt2.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/wordpiece.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/generation_utils.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/mglm_for_text_summarization.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/model/distributed.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/model/downstream.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/model/modeling_bert.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/model/modeling_glm.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/model/prompt.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/model/transformer.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/process_grid.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/run_test.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/data_utils.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/eval_utils.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/language_model/dataset.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/language_model/detokenizer.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/language_model/finetune.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/seq2seq/dataset.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/seq2seq/evaluate.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/seq2seq/finetune.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/dataset.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/evaluate.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/finetune.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/pvp.py": 1679382846.5439725,
-        "TEMPLATE_PATH/models/nlp/mglm/test/test_block.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/mglm/test/test_rel_shift.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/mglm/train_utils.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/mglm/utils.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/palm_v2/configuration.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/palm_v2/dureader_eval.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/palm_v2/text_generation.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/peer/backbone.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/peer/configuration.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/peer/sas_utils.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/peer/text_classification.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/plug/AnnealingLR.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/plug/backbone.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/plug/configuration.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/plug/distributed_plug.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/plug/generator.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/plug_mental/adv_utils.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/plug_mental/backbone.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/plug_mental/configuration.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/plug_mental/text_classification.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/ponet/backbone.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/ponet/configuration.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/ponet/document_segmentation.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/ponet/fill_mask.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/ponet/tokenization.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/space/configuration.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/space/dialog_intent_prediction.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/space/dialog_modeling.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/space/dialog_state_tracking.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/space/model/gen_unified_transformer.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/space/model/generator.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/space/model/intent_unified_transformer.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/space/model/model_base.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/space/model/tokenization_space.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/space/model/unified_transformer.py": 1679382846.5479724,
-        "TEMPLATE_PATH/models/nlp/space/modules/embedder.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/space/modules/feedforward.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/space/modules/functions.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/space/modules/multihead_attention.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/space/modules/transformer_block.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/space_T_cn/backbone.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/space_T_cn/configuration.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/space_T_cn/table_question_answering.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/space_T_en/text_to_sql.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/structbert/adv_utils.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/structbert/backbone.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/structbert/configuration.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/structbert/faq_question_answering.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/structbert/fill_mask.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/structbert/text_classification.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/structbert/token_classification.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/task_models/feature_extraction.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/task_models/fill_mask.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/task_models/information_extraction.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/task_models/task_model.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/task_models/text_classification.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/task_models/text_generation.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/task_models/text_ranking.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/task_models/token_classification.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/unite/configuration_unite.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/unite/modeling_unite.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/use/transformer.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/use/user_satisfaction_estimation.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/veco/backbone.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/veco/configuration.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/veco/fill_mask.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/veco/text_classification.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/veco/token_classification.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/xlm_roberta/backbone.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/nlp/xlm_roberta/configuration.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/science/unifold/config.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/science/unifold/data/data_ops.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/science/unifold/data/msa_pairing.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/science/unifold/data/process.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/science/unifold/data/process_multimer.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/science/unifold/data/protein.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/science/unifold/data/residue_constants.py": 1679382846.5519726,
-        "TEMPLATE_PATH/models/science/unifold/data/utils.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/dataset.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/model.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/modules/alphafold.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/modules/attentions.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/modules/auxillary_heads.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/modules/common.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/modules/confidence.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/modules/embedders.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/modules/evoformer.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/modules/featurization.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/modules/frame.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/modules/structure_module.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/modules/template.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/modules/triangle_multiplication.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/msa/mmcif.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/msa/msa_identifiers.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/msa/parsers.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/msa/pipeline.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/msa/templates.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/msa/tools/hhblits.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/msa/tools/hhsearch.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/msa/tools/hmmbuild.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/msa/tools/hmmsearch.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/msa/tools/jackhmmer.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/msa/tools/kalign.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/msa/tools/utils.py": 1679382846.5559726,
-        "TEMPLATE_PATH/models/science/unifold/msa/utils.py": 1679382846.5559726,
-        "TEMPLATE_PATH/msdatasets/audio/asr_dataset.py": 1679382846.5559726,
-        "TEMPLATE_PATH/msdatasets/auth/auth_config.py": 1679382846.5559726,
-        "TEMPLATE_PATH/msdatasets/context/dataset_context_config.py": 1679382846.5559726,
-        "TEMPLATE_PATH/msdatasets/data_files/data_files_manager.py": 1679382846.5559726,
-        "TEMPLATE_PATH/msdatasets/data_loader/data_loader.py": 1679382846.5559726,
-        "TEMPLATE_PATH/msdatasets/data_loader/data_loader_manager.py": 1679382846.5559726,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/asr_dataset.py": 1679382846.5559726,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/kws_farfield_dataset.py": 1679382846.5559726,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_dataset.py": 1679382846.5559726,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_processor.py": 1679382846.5559726,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/bad_image_detecting_dataset.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/builder.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/build.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/collate_batch.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/coco.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/mosaic_wrapper.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/coco_eval.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/distributed.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/grouped_batch_sampler.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/iteration_based_batch_sampler.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/build.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/transforms.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/easycv_base.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/face_2d_keypoins/face_2d_keypoints_dataset.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/gopro_image_deblurring_dataset.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/hand_2d_keypoints/hand_2d_keypoints_dataset.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/human_wholebody_keypoint/human_wholebody_keypoint_dataset.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_classification/classification_dataset.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_colorization/image_colorization_dataset.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_inpainting/aug.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_instance_segmentation_coco_dataset.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/data_utils.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/image_portrait_enhancement_dataset.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/image_quality_assessment_degradation_dataset.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/image_quality_assessment_mos_dataset.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_semantic_segmentation/segmentation_dataset.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/language_guided_video_summarization_dataset.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/mgeo_ranking_dataset.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/sampler.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/object_detection/detection_dataset.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/augmenter.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/data_loader.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/image_dataset.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/iou_evaluator.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/quad_measurer.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/augment_data.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/data_process.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_border_map.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_icdar_data.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_seg_detection_data.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/normalize_image.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/random_crop_data.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_recognition_dataset.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/reds_image_deblurring_dataset.py": 1679382846.5599725,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/referring_video_object_segmentation_dataset.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/transformers.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/data_utils.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/sidd_image_denoising_dataset.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/transforms.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/torch_custom_dataset.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/veco_dataset.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/data_utils.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/video_frame_interpolation_dataset.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_stabilization/video_stabilization_dataset.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_summarization_dataset.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_super_resolution/video_super_resolution_dataset.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/dataset.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/download/dataset_builder.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/download/download_config.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/download/download_manager.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/meta/data_meta_config.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/meta/data_meta_manager.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/ms_dataset.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/task_datasets/gopro_image_deblurring_dataset.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/task_datasets/reds_image_deblurring_dataset.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/task_datasets/sidd_image_denoising.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/task_datasets/torch_base_dataset.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/task_datasets/video_summarization_dataset.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/utils/dataset_utils.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/utils/delete_utils.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/utils/oss_utils.py": 1679382846.5639727,
-        "TEMPLATE_PATH/msdatasets/utils/upload_utils.py": 1679382846.5639727,
-        "TEMPLATE_PATH/pipelines/audio/ans_dfsmn_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/audio/ans_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/audio/asr_inference_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/audio/asr_wenet_inference_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/audio/inverse_text_processing_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/audio/kws_farfield_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/audio/kws_kwsbp_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/audio/linear_aec_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/audio/lm_infer_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/audio/punctuation_processing_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/audio/separation_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/audio/speaker_diarization_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/audio/speaker_verification_light_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/audio/speaker_verification_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/audio/text_to_speech_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/audio/timestamp_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/audio/voice_activity_detection_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/base.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/builder.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/action_detection_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/action_recognition_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/animal_recognition_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/arc_face_recognition_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/bad_image_detecting_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/body_2d_keypoints_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/body_3d_keypoints_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/card_detection_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/cmdssl_video_embedding_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/content_check_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/controllable_image_generation_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/crowd_counting_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/ddcolor_image_colorization_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/ddpm_semantic_segmentation_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/base.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/detection_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/face_2d_keypoints_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/human_wholebody_keypoint_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/segmentation_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/face_attribute_recognition_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/face_detection_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/face_emotion_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/face_human_hand_detection_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/face_image_generation_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/face_liveness_ir_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/face_liveness_xc_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/face_processing_base_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/face_quality_assessment_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/face_recognition_onnx_fm_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/face_recognition_onnx_ir_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/face_recognition_ood_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/face_recognition_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/face_reconstruction_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/facial_expression_recognition_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/facial_landmark_confidence_pipeline.py": 1679382846.5679727,
-        "TEMPLATE_PATH/pipelines/cv/general_recognition_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/hand_2d_keypoints_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/hand_static_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/hicossl_video_embedding_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/human_reconstruction_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_body_reshaping_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_bts_depth_estimation_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_cartoon_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_color_enhance_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_colorization_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_debanding_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_deblur_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_defrcn_fewshot_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_denoise_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_depth_estimation_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_detection_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_driving_perception_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_face_fusion_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_human_parsing_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_inpainting_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_inpainting_sdv2_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_instance_segmentation_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_matching_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_matting_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_mvs_depth_estimation_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_open_vocabulary_detection_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_paintbyexample_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_panoptic_segmentation_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_portrait_enhancement_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_degradation_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_man_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_mos_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_reid_person_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_restoration_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_salient_detection_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_semantic_segmentation_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_skychange_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_structured_model_probing_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_style_transfer_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_super_resolution_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_to_image_generate_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/image_to_image_translation_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/indoor_layout_estimation_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/language_guided_video_summarization_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/license_plate_detection_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/lineless_table_recognition_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/live_category_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/mask_face_recognition_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/maskdino_instance_segmentation_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/mobile_image_super_resolution_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/mog_face_detection_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/motion_generation_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/movie_scene_segmentation_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/mtcnn_face_detection_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/nerf_recon_acc_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/object_detection_3d_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/ocr_detection_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/ocr_recognition_pipeline.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_convnext_transformer.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_dla34.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_resnet18_half.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_resnet_mutex_v4_linewithchar.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_vlpt.py": 1679382846.5719726,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ocr_modules/convnext.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ocr_modules/timm_tinyc.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ocr_modules/vitstr.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ops.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/resnet18_v1.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/resnet_utils.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/table_process.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/utils.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/panorama_depth_estimation_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/product_retrieval_embedding_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/product_segmentation_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/realtime_video_object_detection_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/referring_video_object_segmentation_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/retina_face_detection_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/shop_segmentation_pipleline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/skin_retouching_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/table_recognition_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/tbs_detection_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/tbs_detection_utils/utils.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/text_driven_segmentation_pipleline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/tinynas_classification_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/tinynas_detection_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/ulfd_face_detection_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/video_category_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/video_colorization_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/video_deinterlace_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/video_depth_estimation_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/video_frame_interpolation_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/video_human_matting_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/video_inpainting_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/video_instance_segmentation_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/video_multi_object_tracking_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/video_object_segmentation_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/video_panoptic_segmentation_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/video_single_object_tracking_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/video_stabilization_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/video_summarization_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/video_super_resolution_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/vidt_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/virtual_try_on_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/vision_efficient_tuning_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/vision_middleware_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/vop_retrieval_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/cv/vop_retrieval_se_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/multi_modal/asr_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/diffusers_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py": 1679382846.5759726,
-        "TEMPLATE_PATH/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/multi_modal/disco_guided_diffusion_pipeline/utils.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/multi_modal/document_vl_embedding_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/multi_modal/gridvlp_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/multi_modal/image_captioning_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/multi_modal/image_text_retrieval_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/multi_modal/mgeo_ranking_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/multi_modal/multi_modal_embedding_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/multi_modal/ocr_recognition_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/multi_modal/sudoku_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/multi_modal/team_multi_modal_similarity_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/multi_modal/text2sql_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/multi_modal/text_to_image_synthesis_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/multi_modal/text_to_video_synthesis_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/multi_modal/video_captioning_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/multi_modal/video_multi_modal_embedding_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/multi_modal/video_question_answering_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/multi_modal/visual_entailment_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/multi_modal/visual_grounding_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/multi_modal/visual_question_answering_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/automatic_post_editing_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/codegeex_code_generation_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/codegeex_code_translation_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/conversational_text_to_sql_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/dialog_intent_prediction_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/dialog_modeling_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/dialog_state_tracking_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/distributed_gpt3_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/distributed_gpt_moe_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/distributed_plug_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_generate_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_rerank_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_retrieval_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/document_segmentation_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/extractive_summarization_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/faq_question_answering_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/fasttext_text_classification_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/feature_extraction_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/fid_dialogue_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/fill_mask_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/information_extraction_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/interactive_translation_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/language_identification_pipline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/mglm_text_summarization_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/named_entity_recognition_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/sentence_embedding_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/siamese_uie_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/summarization_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/table_question_answering_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/text_error_correction_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/text_generation_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/text_ranking_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/token_classification_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/translation_evaluation_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/translation_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/translation_quality_estimation_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/user_satisfaction_estimation_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/word_alignment_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/word_segmentation_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/nlp/zero_shot_classification_pipeline.py": 1679382846.5799727,
-        "TEMPLATE_PATH/pipelines/science/protein_structure_pipeline.py": 1679382846.5839727,
-        "TEMPLATE_PATH/pipelines/util.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/asr.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/audio.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/base.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/builder.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/common.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/cv/action_detection_mapper.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/cv/bad_image_detecting_preprocessor.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/cv/controllable_image_generation.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/cv/cv2_transforms.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/cv/image_quality_assessment_man.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/cv/image_quality_assessment_mos.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/cv/image_restoration_preprocessor.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/cv/mmcls_preprocessor.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/cv/timer.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/cv/util.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/cv/video_stabilization.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/cv/video_super_resolution.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/image.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/kws.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/movie_scene_segmentation/transforms.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/multi_modal.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/bert_seq_cls_tokenizer.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/dialog_classification_use_preprocessor.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/document_segmentation_preprocessor.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/faq_question_answering_preprocessor.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/feature_extraction_preprocessor.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/fill_mask_preprocessor.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/mgeo_ranking_preprocessor.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/mglm_summarization_preprocessor.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/relation_extraction_preprocessor.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/sentence_embedding_preprocessor.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/siamese_uie_preprocessor.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/space/args.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/space/batch.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/space/data_loader.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/space/dialog_intent_prediction_preprocessor.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/space/dialog_modeling_preprocessor.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/space/dialog_state_tracking_preprocessor.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/space/dst_processors.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/space/fields/gen_field.py": 1679382846.5839727,
-        "TEMPLATE_PATH/preprocessors/nlp/space/fields/intent_field.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/space/lazy_dataset.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/space/preprocess.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/space/sampler.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/space/tensorlistdataset.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/space/tokenizer.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/fields/database.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/fields/schema_link.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/fields/struct.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/table_question_answering_preprocessor.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/conversational_text_to_sql_preprocessor.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/common_utils.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/parse.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/preprocess_dataset.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/process_dataset.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/text_classification_preprocessor.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/text_error_correction.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/text_generation_preprocessor.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/text_ranking_preprocessor.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/token_classification_preprocessor.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/token_classification_thai_preprocessor.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/token_classification_viet_preprocessor.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/transformers_tokenizer.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/translation_evaluation_preprocessor.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/utils.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/word_alignment_preprocessor.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/nlp/zero_shot_classification_preprocessor.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/asr.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/base.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/image_captioning.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/image_classification.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/ocr_recognition.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/sudoku.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/summarization.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/text2sql.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/text_classification.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/text_to_image_synthesis.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/audio_helper.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/bridge_content_encoder.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/collate.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/constant.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/get_tables.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/random_help.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/text2phone.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/transforms.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/vision_helper.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/visual_entailment.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/visual_grounding.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/ofa/visual_question_answering.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/science/uni_fold.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/tts.py": 1679382846.5879726,
-        "TEMPLATE_PATH/preprocessors/video.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/audio/ans_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/audio/asr_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/audio/kws_farfield_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/audio/kws_nearfield_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/audio/kws_utils/batch_utils.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/audio/kws_utils/det_utils.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/audio/kws_utils/file_utils.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/audio/kws_utils/model_utils.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/audio/kws_utils/runtime_utils.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/audio/separation_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/audio/tts_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/base.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/builder.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/cv/action_detection_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/cv/card_detection_scrfd_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/cv/cartoon_translation_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/cv/face_detection_scrfd_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/cv/image_classifition_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/cv/image_defrcn_fewshot_detection_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/cv/image_detection_damoyolo_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/cv/image_inpainting_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/cv/image_instance_segmentation_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/cv/image_portrait_enhancement_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/cv/movie_scene_segmentation_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/cv/nerf_recon_acc_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/cv/ocr_detection_db_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/cv/ocr_recognition_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/cv/referring_video_object_segmentation_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/cv/vision_efficient_tuning_trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/default_config.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/easycv/trainer.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/easycv/utils/hooks.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/easycv/utils/metric.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/easycv/utils/register_util.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/hooks/builder.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/hooks/checkpoint_hook.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/hooks/clip_clamp_logit_scale_hook.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/hooks/compression/sparsity_hook.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/hooks/compression/utils.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/hooks/ddp_hook.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/hooks/deepspeed_hook.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/hooks/early_stop_hook.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/hooks/evaluation_hook.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/hooks/hook.py": 1679382846.5919728,
-        "TEMPLATE_PATH/trainers/hooks/iter_timer_hook.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/hooks/logger/base.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/hooks/logger/tensorboard_hook.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/hooks/logger/text_logger_hook.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/hooks/lr_scheduler_hook.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/hooks/megatron_hook.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/hooks/optimizer/apex_optimizer_hook.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/hooks/optimizer/base.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/hooks/optimizer/torch_optimizer_hook.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/hooks/priority.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/lrscheduler/builder.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/lrscheduler/warmup/base.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/lrscheduler/warmup/warmup.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/multi_modal/clip/clip_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/multi_modal/clip/clip_trainer_utils.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/multi_modal/mgeo_ranking_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/multi_modal/mplug/mplug_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/multi_modal/ofa/ofa_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/multi_modal/ofa/ofa_trainer_utils.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/multi_modal/team/team_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/multi_modal/team/team_trainer_utils.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/nlp/csanmt_translation_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_generate_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_rerank_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_retrieval_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/nlp/faq_question_answering_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/nlp/gpt3_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/nlp/gpt_moe_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/nlp/plug_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/nlp/sentence_embedding_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/nlp/sequence_classification_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/nlp/siamese_uie_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/nlp/space/dialog_intent_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/nlp/space/dialog_modeling_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/nlp/space/eval.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/nlp/space/metrics/metrics_tracker.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/nlp/space/trainer/gen_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/nlp/space/trainer/intent_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/nlp/table_question_answering_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/nlp/text_generation_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/nlp/text_ranking_trainer.py": 1679382846.5959728,
-        "TEMPLATE_PATH/trainers/nlp_trainer.py": 1679382846.5999727,
-        "TEMPLATE_PATH/trainers/optimizer/builder.py": 1679382846.5999727,
-        "TEMPLATE_PATH/trainers/optimizer/child_tuning_adamw_optimizer.py": 1679382846.5999727,
-        "TEMPLATE_PATH/trainers/parallel/builder.py": 1679382846.5999727,
-        "TEMPLATE_PATH/trainers/parallel/utils.py": 1679382846.5999727,
-        "TEMPLATE_PATH/trainers/trainer.py": 1679382846.5999727,
-        "TEMPLATE_PATH/trainers/training_args.py": 1679382846.5999727,
-        "TEMPLATE_PATH/trainers/utils/inference.py": 1679382846.5999727,
-        "TEMPLATE_PATH/trainers/utils/log_buffer.py": 1679382846.5999727
+        "TEMPLATE_PATH/exporters/base.py": 1681699605.604635,
+        "TEMPLATE_PATH/exporters/builder.py": 1681699605.604635,
+        "TEMPLATE_PATH/exporters/cv/cartoon_translation_exporter.py": 1681699605.604635,
+        "TEMPLATE_PATH/exporters/cv/face_detection_scrfd_exporter.py": 1681699605.604635,
+        "TEMPLATE_PATH/exporters/cv/object_detection_damoyolo_exporter.py": 1681699605.604635,
+        "TEMPLATE_PATH/exporters/nlp/csanmt_for_translation_exporter.py": 1681699605.604635,
+        "TEMPLATE_PATH/exporters/nlp/model_for_token_classification_exporter.py": 1681699605.604635,
+        "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py": 1681699605.604635,
+        "TEMPLATE_PATH/exporters/nlp/sbert_for_zero_shot_classification_exporter.py": 1681699605.604635,
+        "TEMPLATE_PATH/exporters/tf_model_exporter.py": 1681699605.604635,
+        "TEMPLATE_PATH/exporters/torch_model_exporter.py": 1681699605.604635,
+        "TEMPLATE_PATH/metrics/accuracy_metric.py": 1681699605.604635,
+        "TEMPLATE_PATH/metrics/action_detection_evaluator.py": 1681699605.604635,
+        "TEMPLATE_PATH/metrics/audio_noise_metric.py": 1681699605.604635,
+        "TEMPLATE_PATH/metrics/base.py": 1681699605.604635,
+        "TEMPLATE_PATH/metrics/bleu_metric.py": 1681699605.604635,
+        "TEMPLATE_PATH/metrics/builder.py": 1681699605.604635,
+        "TEMPLATE_PATH/metrics/ciderD/ciderD.py": 1681699605.604635,
+        "TEMPLATE_PATH/metrics/ciderD/ciderD_scorer.py": 1681699605.604635,
+        "TEMPLATE_PATH/metrics/image_color_enhance_metric.py": 1681699605.604635,
+        "TEMPLATE_PATH/metrics/image_colorization_metric.py": 1681699605.604635,
+        "TEMPLATE_PATH/metrics/image_denoise_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/image_inpainting_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/image_instance_segmentation_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/image_portrait_enhancement_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/image_quality_assessment_degradation_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/image_quality_assessment_mos_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/inbatch_recall_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/loss_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/map_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/movie_scene_segmentation_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/ned_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/ocr_recognition_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/ppl_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/prediction_saving_wrapper.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/referring_video_object_segmentation_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/sequence_classification_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/text_generation_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/text_ranking_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/token_classification_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/video_frame_interpolation_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/video_stabilization_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/video_summarization_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/video_super_resolution_metric/matlab_functions.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/video_super_resolution_metric/metric_util.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/video_super_resolution_metric/niqe.py": 1681699605.608635,
+        "TEMPLATE_PATH/metrics/video_super_resolution_metric/video_super_resolution_metric.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/aec/layers/activations.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/aec/layers/affine_transform.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/aec/layers/deep_fsmn.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/aec/layers/layer_base.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/aec/layers/uni_deep_fsmn.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/aec/network/loss.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/aec/network/modulation_loss.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/aec/network/se_net.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/ans/complex_nn.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/ans/conv_stft.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/ans/denoise_net.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/ans/frcrn.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/ans/layers/activations.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/ans/layers/affine_transform.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/ans/layers/layer_base.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/ans/layers/uni_deep_fsmn.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/ans/se_module_complex.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/ans/unet.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/asr/generic_automatic_speech_recognition.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/asr/wenet_automatic_speech_recognition.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/itn/generic_inverse_text_processing.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/kws/farfield/fsmn.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/kws/farfield/fsmn_sele_v2.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/kws/farfield/model.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/kws/farfield/model_def.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/kws/generic_key_word_spotting.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/kws/nearfield/cmvn.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/kws/nearfield/fsmn.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/kws/nearfield/model.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/punc/generic_punctuation.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/separation/layer_norm.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/separation/mossformer.py": 1681699605.608635,
+        "TEMPLATE_PATH/models/audio/separation/mossformer_block.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/audio/separation/mossformer_conv_module.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/audio/sv/DTDNN.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/audio/sv/DTDNN_layers.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/audio/sv/ecapa_tdnn.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/audio/sv/generic_speaker_verification.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/audio/tts/sambert_hifi.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/audio/tts/voice.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/base/base_head.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/base/base_model.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/base/base_torch_head.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/base/base_torch_model.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/builder.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_model.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/action_detection/action_detection_onnx.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/action_detection/modules/action_detection_pytorch.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/action_detection/modules/resnet.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/action_recognition/models.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/action_recognition/s3dg.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/action_recognition/tada_convnext.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/action_recognition/temporal_patch_shift_transformer.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/animal_recognition/resnet.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/animal_recognition/splat.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/bad_image_detecting/bad_image_detecting.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/body_2d_keypoints/hrnet_basic_modules.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/body_2d_keypoints/hrnet_v2.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/body_2d_keypoints/w48.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/cannonical_pose/body_3d_pose.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/cannonical_pose/canonical_pose_modules.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/backbone.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/block.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/directed_graph.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/hdformer.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/hdformer_detector.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/skeleton.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/cartoon/facelib/LK/lk.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/cartoon/facelib/config.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/cartoon/facelib/face_detector.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/cartoon/facelib/face_landmark.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/cartoon/facelib/facer.py": 1681699605.6126351,
+        "TEMPLATE_PATH/models/cv/cartoon/loss.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/cartoon/model_tf.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/cartoon/mtcnn_pytorch/src/align_trans.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/cartoon/mtcnn_pytorch/src/matlab_cp2tform.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/cartoon/network.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/cartoon/utils.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/cmdssl_video_embedding/c3d.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/cmdssl_video_embedding/resnet2p1d.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/cmdssl_video_embedding/resnet3d.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/annotator.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/api.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/base_model.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/blocks.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/dpt_depth.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/midas_net.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/midas_net_custom.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/transforms.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/vit.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/utils.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/mlsd/mbv2_mlsd_large.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/mlsd/utils.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/body.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/hand.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/model.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/util.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/controlnet.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/crowd_counting/cc_model.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/crowd_counting/hrnet_aspp_relu.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/easycv_base.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/face_2d_keypoints/face_2d_keypoints_align.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/detectors.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/mogface.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/mogprednet.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/resnet.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/utils.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/box_utils.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/detector.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/first_stage.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/get_nets.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/LK/lk.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/face_detector.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/face_landmark.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/facer.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/face_detection/retinaface/detection.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/face_detection/retinaface/models/net.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/face_detection/retinaface/models/retinaface.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/face_detection/retinaface/utils.py": 1681699605.616635,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/damofd_detect.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/transforms.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/bbox_nms.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/retinaface.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/master_net.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/base.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/single_stage.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/tinymog.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/preprocessor.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/scrfd_detect.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/tinymog_detect.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/detection.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/box_utils.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/mb_tiny.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/data_preprocessing.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/fd_config.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/mb_tiny_fd.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/predictor.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/ssd.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/transforms.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_emotion/efficient/model.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_emotion/efficient/utils.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_emotion/emotion_infer.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_emotion/emotion_model.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_emotion/face_alignment/face.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_emotion/face_alignment/face_align.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_generation/op/conv2d_gradfix.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_generation/op/fused_act.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_generation/op/upfirdn2d.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_generation/stylegan2.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_human_hand_detection/det_infer.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_human_hand_detection/ghost_pan.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_human_hand_detection/nanodet_plus_head.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_human_hand_detection/one_stage_detector.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_human_hand_detection/shufflenetv2.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_human_hand_detection/utils.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_recognition/align_face.py": 1681699605.6206353,
+        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/arcface_backbone.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/common.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/facemask_backbone.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/model_irse.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/model_resnet.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/rts_backbone.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/bfm.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/de_retouching_module.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facelandmark/large_base_lmks_infer.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facelandmark/nets/large_base_lmks_net.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facelandmark/nets/large_eyeball_net.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facerecon_model.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/losses.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/networks.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/nv_diffrast.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/opt.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/pix2pix/networks.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/pix2pix/pix2pix_model.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/pix2pix/pix2pix_options.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/renderer.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/unet.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/utils.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/facial_expression_recognition/fer/transforms.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/facial_expression_recognition/fer/vgg.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/facial_landmark_confidence/flc/manual_landmark_net.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/hand_2d_keypoints/hand_2d_keypoints.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/hand_static/hand_model.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/hand_static/networks.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/Reconstruction.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/Embedding.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/PixToMesh.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/Res_backbone.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/Surface_head.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/detectors.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/geometry.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/human_segmenter.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/networks.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/utils.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/human_wholebody_keypoint/human_wholebody_keypoint.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/image_binary_quant_classification/binary_quant_model.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/image_binary_quant_classification/bnext.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/image_body_reshaping/image_body_reshaping.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/image_body_reshaping/model.py": 1681699605.6246352,
+        "TEMPLATE_PATH/models/cv/image_body_reshaping/person_info.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_body_reshaping/pose_estimator/body.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_body_reshaping/pose_estimator/model.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_body_reshaping/pose_estimator/util.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_body_reshaping/slim_utils.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_classification/backbones/beit_v2.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_classification/backbones/nextvit.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_classification/mmcls_model.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_classification/resnet50_cc.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_classification/utils.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_color_enhance/adaint/adaint.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_color_enhance/csrnet.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_color_enhance/deeplpf/deeplpf_image_color_enhance.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_color_enhance/deeplpf/deeplpfnet.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_color_enhance/image_color_enhance.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/ddcolor.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/ddcolor_for_image_colorization.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/loss.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/convnext.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/position_encoding.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/transformer_utils.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/unet.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/vgg.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_colorization/unet/unet.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_colorization/unet/utils.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_debanding/rrdb/rrdb_image_debanding.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_deblur/nafnet_for_image_deblur.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/defrcn_for_fewshot.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/evaluation/coco_evaluation.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/evaluation/evaluator.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/evaluation/pascal_voc_evaluation.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/calibration_layer.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/defrcn.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/fast_rcnn.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/gdl.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/resnet.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/roi_heads.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/coco_register.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/configuration_mapper.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/model_surgery_op.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/register_data.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/requirements_check.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/voc_register.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_denoise/nafnet/NAFNet_arch.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_denoise/nafnet/arch_util.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_denoise/nafnet_for_image_denoise.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/newcrf_depth.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/newcrf_layers.py": 1681699605.6286352,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/newcrf_utils.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/swin_transformer.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/uper_crf_head.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation/newcrfs_model.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/depth_estimation_bts_model.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/bts_model.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/decoder.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/encoder.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/utils.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_driving_perception/image_driving_percetion_model.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_driving_perception/preprocessor.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_driving_perception/utils.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/gan_wrap.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/model.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/op/conv2d_gradfix.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/op/fused_act.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/op/upfirdn2d.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/facelib/align_trans.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/facelib/matlab_cp2tform.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/image_face_fusion.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/network/aad_layer.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/network/aei_flow_net.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/network/bfm.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/network/dense_motion.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/network/facerecon_model.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/network/model_irse.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/network/ops.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_human_parsing/backbone/deeplab_resnet.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_human_parsing/m2fp/m2fp_decoder.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_human_parsing/m2fp/m2fp_encoder.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_human_parsing/m2fp_net.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_human_parsing/parsing_utils.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_inpainting/base.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_inpainting/default.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_inpainting/model.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/ade20k/base.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/ade20k/resnet.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/adversarial.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/feature_matching.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/ffc.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/inception.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/perceptual.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/pix2pixhd.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_inpainting/refinement.py": 1681699605.6326354,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/backbones/swin_transformer.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/cascade_mask_rcnn_swin.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/datasets/transforms.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/dino_decoder.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/maskdino_decoder.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/maskdino_encoder.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/ms_deform_attn.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/position_encoding.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/utils.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino_model.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino_swin.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/model.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/postprocess_utils.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_matching/config/default.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/backbone/resnet_fpn.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/fine_preprocess.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/linear_attention.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/quadtree_attention.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/transformer.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/utils/coarse_matching.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/utils/fine_matching.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/utils/position_encoding.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_matching/quadtree_attention_model.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_matching/utils/misc.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/cas_mvsnet.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/casmvs_model.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/colmap2mvsnet.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/depth_filter.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/general_eval_dataset.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/module.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/utils.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_paintbyexample/model.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_panoptic_segmentation/panseg_model.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_panoptic_segmentation/r50_panseg_model.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/align_faces.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/eqface/fqa.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/eqface/model_resnet.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/gpen.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/image_portrait_enhancement.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/losses/helpers.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/losses/losses.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/losses/model_irse.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/detection.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/models/net.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/models/retinaface.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/utils.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_probing_model/backbone.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_probing_model/model.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_probing_model/utils.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_degradation/degradation_model.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_man/image_quality_assessment_man.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_man/maniqa.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_man/swin.py": 1681699605.6366353,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/backbones/resnet.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/censeo_ivqa_model.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/heads/simple_head.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/image_quality_assessment_mos.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_reid_person/pass_model.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_reid_person/transreid_model.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_restoration/demoire_models/nets.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_restoration/image_restoration_model.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/data_util.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/feature_extractors.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/pixel_classifier.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/utils.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/pan_merge/base_panoptic_fusion_head.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/pan_merge/maskformer_semantic_head.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/segformer.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/semantic_seg_model.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/adapter_modules.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/beit.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/beit_adapter.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/base_decode_head.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/mask2former_head_from_mmseg.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/base_segmentor.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/encoder_decoder_mask2former.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/utils/builder.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/utils/data_process_func.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/utils/seg_func.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_skychange/preprocessor.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/BlockModules.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/hrnet_backnone.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/hrnet_super_and_ocr.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/unet.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_skychange/skychange.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_skychange/skychange_model.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_to_image_generation/data/transforms.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_to_image_generation/model.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_to_image_generation/models/autoencoder.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_to_image_generation/models/clip.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_to_image_generation/ops/diffusion.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_to_image_generation/ops/losses.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/data/transforms.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/model_translation.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/models/autoencoder.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/models/clip.py": 1681699605.6406353,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/apps.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/degradation.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/diffusion.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/losses.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/metrics.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/random_color.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/random_mask.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/svd.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/utils.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/backbone/resnet_DA.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/backbone/vit_horizon_pry_image.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/misc/fourier.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/misc/panostretch.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/misc/post_proc.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/modality/layout.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/panovit.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/utils.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/panovit.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/summarizer.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/layers.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/models.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/modules.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/sub_layers.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/motion_generation/model.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/motion_generation/modules/cfg_sampler.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/motion_generation/modules/gaussian_diffusion.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/motion_generation/modules/mdm.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/motion_generation/modules/respace.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/motion_generation/modules/rotation2xyz.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/motion_generation/modules/smpl.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/get_model.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/model.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/head.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/save_op.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/shot_encoder.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/trn.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/nerf_recon_acc/dataloader/nerf_dataset.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/nerf_recon_acc/dataloader/read_write_model.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/nerf_recon_acc/nerf_preprocess.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/nerf_recon_acc/nerf_recon_acc.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/nerf_recon_acc/network/nerf.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/nerf_recon_acc/network/segmenter.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/nerf_recon_acc/network/utils.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/object_detection/dino.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_model.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/backbones/vit.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/dense_heads/anchor_head.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/necks/fpn.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py": 1681699605.6446354,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/fcn_mask_head.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/utils/checkpoint.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/utils/convModule_norm.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/object_detection/yolox_pai.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/depe_detect.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/match_cost.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/util.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/nuscenes_dataset.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/loading.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/vovnet.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/depth_net.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/petrv2_dednhead.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/petr3d.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/cp_fpn.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/positional_encoding.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/result_vis.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/ocr_detection/model.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/ocr_detection/modules/dbnet.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/ocr_detection/modules/seg_detector_loss.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/ocr_detection/preprocessor.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/ocr_detection/utils.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/model.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/convnext.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/convnextvit.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/crnn.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/timm_tinyc.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/vitstr.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/preprocessor.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/open_vocabulary_detection_vild/vild.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/equi.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/layers.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/mobilenet.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/resnet.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/unifuse.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/util.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/unifuse_model.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/pedestrian_attribute_recognition/model.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/common.py": 1681699605.6486354,
+        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/pointnet2_utils.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/rcp_model.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/sf_rcp.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/product_retrieval_embedding/item_detection.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/product_retrieval_embedding/item_embedding.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/product_retrieval_embedding/item_model.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/product_segmentation/net.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/product_segmentation/seg_infer.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/model.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/backbone.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/criterion.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/matcher.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/misc.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/mttr.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/multimodal_transformer.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/position_encoding_2d.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/postprocessing.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/segmentation.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/swin_transformer.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/robust_image_classification/easyrobust_model.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/salient_detection/models/backbone/Res2Net_v1b.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/salient_detection/models/modules.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/salient_detection/models/senet.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/salient_detection/models/u2net.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/salient_detection/models/utils.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/salient_detection/salient_model.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/shop_segmentation/common.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/shop_segmentation/head_fpn.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/shop_segmentation/models.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/shop_segmentation/neck_fpn.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/shop_segmentation/shop_seg_base.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/shop_segmentation/shop_seg_model.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/shop_segmentation/utils.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/skin_retouching/detection_model/detection_module.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/skin_retouching/detection_model/detection_unet_in.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/skin_retouching/inpainting_model/gconv.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/skin_retouching/inpainting_model/inpainting_unet.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/box_utils.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/net.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/network.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/predict_single.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/prior_box.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/utils.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/skin_retouching/unet_deploy.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/skin_retouching/utils.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/skin_retouching/weights_init.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/stream_yolo/data/data_augment.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/stream_yolo/exp/base_exp.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/stream_yolo/exp/build.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/stream_yolo/exp/default/streamyolo.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/stream_yolo/exp/yolox_base.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/stream_yolo/models/darknet.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/stream_yolo/models/dfp_pafpn.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/stream_yolo/models/network_blocks.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/stream_yolo/models/streamyolo.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/stream_yolo/models/tal_head.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/stream_yolo/realtime_video_detector.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/stream_yolo/utils/boxes.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/stream_yolo/utils/format.py": 1681699605.6526353,
+        "TEMPLATE_PATH/models/cv/super_resolution/arch_util.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/super_resolution/ecb.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/super_resolution/ecbsr_model.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/super_resolution/rrdbnet_arch.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/table_recognition/lineless_table_process.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/table_recognition/model_lore.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/table_recognition/modules/lore_detector.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/table_recognition/modules/lore_processor.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/clip.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_base.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_blocks.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_model.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_net.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_vit.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/model.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/simple_tokenizer.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/basic_blocks.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/global_utils.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/master_net.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/model_zoo.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/plain_net_utils.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_blocks.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_res_idwexkx.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_res_k1kxk1.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_res_kxkx.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/apis/detector_evaluater.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/apis/detector_inference.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/box_level_augs.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/color_augs.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/gaussian_maps.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/geometric_augs.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/scale_aware_aug.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/backbones/darknet.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_csp.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_res.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/base_ops.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/neck_ops.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/ops.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/ota_assigner.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/repvgg_block.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/utils.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/weight_init.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/heads/gfocal_v2_tiny.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/heads/zero_head.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/losses/distill_loss.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/losses/gfocal_loss.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/necks/giraffe_config.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn_btn.py": 1681699605.6566355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/detectors/detector.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/structures/bounding_box.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/structures/boxlist_ops.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/structures/image_list.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/utils/boxes.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/utils/model_utils.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/utils/scheduler.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/detector.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/tinynas_damoyolo.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/tinynas_detector.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/utils.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_deinterlace/UNet_for_video_deinterlace.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_deinterlace/deinterlace_arch.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_deinterlace/models/archs.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_deinterlace/models/deep_fourier_upsampling.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_deinterlace/models/enh.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_deinterlace/models/fre.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_deinterlace/models/utils.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/configs/default_config.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/dro_model.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/camera.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/camera_utils.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/pose.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/pose_utils.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/model_checkpoint.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/model_utils.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/model_wrapper.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/sfm_model_mf.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/sup_model_mf.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/depth_pose/depth_pose_net.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/depth_decoder.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/layers.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/pose_decoder.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/resnet_encoder.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/optim/extractor.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/optim/update.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/augmentations.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/config.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/depth.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/horovod.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/image.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/image_gt.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/load.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/misc.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/types.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/VFINet_arch.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/VFINet_for_video_frame_interpolation.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/corr.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/extractor.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/raft.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/update.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/IFNet_swin.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/UNet.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/flow_reversal.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/refinenet_arch.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/transformer_layers.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/utils/scene_change_detection.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/utils/utils.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_human_matting/model.py": 1681699605.6606355,
+        "TEMPLATE_PATH/models/cv/video_human_matting/models/decoder.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_human_matting/models/deep_guided_filter.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_human_matting/models/effv2.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_human_matting/models/lraspp.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_human_matting/models/matting.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_inpainting/inpainting.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_inpainting/inpainting_model.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_frame_iter_head.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_head.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_iter_head.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_update_head.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_updator.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/neck/msdeformattn_decoder.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/track/kernel_update_head.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/utils.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/video_knet.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/common.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/decode.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/model.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/yolo.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/tracker/basetrack.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/tracker/matching.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/tracker/multitracker.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/image.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/kalman_filter.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/utils.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/visualization.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/aggregate.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/cbam.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/eval_network.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/inference_core.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/inference_memory_bank.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/mod_resnet.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/model.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/modules.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/network.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/backbone/swin_checkpoint.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/backbone/swin_transformer.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_head.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_iter_head.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_update_head.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_updator.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/mask.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/semantic_fpn_wrapper.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/track_heads.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/neck/fpn.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/track/quasi_dense_embed_tracker.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/video_k_net.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/visualizer.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/config/ostrack.py": 1681699605.6646357,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/attn.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/attn_blocks.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/head.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/patch_embed.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/base_backbone.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/ostrack.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/utils.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/vit_ce.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/procontext/procontext.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/procontext/utils.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/procontext/vit_ce.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/tracker/ostrack.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/tracker/procontext.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/utils/utils.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/DUT_raft.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/MotionPro.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/corr.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/extractor.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/raft.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/update.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/Smoother.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/config.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/rf_det_module.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/rf_det_so.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUTRAFTStabilizer.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_stabilization/utils/IterativeSmooth.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_stabilization/utils/MedianFilter.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_stabilization/utils/ProjectionUtils.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_stabilization/utils/RAFTUtils.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_stabilization/utils/WarpUtils.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_stabilization/utils/image_utils.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_stabilization/utils/math_utils.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/exp/longshortnet_base.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/longshortnet.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_long.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_short.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/longshort.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/longshort_backbone_neck.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_summarization/base_model.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_summarization/kts/cpd_auto.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_summarization/kts/cpd_nonlin.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_summarization/pgl_sum.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_summarization/summarizer.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_super_resolution/basicvsr_net.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_super_resolution/common.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_super_resolution/msrresnet_lite_model.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_super_resolution/real_basicvsr_for_video_super_resolution.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/video_super_resolution/real_basicvsr_net.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/vidt/backbone.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/vidt/deformable_transformer.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/vidt/fpn_fusion.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/vidt/head.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/vidt/model.py": 1681699605.6686356,
+        "TEMPLATE_PATH/models/cv/virual_tryon/sdafnet.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/backbone.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/head.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/model.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/petl.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/timm_helpers.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/timm_vision_transformer.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/timm_weight_init.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/vision_efficient_tuning.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/cv/vision_middleware/backbone.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/cv/vision_middleware/head.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/cv/vision_middleware/model.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/cv/vision_middleware/vim.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/cv/vop_retrieval/backbone.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/cv/vop_retrieval/basic_utils.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/cv/vop_retrieval/model.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/cv/vop_retrieval/model_se.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/cv/vop_retrieval/tokenization_clip.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/multi_modal/clip/bert_tokenizer.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/multi_modal/clip/configuration_bert.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/multi_modal/clip/model.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/multi_modal/clip/modeling_bert.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/multi_modal/diffusion/diffusion.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/multi_modal/diffusion/model.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/multi_modal/diffusion/structbert.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/multi_modal/diffusion/tokenizer.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/multi_modal/diffusion/unet_generator.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/multi_modal/diffusion/unet_upsampler_1024.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/multi_modal/diffusion/unet_upsampler_256.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/multi_modal/dpm_solver_pytorch.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/multi_modal/efficient_diffusion_tuning/efficient_stable_diffusion.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/multi_modal/gemm/gemm_base.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/multi_modal/gemm/gemm_model.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/multi_modal/gemm/tokenizer.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/gaussian_diffusion.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/respace.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/script.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/unet.py": 1681699605.6726356,
+        "TEMPLATE_PATH/models/multi_modal/mgeo/backbone.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/mgeo/text_classification.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/mgeo/text_ranking.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/mgeo/token_classification.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/mmr/dataloaders/rawvideo_util.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/mmr/models/clip_for_mm_video_embedding.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/mmr/models/dynamic_inverted_softmax.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/mmr/models/modeling.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/mmr/models/module_clip.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/mmr/models/module_cross.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/mmr/models/tokenization_clip.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/mmr/models/until_module.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/mplug/clip/clip.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/mplug/configuration_mplug.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/mplug/modeling_mplug.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/mplug/mvit.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/mplug/predictor.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/mplug_for_all_tasks.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/clip.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/decoder.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/gaussian_diffusion.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/model.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/prior.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/tokenizer.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/upsampler.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/xglm.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/ofa/configuration_mmspeech.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/ofa/configuration_ofa.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/ofa/generate/incremental_decoding_utils.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/ofa/generate/multihead_attention.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/ofa/generate/ngram_repeat_block.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/ofa/generate/search.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/ofa/generate/sequence_generator.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/ofa/generate/token_generation_constraints.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/ofa/generate/utils.py": 1681699605.6766357,
+        "TEMPLATE_PATH/models/multi_modal/ofa/modeling_mmspeech.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/ofa/modeling_ofa.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/ofa/resnet.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/ofa/tokenization_ofa.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/ofa/tokenization_ofa_fast.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/ofa/utils/constant.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/ofa/utils/utils.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/ofa/vit.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/ofa_for_text_to_image_synthesis_model.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/rleg/model.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/rleg/rleg.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/soonet/blocks.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/soonet/clip.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/soonet/model.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/soonet/swin_transformer.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/soonet/tokenizer.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/soonet/utils.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/team/team_model.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/team/utils.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/video_synthesis/autoencoder.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/video_synthesis/diffusion.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/video_synthesis/unet_sd.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/vldoc/conv_fpn_trans.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/vldoc/convnext.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/vldoc/model.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/vldoc/modeling_layout_roberta.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/vldoc/processing.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/vldoc/tokenization.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/multi_modal/vldoc/transformer_local.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/nlp/T5/backbone.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/nlp/T5/configuration.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/nlp/T5/text2text_generation.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/nlp/bart/text_error_correction.py": 1681699605.6806357,
+        "TEMPLATE_PATH/models/nlp/bert/backbone.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/bert/configuration.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/bert/document_segmentation.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/bert/fill_mask.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/bert/sentence_embedding.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/bert/siamese_uie.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/bert/text_classification.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/bert/text_ranking.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/bert/token_classification.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/bert/word_alignment.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/bloom/backbone.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/canmt/canmt_model.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/canmt/canmt_translation.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/canmt/sequence_generator.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/codegeex/codegeex.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/codegeex/codegeex_for_code_generation.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/codegeex/codegeex_for_code_translation.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/codegeex/inference.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/codegeex/tokenizer.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/csanmt/translation.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/deberta_v2/backbone.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/deberta_v2/configuration.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/deberta_v2/fill_mask.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/deberta_v2/tokenization.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/deberta_v2/tokenization_fast.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/dgds/backbone.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_generate.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_rerank.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_retrieval.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/fid_T5/text_generation.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/fid_plug/backbone.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/fid_plug/configuration.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/fid_plug/text_generation.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/glm_130b/generation/strategies.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/glm_130b/initialize.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/glm_130b/quantization/functional.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/glm_130b/quantization/layers.py": 1681699605.6846356,
+        "TEMPLATE_PATH/models/nlp/glm_130b/text_generation.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/gpt2/backbone.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/gpt3/backbone.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/gpt3/configuration.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/gpt3/distributed_gpt3.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/gpt3/text_generation.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/gpt3/tokenizer.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/backbone.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/checkpointing.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/configuration.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/distributed_gpt_moe.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/experts.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/layer.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/mappings.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/sharded_moe.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/utils.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/text_generation.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/tokenizer.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/gpt_neo/backbone.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/heads/crf_head.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/heads/fill_mask_head.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/heads/infromation_extraction_head.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/heads/text_classification_head.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/heads/text_generation_head.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/heads/text_ranking_head.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/heads/token_classification_head.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/heads/torch_pretrain_head.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/hf_transformers/backbone.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/llama/backbone.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/llama/configuration.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/llama/convert_llama_weights_to_hf.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/llama/text_generation.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/llama/tokenization.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/llama/tokenization_fast.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/lstm/backbone.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/lstm/token_classification.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/megatron_bert/backbone.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/megatron_bert/configuration.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/megatron_bert/fill_mask.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/mglm/arguments.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/mglm/blocklm_utils.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/mglm/configure_data.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/corpora.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/datasets.py": 1681699605.6886358,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/extraction.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/file_utils.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/lazy_loader.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/samplers.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/sp_tokenizer.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/tokenization.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/tokenization_gpt2.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/wordpiece.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/generation_utils.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/mglm_for_text_summarization.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/model/distributed.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/model/downstream.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/model/modeling_bert.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/model/modeling_glm.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/model/prompt.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/model/transformer.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/process_grid.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/run_test.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/data_utils.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/eval_utils.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/language_model/dataset.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/language_model/detokenizer.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/language_model/finetune.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/seq2seq/dataset.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/seq2seq/evaluate.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/seq2seq/finetune.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/dataset.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/evaluate.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/finetune.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/pvp.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/test/test_block.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/test/test_rel_shift.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/train_utils.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/mglm/utils.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/palm_v2/configuration.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/palm_v2/dureader_eval.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/palm_v2/text_generation.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/peer/backbone.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/peer/configuration.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/peer/sas_utils.py": 1681699605.6926358,
+        "TEMPLATE_PATH/models/nlp/peer/text_classification.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/plug/AnnealingLR.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/plug/backbone.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/plug/configuration.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/plug/distributed_plug.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/plug/generator.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/plug_mental/adv_utils.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/plug_mental/backbone.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/plug_mental/configuration.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/plug_mental/text_classification.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/ponet/backbone.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/ponet/configuration.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/ponet/document_segmentation.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/ponet/fill_mask.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/ponet/tokenization.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/space/configuration.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/space/dialog_intent_prediction.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/space/dialog_modeling.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/space/dialog_state_tracking.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/space/model/gen_unified_transformer.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/space/model/generator.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/space/model/intent_unified_transformer.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/space/model/model_base.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/space/model/tokenization_space.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/space/model/unified_transformer.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/space/modules/embedder.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/space/modules/feedforward.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/space/modules/functions.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/space/modules/multihead_attention.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/space/modules/transformer_block.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/space_T_cn/backbone.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/space_T_cn/configuration.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/space_T_cn/table_question_answering.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/space_T_en/text_to_sql.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/structbert/adv_utils.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/structbert/backbone.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/structbert/configuration.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/structbert/faq_question_answering.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/structbert/fill_mask.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/structbert/text_classification.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/structbert/token_classification.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/task_models/feature_extraction.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/task_models/fill_mask.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/task_models/information_extraction.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/task_models/task_model.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/task_models/text_classification.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/task_models/text_generation.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/task_models/text_ranking.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/task_models/token_classification.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/unite/configuration_unite.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/unite/modeling_unite.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/use/transformer.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/use/user_satisfaction_estimation.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/veco/backbone.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/veco/configuration.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/veco/fill_mask.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/veco/text_classification.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/veco/token_classification.py": 1681699605.6966357,
+        "TEMPLATE_PATH/models/nlp/xlm_roberta/backbone.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/nlp/xlm_roberta/configuration.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/config.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/data/data_ops.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/data/msa_pairing.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/data/process.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/data/process_multimer.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/data/protein.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/data/residue_constants.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/data/utils.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/dataset.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/model.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/modules/alphafold.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/modules/attentions.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/modules/auxillary_heads.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/modules/common.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/modules/confidence.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/modules/embedders.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/modules/evoformer.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/modules/featurization.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/modules/frame.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/modules/structure_module.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/modules/template.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/modules/triangle_multiplication.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/msa/mmcif.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/msa/msa_identifiers.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/msa/parsers.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/msa/pipeline.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/msa/templates.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/msa/tools/hhblits.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/msa/tools/hhsearch.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/msa/tools/hmmbuild.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/msa/tools/hmmsearch.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/msa/tools/jackhmmer.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/msa/tools/kalign.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/msa/tools/utils.py": 1681699605.700636,
+        "TEMPLATE_PATH/models/science/unifold/msa/utils.py": 1681699605.700636,
+        "TEMPLATE_PATH/msdatasets/audio/asr_dataset.py": 1681699605.700636,
+        "TEMPLATE_PATH/msdatasets/auth/auth_config.py": 1681699605.700636,
+        "TEMPLATE_PATH/msdatasets/context/dataset_context_config.py": 1681699605.700636,
+        "TEMPLATE_PATH/msdatasets/data_files/data_files_manager.py": 1681699605.700636,
+        "TEMPLATE_PATH/msdatasets/data_loader/data_loader.py": 1681699605.700636,
+        "TEMPLATE_PATH/msdatasets/data_loader/data_loader_manager.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/asr_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/kws_farfield_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_processor.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/bad_image_detecting_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/builder.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/build.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/collate_batch.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/coco.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/mosaic_wrapper.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/coco_eval.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/distributed.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/grouped_batch_sampler.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/iteration_based_batch_sampler.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/build.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/transforms.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/easycv_base.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/face_2d_keypoins/face_2d_keypoints_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/gopro_image_deblurring_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/hand_2d_keypoints/hand_2d_keypoints_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/human_wholebody_keypoint/human_wholebody_keypoint_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_classification/classification_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_colorization/image_colorization_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_inpainting/aug.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_instance_segmentation_coco_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/data_utils.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/image_portrait_enhancement_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/image_quality_assessment_degradation_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/image_quality_assessment_mos_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_semantic_segmentation/segmentation_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/language_guided_video_summarization_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/mgeo_ranking_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/sampler.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/object_detection/detection_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/augmenter.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/data_loader.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/image_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/iou_evaluator.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/quad_measurer.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/augment_data.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/data_process.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_border_map.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_icdar_data.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_seg_detection_data.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/normalize_image.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/random_crop_data.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_recognition_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/reds_image_deblurring_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/referring_video_object_segmentation_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/transformers.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/data_utils.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/sidd_image_denoising_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/transforms.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/torch_custom_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/veco_dataset.py": 1681699605.7046359,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/data_utils.py": 1681699605.7086358,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/video_frame_interpolation_dataset.py": 1681699605.7086358,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_stabilization/video_stabilization_dataset.py": 1681699605.7086358,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_summarization_dataset.py": 1681699605.7086358,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_super_resolution/video_super_resolution_dataset.py": 1681699605.7086358,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/dataset.py": 1681699605.7086358,
+        "TEMPLATE_PATH/msdatasets/download/dataset_builder.py": 1681699605.7086358,
+        "TEMPLATE_PATH/msdatasets/download/download_config.py": 1681699605.7086358,
+        "TEMPLATE_PATH/msdatasets/download/download_manager.py": 1681699605.7086358,
+        "TEMPLATE_PATH/msdatasets/meta/data_meta_config.py": 1681699605.7086358,
+        "TEMPLATE_PATH/msdatasets/meta/data_meta_manager.py": 1681699605.7086358,
+        "TEMPLATE_PATH/msdatasets/ms_dataset.py": 1681699605.7086358,
+        "TEMPLATE_PATH/msdatasets/task_datasets/gopro_image_deblurring_dataset.py": 1681699605.7086358,
+        "TEMPLATE_PATH/msdatasets/task_datasets/reds_image_deblurring_dataset.py": 1681699605.7086358,
+        "TEMPLATE_PATH/msdatasets/task_datasets/sidd_image_denoising.py": 1681699605.7086358,
+        "TEMPLATE_PATH/msdatasets/task_datasets/torch_base_dataset.py": 1681699605.7086358,
+        "TEMPLATE_PATH/msdatasets/task_datasets/video_summarization_dataset.py": 1681699605.7086358,
+        "TEMPLATE_PATH/msdatasets/utils/dataset_utils.py": 1681699605.7086358,
+        "TEMPLATE_PATH/msdatasets/utils/delete_utils.py": 1681699605.7086358,
+        "TEMPLATE_PATH/msdatasets/utils/oss_utils.py": 1681699605.7086358,
+        "TEMPLATE_PATH/msdatasets/utils/upload_utils.py": 1681699605.7086358,
+        "TEMPLATE_PATH/pipelines/audio/ans_dfsmn_pipeline.py": 1681699605.7086358,
+        "TEMPLATE_PATH/pipelines/audio/ans_pipeline.py": 1681699605.7086358,
+        "TEMPLATE_PATH/pipelines/audio/asr_inference_pipeline.py": 1681699605.7086358,
+        "TEMPLATE_PATH/pipelines/audio/asr_wenet_inference_pipeline.py": 1681699605.7086358,
+        "TEMPLATE_PATH/pipelines/audio/inverse_text_processing_pipeline.py": 1681699605.7086358,
+        "TEMPLATE_PATH/pipelines/audio/kws_farfield_pipeline.py": 1681699605.7086358,
+        "TEMPLATE_PATH/pipelines/audio/kws_kwsbp_pipeline.py": 1681699605.7086358,
+        "TEMPLATE_PATH/pipelines/audio/linear_aec_pipeline.py": 1681699605.7086358,
+        "TEMPLATE_PATH/pipelines/audio/lm_infer_pipeline.py": 1681699605.7086358,
+        "TEMPLATE_PATH/pipelines/audio/punctuation_processing_pipeline.py": 1681699605.7086358,
+        "TEMPLATE_PATH/pipelines/audio/separation_pipeline.py": 1681699605.7086358,
+        "TEMPLATE_PATH/pipelines/audio/speaker_diarization_pipeline.py": 1681699605.7086358,
+        "TEMPLATE_PATH/pipelines/audio/speaker_verification_light_pipeline.py": 1681699605.7086358,
+        "TEMPLATE_PATH/pipelines/audio/speaker_verification_pipeline.py": 1681699605.7086358,
+        "TEMPLATE_PATH/pipelines/audio/text_to_speech_pipeline.py": 1681699605.7086358,
+        "TEMPLATE_PATH/pipelines/audio/timestamp_pipeline.py": 1681699605.7086358,
+        "TEMPLATE_PATH/pipelines/audio/voice_activity_detection_pipeline.py": 1681699605.7086358,
+        "TEMPLATE_PATH/pipelines/base.py": 1681699605.7086358,
+        "TEMPLATE_PATH/pipelines/builder.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/action_detection_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/action_recognition_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/animal_recognition_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/arc_face_recognition_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/bad_image_detecting_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/body_2d_keypoints_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/body_3d_keypoints_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/card_detection_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/cmdssl_video_embedding_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/content_check_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/controllable_image_generation_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/crowd_counting_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/ddcolor_image_colorization_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/ddpm_semantic_segmentation_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/base.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/detection_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/face_2d_keypoints_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/human_wholebody_keypoint_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/segmentation_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/face_attribute_recognition_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/face_detection_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/face_emotion_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/face_human_hand_detection_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/face_image_generation_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/face_liveness_ir_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/face_liveness_xc_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/face_processing_base_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/face_quality_assessment_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/face_recognition_onnx_fm_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/face_recognition_onnx_ir_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/face_recognition_ood_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/face_recognition_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/face_reconstruction_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/facial_expression_recognition_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/facial_landmark_confidence_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/general_recognition_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/hand_2d_keypoints_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/hand_static_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/hicossl_video_embedding_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/human_reconstruction_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_body_reshaping_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_bts_depth_estimation_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_cartoon_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_color_enhance_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_colorization_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_debanding_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_deblur_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_defrcn_fewshot_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_denoise_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_depth_estimation_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_detection_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_driving_perception_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_face_fusion_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_human_parsing_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_inpainting_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_inpainting_sdv2_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_instance_segmentation_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_matching_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_matting_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_mvs_depth_estimation_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_open_vocabulary_detection_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_paintbyexample_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_panoptic_segmentation_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_portrait_enhancement_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_degradation_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_man_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_mos_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_reid_person_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_restoration_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_salient_detection_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_semantic_segmentation_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_skychange_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_structured_model_probing_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_style_transfer_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_super_resolution_pipeline.py": 1681699605.712636,
+        "TEMPLATE_PATH/pipelines/cv/image_to_image_generate_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/image_to_image_translation_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/indoor_layout_estimation_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/language_guided_video_summarization_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/license_plate_detection_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/lineless_table_recognition_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/live_category_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/mask_face_recognition_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/maskdino_instance_segmentation_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/mobile_image_super_resolution_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/mog_face_detection_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/motion_generation_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/movie_scene_segmentation_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/mtcnn_face_detection_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/nerf_recon_acc_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/object_detection_3d_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/ocr_detection_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/ocr_recognition_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_convnext_transformer.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_dla34.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_resnet18_half.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_resnet_mutex_v4_linewithchar.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_vlpt.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ocr_modules/convnext.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ocr_modules/timm_tinyc.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ocr_modules/vitstr.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ops.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/resnet18_v1.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/resnet_utils.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/table_process.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/utils.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/panorama_depth_estimation_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/pedestrian_attribute_recognition_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/product_retrieval_embedding_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/product_segmentation_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/realtime_video_object_detection_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/referring_video_object_segmentation_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/retina_face_detection_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/shop_segmentation_pipleline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/skin_retouching_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/table_recognition_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/tbs_detection_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/tbs_detection_utils/utils.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/text_driven_segmentation_pipleline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/tinynas_classification_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/tinynas_detection_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/ulfd_face_detection_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/video_category_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/video_colorization_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/video_deinterlace_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/video_depth_estimation_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/video_frame_interpolation_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/video_human_matting_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/video_inpainting_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/video_instance_segmentation_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/video_multi_object_tracking_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/video_object_segmentation_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/video_panoptic_segmentation_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/video_single_object_tracking_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/video_stabilization_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/video_summarization_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/video_super_resolution_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/vidt_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/virtual_try_on_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/vision_efficient_tuning_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/vision_middleware_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/vop_retrieval_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/cv/vop_retrieval_se_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/multi_modal/asr_pipeline.py": 1681699605.716636,
+        "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/diffusers_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/disco_guided_diffusion_pipeline/utils.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/document_vl_embedding_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/efficient_diffusion_tuning_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/gridvlp_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/image_captioning_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/image_text_retrieval_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/mgeo_ranking_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/multi_modal_embedding_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/ocr_recognition_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/sudoku_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/team_multi_modal_similarity_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/text2sql_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/text_to_image_synthesis_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/text_to_video_synthesis_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/video_captioning_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/video_multi_modal_embedding_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/video_question_answering_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/visual_entailment_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/visual_grounding_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/multi_modal/visual_question_answering_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/automatic_post_editing_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/canmt_translation_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/codegeex_code_generation_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/codegeex_code_translation_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/conversational_text_to_sql_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/dialog_intent_prediction_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/dialog_modeling_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/dialog_state_tracking_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/distributed_gpt3_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/distributed_gpt_moe_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/distributed_plug_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_generate_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_rerank_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_retrieval_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/document_segmentation_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/extractive_summarization_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/faq_question_answering_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/fasttext_text_classification_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/feature_extraction_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/fid_dialogue_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/fill_mask_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/glm130b_text_generation_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/information_extraction_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/interactive_translation_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/language_identification_pipline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/mglm_text_summarization_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/named_entity_recognition_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/sentence_embedding_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/siamese_uie_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/summarization_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/table_question_answering_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/text_error_correction_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/text_generation_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/text_ranking_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/token_classification_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/translation_evaluation_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/translation_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/translation_quality_estimation_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/user_satisfaction_estimation_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/word_alignment_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/word_segmentation_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/nlp/zero_shot_classification_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/science/protein_structure_pipeline.py": 1681699605.7206361,
+        "TEMPLATE_PATH/pipelines/util.py": 1681699605.7206361,
+        "TEMPLATE_PATH/preprocessors/asr.py": 1681699605.7206361,
+        "TEMPLATE_PATH/preprocessors/audio.py": 1681699605.7206361,
+        "TEMPLATE_PATH/preprocessors/base.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/builder.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/common.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/cv/action_detection_mapper.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/cv/bad_image_detecting_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/cv/controllable_image_generation.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/cv/cv2_transforms.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/cv/image_quality_assessment_man.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/cv/image_quality_assessment_mos.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/cv/image_restoration_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/cv/mmcls_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/cv/timer.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/cv/util.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/cv/video_stabilization.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/cv/video_super_resolution.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/image.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/kws.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/movie_scene_segmentation/transforms.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/multi_modal.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/bert_seq_cls_tokenizer.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/canmt_translation.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/dialog_classification_use_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/document_segmentation_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/faq_question_answering_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/feature_extraction_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/fill_mask_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/mgeo_ranking_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/mglm_summarization_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/relation_extraction_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/sentence_embedding_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/siamese_uie_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space/args.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space/batch.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space/data_loader.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space/dialog_intent_prediction_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space/dialog_modeling_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space/dialog_state_tracking_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space/dst_processors.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space/fields/gen_field.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space/fields/intent_field.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space/lazy_dataset.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space/preprocess.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space/sampler.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space/tensorlistdataset.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space/tokenizer.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/fields/database.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/fields/schema_link.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/fields/struct.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/table_question_answering_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/conversational_text_to_sql_preprocessor.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/common_utils.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/parse.py": 1681699605.724636,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/preprocess_dataset.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/process_dataset.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/nlp/text_classification_preprocessor.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/nlp/text_clean.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/nlp/text_error_correction.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/nlp/text_generation_preprocessor.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/nlp/text_ranking_preprocessor.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/nlp/token_classification_preprocessor.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/nlp/token_classification_thai_preprocessor.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/nlp/token_classification_viet_preprocessor.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/nlp/transformers_tokenizer.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/nlp/translation_evaluation_preprocessor.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/nlp/utils.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/nlp/word_alignment_preprocessor.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/nlp/zero_shot_classification_preprocessor.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/asr.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/base.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/image_captioning.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/image_classification.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/ocr_recognition.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/sudoku.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/summarization.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/text2sql.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/text_classification.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/text_to_image_synthesis.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/audio_helper.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/bridge_content_encoder.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/collate.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/constant.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/get_tables.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/random_help.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/text2phone.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/transforms.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/vision_helper.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/visual_entailment.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/visual_grounding.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/ofa/visual_question_answering.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/science/uni_fold.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/tts.py": 1681699605.728636,
+        "TEMPLATE_PATH/preprocessors/video.py": 1681699605.728636,
+        "TEMPLATE_PATH/trainers/audio/ans_trainer.py": 1681699605.728636,
+        "TEMPLATE_PATH/trainers/audio/asr_trainer.py": 1681699605.728636,
+        "TEMPLATE_PATH/trainers/audio/kws_farfield_trainer.py": 1681699605.728636,
+        "TEMPLATE_PATH/trainers/audio/kws_nearfield_trainer.py": 1681699605.728636,
+        "TEMPLATE_PATH/trainers/audio/kws_utils/batch_utils.py": 1681699605.728636,
+        "TEMPLATE_PATH/trainers/audio/kws_utils/det_utils.py": 1681699605.728636,
+        "TEMPLATE_PATH/trainers/audio/kws_utils/file_utils.py": 1681699605.728636,
+        "TEMPLATE_PATH/trainers/audio/kws_utils/model_utils.py": 1681699605.728636,
+        "TEMPLATE_PATH/trainers/audio/kws_utils/runtime_utils.py": 1681699605.728636,
+        "TEMPLATE_PATH/trainers/audio/separation_trainer.py": 1681699605.728636,
+        "TEMPLATE_PATH/trainers/audio/tts_trainer.py": 1681699605.728636,
+        "TEMPLATE_PATH/trainers/base.py": 1681699605.728636,
+        "TEMPLATE_PATH/trainers/builder.py": 1681699605.728636,
+        "TEMPLATE_PATH/trainers/cv/action_detection_trainer.py": 1681699605.728636,
+        "TEMPLATE_PATH/trainers/cv/card_detection_scrfd_trainer.py": 1681699605.728636,
+        "TEMPLATE_PATH/trainers/cv/cartoon_translation_trainer.py": 1681699605.728636,
+        "TEMPLATE_PATH/trainers/cv/face_detection_scrfd_trainer.py": 1681699605.728636,
+        "TEMPLATE_PATH/trainers/cv/image_classifition_trainer.py": 1681699605.728636,
+        "TEMPLATE_PATH/trainers/cv/image_defrcn_fewshot_detection_trainer.py": 1681699605.728636,
+        "TEMPLATE_PATH/trainers/cv/image_detection_damoyolo_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/cv/image_inpainting_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/cv/image_instance_segmentation_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/cv/image_portrait_enhancement_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/cv/movie_scene_segmentation_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/cv/nerf_recon_acc_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/cv/ocr_detection_db_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/cv/ocr_recognition_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/cv/referring_video_object_segmentation_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/cv/vision_efficient_tuning_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/default_config.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/easycv/trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/easycv/utils/hooks.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/easycv/utils/metric.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/easycv/utils/register_util.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/hooks/builder.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/hooks/checkpoint_hook.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/hooks/clip_clamp_logit_scale_hook.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/hooks/compression/sparsity_hook.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/hooks/compression/utils.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/hooks/ddp_hook.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/hooks/deepspeed_hook.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/hooks/early_stop_hook.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/hooks/evaluation_hook.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/hooks/hook.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/hooks/iter_timer_hook.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/hooks/logger/base.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/hooks/logger/tensorboard_hook.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/hooks/logger/text_logger_hook.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/hooks/lr_scheduler_hook.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/hooks/megatron_hook.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/hooks/optimizer/apex_optimizer_hook.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/hooks/optimizer/base.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/hooks/optimizer/torch_optimizer_hook.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/hooks/priority.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/lrscheduler/builder.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/lrscheduler/warmup/base.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/lrscheduler/warmup/warmup.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/multi_modal/clip/clip_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/multi_modal/clip/clip_trainer_utils.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/multi_modal/efficient_diffusion_tuning/efficient_diffusion_tuning_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/multi_modal/mgeo_ranking_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/multi_modal/mplug/mplug_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/multi_modal/ofa/ofa_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/multi_modal/ofa/ofa_trainer_utils.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/multi_modal/team/team_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/multi_modal/team/team_trainer_utils.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/nlp/csanmt_translation_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_generate_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_rerank_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_retrieval_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/nlp/faq_question_answering_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/nlp/gpt3_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/nlp/gpt_moe_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/nlp/plug_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/nlp/sentence_embedding_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/nlp/sequence_classification_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/nlp/siamese_uie_trainer.py": 1681699605.7326362,
+        "TEMPLATE_PATH/trainers/nlp/space/dialog_intent_trainer.py": 1681699605.7366362,
+        "TEMPLATE_PATH/trainers/nlp/space/dialog_modeling_trainer.py": 1681699605.7366362,
+        "TEMPLATE_PATH/trainers/nlp/space/eval.py": 1681699605.7366362,
+        "TEMPLATE_PATH/trainers/nlp/space/metrics/metrics_tracker.py": 1681699605.7366362,
+        "TEMPLATE_PATH/trainers/nlp/space/trainer/gen_trainer.py": 1681699605.7366362,
+        "TEMPLATE_PATH/trainers/nlp/space/trainer/intent_trainer.py": 1681699605.7366362,
+        "TEMPLATE_PATH/trainers/nlp/table_question_answering_trainer.py": 1681699605.7366362,
+        "TEMPLATE_PATH/trainers/nlp/text_generation_trainer.py": 1681699605.7366362,
+        "TEMPLATE_PATH/trainers/nlp/text_ranking_trainer.py": 1681699605.7366362,
+        "TEMPLATE_PATH/trainers/nlp_trainer.py": 1681699605.7366362,
+        "TEMPLATE_PATH/trainers/optimizer/builder.py": 1681699605.7366362,
+        "TEMPLATE_PATH/trainers/optimizer/child_tuning_adamw_optimizer.py": 1681699605.7366362,
+        "TEMPLATE_PATH/trainers/parallel/builder.py": 1681699605.7366362,
+        "TEMPLATE_PATH/trainers/parallel/utils.py": 1681699605.7366362,
+        "TEMPLATE_PATH/trainers/trainer.py": 1681699605.7366362,
+        "TEMPLATE_PATH/trainers/training_args.py": 1681699605.7366362,
+        "TEMPLATE_PATH/trainers/utils/inference.py": 1681699605.7366362,
+        "TEMPLATE_PATH/trainers/utils/log_buffer.py": 1681699605.7366362
     },
     "index": {
         "('ATTENTION', 'default', 'PETRMultiheadAttention')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py",
             "imports": [
+                "warnings",
+                "mmcv",
+                "copy",
                 "mmdet",
                 "typing",
                 "math",
-                "mmcv",
-                "warnings",
-                "copy",
                 "torch"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.petr_transformer"
         },
         "('BACKBONES', 'backbone', 'bloom')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bloom/backbone.py",
             "imports": [
@@ -1653,131 +1686,139 @@
                 "transformers"
             ],
             "module": "modelscope.models.nlp.gpt2.backbone"
         },
         "('BACKBONES', 'default', 'BASEBEiT')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/beit.py",
             "imports": [
-                "math",
-                "functools",
                 "mmcv",
-                "timm",
                 "mmdet",
-                "torch"
+                "timm",
+                "math",
+                "torch",
+                "functools"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.backbone.base.beit"
         },
         "('BACKBONES', 'default', 'BEiTAdapter')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/beit_adapter.py",
             "imports": [
+                "logging",
+                "mmdet",
                 "torch",
-                "math",
                 "timm",
-                "logging",
-                "mmdet"
+                "math"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.backbone.beit_adapter"
         },
         "('BACKBONES', 'default', 'BEiTv2')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_classification/backbones/beit_v2.py",
             "imports": [
+                "warnings",
+                "mmcv",
                 "typing",
-                "collections",
+                "itertools",
+                "torch",
                 "functools",
-                "mmcv",
+                "collections",
                 "os",
-                "torch",
                 "math",
-                "itertools",
                 "einops",
-                "warnings",
                 "mmcls"
             ],
             "module": "modelscope.models.cv.image_classification.backbones.beit_v2"
         },
+        "('BACKBONES', 'default', 'MasterNet')": {
+            "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/master_net.py",
+            "imports": [
+                "torch",
+                "mmdet"
+            ],
+            "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.backbones.master_net"
+        },
         "('BACKBONES', 'default', 'MobileNetV1')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py",
             "imports": [
                 "mmcv",
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.backbones.mobilenet"
         },
         "('BACKBONES', 'default', 'NextViT')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_classification/backbones/nextvit.py",
             "imports": [
+                "warnings",
+                "mmcv",
                 "typing",
-                "collections",
+                "itertools",
+                "torch",
                 "functools",
-                "mmcv",
+                "collections",
                 "os",
-                "torch",
                 "math",
-                "itertools",
                 "einops",
-                "warnings",
                 "mmcls"
             ],
             "module": "modelscope.models.cv.image_classification.backbones.nextvit"
         },
         "('BACKBONES', 'default', 'ResNetV1e')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py",
             "imports": [
                 "mmcv",
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.backbones.resnet"
         },
         "('BACKBONES', 'default', 'ViT')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/backbones/vit.py",
             "imports": [
-                "functools",
+                "mmdet",
                 "torch",
-                "math",
+                "functools",
                 "timm",
-                "mmdet"
+                "math"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.backbones.vit"
         },
         "('BACKBONES', 'default', 'VoVNet')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/vovnet.py",
             "imports": [
-                "mmcv",
                 "collections",
-                "mmdet",
-                "torch"
+                "mmcv",
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.backbones.vovnet"
         },
         "('BBOX_ASSIGNERS', 'default', 'HungarianAssigner3D')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py",
             "imports": [
+                "torch",
                 "scipy",
-                "mmdet",
-                "torch"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.core.bbox.assigners.hungarian_assigner_3d"
         },
         "('BBOX_ASSIGNERS', 'default', 'MaskHungarianAssignerVideo')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py",
             "imports": [
                 "numpy",
+                "torch",
                 "scipy",
-                "mmdet",
-                "torch"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.track.mask_hungarian_assigner"
         },
         "('BBOX_CODERS', 'default', 'NMSFreeCoder')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.core.bbox.coders.nms_free_coder"
         },
         "('CUSTOM_DATASETS', 'bad-image-detecting', 'bad-image-detecting')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/bad_image_detecting_dataset.py",
             "imports": [],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.bad_image_detecting.bad_image_detecting_dataset"
@@ -1851,18 +1892,18 @@
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.sidd_image_denoising.sidd_image_denoising_dataset"
         },
         "('CUSTOM_DATASETS', 'image-inpainting', 'FFTInpainting')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py",
             "imports": [
                 "numpy",
                 "albumentations",
-                "cv2",
                 "enum",
+                "glob",
                 "os",
-                "glob"
+                "cv2"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.image_inpainting.image_inpainting_dataset"
         },
         "('CUSTOM_DATASETS', 'image-object-detection', 'DetDataset')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/object_detection/detection_dataset.py",
             "imports": [
                 "easycv"
@@ -1918,31 +1959,31 @@
                 "os"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.image_instance_segmentation_coco_dataset"
         },
         "('CUSTOM_DATASETS', 'language-guided-video-summarization', 'clip-it-language-guided-video-summarization')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/language_guided_video_summarization_dataset.py",
             "imports": [
-                "numpy",
                 "h5py",
-                "os",
+                "numpy",
                 "torch",
+                "os",
                 "json"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.language_guided_video_summarization_dataset"
         },
         "('CUSTOM_DATASETS', 'movie-scene-segmentation', 'resnet50-bert')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py",
             "imports": [
-                "random",
-                "os",
-                "copy",
                 "torch",
-                "json",
-                "torchvision"
+                "copy",
+                "os",
+                "torchvision",
+                "random",
+                "json"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.movie_scene_segmentation.movie_scene_segmentation_dataset"
         },
         "('CUSTOM_DATASETS', 'nli', 'veco')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/veco_dataset.py",
             "imports": [
                 "numpy",
@@ -1951,65 +1992,65 @@
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.veco_dataset"
         },
         "('CUSTOM_DATASETS', 'ocr-recognition', 'OCRRecognition')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_recognition_dataset.py",
             "imports": [
                 "numpy",
-                "cv2",
-                "os",
                 "torch",
-                "json",
                 "PIL",
+                "os",
+                "cv2",
                 "lmdb",
+                "json",
                 "six"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_recognition_dataset"
         },
         "('CUSTOM_DATASETS', 'referring-video-object-segmentation', 'swinT-referring-video-object-segmentation')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/referring_video_object_segmentation_dataset.py",
             "imports": [
-                "numpy",
                 "h5py",
-                "pycocotools",
-                "os",
-                "torch",
-                "json",
+                "numpy",
+                "tqdm",
                 "glob",
+                "torch",
+                "torchvision",
                 "pandas",
-                "tqdm",
-                "torchvision"
+                "os",
+                "pycocotools",
+                "json"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.referring_video_object_segmentation.referring_video_object_segmentation_dataset"
         },
         "('CUSTOM_DATASETS', 'sentence-embedding', 'bert')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py",
             "imports": [
                 "typing",
-                "random",
-                "torch"
+                "torch",
+                "random"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.text_ranking_dataset"
         },
         "('CUSTOM_DATASETS', 'text-ranking', 'bert')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py",
             "imports": [
                 "typing",
-                "random",
-                "torch"
+                "torch",
+                "random"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.text_ranking_dataset"
         },
         "('CUSTOM_DATASETS', 'text-ranking', 'mgeo')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/mgeo_ranking_dataset.py",
             "imports": [
-                "typing",
-                "json",
                 "random",
-                "torch"
+                "typing",
+                "torch",
+                "json"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.mgeo_ranking_dataset"
         },
         "('CUSTOM_DATASETS', 'video-frame-interpolation', 'video-frame-interpolation')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/video_frame_interpolation_dataset.py",
             "imports": [
                 "numpy",
@@ -2049,192 +2090,192 @@
                 "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.retinaface"
         },
         "('DETECTORS', 'default', 'CustomSingleStageDetector')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/single_stage.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.detectors.single_stage"
         },
         "('DETECTORS', 'default', 'EncoderDecoderMask2Former')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/encoder_decoder_mask2former.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.segmentors.encoder_decoder_mask2former"
         },
         "('DETECTORS', 'default', 'Petr3D')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/petr3d.py",
             "imports": [
                 "numpy",
                 "mmcv",
+                "mmdet",
                 "torch",
-                "mmdet3d",
-                "mmdet"
+                "mmdet3d"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.detectors.petr3d"
         },
         "('DETECTORS', 'default', 'SCRFD')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.detectors.scrfd"
         },
         "('DETECTORS', 'default', 'TinyMog')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/tinymog.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.detectors.tinymog"
         },
         "('EXPORTERS', 'default', 'cartoon-translation')": {
             "filepath": "TEMPLATE_PATH/exporters/cv/cartoon_translation_exporter.py",
             "imports": [
-                "typing",
                 "tensorflow",
+                "typing",
                 "os",
                 "packaging"
             ],
             "module": "modelscope.exporters.cv.cartoon_translation_exporter"
         },
         "('EXPORTERS', 'face-detection', 'scrfd')": {
             "filepath": "TEMPLATE_PATH/exporters/cv/face_detection_scrfd_exporter.py",
             "imports": [
                 "numpy",
                 "typing",
+                "torch",
                 "functools",
                 "os",
-                "torch",
                 "onnx"
             ],
             "module": "modelscope.exporters.cv.face_detection_scrfd_exporter"
         },
         "('EXPORTERS', 'image-object-detection', 'tinynas-damoyolo')": {
             "filepath": "TEMPLATE_PATH/exporters/cv/object_detection_damoyolo_exporter.py",
             "imports": [
                 "numpy",
                 "typing",
+                "torch",
                 "functools",
                 "os",
-                "torch",
                 "onnx"
             ],
             "module": "modelscope.exporters.cv.object_detection_damoyolo_exporter"
         },
         "('EXPORTERS', 'named-entity-recognition', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/model_for_token_classification_exporter.py",
             "imports": [
-                "typing",
                 "collections",
+                "typing",
                 "torch"
             ],
             "module": "modelscope.exporters.nlp.model_for_token_classification_exporter"
         },
         "('EXPORTERS', 'nli', 'bert')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py",
             "imports": [
-                "typing",
                 "collections",
+                "typing",
                 "torch"
             ],
             "module": "modelscope.exporters.nlp.sbert_for_sequence_classification_exporter"
         },
         "('EXPORTERS', 'nli', 'structbert')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py",
             "imports": [
-                "typing",
                 "collections",
+                "typing",
                 "torch"
             ],
             "module": "modelscope.exporters.nlp.sbert_for_sequence_classification_exporter"
         },
         "('EXPORTERS', 'part-of-speech', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/model_for_token_classification_exporter.py",
             "imports": [
-                "typing",
                 "collections",
+                "typing",
                 "torch"
             ],
             "module": "modelscope.exporters.nlp.model_for_token_classification_exporter"
         },
         "('EXPORTERS', 'sentence-similarity', 'bert')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py",
             "imports": [
-                "typing",
                 "collections",
+                "typing",
                 "torch"
             ],
             "module": "modelscope.exporters.nlp.sbert_for_sequence_classification_exporter"
         },
         "('EXPORTERS', 'sentence-similarity', 'structbert')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py",
             "imports": [
-                "typing",
                 "collections",
+                "typing",
                 "torch"
             ],
             "module": "modelscope.exporters.nlp.sbert_for_sequence_classification_exporter"
         },
         "('EXPORTERS', 'sentiment-classification', 'bert')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py",
             "imports": [
-                "typing",
                 "collections",
+                "typing",
                 "torch"
             ],
             "module": "modelscope.exporters.nlp.sbert_for_sequence_classification_exporter"
         },
         "('EXPORTERS', 'sentiment-classification', 'structbert')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py",
             "imports": [
-                "typing",
                 "collections",
+                "typing",
                 "torch"
             ],
             "module": "modelscope.exporters.nlp.sbert_for_sequence_classification_exporter"
         },
         "('EXPORTERS', 'text-classification', 'bert')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py",
             "imports": [
-                "typing",
                 "collections",
+                "typing",
                 "torch"
             ],
             "module": "modelscope.exporters.nlp.sbert_for_sequence_classification_exporter"
         },
         "('EXPORTERS', 'text-classification', 'structbert')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py",
             "imports": [
-                "typing",
                 "collections",
+                "typing",
                 "torch"
             ],
             "module": "modelscope.exporters.nlp.sbert_for_sequence_classification_exporter"
         },
         "('EXPORTERS', 'token-classification', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/model_for_token_classification_exporter.py",
             "imports": [
-                "typing",
                 "collections",
+                "typing",
                 "torch"
             ],
             "module": "modelscope.exporters.nlp.model_for_token_classification_exporter"
         },
         "('EXPORTERS', 'transformer-crf', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/model_for_token_classification_exporter.py",
             "imports": [
-                "typing",
                 "collections",
+                "typing",
                 "torch"
             ],
             "module": "modelscope.exporters.nlp.model_for_token_classification_exporter"
         },
         "('EXPORTERS', 'translation', 'csanmt-translation')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/csanmt_for_translation_exporter.py",
             "imports": [
@@ -2243,241 +2284,250 @@
                 "os"
             ],
             "module": "modelscope.exporters.nlp.csanmt_for_translation_exporter"
         },
         "('EXPORTERS', 'word-segmentation', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/model_for_token_classification_exporter.py",
             "imports": [
-                "typing",
                 "collections",
+                "typing",
                 "torch"
             ],
             "module": "modelscope.exporters.nlp.model_for_token_classification_exporter"
         },
         "('EXPORTERS', 'zero-shot-classification', 'bert')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/sbert_for_zero_shot_classification_exporter.py",
             "imports": [
-                "typing",
-                "collections"
+                "collections",
+                "typing"
             ],
             "module": "modelscope.exporters.nlp.sbert_for_zero_shot_classification_exporter"
         },
         "('EXPORTERS', 'zero-shot-classification', 'structbert')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/sbert_for_zero_shot_classification_exporter.py",
             "imports": [
-                "typing",
-                "collections"
+                "collections",
+                "typing"
             ],
             "module": "modelscope.exporters.nlp.sbert_for_zero_shot_classification_exporter"
         },
         "('HEADS', 'default', 'AnchorNHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/dense_heads/anchor_head.py",
             "imports": [
                 "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.dense_heads.anchor_head"
         },
         "('HEADS', 'default', 'ConvFCBBoxNHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.roi_heads.bbox_heads.convfc_bbox_head"
         },
         "('HEADS', 'default', 'ConvKernelHeadVideo')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_head.py",
             "imports": [
                 "mmcv",
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.head.kernel_head"
         },
         "('HEADS', 'default', 'FCNMaskNHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/fcn_mask_head.py",
             "imports": [
                 "numpy",
-                "mmcv",
-                "torch",
                 "warnings",
-                "mmdet"
+                "mmcv",
+                "mmdet",
+                "torch"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.roi_heads.mask_heads.fcn_mask_head"
         },
         "('HEADS', 'default', 'KernelFrameIterHeadVideo')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_frame_iter_head.py",
             "imports": [
                 "mmcv",
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.head.kernel_frame_iter_head"
         },
         "('HEADS', 'default', 'KernelIterHeadVideo')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_iter_head.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.head.kernel_iter_head"
         },
         "('HEADS', 'default', 'KernelUpdateHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_update_head.py",
             "imports": [
                 "numpy",
-                "mmdet",
                 "mmcv",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.head.kernel_update_head"
         },
         "('HEADS', 'default', 'KernelUpdateHeadVideo')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/track/kernel_update_head.py",
             "imports": [
                 "numpy",
-                "mmdet",
                 "mmcv",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.track.kernel_update_head"
         },
         "('HEADS', 'default', 'Mask2FormerHeadFromMMSeg')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/mask2former_head_from_mmseg.py",
             "imports": [
-                "mmcv",
-                "mmdet",
                 "copy",
-                "torch"
+                "mmcv",
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.decode_heads.mask2former_head_from_mmseg"
         },
         "('HEADS', 'default', 'MaskFormerSemanticHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/pan_merge/maskformer_semantic_head.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.pan_merge.maskformer_semantic_head"
         },
         "('HEADS', 'default', 'MaskScoringNRoIHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.abnormal_object_detection.mmdet_ms.roi_head.mask_scoring_roi_head"
         },
         "('HEADS', 'default', 'PETRv2DEDNHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/petrv2_dednhead.py",
             "imports": [
                 "numpy",
                 "mmcv",
-                "copy",
+                "mmdet",
                 "torch",
-                "math",
+                "copy",
                 "mmdet3d",
-                "mmdet"
+                "math"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.dense_heads.petrv2_dednhead"
         },
         "('HEADS', 'default', 'RPNNHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py",
             "imports": [
-                "mmcv",
-                "mmdet",
                 "copy",
-                "torch"
+                "mmcv",
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.dense_heads.rpn_head"
         },
         "('HEADS', 'default', 'SCRFDHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py",
             "imports": [
                 "numpy",
-                "mmdet",
                 "mmcv",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.dense_heads.scrfd_head"
         },
         "('HEADS', 'default', 'Shared2FCBBoxNHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.roi_heads.bbox_heads.convfc_bbox_head"
         },
         "('HEADS', 'default', 'Shared4Conv1FCBBoxNHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.roi_heads.bbox_heads.convfc_bbox_head"
         },
         "('HEADS', 'default', 'VideoKernelIterHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_iter_head.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_panoptic_segmentation.head.kernel_iter_head"
         },
         "('HEADS', 'default', 'VideoKernelUpdateHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_update_head.py",
             "imports": [
                 "numpy",
-                "mmdet",
                 "mmcv",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_panoptic_segmentation.head.kernel_update_head"
         },
         "('HEADS', 'fill-mask', 'bert-mlm')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/fill_mask_head.py",
             "imports": [
                 "typing",
-                "transformers",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.heads.fill_mask_head"
         },
         "('HEADS', 'fill-mask', 'fill-mask')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/fill_mask_head.py",
             "imports": [
                 "typing",
-                "transformers",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.heads.fill_mask_head"
         },
         "('HEADS', 'fill-mask', 'roberta-mlm')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/torch_pretrain_head.py",
             "imports": [
                 "typing",
-                "transformers",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.heads.torch_pretrain_head"
         },
+        "('HEADS', 'fill-mask', 'xlm-roberta-mlm')": {
+            "filepath": "TEMPLATE_PATH/models/nlp/heads/fill_mask_head.py",
+            "imports": [
+                "typing",
+                "torch",
+                "transformers"
+            ],
+            "module": "modelscope.models.nlp.heads.fill_mask_head"
+        },
         "('HEADS', 'information-extraction', 'information-extraction')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/infromation_extraction_head.py",
             "imports": [
                 "torch"
             ],
             "module": "modelscope.models.nlp.heads.infromation_extraction_head"
         },
         "('HEADS', 'named-entity-recognition', 'lstm-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
                 "typing",
-                "transformers",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'named-entity-recognition', 'token-classification')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/token_classification_head.py",
             "imports": [
                 "typing",
@@ -2485,16 +2535,16 @@
             ],
             "module": "modelscope.models.nlp.heads.token_classification_head"
         },
         "('HEADS', 'named-entity-recognition', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
                 "typing",
-                "transformers",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'nli', 'text-classification')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/text_classification_head.py",
             "imports": [
                 "typing",
@@ -2502,16 +2552,16 @@
             ],
             "module": "modelscope.models.nlp.heads.text_classification_head"
         },
         "('HEADS', 'part-of-speech', 'lstm-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
                 "typing",
-                "transformers",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'part-of-speech', 'token-classification')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/token_classification_head.py",
             "imports": [
                 "typing",
@@ -2519,16 +2569,16 @@
             ],
             "module": "modelscope.models.nlp.heads.token_classification_head"
         },
         "('HEADS', 'part-of-speech', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
                 "typing",
-                "transformers",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'relation-extraction', 'information-extraction')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/infromation_extraction_head.py",
             "imports": [
                 "torch"
@@ -2575,16 +2625,16 @@
             ],
             "module": "modelscope.models.nlp.heads.text_ranking_head"
         },
         "('HEADS', 'token-classification', 'lstm-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
                 "typing",
-                "transformers",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'token-classification', 'token-classification')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/token_classification_head.py",
             "imports": [
                 "typing",
@@ -2592,81 +2642,83 @@
             ],
             "module": "modelscope.models.nlp.heads.token_classification_head"
         },
         "('HEADS', 'token-classification', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
                 "typing",
-                "transformers",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'transformer-crf', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
                 "typing",
-                "transformers",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'word-segmentation', 'lstm-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
                 "typing",
-                "transformers",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'word-segmentation', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
                 "typing",
-                "transformers",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HOOKS', 'default', 'AddLrLogHook')": {
             "filepath": "TEMPLATE_PATH/trainers/easycv/utils/hooks.py",
             "imports": [],
             "module": "modelscope.trainers.easycv.utils.hooks"
         },
         "('HOOKS', 'default', 'ApexAMPOptimizerHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/optimizer/apex_optimizer_hook.py",
             "imports": [
-                "torch",
                 "logging",
+                "torch",
                 "packaging"
             ],
             "module": "modelscope.trainers.hooks.optimizer.apex_optimizer_hook"
         },
         "('HOOKS', 'default', 'BestCkptSaverHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/checkpoint_hook.py",
             "imports": [
                 "numpy",
-                "random",
-                "os",
                 "packaging",
+                "time",
+                "re",
                 "torch",
-                "re"
+                "os",
+                "random"
             ],
             "module": "modelscope.trainers.hooks.checkpoint_hook"
         },
         "('HOOKS', 'default', 'CheckpointHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/checkpoint_hook.py",
             "imports": [
                 "numpy",
-                "random",
-                "os",
                 "packaging",
+                "time",
+                "re",
                 "torch",
-                "re"
+                "os",
+                "random"
             ],
             "module": "modelscope.trainers.hooks.checkpoint_hook"
         },
         "('HOOKS', 'default', 'ClipClampLogitScaleHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/clip_clamp_logit_scale_hook.py",
             "imports": [
                 "torch"
@@ -2677,19 +2729,19 @@
             "filepath": "TEMPLATE_PATH/trainers/hooks/ddp_hook.py",
             "imports": [],
             "module": "modelscope.trainers.hooks.ddp_hook"
         },
         "('HOOKS', 'default', 'DeepspeedHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/deepspeed_hook.py",
             "imports": [
-                "megatron_util",
                 "deepspeed",
                 "shutil",
+                "torch",
                 "os",
-                "torch"
+                "megatron_util"
             ],
             "module": "modelscope.trainers.hooks.deepspeed_hook"
         },
         "('HOOKS', 'default', 'EarlyStopHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/early_stop_hook.py",
             "imports": [
                 "numpy"
@@ -2710,33 +2762,34 @@
             ],
             "module": "modelscope.trainers.hooks.iter_timer_hook"
         },
         "('HOOKS', 'default', 'LoadCheckpointHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/checkpoint_hook.py",
             "imports": [
                 "numpy",
-                "random",
-                "os",
                 "packaging",
+                "time",
+                "re",
                 "torch",
-                "re"
+                "os",
+                "random"
             ],
             "module": "modelscope.trainers.hooks.checkpoint_hook"
         },
         "('HOOKS', 'default', 'LrSchedulerHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/lr_scheduler_hook.py",
             "imports": [],
             "module": "modelscope.trainers.hooks.lr_scheduler_hook"
         },
         "('HOOKS', 'default', 'MegatronHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/megatron_hook.py",
             "imports": [
-                "torch",
-                "os",
                 "copy",
+                "os",
+                "torch",
                 "megatron_util"
             ],
             "module": "modelscope.trainers.hooks.megatron_hook"
         },
         "('HOOKS', 'default', 'NoneLrSchedulerHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/lr_scheduler_hook.py",
             "imports": [],
@@ -2778,18 +2831,18 @@
                 "torch"
             ],
             "module": "modelscope.trainers.hooks.logger.tensorboard_hook"
         },
         "('HOOKS', 'default', 'TextLoggerHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/logger/text_logger_hook.py",
             "imports": [
-                "os",
                 "torch",
-                "json",
                 "collections",
+                "os",
+                "json",
                 "datetime"
             ],
             "module": "modelscope.trainers.hooks.logger.text_logger_hook"
         },
         "('HOOKS', 'default', 'TorchAMPOptimizerHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/optimizer/torch_optimizer_hook.py",
             "imports": [
@@ -2811,26 +2864,26 @@
             "filepath": "TEMPLATE_PATH/trainers/lrscheduler/warmup/warmup.py",
             "imports": [],
             "module": "modelscope.trainers.lrscheduler.warmup.warmup"
         },
         "('MATCH_COST', 'default', 'BBox3DL1Cost')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/match_cost.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.core.bbox.match_costs.match_cost"
         },
         "('MATCH_COST', 'default', 'MaskCost')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py",
             "imports": [
                 "numpy",
+                "torch",
                 "scipy",
-                "mmdet",
-                "torch"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.track.mask_hungarian_assigner"
         },
         "('METRICS', 'default', 'EasyCVMetric')": {
             "filepath": "TEMPLATE_PATH/trainers/easycv/utils/metric.py",
             "imports": [
                 "numpy",
@@ -2873,17 +2926,19 @@
             ],
             "module": "modelscope.metrics.image_color_enhance_metric"
         },
         "('METRICS', 'default', 'image-colorization-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_colorization_metric.py",
             "imports": [
                 "numpy",
-                "typing",
                 "scipy",
-                "torch"
+                "typing",
+                "torch",
+                "torchvision",
+                "cv2"
             ],
             "module": "modelscope.metrics.image_colorization_metric"
         },
         "('METRICS', 'default', 'image-denoise-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_denoise_metric.py",
             "imports": [
                 "numpy",
@@ -2894,28 +2949,28 @@
             "module": "modelscope.metrics.image_denoise_metric"
         },
         "('METRICS', 'default', 'image-inpainting-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_inpainting_metric.py",
             "imports": [
                 "numpy",
                 "typing",
-                "scipy",
-                "torch"
+                "torch",
+                "scipy"
             ],
             "module": "modelscope.metrics.image_inpainting_metric"
         },
         "('METRICS', 'default', 'image-ins-seg-coco-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_instance_segmentation_metric.py",
             "imports": [
                 "numpy",
                 "typing",
-                "pycocotools",
-                "os",
                 "tempfile",
-                "collections"
+                "collections",
+                "os",
+                "pycocotools"
             ],
             "module": "modelscope.metrics.image_instance_segmentation_metric"
         },
         "('METRICS', 'default', 'image-portrait-enhancement-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_portrait_enhancement_metric.py",
             "imports": [
                 "numpy",
@@ -2924,37 +2979,37 @@
             ],
             "module": "modelscope.metrics.image_portrait_enhancement_metric"
         },
         "('METRICS', 'default', 'image-quality-assessment-degradation-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_quality_assessment_degradation_metric.py",
             "imports": [
                 "numpy",
-                "typing",
                 "scipy",
-                "cv2",
+                "sys",
+                "typing",
+                "torch",
                 "tempfile",
+                "collections",
                 "os",
-                "torch",
-                "sys",
-                "tqdm",
-                "collections"
+                "cv2",
+                "tqdm"
             ],
             "module": "modelscope.metrics.image_quality_assessment_degradation_metric"
         },
         "('METRICS', 'default', 'image-quality-assessment-mos-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_quality_assessment_mos_metric.py",
             "imports": [
                 "numpy",
-                "typing",
                 "scipy",
-                "cv2",
+                "sys",
+                "typing",
+                "torch",
                 "tempfile",
                 "os",
-                "torch",
-                "sys",
+                "cv2",
                 "tqdm"
             ],
             "module": "modelscope.metrics.image_quality_assessment_mos_metric"
         },
         "('METRICS', 'default', 'inbatch_recall')": {
             "filepath": "TEMPLATE_PATH/metrics/inbatch_recall_metric.py",
             "imports": [
@@ -2997,16 +3052,16 @@
             ],
             "module": "modelscope.metrics.ned_metric"
         },
         "('METRICS', 'default', 'ocr-recognition-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/ocr_recognition_metric.py",
             "imports": [
                 "numpy",
-                "typing",
                 "edit_distance",
+                "typing",
                 "torch"
             ],
             "module": "modelscope.metrics.ocr_recognition_metric"
         },
         "('METRICS', 'default', 'ppl')": {
             "filepath": "TEMPLATE_PATH/metrics/ppl_metric.py",
             "imports": [
@@ -3026,16 +3081,16 @@
             ],
             "module": "modelscope.metrics.prediction_saving_wrapper"
         },
         "('METRICS', 'default', 'referring-video-object-segmentation-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/referring_video_object_segmentation_metric.py",
             "imports": [
                 "numpy",
-                "typing",
                 "pycocotools",
+                "typing",
                 "torch",
                 "tqdm"
             ],
             "module": "modelscope.metrics.referring_video_object_segmentation_metric"
         },
         "('METRICS', 'default', 'seq-cls-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/sequence_classification_metric.py",
@@ -3045,16 +3100,16 @@
                 "sklearn"
             ],
             "module": "modelscope.metrics.sequence_classification_metric"
         },
         "('METRICS', 'default', 'text-gen-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/text_generation_metric.py",
             "imports": [
-                "nltk",
                 "typing",
+                "nltk",
                 "rouge"
             ],
             "module": "modelscope.metrics.text_generation_metric"
         },
         "('METRICS', 'default', 'text-ranking-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/text_ranking_metric.py",
             "imports": [
@@ -3071,31 +3126,31 @@
                 "importlib"
             ],
             "module": "modelscope.metrics.token_classification_metric"
         },
         "('METRICS', 'default', 'video-frame-interpolation-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/video_frame_interpolation_metric.py",
             "imports": [
-                "lpips",
-                "typing",
                 "numpy",
+                "typing",
                 "torch",
+                "lpips",
                 "math"
             ],
             "module": "modelscope.metrics.video_frame_interpolation_metric"
         },
         "('METRICS', 'default', 'video-stabilization-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/video_stabilization_metric.py",
             "imports": [
                 "numpy",
+                "sys",
                 "typing",
-                "cv2",
-                "os",
                 "tempfile",
-                "sys",
+                "os",
+                "cv2",
                 "tqdm"
             ],
             "module": "modelscope.metrics.video_stabilization_metric"
         },
         "('METRICS', 'default', 'video-summarization-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/video_summarization_metric.py",
             "imports": [
@@ -3136,129 +3191,139 @@
             ],
             "module": "modelscope.models.audio.asr.generic_automatic_speech_recognition"
         },
         "('MODELS', 'auto-speech-recognition', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
                 "typing",
+                "re",
+                "string",
+                "torch",
                 "functools",
                 "os",
-                "torch",
-                "json",
                 "math",
-                "re",
-                "string"
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'auto-speech-recognition', 'wenet-asr')": {
             "filepath": "TEMPLATE_PATH/models/audio/asr/wenet_automatic_speech_recognition.py",
             "imports": [
-                "typing",
-                "json",
                 "wenetruntime",
-                "os"
+                "typing",
+                "os",
+                "json"
             ],
             "module": "modelscope.models.audio.asr.wenet_automatic_speech_recognition"
         },
         "('MODELS', 'backbone', 'T5')": {
             "filepath": "TEMPLATE_PATH/models/nlp/T5/backbone.py",
             "imports": [
+                "warnings",
                 "typing",
-                "transformers",
-                "os",
-                "copy",
                 "torch",
+                "copy",
+                "os",
                 "math",
-                "warnings"
+                "transformers"
             ],
             "module": "modelscope.models.nlp.T5.backbone"
         },
         "('MODELS', 'backbone', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/backbone.py",
             "imports": [
-                "transformers",
                 "math",
                 "torch",
-                "packaging"
+                "packaging",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.bert.backbone"
         },
         "('MODELS', 'backbone', 'deberta_v2')": {
             "filepath": "TEMPLATE_PATH/models/nlp/deberta_v2/backbone.py",
             "imports": [
-                "typing",
-                "transformers",
                 "collections",
-                "torch"
+                "typing",
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.deberta_v2.backbone"
         },
+        "('MODELS', 'backbone', 'llama')": {
+            "filepath": "TEMPLATE_PATH/models/nlp/llama/backbone.py",
+            "imports": [
+                "typing",
+                "math",
+                "torch",
+                "transformers"
+            ],
+            "module": "modelscope.models.nlp.llama.backbone"
+        },
         "('MODELS', 'backbone', 'lstm')": {
             "filepath": "TEMPLATE_PATH/models/nlp/lstm/backbone.py",
             "imports": [
                 "torch"
             ],
             "module": "modelscope.models.nlp.lstm.backbone"
         },
         "('MODELS', 'backbone', 'megatron-bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/megatron_bert/backbone.py",
             "imports": [
-                "transformers",
                 "math",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.megatron_bert.backbone"
         },
         "('MODELS', 'backbone', 'mgeo')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/mgeo/backbone.py",
             "imports": [
+                "warnings",
+                "dataclasses",
                 "typing",
-                "transformers",
-                "random",
-                "os",
                 "torch",
+                "os",
+                "random",
                 "math",
-                "dataclasses",
-                "warnings"
+                "transformers"
             ],
             "module": "modelscope.models.multi_modal.mgeo.backbone"
         },
         "('MODELS', 'backbone', 'plug-mental')": {
             "filepath": "TEMPLATE_PATH/models/nlp/plug_mental/backbone.py",
             "imports": [
+                "packaging",
+                "dataclasses",
                 "typing",
-                "transformers",
                 "torch",
-                "packaging",
                 "math",
-                "dataclasses"
+                "transformers"
             ],
             "module": "modelscope.models.nlp.plug_mental.backbone"
         },
         "('MODELS', 'backbone', 'ponet')": {
             "filepath": "TEMPLATE_PATH/models/nlp/ponet/backbone.py",
             "imports": [
-                "transformers",
                 "distutils",
-                "torch",
                 "packaging",
-                "math"
+                "torch",
+                "math",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.ponet.backbone"
         },
         "('MODELS', 'backbone', 'structbert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/structbert/backbone.py",
             "imports": [
+                "packaging",
+                "dataclasses",
                 "typing",
-                "transformers",
                 "torch",
-                "packaging",
                 "math",
-                "dataclasses"
+                "transformers"
             ],
             "module": "modelscope.models.nlp.structbert.backbone"
         },
         "('MODELS', 'backbone', 'transformers')": {
             "filepath": "TEMPLATE_PATH/models/nlp/hf_transformers/backbone.py",
             "imports": [
                 "transformers"
@@ -3271,28 +3336,28 @@
                 "transformers"
             ],
             "module": "modelscope.models.nlp.veco.backbone"
         },
         "('MODELS', 'backbone', 'xlm-roberta')": {
             "filepath": "TEMPLATE_PATH/models/nlp/xlm_roberta/backbone.py",
             "imports": [
-                "transformers",
                 "math",
                 "torch",
-                "packaging"
+                "packaging",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.xlm_roberta.backbone"
         },
         "('MODELS', 'bad-image-detecting', 'bad-image-detecting')": {
             "filepath": "TEMPLATE_PATH/models/cv/bad_image_detecting/bad_image_detecting.py",
             "imports": [
                 "numpy",
                 "typing",
-                "os",
                 "torch",
+                "os",
                 "torchvision"
             ],
             "module": "modelscope.models.cv.bad_image_detecting.bad_image_detecting"
         },
         "('MODELS', 'body-2d-keypoints', 'body-2d-keypoints')": {
             "filepath": "TEMPLATE_PATH/models/cv/body_2d_keypoints/hrnet_v2.py",
             "imports": [
@@ -3301,19 +3366,19 @@
                 "torch"
             ],
             "module": "modelscope.models.cv.body_2d_keypoints.hrnet_v2"
         },
         "('MODELS', 'body-3d-keypoints', 'body-3d-keypoints')": {
             "filepath": "TEMPLATE_PATH/models/cv/body_3d_keypoints/cannonical_pose/body_3d_pose.py",
             "imports": [
+                "logging",
                 "numpy",
                 "typing",
-                "os",
                 "torch",
-                "logging"
+                "os"
             ],
             "module": "modelscope.models.cv.body_3d_keypoints.cannonical_pose.body_3d_pose"
         },
         "('MODELS', 'body-3d-keypoints', 'hdformer')": {
             "filepath": "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/hdformer_detector.py",
             "imports": [
                 "numpy",
@@ -3324,17 +3389,17 @@
             "module": "modelscope.models.cv.body_3d_keypoints.hdformer.hdformer_detector"
         },
         "('MODELS', 'card-detection', 'scrfd')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/scrfd_detect.py",
             "imports": [
                 "numpy",
                 "typing",
-                "os",
+                "torch",
                 "copy",
-                "torch"
+                "os"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.scrfd_detect"
         },
         "('MODELS', 'code-generation', 'codegeex')": {
             "filepath": "TEMPLATE_PATH/models/nlp/codegeex/codegeex_for_code_generation.py",
             "imports": [
                 "typing",
@@ -3348,29 +3413,40 @@
             "imports": [
                 "typing",
                 "copy",
                 "torch"
             ],
             "module": "modelscope.models.nlp.codegeex.codegeex_for_code_translation"
         },
+        "('MODELS', 'competency-aware-translation', 'canmt')": {
+            "filepath": "TEMPLATE_PATH/models/nlp/canmt/canmt_translation.py",
+            "imports": [
+                "numpy",
+                "typing",
+                "torch",
+                "os",
+                "math"
+            ],
+            "module": "modelscope.models.nlp.canmt.canmt_translation"
+        },
         "('MODELS', 'controllable-image-generation', 'controllable-image-generation')": {
             "filepath": "TEMPLATE_PATH/models/cv/controllable_image_generation/controlnet.py",
             "imports": [
                 "numpy",
+                "sys",
                 "typing",
-                "random",
-                "cv2",
-                "tempfile",
-                "os",
                 "torch",
-                "sys",
                 "control_ldm",
+                "tempfile",
+                "PIL",
+                "os",
+                "cv2",
+                "random",
                 "math",
-                "einops",
-                "PIL"
+                "einops"
             ],
             "module": "modelscope.models.cv.controllable_image_generation.controlnet"
         },
         "('MODELS', 'crowd-counting', 'HRNetCrowdCounting')": {
             "filepath": "TEMPLATE_PATH/models/cv/crowd_counting/cc_model.py",
             "imports": [
                 "typing",
@@ -3421,23 +3497,23 @@
                 "torch"
             ],
             "module": "modelscope.models.nlp.ponet.document_segmentation"
         },
         "('MODELS', 'document-vl-embedding', 'vldoc')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/vldoc/model.py",
             "imports": [
-                "os",
-                "copy",
-                "torch",
+                "logging",
                 "sys",
-                "math",
-                "json",
                 "re",
+                "torch",
+                "copy",
+                "os",
                 "torchvision",
-                "logging"
+                "math",
+                "json"
             ],
             "module": "modelscope.models.multi_modal.vldoc.model"
         },
         "('MODELS', 'domain-specific-object-detection', 'YOLOX')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/yolox_pai.py",
             "imports": [
                 "easycv"
@@ -3445,14 +3521,26 @@
             "module": "modelscope.models.cv.object_detection.yolox_pai"
         },
         "('MODELS', 'domain-specific-object-detection', 'tinynas-damoyolo')": {
             "filepath": "TEMPLATE_PATH/models/cv/tinynas_detection/tinynas_damoyolo.py",
             "imports": [],
             "module": "modelscope.models.cv.tinynas_detection.tinynas_damoyolo"
         },
+        "('MODELS', 'efficient-diffusion-tuning', 'efficient-diffusion-tuning')": {
+            "filepath": "TEMPLATE_PATH/models/multi_modal/efficient_diffusion_tuning/efficient_stable_diffusion.py",
+            "imports": [
+                "diffusers",
+                "typing",
+                "torch",
+                "functools",
+                "os",
+                "transformers"
+            ],
+            "module": "modelscope.models.multi_modal.efficient_diffusion_tuning.efficient_stable_diffusion"
+        },
         "('MODELS', 'extractive-summarization', 'ponet-for-document-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/nlp/ponet/document_segmentation.py",
             "imports": [
                 "typing",
                 "torch"
             ],
             "module": "modelscope.models.nlp.ponet.document_segmentation"
@@ -3464,50 +3552,60 @@
             ],
             "module": "modelscope.models.cv.face_2d_keypoints.face_2d_keypoints_align"
         },
         "('MODELS', 'face-2d-keypoints', 'flc')": {
             "filepath": "TEMPLATE_PATH/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py",
             "imports": [
                 "numpy",
-                "cv2",
-                "os",
                 "torch",
-                "PIL"
+                "PIL",
+                "os",
+                "cv2"
             ],
             "module": "modelscope.models.cv.facial_landmark_confidence.flc.facial_landmark_confidence"
         },
         "('MODELS', 'face-attribute-recognition', 'fairface')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py",
             "imports": [
                 "numpy",
-                "cv2",
-                "os",
                 "torch",
                 "PIL",
+                "os",
+                "cv2",
                 "torchvision"
             ],
             "module": "modelscope.models.cv.face_attribute_recognition.fair_face.face_attribute_recognition"
         },
+        "('MODELS', 'face-detection', 'damofd')": {
+            "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/damofd_detect.py",
+            "imports": [
+                "typing",
+                "copy",
+                "os",
+                "torch"
+            ],
+            "module": "modelscope.models.cv.face_detection.scrfd.damofd_detect"
+        },
         "('MODELS', 'face-detection', 'mogface')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/mogface/models/detectors.py",
             "imports": [
                 "numpy",
-                "cv2",
                 "os",
+                "cv2",
                 "torch"
             ],
             "module": "modelscope.models.cv.face_detection.mogface.models.detectors"
         },
         "('MODELS', 'face-detection', 'mtcnn')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/detector.py",
             "imports": [
                 "numpy",
                 "os",
-                "PIL",
-                "torch"
+                "torch",
+                "PIL"
             ],
             "module": "modelscope.models.cv.face_detection.mtcnn.models.detector"
         },
         "('MODELS', 'face-detection', 'retinaface')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/retinaface/detection.py",
             "imports": [
                 "numpy",
@@ -3517,36 +3615,36 @@
             "module": "modelscope.models.cv.face_detection.retinaface.detection"
         },
         "('MODELS', 'face-detection', 'scrfd')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/scrfd_detect.py",
             "imports": [
                 "numpy",
                 "typing",
-                "os",
+                "torch",
                 "copy",
-                "torch"
+                "os"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.scrfd_detect"
         },
         "('MODELS', 'face-detection', 'tinymog')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/tinymog_detect.py",
             "imports": [
                 "typing",
-                "os",
                 "copy",
+                "os",
                 "torch"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.tinymog_detect"
         },
         "('MODELS', 'face-detection', 'ulfd')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/detection.py",
             "imports": [
                 "numpy",
-                "cv2",
                 "os",
+                "cv2",
                 "torch"
             ],
             "module": "modelscope.models.cv.face_detection.ulfd_slim.detection"
         },
         "('MODELS', 'face-emotion', 'face-emotion')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_emotion/emotion_model.py",
             "imports": [
@@ -3565,81 +3663,92 @@
             ],
             "module": "modelscope.models.cv.face_human_hand_detection.det_infer"
         },
         "('MODELS', 'face-recognition', 'rts-backbone')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_recognition/torchkit/rts_backbone.py",
             "imports": [
                 "collections",
-                "math",
                 "os",
+                "math",
                 "torch"
             ],
             "module": "modelscope.models.cv.face_recognition.torchkit.rts_backbone"
         },
         "('MODELS', 'face-reconstruction', 'face_reconstruction')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_reconstruction/models/facerecon_model.py",
             "imports": [
                 "numpy",
-                "cv2",
+                "torch",
+                "collections",
                 "os",
-                "torch"
+                "cv2"
             ],
             "module": "modelscope.models.cv.face_reconstruction.models.facerecon_model"
         },
         "('MODELS', 'facial-expression-recognition', 'fer')": {
             "filepath": "TEMPLATE_PATH/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py",
             "imports": [
                 "numpy",
-                "cv2",
-                "os",
                 "torch",
-                "PIL"
+                "PIL",
+                "os",
+                "cv2"
             ],
             "module": "modelscope.models.cv.facial_expression_recognition.fer.facial_expression_recognition"
         },
         "('MODELS', 'faq-question-answering', 'structbert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/structbert/faq_question_answering.py",
             "imports": [
                 "typing",
-                "os",
                 "torch",
-                "math",
-                "collections"
+                "collections",
+                "os",
+                "math"
             ],
             "module": "modelscope.models.nlp.structbert.faq_question_answering"
         },
         "('MODELS', 'feature-extraction', 'feature-extraction')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/feature_extraction.py",
             "imports": [
                 "numpy",
                 "typing"
             ],
             "module": "modelscope.models.nlp.task_models.feature_extraction"
         },
+        "('MODELS', 'fid-dialogue', 'fid-T5')": {
+            "filepath": "TEMPLATE_PATH/models/nlp/fid_T5/text_generation.py",
+            "imports": [
+                "io",
+                "os",
+                "torch",
+                "transformers"
+            ],
+            "module": "modelscope.models.nlp.fid_T5.text_generation"
+        },
         "('MODELS', 'fid-dialogue', 'fid-plug')": {
             "filepath": "TEMPLATE_PATH/models/nlp/fid_plug/text_generation.py",
             "imports": [
-                "transformers",
                 "io",
                 "os",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.fid_plug.text_generation"
         },
         "('MODELS', 'fill-mask', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/fill_mask.py",
             "imports": [],
             "module": "modelscope.models.nlp.bert.fill_mask"
         },
         "('MODELS', 'fill-mask', 'deberta_v2')": {
             "filepath": "TEMPLATE_PATH/models/nlp/deberta_v2/fill_mask.py",
             "imports": [
                 "typing",
-                "transformers",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.deberta_v2.fill_mask"
         },
         "('MODELS', 'fill-mask', 'fill-mask')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/fill_mask.py",
             "imports": [
                 "numpy",
@@ -3647,32 +3756,32 @@
                 "torch"
             ],
             "module": "modelscope.models.nlp.task_models.fill_mask"
         },
         "('MODELS', 'fill-mask', 'megatron-bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/megatron_bert/fill_mask.py",
             "imports": [
-                "transformers",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.megatron_bert.fill_mask"
         },
         "('MODELS', 'fill-mask', 'ponet')": {
             "filepath": "TEMPLATE_PATH/models/nlp/ponet/fill_mask.py",
             "imports": [
-                "transformers",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.ponet.fill_mask"
         },
         "('MODELS', 'fill-mask', 'structbert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/structbert/fill_mask.py",
             "imports": [
-                "transformers",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.structbert.fill_mask"
         },
         "('MODELS', 'fill-mask', 'veco')": {
             "filepath": "TEMPLATE_PATH/models/nlp/veco/fill_mask.py",
             "imports": [
                 "transformers"
@@ -3680,19 +3789,19 @@
             "module": "modelscope.models.nlp.veco.fill_mask"
         },
         "('MODELS', 'generative-multi-modal-embedding', 'gemm-generative-multi-modal')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/gemm/gemm_model.py",
             "imports": [
                 "numpy",
                 "typing",
-                "os",
                 "torch",
-                "json",
                 "PIL",
-                "torchvision"
+                "os",
+                "torchvision",
+                "json"
             ],
             "module": "modelscope.models.multi_modal.gemm.gemm_model"
         },
         "('MODELS', 'generative-multi-modal-embedding', 'rleg-generative-multi-modal')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/rleg/rleg.py",
             "imports": [
                 "typing",
@@ -3708,19 +3817,19 @@
             ],
             "module": "modelscope.models.cv.hand_2d_keypoints.hand_2d_keypoints"
         },
         "('MODELS', 'hand-static', 'hand-static')": {
             "filepath": "TEMPLATE_PATH/models/cv/hand_static/hand_model.py",
             "imports": [
                 "numpy",
-                "cv2",
-                "os",
-                "torch",
                 "sys",
+                "torch",
                 "PIL",
+                "os",
+                "cv2",
                 "torchvision"
             ],
             "module": "modelscope.models.cv.hand_static.hand_model"
         },
         "('MODELS', 'human-detection', 'detection')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_model.py",
             "imports": [
@@ -3731,19 +3840,19 @@
             "module": "modelscope.models.cv.object_detection.mmdet_model"
         },
         "('MODELS', 'human-reconstruction', 'human-reconstruction')": {
             "filepath": "TEMPLATE_PATH/models/cv/human_reconstruction/Reconstruction.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
                 "PIL",
                 "skimage",
+                "os",
+                "cv2",
                 "torchvision"
             ],
             "module": "modelscope.models.cv.human_reconstruction.Reconstruction"
         },
         "('MODELS', 'human-wholebody-keypoint', 'human-wholebody-keypoint')": {
             "filepath": "TEMPLATE_PATH/models/cv/human_wholebody_keypoint/human_wholebody_keypoint.py",
             "imports": [
@@ -3752,17 +3861,17 @@
             "module": "modelscope.models.cv.human_wholebody_keypoint.human_wholebody_keypoint"
         },
         "('MODELS', 'image-body-reshaping', 'image-body-reshaping')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_body_reshaping/image_body_reshaping.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
+                "torch",
                 "os",
-                "torch"
+                "cv2"
             ],
             "module": "modelscope.models.cv.image_body_reshaping.image_body_reshaping"
         },
         "('MODELS', 'image-captioning', 'mplug')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/mplug_for_all_tasks.py",
             "imports": [
                 "typing",
@@ -3770,21 +3879,21 @@
             ],
             "module": "modelscope.models.multi_modal.mplug_for_all_tasks"
         },
         "('MODELS', 'image-captioning', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
                 "typing",
+                "re",
+                "string",
+                "torch",
                 "functools",
                 "os",
-                "torch",
-                "json",
                 "math",
-                "re",
-                "string"
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'image-classification', 'ClassificationModel')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_classification/mmcls_model.py",
             "imports": [
                 "os"
@@ -3807,54 +3916,54 @@
                 "torch"
             ],
             "module": "modelscope.models.cv.image_binary_quant_classification.binary_quant_model"
         },
         "('MODELS', 'image-classification', 'content-check')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_classification/resnet50_cc.py",
             "imports": [
+                "torch",
                 "collections",
                 "os",
-                "torch",
-                "math",
-                "torchvision"
+                "torchvision",
+                "math"
             ],
             "module": "modelscope.models.cv.image_classification.resnet50_cc"
         },
         "('MODELS', 'image-classification', 'image-probing-model')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_probing_model/model.py",
             "imports": [
                 "typing",
-                "json",
                 "os",
-                "torch"
+                "torch",
+                "json"
             ],
             "module": "modelscope.models.cv.image_probing_model.model"
         },
         "('MODELS', 'image-classification', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
                 "typing",
+                "re",
+                "string",
+                "torch",
                 "functools",
                 "os",
-                "torch",
-                "json",
                 "math",
-                "re",
-                "string"
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'image-color-enhancement', 'adaint')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_color_enhance/adaint/adaint.py",
             "imports": [
+                "numbers",
                 "typing",
-                "os",
                 "torch",
-                "numbers",
-                "torchvision"
+                "torchvision",
+                "os"
             ],
             "module": "modelscope.models.cv.image_color_enhance.adaint.adaint"
         },
         "('MODELS', 'image-color-enhancement', 'csrnet')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_color_enhance/image_color_enhance.py",
             "imports": [
                 "typing",
@@ -3873,17 +3982,17 @@
             "module": "modelscope.models.cv.image_color_enhance.deeplpf.deeplpf_image_color_enhance"
         },
         "('MODELS', 'image-colorization', 'ddcolor')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/ddcolor_for_image_colorization.py",
             "imports": [
                 "numpy",
                 "typing",
-                "os",
+                "torch",
                 "copy",
-                "torch"
+                "os"
             ],
             "module": "modelscope.models.cv.image_colorization.ddcolor.ddcolor_for_image_colorization"
         },
         "('MODELS', 'image-debanding', 'rrdb')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_debanding/rrdb/rrdb_image_debanding.py",
             "imports": [
                 "typing",
@@ -3901,16 +4010,16 @@
             ],
             "module": "modelscope.models.cv.image_deblur.nafnet_for_image_deblur"
         },
         "('MODELS', 'image-demoireing', 'image-restoration')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_restoration/image_restoration_model.py",
             "imports": [
                 "numpy",
-                "cv2",
                 "os",
+                "cv2",
                 "torch"
             ],
             "module": "modelscope.models.cv.image_restoration.image_restoration_model"
         },
         "('MODELS', 'image-denoising', 'nafnet')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_denoise/nafnet_for_image_denoise.py",
             "imports": [
@@ -3938,31 +4047,31 @@
             "module": "modelscope.models.cv.image_depth_estimation.newcrfs_model"
         },
         "('MODELS', 'image-driving-perception', 'yolopv2')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_driving_perception/image_driving_percetion_model.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
+                "torch",
                 "os",
-                "torch"
+                "cv2"
             ],
             "module": "modelscope.models.cv.image_driving_perception.image_driving_percetion_model"
         },
         "('MODELS', 'image-face-fusion', 'image-face-fusion')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_face_fusion/image_face_fusion.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
-                "torchvision",
                 "PIL",
-                "collections"
+                "collections",
+                "os",
+                "cv2",
+                "torchvision"
             ],
             "module": "modelscope.models.cv.image_face_fusion.image_face_fusion"
         },
         "('MODELS', 'image-fewshot-detection', 'defrcn')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/defrcn_for_fewshot.py",
             "imports": [
                 "typing",
@@ -3980,29 +4089,29 @@
             ],
             "module": "modelscope.models.cv.image_inpainting.model"
         },
         "('MODELS', 'image-matching', 'quadtree-attention-image-matching')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_matching/quadtree_attention_model.py",
             "imports": [
                 "numpy",
-                "cv2",
+                "torch",
                 "os",
-                "pathlib",
-                "torch"
+                "cv2",
+                "pathlib"
             ],
             "module": "modelscope.models.cv.image_matching.quadtree_attention_model"
         },
         "('MODELS', 'image-multi-view-depth-estimation', 'image-casmvs-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/casmvs_model.py",
             "imports": [
                 "numpy",
-                "cv2",
+                "torch",
                 "os",
-                "easydict",
-                "torch"
+                "cv2",
+                "easydict"
             ],
             "module": "modelscope.models.cv.image_mvs_depth_estimation.casmvs_model"
         },
         "('MODELS', 'image-object-detection', 'DINO')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/dino.py",
             "imports": [
                 "easycv"
@@ -4059,27 +4168,27 @@
             ],
             "module": "modelscope.models.cv.vidt.model"
         },
         "('MODELS', 'image-paintbyexample', 'Stablediffusion-Paintbyexample')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_paintbyexample/model.py",
             "imports": [
                 "typing",
-                "paint_ldm",
-                "os",
                 "torch",
-                "omegaconf"
+                "omegaconf",
+                "os",
+                "paint_ldm"
             ],
             "module": "modelscope.models.cv.image_paintbyexample.model"
         },
         "('MODELS', 'image-portrait-enhancement', 'gpen')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_portrait_enhancement/image_portrait_enhancement.py",
             "imports": [
                 "typing",
-                "math",
                 "os",
+                "math",
                 "torch"
             ],
             "module": "modelscope.models.cv.image_portrait_enhancement.image_portrait_enhancement"
         },
         "('MODELS', 'image-quality-assessment-degradation', 'image-quality-assessment-degradation')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py",
             "imports": [
@@ -4106,17 +4215,17 @@
                 "torch"
             ],
             "module": "modelscope.models.cv.image_quality_assessment_mos.image_quality_assessment_mos"
         },
         "('MODELS', 'image-reid-person', 'passvitb')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_reid_person/pass_model.py",
             "imports": [
+                "torch",
                 "os",
-                "enum",
-                "torch"
+                "enum"
             ],
             "module": "modelscope.models.cv.image_reid_person.pass_model"
         },
         "('MODELS', 'image-segmentation', 'Segformer')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/segformer.py",
             "imports": [
                 "easycv"
@@ -4174,17 +4283,17 @@
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.semantic_seg_model"
         },
         "('MODELS', 'image-segmentation', 'vision-middleware')": {
             "filepath": "TEMPLATE_PATH/models/cv/vision_middleware/model.py",
             "imports": [
                 "typing",
-                "json",
                 "os",
-                "torch"
+                "torch",
+                "json"
             ],
             "module": "modelscope.models.cv.vision_middleware.model"
         },
         "('MODELS', 'image-segmentation', 'vitadapter-semantic-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/semantic_seg_model.py",
             "imports": [
                 "numpy",
@@ -4192,23 +4301,23 @@
                 "torch"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.semantic_seg_model"
         },
         "('MODELS', 'image-skychange', 'image-skychange')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_skychange/skychange_model.py",
             "imports": [
-                "typing",
                 "time",
-                "cv2",
-                "os",
+                "pdb",
+                "typing",
                 "torch",
+                "collections",
+                "os",
+                "cv2",
                 "math",
-                "json",
-                "pdb",
-                "collections"
+                "json"
             ],
             "module": "modelscope.models.cv.image_skychange.skychange_model"
         },
         "('MODELS', 'image-super-resolution', 'ecbsr')": {
             "filepath": "TEMPLATE_PATH/models/cv/super_resolution/ecbsr_model.py",
             "imports": [
                 "typing",
@@ -4267,32 +4376,32 @@
                 "tempfile"
             ],
             "module": "modelscope.models.audio.kws.farfield.model"
         },
         "('MODELS', 'keyword-spotting', 'speech_kws_fsmn_char_ctc_nearfield')": {
             "filepath": "TEMPLATE_PATH/models/audio/kws/nearfield/model.py",
             "imports": [
+                "sys",
                 "typing",
-                "os",
-                "tempfile",
                 "torch",
-                "sys"
+                "tempfile",
+                "os"
             ],
             "module": "modelscope.models.audio.kws.nearfield.model"
         },
         "('MODELS', 'language-guided-video-summarization', 'clip-it-language-guided-video-summarization')": {
             "filepath": "TEMPLATE_PATH/models/cv/language_guided_video_summarization/summarizer.py",
             "imports": [
+                "argparse",
                 "numpy",
+                "videofeatures_clipit",
+                "bmt_clipit",
                 "typing",
-                "os",
                 "torch",
-                "bmt_clipit",
-                "videofeatures_clipit",
-                "argparse"
+                "os"
             ],
             "module": "modelscope.models.cv.language_guided_video_summarization.summarizer"
         },
         "('MODELS', 'language-score-prediction', 'generic-asr')": {
             "filepath": "TEMPLATE_PATH/models/audio/asr/generic_automatic_speech_recognition.py",
             "imports": [
                 "typing",
@@ -4301,58 +4410,58 @@
             "module": "modelscope.models.audio.asr.generic_automatic_speech_recognition"
         },
         "('MODELS', 'lineless-table-recognition', 'LoreModel')": {
             "filepath": "TEMPLATE_PATH/models/cv/table_recognition/model_lore.py",
             "imports": [
                 "numpy",
                 "typing",
-                "os",
-                "copy",
                 "torch",
+                "copy",
+                "os",
                 "math"
             ],
             "module": "modelscope.models.cv.table_recognition.model_lore"
         },
         "('MODELS', 'movie-scene-segmentation', 'resnet50-bert')": {
             "filepath": "TEMPLATE_PATH/models/cv/movie_scene_segmentation/model.py",
             "imports": [
                 "numpy",
                 "typing",
-                "os",
                 "torch",
-                "math",
-                "shotdetect_scenedetect_lgss",
-                "einops",
                 "PIL",
-                "torchvision"
+                "os",
+                "torchvision",
+                "shotdetect_scenedetect_lgss",
+                "math",
+                "einops"
             ],
             "module": "modelscope.models.cv.movie_scene_segmentation.model"
         },
         "('MODELS', 'multi-modal-embedding', 'clip-multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/clip/model.py",
             "imports": [
                 "numpy",
                 "typing",
-                "os",
                 "torch",
-                "json",
-                "collections"
+                "collections",
+                "os",
+                "json"
             ],
             "module": "modelscope.models.multi_modal.clip.model"
         },
         "('MODELS', 'multi-modal-similarity', 'team-multi-modal-similarity')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/team/team_model.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "torch",
                 "PIL",
+                "tokenizers",
                 "torchvision",
-                "tokenizers"
+                "cv2"
             ],
             "module": "modelscope.models.multi_modal.team.team_model"
         },
         "('MODELS', 'named-entity-recognition', 'lstm-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/lstm/token_classification.py",
             "imports": [],
             "module": "modelscope.models.nlp.lstm.token_classification"
@@ -4374,18 +4483,18 @@
             "module": "modelscope.models.nlp.task_models.token_classification"
         },
         "('MODELS', 'nerf-recon-acc', 'nerf-recon-acc')": {
             "filepath": "TEMPLATE_PATH/models/cv/nerf_recon_acc/nerf_recon_acc.py",
             "imports": [
                 "numpy",
                 "time",
-                "cv2",
-                "os",
-                "torch",
                 "glob",
+                "torch",
+                "os",
+                "cv2",
                 "tqdm"
             ],
             "module": "modelscope.models.cv.nerf_recon_acc.nerf_recon_acc"
         },
         "('MODELS', 'nli', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/text_classification.py",
             "imports": [],
@@ -4455,44 +4564,44 @@
             ],
             "module": "modelscope.models.cv.ocr_recognition.model"
         },
         "('MODELS', 'ocr-recognition', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
                 "typing",
+                "re",
+                "string",
+                "torch",
                 "functools",
                 "os",
-                "torch",
-                "json",
                 "math",
-                "re",
-                "string"
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'open-vocabulary-detection', 'open-vocabulary-detection-vild')": {
             "filepath": "TEMPLATE_PATH/models/cv/open_vocabulary_detection_vild/vild.py",
             "imports": [
                 "numpy",
-                "typing",
-                "clip",
                 "scipy",
-                "os",
+                "typing",
                 "torch",
+                "os",
+                "clip",
                 "tensorflow"
             ],
             "module": "modelscope.models.cv.open_vocabulary_detection_vild.vild"
         },
         "('MODELS', 'panorama-depth-estimation', 'unifuse-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/models/cv/panorama_depth_estimation/unifuse_model.py",
             "imports": [
                 "numpy",
-                "torchvision",
                 "os",
-                "torch"
+                "torch",
+                "torchvision"
             ],
             "module": "modelscope.models.cv.panorama_depth_estimation.unifuse_model"
         },
         "('MODELS', 'part-of-speech', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/token_classification.py",
             "imports": [],
             "module": "modelscope.models.nlp.bert.token_classification"
@@ -4528,14 +4637,24 @@
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/token_classification.py",
             "imports": [
                 "typing",
                 "torch"
             ],
             "module": "modelscope.models.nlp.task_models.token_classification"
         },
+        "('MODELS', 'pedestrian-attribute-recognition', 'pedestrian-attribute-recognition')": {
+            "filepath": "TEMPLATE_PATH/models/cv/pedestrian_attribute_recognition/model.py",
+            "imports": [
+                "numpy",
+                "os",
+                "torch",
+                "torchvision"
+            ],
+            "module": "modelscope.models.cv.pedestrian_attribute_recognition.model"
+        },
         "('MODELS', 'pointcloud-sceneflow-estimation', 'rcp-sceneflow-estimation')": {
             "filepath": "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/rcp_model.py",
             "imports": [
                 "numpy",
                 "os",
                 "torch"
             ],
@@ -4552,24 +4671,24 @@
             "module": "modelscope.models.cv.product_retrieval_embedding.item_model"
         },
         "('MODELS', 'product-segmentation', 'product-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/product_segmentation/seg_infer.py",
             "imports": [
                 "numpy",
                 "cv2",
-                "PIL",
-                "torch"
+                "torch",
+                "PIL"
             ],
             "module": "modelscope.models.cv.product_segmentation.seg_infer"
         },
         "('MODELS', 'protein-structure', 'unifold')": {
             "filepath": "TEMPLATE_PATH/models/science/unifold/model.py",
             "imports": [
-                "typing",
                 "argparse",
+                "typing",
                 "os",
                 "torch"
             ],
             "module": "modelscope.models.science.unifold.model"
         },
         "('MODELS', 'punctuation', 'generic-punc')": {
             "filepath": "TEMPLATE_PATH/models/audio/punc/generic_punctuation.py",
@@ -4597,26 +4716,26 @@
             "module": "modelscope.models.nlp.task_models.information_extraction"
         },
         "('MODELS', 'semantic-segmentation', 'ddpm')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py",
             "imports": [
                 "typing",
                 "os",
-                "ddpm_guided_diffusion",
-                "torch"
+                "torch",
+                "ddpm_guided_diffusion"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.ddpm_segmentation_model"
         },
         "('MODELS', 'semantic-segmentation', 'detection')": {
             "filepath": "TEMPLATE_PATH/models/cv/salient_detection/salient_model.py",
             "imports": [
-                "cv2",
-                "os",
                 "torch",
                 "PIL",
+                "os",
+                "cv2",
                 "torchvision"
             ],
             "module": "modelscope.models.cv.salient_detection.salient_model"
         },
         "('MODELS', 'sentence-embedding', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/sentence_embedding.py",
             "imports": [
@@ -4707,18 +4826,18 @@
             "module": "modelscope.models.nlp.veco.text_classification"
         },
         "('MODELS', 'shop-segmentation', 'shop-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/shop_segmentation/shop_seg_model.py",
             "imports": [
                 "numpy",
                 "typing",
-                "os",
                 "torch",
-                "json",
-                "PIL"
+                "PIL",
+                "os",
+                "json"
             ],
             "module": "modelscope.models.cv.shop_segmentation.shop_seg_model"
         },
         "('MODELS', 'siamese-uie', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/siamese_uie.py",
             "imports": [
                 "copy",
@@ -4730,22 +4849,33 @@
             "filepath": "TEMPLATE_PATH/models/audio/sv/generic_speaker_verification.py",
             "imports": [
                 "typing",
                 "os"
             ],
             "module": "modelscope.models.audio.sv.generic_speaker_verification"
         },
+        "('MODELS', 'speaker-verification', 'cam++-sv')": {
+            "filepath": "TEMPLATE_PATH/models/audio/sv/DTDNN.py",
+            "imports": [
+                "torchaudio",
+                "typing",
+                "torch",
+                "collections",
+                "os"
+            ],
+            "module": "modelscope.models.audio.sv.DTDNN"
+        },
         "('MODELS', 'speaker-verification', 'ecapa-tdnn-sv')": {
             "filepath": "TEMPLATE_PATH/models/audio/sv/ecapa_tdnn.py",
             "imports": [
+                "torchaudio",
                 "typing",
-                "os",
                 "torch",
-                "math",
-                "torchaudio"
+                "os",
+                "math"
             ],
             "module": "modelscope.models.audio.sv.ecapa_tdnn"
         },
         "('MODELS', 'speaker-verification', 'generic-sv')": {
             "filepath": "TEMPLATE_PATH/models/audio/sv/generic_speaker_verification.py",
             "imports": [
                 "typing",
@@ -4753,16 +4883,16 @@
             ],
             "module": "modelscope.models.audio.sv.generic_speaker_verification"
         },
         "('MODELS', 'speech-separation', 'speech_mossformer_separation_temporal_8k')": {
             "filepath": "TEMPLATE_PATH/models/audio/separation/mossformer.py",
             "imports": [
                 "typing",
-                "os",
                 "copy",
+                "os",
                 "torch"
             ],
             "module": "modelscope.models.audio.separation.mossformer"
         },
         "('MODELS', 'speech-timestamp', 'generic-asr')": {
             "filepath": "TEMPLATE_PATH/models/audio/asr/generic_automatic_speech_recognition.py",
             "imports": [
@@ -4771,32 +4901,32 @@
             ],
             "module": "modelscope.models.audio.asr.generic_automatic_speech_recognition"
         },
         "('MODELS', 'sudoku', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
                 "typing",
+                "re",
+                "string",
+                "torch",
                 "functools",
                 "os",
-                "torch",
-                "json",
                 "math",
-                "re",
-                "string"
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'table-question-answering', 'space-T-cn')": {
             "filepath": "TEMPLATE_PATH/models/nlp/space_T_cn/table_question_answering.py",
             "imports": [
                 "numpy",
                 "typing",
-                "transformers",
+                "torch",
                 "os",
-                "torch"
+                "transformers"
             ],
             "module": "modelscope.models.nlp.space_T_cn.table_question_answering"
         },
         "('MODELS', 'table-question-answering', 'space-T-en')": {
             "filepath": "TEMPLATE_PATH/models/nlp/space_T_en/text_to_sql.py",
             "imports": [
                 "typing",
@@ -4806,16 +4936,16 @@
             ],
             "module": "modelscope.models.nlp.space_T_en.text_to_sql"
         },
         "('MODELS', 'task-oriented-conversation', 'space-dst')": {
             "filepath": "TEMPLATE_PATH/models/nlp/space/dialog_state_tracking.py",
             "imports": [
                 "typing",
-                "transformers",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.space.dialog_state_tracking"
         },
         "('MODELS', 'task-oriented-conversation', 'space-intent')": {
             "filepath": "TEMPLATE_PATH/models/nlp/space/dialog_intent_prediction.py",
             "imports": [
                 "typing",
@@ -4843,21 +4973,21 @@
             ],
             "module": "modelscope.models.multi_modal.mgeo.text_classification"
         },
         "('MODELS', 'text-classification', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
                 "typing",
+                "re",
+                "string",
+                "torch",
                 "functools",
                 "os",
-                "torch",
-                "json",
                 "math",
-                "re",
-                "string"
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'text-classification', 'peer')": {
             "filepath": "TEMPLATE_PATH/models/nlp/peer/text_classification.py",
             "imports": [
                 "copy",
@@ -4888,17 +5018,17 @@
             "module": "modelscope.models.nlp.task_models.text_classification"
         },
         "('MODELS', 'text-classification', 'user-satisfaction-estimation')": {
             "filepath": "TEMPLATE_PATH/models/nlp/use/user_satisfaction_estimation.py",
             "imports": [
                 "numpy",
                 "typing",
-                "transformers",
+                "torch",
                 "os",
-                "torch"
+                "transformers"
             ],
             "module": "modelscope.models.nlp.use.user_satisfaction_estimation"
         },
         "('MODELS', 'text-classification', 'veco')": {
             "filepath": "TEMPLATE_PATH/models/nlp/veco/text_classification.py",
             "imports": [
                 "transformers"
@@ -4906,70 +5036,96 @@
             "module": "modelscope.models.nlp.veco.text_classification"
         },
         "('MODELS', 'text-driven-segmentation', 'text-driven-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_model.py",
             "imports": [
                 "numpy",
                 "typing",
-                "os",
                 "torch",
-                "json",
-                "PIL"
+                "PIL",
+                "os",
+                "json"
             ],
             "module": "modelscope.models.cv.text_driven_segmentation.lseg_model"
         },
         "('MODELS', 'text-error-correction', 'bart')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bart/text_error_correction.py",
             "imports": [
                 "typing",
                 "os",
                 "torch"
             ],
             "module": "modelscope.models.nlp.bart.text_error_correction"
         },
+        "('MODELS', 'text-generation', 'glm130b')": {
+            "filepath": "TEMPLATE_PATH/models/nlp/glm_130b/text_generation.py",
+            "imports": [
+                "time",
+                "sys",
+                "typing",
+                "re",
+                "torch",
+                "functools",
+                "SwissArmyTransformer",
+                "copy",
+                "os",
+                "random",
+                "stat"
+            ],
+            "module": "modelscope.models.nlp.glm_130b.text_generation"
+        },
         "('MODELS', 'text-generation', 'gpt-moe')": {
             "filepath": "TEMPLATE_PATH/models/nlp/gpt_moe/text_generation.py",
             "imports": [
                 "typing",
                 "transformers"
             ],
             "module": "modelscope.models.nlp.gpt_moe.text_generation"
         },
         "('MODELS', 'text-generation', 'gpt3')": {
             "filepath": "TEMPLATE_PATH/models/nlp/gpt3/text_generation.py",
             "imports": [
-                "typing",
-                "transformers",
                 "collections",
-                "torch"
+                "typing",
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.gpt3.text_generation"
         },
+        "('MODELS', 'text-generation', 'llama')": {
+            "filepath": "TEMPLATE_PATH/models/nlp/llama/text_generation.py",
+            "imports": [
+                "typing",
+                "torch"
+            ],
+            "module": "modelscope.models.nlp.llama.text_generation"
+        },
         "('MODELS', 'text-generation', 'palm-v2')": {
             "filepath": "TEMPLATE_PATH/models/nlp/palm_v2/text_generation.py",
             "imports": [
                 "numpy",
+                "dataclasses",
                 "typing",
-                "transformers",
-                "subprocess",
-                "os",
-                "copy",
                 "torch",
+                "codecs",
+                "copy",
+                "os",
+                "subprocess",
                 "math",
                 "json",
-                "dataclasses",
-                "codecs"
+                "transformers"
             ],
             "module": "modelscope.models.nlp.palm_v2.text_generation"
         },
         "('MODELS', 'text-generation', 'text-generation')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/text_generation.py",
             "imports": [
                 "numpy",
                 "typing",
+                "torch",
                 "transformers"
             ],
             "module": "modelscope.models.nlp.task_models.text_generation"
         },
         "('MODELS', 'text-ranking', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/text_ranking.py",
             "imports": [],
@@ -4991,123 +5147,123 @@
             "module": "modelscope.models.nlp.task_models.text_ranking"
         },
         "('MODELS', 'text-summarization', 'mglm')": {
             "filepath": "TEMPLATE_PATH/models/nlp/mglm/mglm_for_text_summarization.py",
             "imports": [
                 "numpy",
                 "typing",
-                "megatron_util",
-                "random",
+                "torch",
                 "os",
-                "torch"
+                "megatron_util",
+                "random"
             ],
             "module": "modelscope.models.nlp.mglm.mglm_for_text_summarization"
         },
         "('MODELS', 'text-summarization', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
                 "typing",
+                "re",
+                "string",
+                "torch",
                 "functools",
                 "os",
-                "torch",
-                "json",
                 "math",
-                "re",
-                "string"
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'text-to-image-synthesis', 'diffusion-text-to-image-synthesis')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/diffusion/model.py",
             "imports": [
                 "numpy",
                 "typing",
-                "os",
                 "torch",
+                "os",
                 "json"
             ],
             "module": "modelscope.models.multi_modal.diffusion.model"
         },
         "('MODELS', 'text-to-image-synthesis', 'multi-stage-diffusion-text-to-image-synthesis')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/model.py",
             "imports": [
                 "numpy",
                 "typing",
-                "os",
                 "torch",
-                "json",
+                "PIL",
+                "os",
                 "math",
-                "PIL"
+                "json"
             ],
             "module": "modelscope.models.multi_modal.multi_stage_diffusion.model"
         },
         "('MODELS', 'text-to-image-synthesis', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_text_to_image_synthesis_model.py",
             "imports": [
                 "numpy",
+                "taming",
                 "typing",
                 "pkg_resources",
-                "os",
                 "torch",
-                "taming",
-                "json",
                 "PIL",
-                "torchvision"
+                "os",
+                "torchvision",
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_text_to_image_synthesis_model"
         },
         "('MODELS', 'text-to-speech', 'sambert-hifigan')": {
             "filepath": "TEMPLATE_PATH/models/audio/tts/sambert_hifi.py",
             "imports": [
                 "numpy",
-                "__future__",
-                "shutil",
-                "os",
-                "zipfile",
-                "json",
                 "wave",
                 "yaml",
+                "shutil",
+                "__future__",
                 "matplotlib",
+                "zipfile",
+                "os",
+                "json",
                 "datetime"
             ],
             "module": "modelscope.models.audio.tts.sambert_hifi"
         },
         "('MODELS', 'text-to-video-synthesis', 'latent-text-to-video-synthesis')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py",
             "imports": [
                 "typing",
-                "open_clip",
-                "os",
                 "torch",
-                "einops"
+                "os",
+                "einops",
+                "open_clip"
             ],
             "module": "modelscope.models.multi_modal.video_synthesis.text_to_video_synthesis_model"
         },
         "('MODELS', 'text2sql', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
                 "typing",
+                "re",
+                "string",
+                "torch",
                 "functools",
                 "os",
-                "torch",
-                "json",
                 "math",
-                "re",
-                "string"
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'text2text-generation', 'T5')": {
             "filepath": "TEMPLATE_PATH/models/nlp/T5/text2text_generation.py",
             "imports": [
+                "warnings",
                 "typing",
-                "transformers",
-                "copy",
                 "torch",
-                "warnings"
+                "copy",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.T5.text2text_generation"
         },
         "('MODELS', 'token-classification', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/token_classification.py",
             "imports": [],
             "module": "modelscope.models.nlp.bert.token_classification"
@@ -5154,48 +5310,48 @@
                 "torch"
             ],
             "module": "modelscope.models.nlp.task_models.token_classification"
         },
         "('MODELS', 'token-classification', 'veco')": {
             "filepath": "TEMPLATE_PATH/models/nlp/veco/token_classification.py",
             "imports": [
-                "transformers",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.models.nlp.veco.token_classification"
         },
         "('MODELS', 'transformer-crf', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/token_classification.py",
             "imports": [
                 "typing",
                 "torch"
             ],
             "module": "modelscope.models.nlp.task_models.token_classification"
         },
         "('MODELS', 'translation', 'csanmt-translation')": {
             "filepath": "TEMPLATE_PATH/models/nlp/csanmt/translation.py",
             "imports": [
+                "collections",
                 "typing",
                 "tensorflow",
-                "collections",
                 "math"
             ],
             "module": "modelscope.models.nlp.csanmt.translation"
         },
         "('MODELS', 'translation-evaluation', 'unite')": {
             "filepath": "TEMPLATE_PATH/models/nlp/unite/modeling_unite.py",
             "imports": [
                 "numpy",
+                "warnings",
+                "dataclasses",
+                "packaging",
                 "typing",
-                "transformers",
                 "torch",
-                "packaging",
                 "math",
-                "dataclasses",
-                "warnings"
+                "transformers"
             ],
             "module": "modelscope.models.nlp.unite.modeling_unite"
         },
         "('MODELS', 'video-captioning', 'hitea')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/mplug_for_all_tasks.py",
             "imports": [
                 "typing",
@@ -5203,49 +5359,49 @@
             ],
             "module": "modelscope.models.multi_modal.mplug_for_all_tasks"
         },
         "('MODELS', 'video-deinterlace', 'video-deinterlace')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_deinterlace/UNet_for_video_deinterlace.py",
             "imports": [
                 "typing",
-                "os",
                 "copy",
+                "os",
                 "torch"
             ],
             "module": "modelscope.models.cv.video_deinterlace.UNet_for_video_deinterlace"
         },
         "('MODELS', 'video-depth-estimation', 'dro-resnet18-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_depth_estimation/dro_model.py",
             "imports": [
                 "numpy",
-                "cv2",
-                "os",
-                "torch",
                 "glob",
+                "torch",
+                "os",
+                "cv2",
                 "tqdm"
             ],
             "module": "modelscope.models.cv.video_depth_estimation.dro_model"
         },
         "('MODELS', 'video-frame-interpolation', 'video-frame-interpolation')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_frame_interpolation/VFINet_for_video_frame_interpolation.py",
             "imports": [
                 "typing",
-                "os",
                 "copy",
+                "os",
                 "torch"
             ],
             "module": "modelscope.models.cv.video_frame_interpolation.VFINet_for_video_frame_interpolation"
         },
         "('MODELS', 'video-human-matting', 'video-human-matting')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_human_matting/model.py",
             "imports": [
                 "numpy",
                 "typing",
-                "os",
                 "torch",
+                "os",
                 "torchvision"
             ],
             "module": "modelscope.models.cv.video_human_matting.model"
         },
         "('MODELS', 'video-inpainting', 'video-inpainting')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_inpainting/inpainting_model.py",
             "imports": [
@@ -5255,63 +5411,63 @@
                 "torch"
             ],
             "module": "modelscope.models.cv.video_inpainting.inpainting_model"
         },
         "('MODELS', 'video-instance-segmentation', 'swinb-video-instance-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/video_knet.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.video_knet"
         },
         "('MODELS', 'video-multi-modal-embedding', 'video-clip-multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/mmr/models/clip_for_mm_video_embedding.py",
             "imports": [
                 "numpy",
-                "typing",
                 "decord",
-                "random",
+                "typing",
                 "urllib",
-                "os",
-                "tempfile",
                 "torch",
-                "json",
                 "PIL",
-                "uuid"
+                "tempfile",
+                "os",
+                "uuid",
+                "random",
+                "json"
             ],
             "module": "modelscope.models.multi_modal.mmr.models.clip_for_mm_video_embedding"
         },
         "('MODELS', 'video-object-detection', 'longshortnet')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/longshortnet.py",
             "imports": [
+                "argparse",
+                "logging",
                 "numpy",
                 "time",
-                "cv2",
-                "os",
                 "torch",
-                "json",
+                "os",
+                "cv2",
                 "tqdm",
-                "argparse",
-                "logging"
+                "json"
             ],
             "module": "modelscope.models.cv.video_streaming_perception.longshortnet.longshortnet"
         },
         "('MODELS', 'video-object-detection', 'realtime-video-object-detection')": {
             "filepath": "TEMPLATE_PATH/models/cv/stream_yolo/realtime_video_detector.py",
             "imports": [
+                "argparse",
+                "logging",
                 "numpy",
                 "time",
-                "cv2",
-                "os",
                 "torch",
-                "json",
+                "os",
+                "cv2",
                 "tqdm",
-                "argparse",
-                "logging"
+                "json"
             ],
             "module": "modelscope.models.cv.stream_yolo.realtime_video_detector"
         },
         "('MODELS', 'video-object-segmentation', 'video-object-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_object_segmentation/model.py",
             "imports": [
                 "typing",
@@ -5320,17 +5476,17 @@
             ],
             "module": "modelscope.models.cv.video_object_segmentation.model"
         },
         "('MODELS', 'video-panoptic-segmentation', 'swinb-video-panoptic-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/video_k_net.py",
             "imports": [
                 "numpy",
-                "mmdet",
                 "mmcv",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_panoptic_segmentation.video_k_net"
         },
         "('MODELS', 'video-question-answering', 'hitea')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/mplug_for_all_tasks.py",
             "imports": [
                 "typing",
@@ -5338,20 +5494,20 @@
             ],
             "module": "modelscope.models.multi_modal.mplug_for_all_tasks"
         },
         "('MODELS', 'video-stabilization', 'video-stabilization')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_stabilization/DUTRAFTStabilizer.py",
             "imports": [
                 "numpy",
+                "sys",
                 "typing",
-                "cv2",
+                "torch",
                 "tempfile",
                 "os",
-                "torch",
-                "sys",
+                "cv2",
                 "math"
             ],
             "module": "modelscope.models.cv.video_stabilization.DUTRAFTStabilizer"
         },
         "('MODELS', 'video-summarization', 'pgl-video-summarization')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_summarization/summarizer.py",
             "imports": [
@@ -5362,17 +5518,17 @@
             ],
             "module": "modelscope.models.cv.video_summarization.summarizer"
         },
         "('MODELS', 'video-super-resolution', 'msrresnet-lite')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_super_resolution/msrresnet_lite_model.py",
             "imports": [
                 "typing",
-                "functools",
                 "os",
-                "torch"
+                "torch",
+                "functools"
             ],
             "module": "modelscope.models.cv.video_super_resolution.msrresnet_lite_model"
         },
         "('MODELS', 'video-super-resolution', 'real-basicvsr')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_super_resolution/real_basicvsr_for_video_super_resolution.py",
             "imports": [
                 "typing",
@@ -5413,35 +5569,35 @@
             ],
             "module": "modelscope.models.cv.vision_efficient_tuning.model"
         },
         "('MODELS', 'visual-entailment', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
                 "typing",
+                "re",
+                "string",
+                "torch",
                 "functools",
                 "os",
-                "torch",
-                "json",
                 "math",
-                "re",
-                "string"
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'visual-grounding', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
                 "typing",
+                "re",
+                "string",
+                "torch",
                 "functools",
                 "os",
-                "torch",
-                "json",
                 "math",
-                "re",
-                "string"
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'visual-question-answering', 'mplug')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/mplug_for_all_tasks.py",
             "imports": [
                 "typing",
@@ -5449,21 +5605,21 @@
             ],
             "module": "modelscope.models.multi_modal.mplug_for_all_tasks"
         },
         "('MODELS', 'visual-question-answering', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
                 "typing",
+                "re",
+                "string",
+                "torch",
                 "functools",
                 "os",
-                "torch",
-                "json",
                 "math",
-                "re",
-                "string"
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'voice-activity-detection', 'generic-asr')": {
             "filepath": "TEMPLATE_PATH/models/audio/asr/generic_automatic_speech_recognition.py",
             "imports": [
                 "typing",
@@ -5557,153 +5713,153 @@
             ],
             "module": "modelscope.models.nlp.structbert.text_classification"
         },
         "('NECKS', 'default', 'CPFPN')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/cp_fpn.py",
             "imports": [
                 "mmcv",
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.necks.cp_fpn"
         },
         "('NECKS', 'default', 'FPNF')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/necks/fpn.py",
             "imports": [
                 "mmcv",
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.necks.fpn"
         },
         "('NECKS', 'default', 'MSDeformAttnPixelDecoder')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/neck/msdeformattn_decoder.py",
             "imports": [
                 "mmcv",
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.neck.msdeformattn_decoder"
         },
         "('NECKS', 'default', 'SemanticFPNWrapper')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/semantic_fpn_wrapper.py",
             "imports": [
                 "mmcv",
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_panoptic_segmentation.head.semantic_fpn_wrapper"
         },
         "('OPTIMIZERS', 'default', 'ChildTuningAdamW')": {
             "filepath": "TEMPLATE_PATH/trainers/optimizer/child_tuning_adamw_optimizer.py",
             "imports": [
                 "numpy",
                 "typing",
+                "types",
                 "torch",
-                "math",
-                "types"
+                "math"
             ],
             "module": "modelscope.trainers.optimizer.child_tuning_adamw_optimizer"
         },
         "('PARALLEL', 'default', 'DistributedDataParallel')": {
             "filepath": "TEMPLATE_PATH/trainers/parallel/builder.py",
             "imports": [
                 "torch"
             ],
             "module": "modelscope.trainers.parallel.builder"
         },
         "('PIPELINES', 'acoustic-echo-cancellation', 'speech-dfsmn-aec-psm-16k')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/linear_aec_pipeline.py",
             "imports": [
                 "numpy",
-                "typing",
                 "scipy",
-                "os",
+                "typing",
                 "torch",
                 "importlib",
+                "os",
                 "yaml"
             ],
             "module": "modelscope.pipelines.audio.linear_aec_pipeline"
         },
         "('PIPELINES', 'acoustic-noise-suppression', 'speech_dfsmn_ans_psm_48k_causal')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/ans_dfsmn_pipeline.py",
             "imports": [
                 "numpy",
-                "typing",
-                "os",
-                "librosa",
-                "soundfile",
                 "sys",
+                "typing",
                 "torch",
                 "collections",
-                "io"
+                "io",
+                "os",
+                "soundfile",
+                "librosa"
             ],
             "module": "modelscope.pipelines.audio.ans_dfsmn_pipeline"
         },
         "('PIPELINES', 'acoustic-noise-suppression', 'speech_frcrn_ans_cirm_16k')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/ans_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "librosa",
-                "soundfile",
                 "torch",
-                "io"
+                "io",
+                "soundfile",
+                "librosa"
             ],
             "module": "modelscope.pipelines.audio.ans_pipeline"
         },
         "('PIPELINES', 'action-detection', 'ResNetC3D-action-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/action_detection_pipeline.py",
             "imports": [
                 "typing",
-                "math",
-                "os"
+                "os",
+                "math"
             ],
             "module": "modelscope.pipelines.cv.action_detection_pipeline"
         },
         "('PIPELINES', 'action-recognition', 'TAdaConv_action-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/action_recognition_pipeline.py",
             "imports": [
                 "typing",
-                "math",
                 "os",
+                "math",
                 "torch"
             ],
             "module": "modelscope.pipelines.cv.action_recognition_pipeline"
         },
         "('PIPELINES', 'action-recognition', 'patchshift-action-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/action_recognition_pipeline.py",
             "imports": [
                 "typing",
-                "math",
                 "os",
+                "math",
                 "torch"
             ],
             "module": "modelscope.pipelines.cv.action_recognition_pipeline"
         },
         "('PIPELINES', 'animal-recognition', 'resnet101-animal-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/animal_recognition_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
                 "PIL",
+                "os",
+                "cv2",
                 "torchvision"
             ],
             "module": "modelscope.pipelines.cv.animal_recognition_pipeline"
         },
         "('PIPELINES', 'auto-speech-recognition', 'asr-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/asr_inference_pipeline.py",
             "imports": [
-                "yaml",
                 "typing",
-                "json",
-                "os"
+                "os",
+                "yaml",
+                "json"
             ],
             "module": "modelscope.pipelines.audio.asr_inference_pipeline"
         },
         "('PIPELINES', 'auto-speech-recognition', 'asr-wenet-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/asr_wenet_inference_pipeline.py",
             "imports": [
                 "typing"
@@ -5728,35 +5884,35 @@
             "module": "modelscope.pipelines.cv.bad_image_detecting_pipeline"
         },
         "('PIPELINES', 'body-2d-keypoints', 'hrnetv2w32_body-2d-keypoints_image')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/body_2d_keypoints_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
-                "json",
                 "PIL",
-                "torchvision"
+                "os",
+                "cv2",
+                "torchvision",
+                "json"
             ],
             "module": "modelscope.pipelines.cv.body_2d_keypoints_pipeline"
         },
         "('PIPELINES', 'body-3d-keypoints', 'canonical_body-3d-keypoints_video')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/body_3d_keypoints_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
+                "torch",
+                "datetime",
+                "tempfile",
                 "mpl_toolkits",
-                "cv2",
                 "os",
-                "tempfile",
-                "torch",
-                "matplotlib",
-                "datetime"
+                "cv2",
+                "matplotlib"
             ],
             "module": "modelscope.pipelines.cv.body_3d_keypoints_pipeline"
         },
         "('PIPELINES', 'card-detection', 'resnet-card-detection-scrfd34gkps')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/card_detection_pipeline.py",
             "imports": [
                 "typing"
@@ -5773,48 +5929,58 @@
         "('PIPELINES', 'code-translation', 'codegeex-code-translation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/codegeex_code_translation_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.codegeex_code_translation_pipeline"
         },
+        "('PIPELINES', 'competency-aware-translation', 'canmt-translation')": {
+            "filepath": "TEMPLATE_PATH/pipelines/nlp/canmt_translation_pipeline.py",
+            "imports": [
+                "typing",
+                "os",
+                "sacremoses",
+                "torch"
+            ],
+            "module": "modelscope.pipelines.nlp.canmt_translation_pipeline"
+        },
         "('PIPELINES', 'controllable-image-generation', 'controllable-image-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/controllable_image_generation_pipeline.py",
             "imports": [
                 "numpy",
+                "glob",
                 "typing",
+                "torch",
+                "tempfile",
+                "os",
                 "subprocess",
                 "cv2",
-                "os",
-                "tempfile",
-                "torch",
-                "math",
-                "glob"
+                "math"
             ],
             "module": "modelscope.pipelines.cv.controllable_image_generation_pipeline"
         },
         "('PIPELINES', 'crowd-counting', 'hrnet-crowd-counting')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/crowd_counting_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
                 "torch",
-                "math",
                 "PIL",
-                "torchvision"
+                "torchvision",
+                "math"
             ],
             "module": "modelscope.pipelines.cv.crowd_counting_pipeline"
         },
         "('PIPELINES', 'default', 'DefaultFormatBundleV2')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py",
             "imports": [
                 "numpy",
-                "mmdet",
                 "mmcv",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.formating"
         },
         "('PIPELINES', 'default', 'LoadAnnotationsV2')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py",
             "imports": [
                 "numpy",
@@ -5823,150 +5989,150 @@
                 "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.loading"
         },
         "('PIPELINES', 'default', 'LoadMultiViewImageFromMultiSweepsFiles')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/loading.py",
             "imports": [
-                "mmcv",
                 "numpy",
+                "mmcv",
                 "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.pipelines.loading"
         },
         "('PIPELINES', 'default', 'NormalizeMultiviewImage')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py",
             "imports": [
                 "numpy",
-                "mmdet",
-                "PIL",
                 "mmcv",
-                "mmdet3d",
                 "copy",
-                "torch"
+                "mmdet3d",
+                "mmdet",
+                "torch",
+                "PIL"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.pipelines.transform_3d"
         },
         "('PIPELINES', 'default', 'PadMultiViewImage')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py",
             "imports": [
                 "numpy",
-                "mmdet",
-                "PIL",
                 "mmcv",
-                "mmdet3d",
                 "copy",
-                "torch"
+                "mmdet3d",
+                "mmdet",
+                "torch",
+                "PIL"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.pipelines.transform_3d"
         },
         "('PIPELINES', 'default', 'RandomFlipV2')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py",
             "imports": [
-                "mmcv",
                 "numpy",
+                "mmcv",
                 "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.transforms"
         },
         "('PIPELINES', 'default', 'RandomSquareCrop')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py",
             "imports": [
-                "mmcv",
                 "numpy",
+                "mmcv",
                 "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.transforms"
         },
         "('PIPELINES', 'default', 'ResizeCropFlipImage')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py",
             "imports": [
                 "numpy",
-                "mmdet",
-                "PIL",
                 "mmcv",
-                "mmdet3d",
                 "copy",
-                "torch"
+                "mmdet3d",
+                "mmdet",
+                "torch",
+                "PIL"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.pipelines.transform_3d"
         },
         "('PIPELINES', 'default', 'ResizeToMultiple')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/utils/data_process_func.py",
             "imports": [
                 "mmcv",
                 "mmdet"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.vit_adapter.utils.data_process_func"
         },
         "('PIPELINES', 'default', 'ResizeV2')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py",
             "imports": [
-                "mmcv",
                 "numpy",
+                "mmcv",
                 "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.transforms"
         },
         "('PIPELINES', 'default', 'RotateV2')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py",
             "imports": [
                 "numpy",
-                "mmdet",
                 "mmcv",
+                "copy",
                 "cv2",
-                "copy"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.auto_augment"
         },
         "('PIPELINES', 'document-grounded-dialog-generate', 'document-grounded-dialog-generate')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_generate_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.document_grounded_dialog_generate_pipeline"
         },
         "('PIPELINES', 'document-grounded-dialog-rerank', 'document-grounded-dialog-rerank')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_rerank_pipeline.py",
             "imports": [
                 "numpy",
-                "typing",
-                "transformers",
-                "random",
                 "time",
-                "os",
-                "torch",
-                "sys",
-                "pprint",
                 "ujson",
+                "sys",
+                "typing",
                 "re",
-                "collections"
+                "torch",
+                "collections",
+                "os",
+                "random",
+                "pprint",
+                "transformers"
             ],
             "module": "modelscope.pipelines.nlp.document_grounded_dialog_rerank_pipeline"
         },
         "('PIPELINES', 'document-grounded-dialog-retrieval', 'document-grounded-dialog-retrieval')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_retrieval_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
                 "os",
-                "json",
-                "faiss"
+                "faiss",
+                "json"
             ],
             "module": "modelscope.pipelines.nlp.document_grounded_dialog_retrieval_pipeline"
         },
         "('PIPELINES', 'document-segmentation', 'document-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/document_segmentation_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "transformers",
-                "datasets",
+                "re",
                 "torch",
-                "re"
+                "datasets",
+                "transformers"
             ],
             "module": "modelscope.pipelines.nlp.document_segmentation_pipeline"
         },
         "('PIPELINES', 'document-vl-embedding', 'document-vl-embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/document_vl_embedding_pipeline.py",
             "imports": [
                 "typing",
@@ -5984,57 +6150,69 @@
         "('PIPELINES', 'domain-specific-object-detection', 'yolox-pai_hand-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/detection_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.easycv_pipelines.detection_pipeline"
         },
+        "('PIPELINES', 'efficient-diffusion-tuning', 'efficient-diffusion-tuning')": {
+            "filepath": "TEMPLATE_PATH/pipelines/multi_modal/efficient_diffusion_tuning_pipeline.py",
+            "imports": [
+                "numpy",
+                "typing",
+                "torch",
+                "PIL",
+                "torchvision",
+                "cv2"
+            ],
+            "module": "modelscope.pipelines.multi_modal.efficient_diffusion_tuning_pipeline"
+        },
         "('PIPELINES', 'extractive-summarization', 'extractive-summarization')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/extractive_summarization_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "datasets",
+                "re",
                 "torch",
-                "re"
+                "datasets"
             ],
             "module": "modelscope.pipelines.nlp.extractive_summarization_pipeline"
         },
         "('PIPELINES', 'face-2d-keypoints', 'manual-facial-landmark-confidence-flcm')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/facial_landmark_confidence_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
-                "PIL"
+                "PIL",
+                "os",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.facial_landmark_confidence_pipeline"
         },
         "('PIPELINES', 'face-2d-keypoints', 'mobilenet_face-2d-keypoints_alignment')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/face_2d_keypoints_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "copy",
+                "cv2",
                 "math"
             ],
             "module": "modelscope.pipelines.cv.easycv_pipelines.face_2d_keypoints_pipeline"
         },
         "('PIPELINES', 'face-attribute-recognition', 'resnet34-face-attribute-recognition-fairface')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_attribute_recognition_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
-                "PIL"
+                "PIL",
+                "os",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.face_attribute_recognition_pipeline"
         },
         "('PIPELINES', 'face-detection', 'manual-face-detection-mtcnn')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/mtcnn_face_detection_pipeline.py",
             "imports": [
                 "typing",
@@ -6044,30 +6222,30 @@
             "module": "modelscope.pipelines.cv.mtcnn_face_detection_pipeline"
         },
         "('PIPELINES', 'face-detection', 'manual-face-detection-ulfd')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/ulfd_face_detection_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
-                "PIL"
+                "PIL",
+                "os",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.ulfd_face_detection_pipeline"
         },
         "('PIPELINES', 'face-detection', 'resnet-face-detection-scrfd10gkps')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_detection_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
-                "PIL"
+                "PIL",
+                "os",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.face_detection_pipeline"
         },
         "('PIPELINES', 'face-detection', 'resnet101-face-detection-cvpr22papermogface')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/mog_face_detection_pipeline.py",
             "imports": [
                 "numpy",
@@ -6077,18 +6255,18 @@
             "module": "modelscope.pipelines.cv.mog_face_detection_pipeline"
         },
         "('PIPELINES', 'face-detection', 'resnet50-face-detection-retinaface')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/retina_face_detection_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
-                "PIL"
+                "PIL",
+                "os",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.retina_face_detection_pipeline"
         },
         "('PIPELINES', 'face-emotion', 'face-emotion')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_emotion_pipeline.py",
             "imports": [
                 "numpy",
@@ -6105,160 +6283,161 @@
             "module": "modelscope.pipelines.cv.face_human_hand_detection_pipeline"
         },
         "('PIPELINES', 'face-image-generation', 'gan-face-image-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_image_generation_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
-                "PIL"
+                "PIL",
+                "os",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.face_image_generation_pipeline"
         },
         "('PIPELINES', 'face-liveness', 'manual-face-liveness-flir')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_liveness_ir_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
-                "torch",
                 "PIL",
+                "torch",
+                "os",
+                "cv2",
                 "onnxruntime"
             ],
             "module": "modelscope.pipelines.cv.face_liveness_ir_pipeline"
         },
         "('PIPELINES', 'face-liveness', 'manual-face-liveness-flxc')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_liveness_xc_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
-                "torch",
                 "PIL",
+                "torch",
+                "os",
+                "cv2",
                 "onnxruntime"
             ],
             "module": "modelscope.pipelines.cv.face_liveness_xc_pipeline"
         },
         "('PIPELINES', 'face-quality-assessment', 'manual-face-quality-assessment-fqa')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_quality_assessment_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
-                "torch",
                 "PIL",
+                "torch",
+                "os",
+                "cv2",
                 "onnxruntime"
             ],
             "module": "modelscope.pipelines.cv.face_quality_assessment_pipeline"
         },
         "('PIPELINES', 'face-recognition', 'ir-face-recognition-rts')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_recognition_ood_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
-                "PIL"
+                "PIL",
+                "os",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.face_recognition_ood_pipeline"
         },
         "('PIPELINES', 'face-recognition', 'ir101-face-recognition-cfglint')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_recognition_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
-                "PIL"
+                "PIL",
+                "os",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.face_recognition_pipeline"
         },
         "('PIPELINES', 'face-recognition', 'ir50-face-recognition-arcface')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/arc_face_recognition_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
-                "PIL"
+                "PIL",
+                "os",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.arc_face_recognition_pipeline"
         },
         "('PIPELINES', 'face-recognition', 'manual-face-recognition-frfm')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_recognition_onnx_fm_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
-                "torch",
                 "PIL",
+                "torch",
+                "os",
+                "cv2",
                 "onnxruntime"
             ],
             "module": "modelscope.pipelines.cv.face_recognition_onnx_fm_pipeline"
         },
         "('PIPELINES', 'face-recognition', 'manual-face-recognition-frir')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_recognition_onnx_ir_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
-                "torch",
                 "PIL",
+                "torch",
+                "os",
+                "cv2",
                 "onnxruntime"
             ],
             "module": "modelscope.pipelines.cv.face_recognition_onnx_ir_pipeline"
         },
         "('PIPELINES', 'face-recognition', 'resnet-face-recognition-facemask')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/mask_face_recognition_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
                 "PIL",
-                "collections"
+                "collections",
+                "os",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.mask_face_recognition_pipeline"
         },
         "('PIPELINES', 'face-reconstruction', 'resnet50-face-reconstruction')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_reconstruction_pipeline.py",
             "imports": [
                 "numpy",
-                "typing",
                 "scipy",
+                "typing",
                 "shutil",
-                "cv2",
-                "os",
-                "torch",
+                "face_alignment",
                 "PIL",
-                "tensorflow",
-                "face_alignment"
+                "torch",
+                "io",
+                "os",
+                "cv2",
+                "tensorflow"
             ],
             "module": "modelscope.pipelines.cv.face_reconstruction_pipeline"
         },
         "('PIPELINES', 'facial-expression-recognition', 'vgg19-facial-expression-recognition-fer')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/facial_expression_recognition_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
-                "PIL"
+                "PIL",
+                "os",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.facial_expression_recognition_pipeline"
         },
         "('PIPELINES', 'faq-question-answering', 'faq-question-answering')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/faq_question_answering_pipeline.py",
             "imports": [
                 "typing"
@@ -6273,16 +6452,16 @@
                 "torch"
             ],
             "module": "modelscope.pipelines.nlp.feature_extraction_pipeline"
         },
         "('PIPELINES', 'fid-dialogue', 'fid-dialogue')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/fid_dialogue_pipeline.py",
             "imports": [
-                "re",
                 "typing",
+                "re",
                 "torch"
             ],
             "module": "modelscope.pipelines.nlp.fid_dialogue_pipeline"
         },
         "('PIPELINES', 'fill-mask', 'fill-mask')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/fill_mask_pipeline.py",
             "imports": [
@@ -6300,18 +6479,18 @@
             "module": "modelscope.pipelines.nlp.fill_mask_pipeline"
         },
         "('PIPELINES', 'general-recognition', 'resnet101-general-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/general_recognition_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
                 "PIL",
+                "os",
+                "cv2",
                 "torchvision"
             ],
             "module": "modelscope.pipelines.cv.general_recognition_pipeline"
         },
         "('PIPELINES', 'generative-multi-modal-embedding', 'generative-multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py",
             "imports": [
@@ -6342,19 +6521,19 @@
             ],
             "module": "modelscope.pipelines.cv.image_detection_pipeline"
         },
         "('PIPELINES', 'human-reconstruction', 'human-reconstruction')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/human_reconstruction_pipeline.py",
             "imports": [
                 "numpy",
+                "trimesh",
                 "typing",
                 "shutil",
-                "trimesh",
-                "os",
-                "torch"
+                "torch",
+                "os"
             ],
             "module": "modelscope.pipelines.cv.human_reconstruction_pipeline"
         },
         "('PIPELINES', 'human-wholebody-keypoint', 'hrnetw48_human-wholebody-keypoint_image')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/human_wholebody_keypoint_pipeline.py",
             "imports": [
                 "typing",
@@ -6422,20 +6601,20 @@
             ],
             "module": "modelscope.pipelines.cv.image_classification_pipeline"
         },
         "('PIPELINES', 'image-classification', 'image-structured-model-probing')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_structured_model_probing_pipeline.py",
             "imports": [
                 "numpy",
-                "typing",
                 "mmcv",
-                "os",
+                "typing",
                 "torch",
-                "math",
-                "torchvision"
+                "os",
+                "torchvision",
+                "math"
             ],
             "module": "modelscope.pipelines.cv.image_structured_model_probing_pipeline"
         },
         "('PIPELINES', 'image-classification', 'nextvit-small_image-classification_Dailylife-labels')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py",
             "imports": [
                 "numpy",
@@ -6445,30 +6624,30 @@
             "module": "modelscope.pipelines.cv.image_classification_pipeline"
         },
         "('PIPELINES', 'image-classification', 'resnet50-image-classification-cc')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/content_check_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
                 "PIL",
+                "os",
+                "cv2",
                 "torchvision"
             ],
             "module": "modelscope.pipelines.cv.content_check_pipeline"
         },
         "('PIPELINES', 'image-classification', 'tinynas-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/tinynas_classification_pipeline.py",
             "imports": [
                 "typing",
-                "os",
                 "torch",
-                "math",
-                "torchvision"
+                "os",
+                "torchvision",
+                "math"
             ],
             "module": "modelscope.pipelines.cv.tinynas_classification_pipeline"
         },
         "('PIPELINES', 'image-classification', 'vit-base_image-classification_Dailylife-labels')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py",
             "imports": [
                 "numpy",
@@ -6514,29 +6693,29 @@
             "module": "modelscope.pipelines.cv.image_color_enhance_pipeline"
         },
         "('PIPELINES', 'image-colorization', 'ddcolor-image-colorization')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/ddcolor_image_colorization_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "torch",
-                "torchvision"
+                "torchvision",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.ddcolor_image_colorization_pipeline"
         },
         "('PIPELINES', 'image-colorization', 'unet-image-colorization')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_colorization_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "torch",
                 "PIL",
-                "torchvision"
+                "torchvision",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.image_colorization_pipeline"
         },
         "('PIPELINES', 'image-debanding', 'rrdb-image-debanding')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_debanding_pipeline.py",
             "imports": [
                 "typing",
@@ -6570,39 +6749,39 @@
             ],
             "module": "modelscope.pipelines.cv.image_denoise_pipeline"
         },
         "('PIPELINES', 'image-depth-estimation', 'image-bts-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_bts_depth_estimation_pipeline.py",
             "imports": [
                 "numpy",
-                "typing",
                 "albumentations",
-                "cv2",
-                "torch"
+                "typing",
+                "torch",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.image_bts_depth_estimation_pipeline"
         },
         "('PIPELINES', 'image-depth-estimation', 'image-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_depth_estimation_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "torch",
-                "PIL"
+                "PIL",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.image_depth_estimation_pipeline"
         },
         "('PIPELINES', 'image-driving-perception', 'yolopv2_image-driving-percetion_bdd100k')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_driving_perception_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os"
+                "os",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.image_driving_perception_pipeline"
         },
         "('PIPELINES', 'image-face-fusion', 'image-face-fusion')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_face_fusion_pipeline.py",
             "imports": [
                 "numpy",
@@ -6621,43 +6800,43 @@
             "module": "modelscope.pipelines.cv.image_defrcn_fewshot_pipeline"
         },
         "('PIPELINES', 'image-inpainting', 'fft-inpainting')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_inpainting_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "torch",
-                "PIL"
+                "PIL",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.image_inpainting_pipeline"
         },
         "('PIPELINES', 'image-inpainting', 'image-inpainting-sdv2')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_inpainting_sdv2_pipeline.py",
             "imports": [
                 "numpy",
-                "typing",
                 "diffusers",
-                "cv2",
+                "sys",
+                "typing",
+                "torch",
                 "tempfile",
                 "os",
-                "torch",
-                "sys",
+                "cv2",
                 "math"
             ],
             "module": "modelscope.pipelines.cv.image_inpainting_sdv2_pipeline"
         },
         "('PIPELINES', 'image-matching', 'image-matching')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_matching_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "torch",
-                "PIL"
+                "PIL",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.image_matching_pipeline"
         },
         "('PIPELINES', 'image-multi-view-depth-estimation', 'image-multi-view-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_mvs_depth_estimation_pipeline.py",
             "imports": [
                 "typing",
@@ -6684,18 +6863,18 @@
         },
         "('PIPELINES', 'image-object-detection', 'tbs-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/tbs_detection_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
                 "colorsys",
-                "cv2",
-                "os",
                 "torch",
-                "PIL"
+                "PIL",
+                "os",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.tbs_detection_pipeline"
         },
         "('PIPELINES', 'image-object-detection', 'tinynas-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/tinynas_detection_pipeline.py",
             "imports": [
                 "typing"
@@ -6727,106 +6906,106 @@
             "module": "modelscope.pipelines.cv.easycv_pipelines.detection_pipeline"
         },
         "('PIPELINES', 'image-paintbyexample', 'stablediffusion-paintbyexample')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_paintbyexample_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "torch",
                 "PIL",
-                "einops",
-                "torchvision"
+                "torchvision",
+                "cv2",
+                "einops"
             ],
             "module": "modelscope.pipelines.cv.image_paintbyexample_pipeline"
         },
         "('PIPELINES', 'image-portrait-enhancement', 'gpen-image-portrait-enhancement')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_portrait_enhancement_pipeline.py",
             "imports": [
                 "numpy",
-                "typing",
                 "scipy",
-                "cv2",
+                "typing",
                 "torch",
-                "math",
-                "PIL"
+                "PIL",
+                "cv2",
+                "math"
             ],
             "module": "modelscope.pipelines.cv.image_portrait_enhancement_pipeline"
         },
         "('PIPELINES', 'image-portrait-stylization', 'unet-person-image-cartoon')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_cartoon_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "os",
+                "cv2",
                 "tensorflow"
             ],
             "module": "modelscope.pipelines.cv.image_cartoon_pipeline"
         },
         "('PIPELINES', 'image-quality-assessment-degradation', 'image-quality-assessment-degradation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_degradation_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "tempfile",
                 "torch",
-                "math",
-                "torchvision"
+                "tempfile",
+                "torchvision",
+                "cv2",
+                "math"
             ],
             "module": "modelscope.pipelines.cv.image_quality_assessment_degradation_pipeline"
         },
         "('PIPELINES', 'image-quality-assessment-mos', 'image-quality-assessment-man')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_man_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "tempfile",
                 "torch",
-                "math",
-                "torchvision"
+                "tempfile",
+                "torchvision",
+                "cv2",
+                "math"
             ],
             "module": "modelscope.pipelines.cv.image_quality_assessment_man_pipeline"
         },
         "('PIPELINES', 'image-quality-assessment-mos', 'image-quality-assessment-mos')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_mos_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "tempfile",
                 "torch",
-                "math",
-                "torchvision"
+                "tempfile",
+                "torchvision",
+                "cv2",
+                "math"
             ],
             "module": "modelscope.pipelines.cv.image_quality_assessment_mos_pipeline"
         },
         "('PIPELINES', 'image-reid-person', 'passvitb-image-reid-person')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_reid_person_pipeline.py",
             "imports": [
                 "typing",
-                "os",
                 "torch",
-                "math",
                 "PIL",
-                "torchvision"
+                "os",
+                "torchvision",
+                "math"
             ],
             "module": "modelscope.pipelines.cv.image_reid_person_pipeline"
         },
         "('PIPELINES', 'image-segmentation', 'cascade-mask-rcnn-swin-image-instance-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_instance_segmentation_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
-                "PIL"
+                "PIL",
+                "os",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.image_instance_segmentation_pipeline"
         },
         "('PIPELINES', 'image-segmentation', 'easycv-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/segmentation_pipeline.py",
             "imports": [
                 "numpy",
@@ -6835,39 +7014,39 @@
             "module": "modelscope.pipelines.cv.easycv_pipelines.segmentation_pipeline"
         },
         "('PIPELINES', 'image-segmentation', 'image-panoptic-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_panoptic_segmentation_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "torch",
-                "PIL"
+                "PIL",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.image_panoptic_segmentation_pipeline"
         },
         "('PIPELINES', 'image-segmentation', 'image-panoptic-segmentation-easycv')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_panoptic_segmentation_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "torch",
-                "PIL"
+                "PIL",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.image_panoptic_segmentation_pipeline"
         },
         "('PIPELINES', 'image-segmentation', 'image-semantic-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_semantic_segmentation_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "torch",
-                "PIL"
+                "PIL",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.image_semantic_segmentation_pipeline"
         },
         "('PIPELINES', 'image-segmentation', 'm2fp-image-human-parsing')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_human_parsing_pipeline.py",
             "imports": [
                 "numpy",
@@ -6886,42 +7065,42 @@
             ],
             "module": "modelscope.pipelines.cv.maskdino_instance_segmentation_pipeline"
         },
         "('PIPELINES', 'image-segmentation', 'vision-middleware-multi-task')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/vision_middleware_pipeline.py",
             "imports": [
                 "numpy",
-                "typing",
                 "mmcv",
-                "os",
+                "typing",
                 "torch",
-                "math",
-                "torchvision"
+                "os",
+                "torchvision",
+                "math"
             ],
             "module": "modelscope.pipelines.cv.vision_middleware_pipeline"
         },
         "('PIPELINES', 'image-skychange', 'image-skychange')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_skychange_pipeline.py",
             "imports": [
                 "numpy",
-                "typing",
                 "time",
-                "cv2",
+                "pdb",
+                "typing",
                 "PIL",
-                "pdb"
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.image_skychange_pipeline"
         },
         "('PIPELINES', 'image-style-transfer', 'AAMS-style-transfer')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_style_transfer_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os"
+                "os",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.image_style_transfer_pipeline"
         },
         "('PIPELINES', 'image-super-resolution', 'mobile-image-super-resolution')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/mobile_image_super_resolution_pipeline.py",
             "imports": [
                 "numpy",
@@ -6933,17 +7112,17 @@
             "module": "modelscope.pipelines.cv.mobile_image_super_resolution_pipeline"
         },
         "('PIPELINES', 'image-super-resolution', 'rrdb-image-super-resolution')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_super_resolution_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "torch",
-                "PIL"
+                "PIL",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.image_super_resolution_pipeline"
         },
         "('PIPELINES', 'image-text-retrieval', 'image-text-retrieval')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/image_text_retrieval_pipeline.py",
             "imports": [
                 "typing",
@@ -6959,34 +7138,34 @@
             "module": "modelscope.pipelines.multi_modal.multi_modal_embedding_pipeline"
         },
         "('PIPELINES', 'image-to-image-generation', 'image-to-image-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_to_image_generate_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
                 "PIL",
+                "os",
+                "cv2",
                 "torchvision"
             ],
             "module": "modelscope.pipelines.cv.image_to_image_generate_pipeline"
         },
         "('PIPELINES', 'image-to-image-translation', 'image-to-image-translation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_to_image_translation_pipeline.py",
             "imports": [
                 "numpy",
+                "sys",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
-                "sys",
                 "PIL",
-                "torchvision",
-                "io"
+                "io",
+                "os",
+                "cv2",
+                "torchvision"
             ],
             "module": "modelscope.pipelines.cv.image_to_image_translation_pipeline"
         },
         "('PIPELINES', 'indoor-layout-estimation', 'indoor-layout-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/indoor_layout_estimation_pipeline.py",
             "imports": [
                 "numpy",
@@ -7002,54 +7181,54 @@
                 "torch"
             ],
             "module": "modelscope.pipelines.nlp.information_extraction_pipeline"
         },
         "('PIPELINES', 'inverse-text-processing', 'itn-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/inverse_text_processing_pipeline.py",
             "imports": [
-                "yaml",
                 "typing",
                 "shutil",
-                "os"
+                "os",
+                "yaml"
             ],
             "module": "modelscope.pipelines.audio.inverse_text_processing_pipeline"
         },
         "('PIPELINES', 'keyword-spotting', 'kws-kwsbp')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/kws_kwsbp_pipeline.py",
             "imports": [
                 "typing",
-                "json",
-                "os"
+                "os",
+                "json"
             ],
             "module": "modelscope.pipelines.audio.kws_kwsbp_pipeline"
         },
         "('PIPELINES', 'keyword-spotting', 'speech_dfsmn_kws_char_farfield')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/kws_farfield_pipeline.py",
             "imports": [
                 "numpy",
+                "wave",
                 "typing",
                 "soundfile",
-                "wave",
                 "io"
             ],
             "module": "modelscope.pipelines.audio.kws_farfield_pipeline"
         },
         "('PIPELINES', 'language-guided-video-summarization', 'clip-it-video-summarization')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/language_guided_video_summarization_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "clip",
                 "shutil",
-                "random",
-                "cv2",
+                "torch",
+                "PIL",
                 "tempfile",
                 "os",
-                "torch",
-                "PIL"
+                "cv2",
+                "random",
+                "clip"
             ],
             "module": "modelscope.pipelines.cv.language_guided_video_summarization_pipeline"
         },
         "('PIPELINES', 'language-score-prediction', 'language-score-prediction')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/lm_infer_pipeline.py",
             "imports": [
                 "typing",
@@ -7058,56 +7237,56 @@
             "module": "modelscope.pipelines.audio.lm_infer_pipeline"
         },
         "('PIPELINES', 'license-plate-detection', 'resnet18-license-plate-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/license_plate_detection_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
+                "PIL",
                 "torch",
-                "math",
-                "PIL"
+                "os",
+                "cv2",
+                "math"
             ],
             "module": "modelscope.pipelines.cv.license_plate_detection_pipeline"
         },
         "('PIPELINES', 'lineless-table-recognition', 'lore-lineless-table-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/lineless_table_recognition_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
+                "PIL",
                 "torch",
-                "math",
-                "PIL"
+                "os",
+                "cv2",
+                "math"
             ],
             "module": "modelscope.pipelines.cv.lineless_table_recognition_pipeline"
         },
         "('PIPELINES', 'live-category', 'live-category')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/live_category_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "decord",
-                "os",
                 "torch",
                 "PIL",
-                "torchvision"
+                "os",
+                "torchvision",
+                "decord"
             ],
             "module": "modelscope.pipelines.cv.live_category_pipeline"
         },
         "('PIPELINES', 'motion-generation', 'mdm-motion-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/motion_generation_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "os",
+                "torch",
                 "tempfile",
-                "torch"
+                "os"
             ],
             "module": "modelscope.pipelines.cv.motion_generation_pipeline"
         },
         "('PIPELINES', 'movie-scene-segmentation', 'resnet50-bert-movie-scene-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/movie_scene_segmentation_pipeline.py",
             "imports": [
                 "typing",
@@ -7115,22 +7294,22 @@
             ],
             "module": "modelscope.pipelines.cv.movie_scene_segmentation_pipeline"
         },
         "('PIPELINES', 'multi-modal-embedding', 'gridvlp-multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/gridvlp_pipeline.py",
             "imports": [
                 "numpy",
-                "typing",
-                "transformers",
-                "traceback",
                 "time",
-                "os",
+                "typing",
                 "torch",
+                "PIL",
+                "os",
+                "traceback",
                 "json",
-                "PIL"
+                "transformers"
             ],
             "module": "modelscope.pipelines.multi_modal.gridvlp_pipeline"
         },
         "('PIPELINES', 'multi-modal-embedding', 'multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/multi_modal_embedding_pipeline.py",
             "imports": [
                 "typing"
@@ -7182,33 +7361,33 @@
             "module": "modelscope.pipelines.nlp.text_classification_pipeline"
         },
         "('PIPELINES', 'object-detection-3d', 'object-detection-3d-depe')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/object_detection_3d_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
-                "tempfile",
                 "torch",
-                "PIL"
+                "PIL",
+                "tempfile",
+                "os",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.object_detection_3d_pipeline"
         },
         "('PIPELINES', 'ocr-detection', 'resnet18-ocr-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/ocr_detection_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
-                "math",
+                "os",
+                "cv2",
+                "tf_slim",
                 "tensorflow",
-                "tf_slim"
+                "math"
             ],
             "module": "modelscope.pipelines.cv.ocr_detection_pipeline"
         },
         "('PIPELINES', 'ocr-recognition', 'convnextTiny-ocr-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/ocr_recognition_pipeline.py",
             "imports": [],
             "module": "modelscope.pipelines.cv.ocr_recognition_pipeline"
@@ -7222,41 +7401,55 @@
             "module": "modelscope.pipelines.multi_modal.ocr_recognition_pipeline"
         },
         "('PIPELINES', 'open-vocabulary-detection', 'open-vocabulary-detection-vild')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_open_vocabulary_detection_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
-                "PIL"
+                "PIL",
+                "os",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.image_open_vocabulary_detection_pipeline"
         },
         "('PIPELINES', 'panorama-depth-estimation', 'panorama-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/panorama_depth_estimation_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "torch",
-                "PIL"
+                "PIL",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.panorama_depth_estimation_pipeline"
         },
         "('PIPELINES', 'part-of-speech', 'part-of-speech')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/token_classification_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
                 "torch"
             ],
             "module": "modelscope.pipelines.nlp.token_classification_pipeline"
         },
+        "('PIPELINES', 'pedestrian-attribute-recognition', 'resnet50_pedestrian-attribute-recognition_image')": {
+            "filepath": "TEMPLATE_PATH/pipelines/cv/pedestrian_attribute_recognition_pipeline.py",
+            "imports": [
+                "numpy",
+                "typing",
+                "torch",
+                "PIL",
+                "os",
+                "cv2",
+                "torchvision",
+                "json"
+            ],
+            "module": "modelscope.pipelines.cv.pedestrian_attribute_recognition_pipeline"
+        },
         "('PIPELINES', 'pointcloud-sceneflow-estimation', 'pointcloud-sceneflow-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
                 "plyfile",
                 "torch"
@@ -7264,29 +7457,29 @@
             "module": "modelscope.pipelines.cv.pointcloud_sceneflow_estimation_pipeline"
         },
         "('PIPELINES', 'portrait-matting', 'unet-image-matting')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_matting_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "os",
+                "cv2",
                 "tensorflow"
             ],
             "module": "modelscope.pipelines.cv.image_matting_pipeline"
         },
         "('PIPELINES', 'product-retrieval-embedding', 'resnet50-product-retrieval-embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/product_retrieval_embedding_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
                 "PIL",
+                "os",
+                "cv2",
                 "torchvision"
             ],
             "module": "modelscope.pipelines.cv.product_retrieval_embedding_pipeline"
         },
         "('PIPELINES', 'product-segmentation', 'product-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/product_segmentation_pipeline.py",
             "imports": [
@@ -7295,45 +7488,45 @@
             ],
             "module": "modelscope.pipelines.cv.product_segmentation_pipeline"
         },
         "('PIPELINES', 'protein-structure', 'unifold-protein-structure')": {
             "filepath": "TEMPLATE_PATH/pipelines/science/protein_structure_pipeline.py",
             "imports": [
                 "numpy",
-                "typing",
                 "unicore",
                 "time",
-                "os",
+                "typing",
                 "torch",
+                "os",
                 "json"
             ],
             "module": "modelscope.pipelines.science.protein_structure_pipeline"
         },
         "('PIPELINES', 'punctuation', 'punc-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/punctuation_processing_pipeline.py",
             "imports": [
-                "yaml",
                 "typing",
                 "shutil",
-                "os"
+                "os",
+                "yaml"
             ],
             "module": "modelscope.pipelines.audio.punctuation_processing_pipeline"
         },
         "('PIPELINES', 'referring-video-object-segmentation', 'referring-video-object-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/referring_video_object_segmentation_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "moviepy",
-                "tempfile",
                 "torch",
-                "einops",
                 "PIL",
+                "tempfile",
+                "torchvision",
+                "moviepy",
                 "tqdm",
-                "torchvision"
+                "einops"
             ],
             "module": "modelscope.pipelines.cv.referring_video_object_segmentation_pipeline"
         },
         "('PIPELINES', 'relation-extraction', 'relation-extraction')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/information_extraction_pipeline.py",
             "imports": [
                 "typing",
@@ -7389,18 +7582,18 @@
             ],
             "module": "modelscope.pipelines.nlp.text_classification_pipeline"
         },
         "('PIPELINES', 'sentence-similarity', 'translation-quality-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/translation_quality_estimation_pipeline.py",
             "imports": [
                 "typing",
-                "transformers",
-                "os",
                 "torch",
-                "io"
+                "io",
+                "os",
+                "transformers"
             ],
             "module": "modelscope.pipelines.nlp.translation_quality_estimation_pipeline"
         },
         "('PIPELINES', 'sentiment-classification', 'sentiment-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py",
             "imports": [
                 "numpy",
@@ -7415,93 +7608,93 @@
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.shop_segmentation_pipleline"
         },
         "('PIPELINES', 'siamese-uie', 'siamese-uie')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/siamese_uie_pipeline.py",
             "imports": [
-                "typing",
-                "scipy",
+                "logging",
                 "time",
-                "os",
-                "pathlib",
+                "scipy",
+                "typing",
                 "torch",
+                "os",
                 "copy",
-                "json",
-                "math",
+                "pathlib",
                 "tqdm",
-                "logging"
+                "math",
+                "json"
             ],
             "module": "modelscope.pipelines.nlp.siamese_uie_pipeline"
         },
         "('PIPELINES', 'skin-retouching', 'unet-skin-retouching')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/skin_retouching_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
-                "torch",
                 "PIL",
-                "tensorflow",
-                "torchvision"
+                "torch",
+                "os",
+                "cv2",
+                "torchvision",
+                "tensorflow"
             ],
             "module": "modelscope.pipelines.cv.skin_retouching_pipeline"
         },
         "('PIPELINES', 'speaker-diarization', 'speaker-diarization-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/speaker_diarization_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
                 "shutil",
+                "yaml",
                 "os",
-                "json",
-                "yaml"
+                "json"
             ],
             "module": "modelscope.pipelines.audio.speaker_diarization_pipeline"
         },
         "('PIPELINES', 'speaker-verification', 'speaker-verification')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/speaker_verification_light_pipeline.py",
             "imports": [
-                "soundfile",
                 "typing",
                 "io",
-                "torch"
+                "torch",
+                "soundfile"
             ],
             "module": "modelscope.pipelines.audio.speaker_verification_light_pipeline"
         },
         "('PIPELINES', 'speaker-verification', 'sv-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/speaker_verification_pipeline.py",
             "imports": [
-                "yaml",
                 "typing",
                 "shutil",
-                "os"
+                "os",
+                "yaml"
             ],
             "module": "modelscope.pipelines.audio.speaker_verification_pipeline"
         },
         "('PIPELINES', 'speech-separation', 'speech-separation')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/separation_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "soundfile",
                 "torch",
-                "io"
+                "io",
+                "soundfile"
             ],
             "module": "modelscope.pipelines.audio.separation_pipeline"
         },
         "('PIPELINES', 'speech-timestamp', 'speech-timestamp-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/timestamp_pipeline.py",
             "imports": [
                 "typing",
-                "os",
-                "json",
+                "yaml",
                 "funasr",
-                "yaml"
+                "os",
+                "json"
             ],
             "module": "modelscope.pipelines.audio.timestamp_pipeline"
         },
         "('PIPELINES', 'sudoku', 'ofa-sudoku')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/sudoku_pipeline.py",
             "imports": [
                 "typing",
@@ -7518,31 +7711,31 @@
             ],
             "module": "modelscope.pipelines.nlp.conversational_text_to_sql_pipeline"
         },
         "('PIPELINES', 'table-question-answering', 'table-question-answering-pipeline')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/table_question_answering_pipeline.py",
             "imports": [
                 "typing",
-                "transformers",
-                "os",
                 "torch",
-                "json"
+                "os",
+                "json",
+                "transformers"
             ],
             "module": "modelscope.pipelines.nlp.table_question_answering_pipeline"
         },
         "('PIPELINES', 'table-recognition', 'dla34-table-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/table_recognition_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
+                "PIL",
                 "torch",
-                "math",
-                "PIL"
+                "os",
+                "cv2",
+                "math"
             ],
             "module": "modelscope.pipelines.cv.table_recognition_pipeline"
         },
         "('PIPELINES', 'task-oriented-conversation', 'dialog-intent-prediction')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/dialog_intent_prediction_pipeline.py",
             "imports": [
                 "typing"
@@ -7564,27 +7757,27 @@
             "module": "modelscope.pipelines.nlp.dialog_state_tracking_pipeline"
         },
         "('PIPELINES', 'text-classification', 'domain-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/fasttext_text_classification_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
+                "sentencepiece",
                 "fasttext",
-                "os",
-                "sentencepiece"
+                "os"
             ],
             "module": "modelscope.pipelines.nlp.fasttext_text_classification_pipeline"
         },
         "('PIPELINES', 'text-classification', 'language_identification')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/language_identification_pipline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "os",
                 "re",
+                "os",
                 "tensorflow"
             ],
             "module": "modelscope.pipelines.nlp.language_identification_pipline"
         },
         "('PIPELINES', 'text-classification', 'sentence-similarity')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py",
             "imports": [
@@ -7641,14 +7834,21 @@
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_error_correction_pipeline.py",
             "imports": [
                 "typing",
                 "torch"
             ],
             "module": "modelscope.pipelines.nlp.text_error_correction_pipeline"
         },
+        "('PIPELINES', 'text-generation', 'glm130b-text-generation')": {
+            "filepath": "TEMPLATE_PATH/pipelines/nlp/glm130b_text_generation_pipeline.py",
+            "imports": [
+                "typing"
+            ],
+            "module": "modelscope.pipelines.nlp.glm130b_text_generation_pipeline"
+        },
         "('PIPELINES', 'text-generation', 'gpt-moe-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/distributed_gpt_moe_pipeline.py",
             "imports": [
                 "typing",
                 "torch"
             ],
             "module": "modelscope.pipelines.nlp.distributed_gpt_moe_pipeline"
@@ -7711,49 +7911,49 @@
             ],
             "module": "modelscope.pipelines.nlp.summarization_pipeline"
         },
         "('PIPELINES', 'text-to-image-synthesis', 'chinese-stable-diffusion')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py",
             "imports": [
                 "numpy",
-                "typing",
-                "transformers",
                 "diffusers",
-                "cv2",
+                "typing",
                 "torch",
-                "PIL"
+                "PIL",
+                "cv2",
+                "transformers"
             ],
             "module": "modelscope.pipelines.multi_modal.diffusers_wrapped.stable_diffusion.chinese_stable_diffusion_pipeline"
         },
         "('PIPELINES', 'text-to-image-synthesis', 'diffusers-stable-diffusion')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py",
             "imports": [
                 "numpy",
-                "typing",
                 "diffusers",
-                "cv2",
+                "typing",
                 "torch",
-                "PIL"
+                "PIL",
+                "cv2"
             ],
             "module": "modelscope.pipelines.multi_modal.diffusers_wrapped.stable_diffusion.stable_diffusion_pipeline"
         },
         "('PIPELINES', 'text-to-image-synthesis', 'disco_guided_diffusion')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py",
             "imports": [
                 "numpy",
-                "clip",
                 "gc",
-                "cv2",
-                "os",
                 "torch",
+                "PIL",
                 "importlib",
+                "os",
+                "cv2",
+                "torchvision",
+                "clip",
                 "math",
-                "json",
-                "PIL",
-                "torchvision"
+                "json"
             ],
             "module": "modelscope.pipelines.multi_modal.disco_guided_diffusion_pipeline.disco_guided_diffusion"
         },
         "('PIPELINES', 'text-to-image-synthesis', 'text-to-image-synthesis')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/text_to_image_synthesis_pipeline.py",
             "imports": [
                 "typing",
@@ -7769,17 +7969,17 @@
             ],
             "module": "modelscope.pipelines.audio.text_to_speech_pipeline"
         },
         "('PIPELINES', 'text-to-video-synthesis', 'latent-text-to-video-synthesis')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/text_to_video_synthesis_pipeline.py",
             "imports": [
                 "typing",
-                "cv2",
-                "tempfile",
                 "torch",
+                "tempfile",
+                "cv2",
                 "einops"
             ],
             "module": "modelscope.pipelines.multi_modal.text_to_video_synthesis_pipeline"
         },
         "('PIPELINES', 'text2sql', 'ofa-text2sql')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/text2sql_pipeline.py",
             "imports": [
@@ -7861,67 +8061,67 @@
             "module": "modelscope.pipelines.nlp.token_classification_pipeline"
         },
         "('PIPELINES', 'translation', 'automatic-post-editing')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/automatic_post_editing_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "sacremoses",
+                "jieba",
+                "sentencepiece",
                 "os",
-                "html",
                 "tensorflow",
-                "sentencepiece",
-                "jieba"
+                "sacremoses",
+                "html"
             ],
             "module": "modelscope.pipelines.nlp.automatic_post_editing_pipeline"
         },
         "('PIPELINES', 'translation', 'csanmt-translation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/translation_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "sacremoses",
                 "os",
-                "tensorflow",
                 "subword_nmt",
+                "tensorflow",
+                "sacremoses",
                 "jieba"
             ],
             "module": "modelscope.pipelines.nlp.translation_pipeline"
         },
         "('PIPELINES', 'translation', 'interactive-translation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/interactive_translation_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "sacremoses",
                 "os",
-                "tensorflow",
                 "subword_nmt",
+                "tensorflow",
+                "sacremoses",
                 "jieba"
             ],
             "module": "modelscope.pipelines.nlp.interactive_translation_pipeline"
         },
         "('PIPELINES', 'translation-evaluation', 'translation-evaluation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/translation_evaluation_pipeline.py",
             "imports": [
                 "numpy",
-                "typing",
-                "os",
                 "enum",
-                "torch"
+                "typing",
+                "torch",
+                "os"
             ],
             "module": "modelscope.pipelines.nlp.translation_evaluation_pipeline"
         },
         "('PIPELINES', 'universal-matting', 'unet-universal-matting')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_matting_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "os",
+                "cv2",
                 "tensorflow"
             ],
             "module": "modelscope.pipelines.cv.image_matting_pipeline"
         },
         "('PIPELINES', 'video-captioning', 'video-captioning')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/video_captioning_pipeline.py",
             "imports": [
@@ -7931,50 +8131,50 @@
             "module": "modelscope.pipelines.multi_modal.video_captioning_pipeline"
         },
         "('PIPELINES', 'video-category', 'video-category')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_category_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "decord",
-                "os",
                 "torch",
-                "json",
                 "PIL",
-                "torchvision"
+                "os",
+                "torchvision",
+                "json",
+                "decord"
             ],
             "module": "modelscope.pipelines.cv.video_category_pipeline"
         },
         "('PIPELINES', 'video-colorization', 'video-colorization')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_colorization_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "subprocess",
-                "cv2",
+                "PIL",
+                "torch",
                 "tempfile",
                 "os",
-                "torch",
-                "PIL",
+                "cv2",
+                "subprocess",
                 "torchvision"
             ],
             "module": "modelscope.pipelines.cv.video_colorization_pipeline"
         },
         "('PIPELINES', 'video-deinterlace', 'video-deinterlace')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_deinterlace_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "subprocess",
-                "cv2",
+                "torch",
                 "tempfile",
                 "os",
-                "torch",
-                "math",
-                "torchvision"
+                "cv2",
+                "subprocess",
+                "torchvision",
+                "math"
             ],
             "module": "modelscope.pipelines.cv.video_deinterlace_pipeline"
         },
         "('PIPELINES', 'video-depth-estimation', 'video-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_depth_estimation_pipeline.py",
             "imports": [
                 "typing"
@@ -7982,76 +8182,76 @@
             "module": "modelscope.pipelines.cv.video_depth_estimation_pipeline"
         },
         "('PIPELINES', 'video-embedding', 'cmdssl-r2p1d_video_embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/cmdssl_video_embedding_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "decord",
-                "os",
                 "torch",
                 "PIL",
-                "torchvision"
+                "os",
+                "torchvision",
+                "decord"
             ],
             "module": "modelscope.pipelines.cv.cmdssl_video_embedding_pipeline"
         },
         "('PIPELINES', 'video-embedding', 'hicossl-s3dg-video_embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/hicossl_video_embedding_pipeline.py",
             "imports": [
                 "typing",
-                "math",
                 "os",
+                "math",
                 "torch"
             ],
             "module": "modelscope.pipelines.cv.hicossl_video_embedding_pipeline"
         },
         "('PIPELINES', 'video-frame-interpolation', 'video-frame-interpolation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_frame_interpolation_pipeline.py",
             "imports": [
                 "numpy",
+                "glob",
                 "typing",
+                "torch",
+                "tempfile",
+                "os",
                 "subprocess",
                 "cv2",
-                "os",
-                "tempfile",
-                "torch",
-                "math",
-                "glob",
-                "torchvision"
+                "torchvision",
+                "math"
             ],
             "module": "modelscope.pipelines.cv.video_frame_interpolation_pipeline"
         },
         "('PIPELINES', 'video-human-matting', 'video-human-matting')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_human_matting_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "moviepy",
-                "cv2",
+                "torch",
                 "os",
-                "torch"
+                "cv2",
+                "moviepy"
             ],
             "module": "modelscope.pipelines.cv.video_human_matting_pipeline"
         },
         "('PIPELINES', 'video-inpainting', 'video-inpainting')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_inpainting_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.video_inpainting_pipeline"
         },
         "('PIPELINES', 'video-instance-segmentation', 'video-instance-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_instance_segmentation_pipeline.py",
             "imports": [
                 "numpy",
-                "typing",
                 "mmcv",
-                "cv2",
-                "os",
+                "typing",
                 "torch",
+                "os",
+                "cv2",
                 "tqdm"
             ],
             "module": "modelscope.pipelines.cv.video_instance_segmentation_pipeline"
         },
         "('PIPELINES', 'video-multi-modal-embedding', 'video-multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/video_multi_modal_embedding_pipeline.py",
             "imports": [
@@ -8069,44 +8269,44 @@
             "module": "modelscope.pipelines.cv.video_multi_object_tracking_pipeline"
         },
         "('PIPELINES', 'video-object-detection', 'cspnet_realtime-video-object-detection_streamyolo')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/realtime_video_object_detection_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
-                "json",
                 "PIL",
-                "torchvision"
+                "os",
+                "cv2",
+                "torchvision",
+                "json"
             ],
             "module": "modelscope.pipelines.cv.realtime_video_object_detection_pipeline"
         },
         "('PIPELINES', 'video-object-segmentation', 'video-object-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_object_segmentation_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "os",
                 "torch",
                 "PIL",
+                "os",
                 "torchvision"
             ],
             "module": "modelscope.pipelines.cv.video_object_segmentation_pipeline"
         },
         "('PIPELINES', 'video-panoptic-segmentation', 'video-panoptic-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_panoptic_segmentation_pipeline.py",
             "imports": [
                 "numpy",
-                "typing",
                 "mmcv",
-                "cv2",
-                "os",
+                "typing",
                 "torch",
+                "os",
+                "cv2",
                 "tqdm"
             ],
             "module": "modelscope.pipelines.cv.video_panoptic_segmentation_pipeline"
         },
         "('PIPELINES', 'video-question-answering', 'video-question-answering')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/video_question_answering_pipeline.py",
             "imports": [
@@ -8115,117 +8315,117 @@
             ],
             "module": "modelscope.pipelines.multi_modal.video_question_answering_pipeline"
         },
         "('PIPELINES', 'video-single-object-tracking', 'ostrack-vitb-video-single-object-tracking')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_single_object_tracking_pipeline.py",
             "imports": [
                 "typing",
-                "cv2",
-                "os"
+                "os",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.video_single_object_tracking_pipeline"
         },
         "('PIPELINES', 'video-single-object-tracking', 'procontext-vitb-video-single-object-tracking')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_single_object_tracking_pipeline.py",
             "imports": [
                 "typing",
-                "cv2",
-                "os"
+                "os",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.video_single_object_tracking_pipeline"
         },
         "('PIPELINES', 'video-stabilization', 'video-stabilization')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_stabilization_pipeline.py",
             "imports": [
                 "numpy",
+                "glob",
                 "typing",
+                "torch",
+                "tempfile",
+                "os",
                 "subprocess",
                 "cv2",
-                "os",
-                "tempfile",
-                "torch",
-                "math",
-                "glob"
+                "math"
             ],
             "module": "modelscope.pipelines.cv.video_stabilization_pipeline"
         },
         "('PIPELINES', 'video-summarization', 'googlenet_pgl_video_summarization')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_summarization_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
+                "os",
+                "cv2",
                 "tqdm"
             ],
             "module": "modelscope.pipelines.cv.video_summarization_pipeline"
         },
         "('PIPELINES', 'video-super-resolution', 'realbasicvsr-video-super-resolution')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_super_resolution_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "subprocess",
-                "cv2",
+                "torch",
                 "tempfile",
                 "os",
-                "torch",
-                "math",
-                "torchvision"
+                "cv2",
+                "subprocess",
+                "torchvision",
+                "math"
             ],
             "module": "modelscope.pipelines.cv.video_super_resolution_pipeline"
         },
         "('PIPELINES', 'video-temporal-grounding', 'soonet-video-temporal-grounding')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "os",
                 "torch",
+                "os",
                 "torchvision"
             ],
             "module": "modelscope.pipelines.multi_modal.soonet_video_temporal_grounding_pipeline"
         },
         "('PIPELINES', 'video-text-retrieval', 'vop-video-text-retrieval')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/vop_retrieval_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "random",
-                "os",
                 "torch",
+                "collections",
+                "os",
                 "gzip",
-                "math",
+                "random",
                 "pickle",
                 "tqdm",
-                "collections"
+                "math"
             ],
             "module": "modelscope.pipelines.cv.vop_retrieval_pipeline"
         },
         "('PIPELINES', 'video-text-retrieval', 'vop-video-text-retrieval-se')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/vop_retrieval_se_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "os",
                 "torch",
+                "os",
                 "gzip"
             ],
             "module": "modelscope.pipelines.cv.vop_retrieval_se_pipeline"
         },
         "('PIPELINES', 'virtual-try-on', 'virtual-try-on')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/virtual_try_on_pipeline.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
-                "PIL"
+                "PIL",
+                "os",
+                "cv2"
             ],
             "module": "modelscope.pipelines.cv.virtual_try_on_pipeline"
         },
         "('PIPELINES', 'vision-efficient-tuning', 'vision-efficient-tuning')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/vision_efficient_tuning_pipeline.py",
             "imports": [
                 "numpy",
@@ -8251,22 +8451,22 @@
             ],
             "module": "modelscope.pipelines.multi_modal.visual_grounding_pipeline"
         },
         "('PIPELINES', 'visual-question-answering', 'gridvlp-multi-modal-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/gridvlp_pipeline.py",
             "imports": [
                 "numpy",
-                "typing",
-                "transformers",
-                "traceback",
                 "time",
-                "os",
+                "typing",
                 "torch",
+                "PIL",
+                "os",
+                "traceback",
                 "json",
-                "PIL"
+                "transformers"
             ],
             "module": "modelscope.pipelines.multi_modal.gridvlp_pipeline"
         },
         "('PIPELINES', 'visual-question-answering', 'visual-question-answering')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/visual_question_answering_pipeline.py",
             "imports": [
                 "typing",
@@ -8274,18 +8474,18 @@
             ],
             "module": "modelscope.pipelines.multi_modal.visual_question_answering_pipeline"
         },
         "('PIPELINES', 'voice-activity-detection', 'vad-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/voice_activity_detection_pipeline.py",
             "imports": [
                 "typing",
-                "os",
-                "json",
+                "yaml",
                 "funasr",
-                "yaml"
+                "os",
+                "json"
             ],
             "module": "modelscope.pipelines.audio.voice_activity_detection_pipeline"
         },
         "('PIPELINES', 'word-alignment', 'word-alignment')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/word_alignment_pipeline.py",
             "imports": [
                 "numpy",
@@ -8316,17 +8516,17 @@
                 "torch"
             ],
             "module": "modelscope.pipelines.nlp.word_segmentation_pipeline"
         },
         "('PIPELINES', 'zero-shot-classification', 'zero-shot-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/zero_shot_classification_pipeline.py",
             "imports": [
-                "scipy",
                 "typing",
-                "torch"
+                "torch",
+                "scipy"
             ],
             "module": "modelscope.pipelines.nlp.zero_shot_classification_pipeline"
         },
         "('POSITIONAL_ENCODING', 'default', 'SinePositionalEncoding3D')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/positional_encoding.py",
             "imports": [
                 "mmcv",
@@ -8335,28 +8535,28 @@
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.positional_encoding"
         },
         "('PREPROCESSORS', 'audio', 'LinearAECAndFbank')": {
             "filepath": "TEMPLATE_PATH/preprocessors/audio.py",
             "imports": [
                 "numpy",
-                "typing",
                 "scipy",
-                "os",
+                "typing",
                 "torch",
-                "io"
+                "io",
+                "os"
             ],
             "module": "modelscope.preprocessors.audio"
         },
         "('PREPROCESSORS', 'audio', 'wav-to-lists')": {
             "filepath": "TEMPLATE_PATH/preprocessors/kws.py",
             "imports": [
-                "yaml",
                 "typing",
-                "os"
+                "os",
+                "yaml"
             ],
             "module": "modelscope.preprocessors.kws"
         },
         "('PREPROCESSORS', 'audio', 'wav-to-scp')": {
             "filepath": "TEMPLATE_PATH/preprocessors/asr.py",
             "imports": [
                 "typing",
@@ -8365,134 +8565,134 @@
             "module": "modelscope.preprocessors.asr"
         },
         "('PREPROCESSORS', 'cv', 'CenterCrop')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
                 "PIL",
+                "os",
+                "cv2",
                 "torchvision"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'ImageToTensor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
                 "PIL",
+                "os",
+                "cv2",
                 "torchvision"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'Normalize')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
                 "PIL",
+                "os",
+                "cv2",
                 "torchvision"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'RandomCrop')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
                 "PIL",
+                "os",
+                "cv2",
                 "torchvision"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'RandomHorizontalFlip')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
                 "PIL",
+                "os",
+                "cv2",
                 "torchvision"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'RandomResizedCrop')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
                 "PIL",
+                "os",
+                "cv2",
                 "torchvision"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'Resize')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
                 "PIL",
+                "os",
+                "cv2",
                 "torchvision"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'bad-image-detecting-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/bad_image_detecting_preprocessor.py",
             "imports": [
                 "numpy",
                 "typing",
                 "torch",
-                "math",
                 "PIL",
-                "torchvision"
+                "torchvision",
+                "math"
             ],
             "module": "modelscope.preprocessors.cv.bad_image_detecting_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'controllable-image-generation-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/controllable_image_generation.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
-                "math",
                 "PIL",
-                "torchvision"
+                "os",
+                "cv2",
+                "torchvision",
+                "math"
             ],
             "module": "modelscope.preprocessors.cv.controllable_image_generation"
         },
         "('PREPROCESSORS', 'cv', 'image-classification-bypass-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "PIL",
-                "io"
+                "io",
+                "cv2"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'image-classification-mmcv-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/mmcls_preprocessor.py",
             "imports": [
                 "numpy",
@@ -8502,64 +8702,64 @@
             "module": "modelscope.preprocessors.cv.mmcls_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'image-classification-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
                 "torch",
                 "PIL",
+                "os",
+                "cv2",
                 "torchvision"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'image-color-enhance-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "PIL",
-                "io"
+                "io",
+                "cv2"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'image-deblur-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "PIL",
-                "io"
+                "io",
+                "cv2"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'image-demoire-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_restoration_preprocessor.py",
             "imports": [
                 "numpy",
                 "typing",
                 "torch",
-                "math",
                 "PIL",
-                "torchvision"
+                "torchvision",
+                "math"
             ],
             "module": "modelscope.preprocessors.cv.image_restoration_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'image-denoise-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "PIL",
-                "io"
+                "io",
+                "cv2"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'image-driving-perception-preprocessor')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_driving_perception/preprocessor.py",
             "imports": [
                 "numpy",
@@ -8570,104 +8770,104 @@
             "module": "modelscope.models.cv.image_driving_perception.preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'image-instance-segmentation-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "PIL",
-                "io"
+                "io",
+                "cv2"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'image-portrait-enhancement-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "PIL",
-                "io"
+                "io",
+                "cv2"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'image-quality_assessment-man-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_quality_assessment_man.py",
             "imports": [
                 "numpy",
                 "typing",
                 "torch",
-                "math",
                 "PIL",
-                "torchvision"
+                "torchvision",
+                "math"
             ],
             "module": "modelscope.preprocessors.cv.image_quality_assessment_man"
         },
         "('PREPROCESSORS', 'cv', 'image-quality_assessment-mos-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_quality_assessment_mos.py",
             "imports": [
                 "numpy",
                 "typing",
+                "torchvision",
                 "cv2",
-                "math",
-                "torchvision"
+                "math"
             ],
             "module": "modelscope.preprocessors.cv.image_quality_assessment_mos"
         },
         "('PREPROCESSORS', 'cv', 'image-sky-change-preprocessor')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_skychange/preprocessor.py",
             "imports": [
                 "numpy",
-                "typing",
-                "cv2",
-                "torch",
-                "json",
                 "numbers",
                 "pdb",
-                "torchvision"
+                "typing",
+                "torch",
+                "torchvision",
+                "cv2",
+                "json"
             ],
             "module": "modelscope.models.cv.image_skychange.preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'load-image')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "PIL",
-                "io"
+                "io",
+                "cv2"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'movie-scene-segmentation-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/video.py",
             "imports": [
                 "numpy",
-                "decord",
-                "random",
                 "urllib",
-                "os",
-                "tempfile",
                 "torch",
+                "tempfile",
+                "os",
+                "uuid",
                 "torchvision",
+                "random",
                 "math",
-                "uuid"
+                "decord"
             ],
             "module": "modelscope.preprocessors.video"
         },
         "('PREPROCESSORS', 'cv', 'nerf-recon-acc-preprocessor')": {
             "filepath": "TEMPLATE_PATH/models/cv/nerf_recon_acc/nerf_preprocess.py",
             "imports": [
                 "numpy",
+                "glob",
                 "typing",
-                "subprocess",
-                "cv2",
                 "os",
-                "glob",
+                "cv2",
+                "subprocess",
                 "tensorflow"
             ],
             "module": "modelscope.models.cv.nerf_recon_acc.nerf_preprocess"
         },
         "('PREPROCESSORS', 'cv', 'object-detection-scrfd')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/preprocessor.py",
             "imports": [
@@ -8678,198 +8878,214 @@
             "module": "modelscope.models.cv.face_detection.scrfd.preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'object-detection-tinynas-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "PIL",
-                "io"
+                "io",
+                "cv2"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'ocr-detection')": {
             "filepath": "TEMPLATE_PATH/models/cv/ocr_detection/preprocessor.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
-                "os",
+                "PIL",
                 "torch",
-                "math",
-                "PIL"
+                "os",
+                "cv2",
+                "math"
             ],
             "module": "modelscope.models.cv.ocr_detection.preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'ocr-recognition')": {
             "filepath": "TEMPLATE_PATH/models/cv/ocr_recognition/preprocessor.py",
             "imports": [
                 "numpy",
-                "cv2",
-                "os",
                 "torch",
-                "PIL"
+                "PIL",
+                "os",
+                "cv2"
             ],
             "module": "modelscope.models.cv.ocr_recognition.preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'video-summarization-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
                 "numpy",
                 "typing",
-                "cv2",
                 "PIL",
-                "io"
+                "io",
+                "cv2"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'default', 'Compose')": {
             "filepath": "TEMPLATE_PATH/preprocessors/common.py",
             "imports": [
                 "numpy",
-                "typing",
                 "time",
+                "typing",
                 "torch",
                 "collections"
             ],
             "module": "modelscope.preprocessors.common"
         },
         "('PREPROCESSORS', 'default', 'Filter')": {
             "filepath": "TEMPLATE_PATH/preprocessors/common.py",
             "imports": [
                 "numpy",
-                "typing",
                 "time",
+                "typing",
                 "torch",
                 "collections"
             ],
             "module": "modelscope.preprocessors.common"
         },
         "('PREPROCESSORS', 'default', 'Identity')": {
             "filepath": "TEMPLATE_PATH/preprocessors/common.py",
             "imports": [
                 "numpy",
-                "typing",
                 "time",
+                "typing",
                 "torch",
                 "collections"
             ],
             "module": "modelscope.preprocessors.common"
         },
         "('PREPROCESSORS', 'default', 'Rename')": {
             "filepath": "TEMPLATE_PATH/preprocessors/common.py",
             "imports": [
                 "numpy",
-                "typing",
                 "time",
+                "typing",
                 "torch",
                 "collections"
             ],
             "module": "modelscope.preprocessors.common"
         },
         "('PREPROCESSORS', 'default', 'ToNumpy')": {
             "filepath": "TEMPLATE_PATH/preprocessors/common.py",
             "imports": [
                 "numpy",
-                "typing",
                 "time",
+                "typing",
                 "torch",
                 "collections"
             ],
             "module": "modelscope.preprocessors.common"
         },
         "('PREPROCESSORS', 'default', 'ToTensor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/common.py",
             "imports": [
                 "numpy",
-                "typing",
                 "time",
+                "typing",
                 "torch",
                 "collections"
             ],
             "module": "modelscope.preprocessors.common"
         },
         "('PREPROCESSORS', 'multi-modal', 'clip-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
             "imports": [
                 "numpy",
                 "typing",
-                "decord",
-                "os",
                 "torch",
-                "json",
                 "PIL",
+                "io",
+                "os",
+                "torchvision",
+                "json",
                 "timm",
+                "decord"
+            ],
+            "module": "modelscope.preprocessors.multi_modal"
+        },
+        "('PREPROCESSORS', 'multi-modal', 'diffusion-image-generation-preprocessor')": {
+            "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
+            "imports": [
+                "numpy",
+                "typing",
+                "torch",
+                "PIL",
+                "io",
+                "os",
                 "torchvision",
-                "io"
+                "json",
+                "timm",
+                "decord"
             ],
             "module": "modelscope.preprocessors.multi_modal"
         },
         "('PREPROCESSORS', 'multi-modal', 'hitea-tasks-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
             "imports": [
                 "numpy",
                 "typing",
-                "decord",
-                "os",
                 "torch",
-                "json",
                 "PIL",
-                "timm",
+                "io",
+                "os",
                 "torchvision",
-                "io"
+                "json",
+                "timm",
+                "decord"
             ],
             "module": "modelscope.preprocessors.multi_modal"
         },
         "('PREPROCESSORS', 'multi-modal', 'mplug-tasks-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
             "imports": [
                 "numpy",
                 "typing",
-                "decord",
-                "os",
                 "torch",
-                "json",
                 "PIL",
-                "timm",
+                "io",
+                "os",
                 "torchvision",
-                "io"
+                "json",
+                "timm",
+                "decord"
             ],
             "module": "modelscope.preprocessors.multi_modal"
         },
         "('PREPROCESSORS', 'multi-modal', 'ofa-tasks-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
             "imports": [
                 "numpy",
                 "typing",
-                "decord",
-                "os",
                 "torch",
-                "json",
                 "PIL",
-                "timm",
+                "io",
+                "os",
                 "torchvision",
-                "io"
+                "json",
+                "timm",
+                "decord"
             ],
             "module": "modelscope.preprocessors.multi_modal"
         },
         "('PREPROCESSORS', 'multi-modal', 'vldoc-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
             "imports": [
                 "numpy",
                 "typing",
-                "decord",
-                "os",
                 "torch",
-                "json",
                 "PIL",
-                "timm",
+                "io",
+                "os",
                 "torchvision",
-                "io"
+                "json",
+                "timm",
+                "decord"
             ],
             "module": "modelscope.preprocessors.multi_modal"
         },
         "('PREPROCESSORS', 'nlp', 'Tokenize')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/bert_seq_cls_tokenizer.py",
             "imports": [
                 "typing",
@@ -8881,31 +9097,43 @@
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/text_classification_preprocessor.py",
             "imports": [
                 "numpy",
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.text_classification_preprocessor"
         },
+        "('PREPROCESSORS', 'nlp', 'canmt-translation')": {
+            "filepath": "TEMPLATE_PATH/preprocessors/nlp/canmt_translation.py",
+            "imports": [
+                "typing",
+                "torch",
+                "os",
+                "subword_nmt",
+                "sacremoses",
+                "jieba"
+            ],
+            "module": "modelscope.preprocessors.nlp.canmt_translation"
+        },
         "('PREPROCESSORS', 'nlp', 'conversational-text-to-sql')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/space_T_en/conversational_text_to_sql_preprocessor.py",
             "imports": [
                 "typing",
+                "torch",
                 "text2sql_lgesql",
                 "os",
-                "torch",
                 "json"
             ],
             "module": "modelscope.preprocessors.nlp.space_T_en.conversational_text_to_sql_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'dialog-intent-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/space/dialog_intent_prediction_preprocessor.py",
             "imports": [
                 "typing",
-                "json",
-                "os"
+                "os",
+                "json"
             ],
             "module": "modelscope.preprocessors.nlp.space.dialog_intent_prediction_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'dialog-modeling-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/space/dialog_modeling_preprocessor.py",
             "imports": [
                 "typing",
@@ -8920,47 +9148,47 @@
             ],
             "module": "modelscope.preprocessors.nlp.space.dialog_state_tracking_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'dialog-use-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/dialog_classification_use_preprocessor.py",
             "imports": [
                 "typing",
-                "transformers",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.preprocessors.nlp.dialog_classification_use_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'document-grounded-dialog-generate')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py",
             "imports": [
                 "typing",
-                "transformers",
                 "os",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.preprocessors.nlp.document_grounded_dialog_generate_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'document-grounded-dialog-rerank')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py",
             "imports": [
                 "typing",
-                "transformers",
-                "os",
+                "torch",
                 "copy",
-                "torch"
+                "os",
+                "transformers"
             ],
             "module": "modelscope.preprocessors.nlp.document_grounded_dialog_rerank_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'document-grounded-dialog-retrieval')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py",
             "imports": [
                 "typing",
-                "transformers",
                 "os",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.preprocessors.nlp.document_grounded_dialog_retrieval_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'document-segmentation')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/document_segmentation_preprocessor.py",
             "imports": [
                 "typing"
@@ -8983,49 +9211,49 @@
             ],
             "module": "modelscope.preprocessors.nlp.feature_extraction_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'fill-mask')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/fill_mask_preprocessor.py",
             "imports": [
                 "numpy",
+                "abc",
                 "typing",
-                "os",
+                "re",
                 "torch",
-                "abc",
-                "re"
+                "os"
             ],
             "module": "modelscope.preprocessors.nlp.fill_mask_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'fill-mask-ponet')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/fill_mask_preprocessor.py",
             "imports": [
                 "numpy",
+                "abc",
                 "typing",
-                "os",
+                "re",
                 "torch",
-                "abc",
-                "re"
+                "os"
             ],
             "module": "modelscope.preprocessors.nlp.fill_mask_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'mgeo-ranking')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/mgeo_ranking_preprocessor.py",
             "imports": [
                 "typing",
-                "transformers",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.preprocessors.nlp.mgeo_ranking_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'mglm-summarization')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/mglm_summarization_preprocessor.py",
             "imports": [
-                "re",
+                "typing",
                 "os",
-                "typing"
+                "re"
             ],
             "module": "modelscope.preprocessors.nlp.mglm_summarization_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'ner-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/token_classification_preprocessor.py",
             "imports": [
                 "numpy",
@@ -9100,27 +9328,27 @@
             ],
             "module": "modelscope.preprocessors.nlp.siamese_uie_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'table-question-answering-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/table_question_answering_preprocessor.py",
             "imports": [
                 "typing",
-                "transformers",
                 "os",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.preprocessors.nlp.space_T_cn.table_question_answering_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'text-error-correction')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/text_error_correction.py",
             "imports": [
                 "typing",
-                "transformers",
                 "os",
-                "torch"
+                "torch",
+                "transformers"
             ],
             "module": "modelscope.preprocessors.nlp.text_error_correction"
         },
         "('PREPROCESSORS', 'nlp', 'text-gen-jieba-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/text_generation_preprocessor.py",
             "imports": [
                 "numpy",
@@ -9198,17 +9426,17 @@
             "module": "modelscope.preprocessors.nlp.token_classification_viet_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'word-alignment')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/word_alignment_preprocessor.py",
             "imports": [
                 "numpy",
                 "typing",
-                "os",
+                "itertools",
                 "torch",
-                "itertools"
+                "os"
             ],
             "module": "modelscope.preprocessors.nlp.word_alignment_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'word-segment-text-to-label-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/token_classification_preprocessor.py",
             "imports": [
                 "numpy",
@@ -9223,70 +9451,70 @@
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.zero_shot_classification_preprocessor"
         },
         "('PREPROCESSORS', 'science', 'unifold-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/science/uni_fold.py",
             "imports": [
+                "logging",
                 "time",
-                "pathlib",
-                "torch",
+                "re",
+                "os",
                 "gzip",
-                "json",
                 "pickle",
                 "tqdm",
-                "logging",
-                "requests",
                 "numpy",
-                "typing",
-                "random",
                 "tarfile",
-                "hashlib",
-                "os",
                 "ipdb",
                 "unittest",
-                "re"
+                "typing",
+                "torch",
+                "requests",
+                "random",
+                "json",
+                "pathlib",
+                "hashlib"
             ],
             "module": "modelscope.preprocessors.science.uni_fold"
         },
         "('PREPROCESSORS', 'text-to-speech', 'kantts-data-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/tts.py",
             "imports": [
-                "kantts",
                 "typing",
-                "os"
+                "os",
+                "kantts"
             ],
             "module": "modelscope.preprocessors.tts"
         },
         "('ROI_EXTRACTORS', 'default', 'SingleRoINExtractor')": {
             "filepath": "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py",
             "imports": [
                 "mmcv",
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.abnormal_object_detection.mmdet_ms.roi_head.roi_extractors.single_level_roi_extractor"
         },
         "('TRACKERS', 'default', 'QuasiDenseEmbedTracker')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/track/quasi_dense_embed_tracker.py",
             "imports": [
                 "mmcv",
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_panoptic_segmentation.track.quasi_dense_embed_tracker"
         },
         "('TRAINERS', 'default', 'action-detection')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/action_detection_trainer.py",
             "imports": [
+                "fvcore",
                 "typing",
-                "detectron2",
+                "torch",
                 "os",
-                "fvcore",
-                "torch"
+                "detectron2"
             ],
             "module": "modelscope.trainers.cv.action_detection_trainer"
         },
         "('TRAINERS', 'default', 'bert-sentiment-analysis')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/sequence_classification_trainer.py",
             "imports": [
                 "numpy",
@@ -9300,39 +9528,39 @@
             "imports": [],
             "module": "modelscope.trainers.cv.card_detection_scrfd_trainer"
         },
         "('TRAINERS', 'default', 'cartoon-translation')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/cartoon_translation_trainer.py",
             "imports": [
                 "numpy",
+                "packaging",
                 "typing",
                 "os",
-                "packaging",
                 "tqdm",
                 "tensorflow"
             ],
             "module": "modelscope.trainers.cv.cartoon_translation_trainer"
         },
         "('TRAINERS', 'default', 'clip-multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/trainers/multi_modal/clip/clip_trainer.py",
             "imports": [
                 "typing",
-                "math",
                 "os",
+                "math",
                 "torch"
             ],
             "module": "modelscope.trainers.multi_modal.clip.clip_trainer"
         },
         "('TRAINERS', 'default', 'csanmt-translation')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/csanmt_translation_trainer.py",
             "imports": [
-                "typing",
                 "tensorflow",
-                "time",
-                "os"
+                "typing",
+                "os",
+                "time"
             ],
             "module": "modelscope.trainers.nlp.csanmt_translation_trainer"
         },
         "('TRAINERS', 'default', 'dialog-intent-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/space/dialog_intent_trainer.py",
             "imports": [
                 "numpy",
@@ -9342,147 +9570,155 @@
             "module": "modelscope.trainers.nlp.space.dialog_intent_trainer"
         },
         "('TRAINERS', 'default', 'dialog-modeling-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/space/dialog_modeling_trainer.py",
             "imports": [
                 "numpy",
                 "typing",
-                "time",
-                "os"
+                "os",
+                "time"
             ],
             "module": "modelscope.trainers.nlp.space.dialog_modeling_trainer"
         },
         "('TRAINERS', 'default', 'document-grounded-dialog-generate-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_generate_trainer.py",
             "imports": [
+                "re",
                 "transformers",
                 "sacrebleu",
-                "os",
+                "string",
                 "torch",
-                "json",
-                "tqdm",
-                "re",
                 "collections",
-                "rouge",
-                "string"
+                "os",
+                "tqdm",
+                "json",
+                "rouge"
             ],
             "module": "modelscope.trainers.nlp.document_grounded_dialog_generate_trainer"
         },
         "('TRAINERS', 'default', 'document-grounded-dialog-rerank-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_rerank_trainer.py",
             "imports": [
                 "numpy",
-                "typing",
-                "transformers",
                 "time",
-                "random",
+                "typing",
+                "torch",
                 "os",
-                "torch"
+                "random",
+                "transformers"
             ],
             "module": "modelscope.trainers.nlp.document_grounded_dialog_rerank_trainer"
         },
         "('TRAINERS', 'default', 'document-grounded-dialog-retrieval-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_retrieval_trainer.py",
             "imports": [
                 "numpy",
-                "transformers",
-                "os",
                 "torch",
-                "json",
+                "os",
+                "faiss",
                 "tqdm",
-                "faiss"
+                "json",
+                "transformers"
             ],
             "module": "modelscope.trainers.nlp.document_grounded_dialog_retrieval_trainer"
         },
         "('TRAINERS', 'default', 'dummy')": {
             "filepath": "TEMPLATE_PATH/trainers/base.py",
             "imports": [
                 "abc",
                 "typing",
-                "time",
-                "os"
+                "os",
+                "time"
             ],
             "module": "modelscope.trainers.base"
         },
         "('TRAINERS', 'default', 'easycv')": {
             "filepath": "TEMPLATE_PATH/trainers/easycv/trainer.py",
             "imports": [
                 "typing",
-                "functools",
-                "easycv",
                 "torch",
-                "copy"
+                "functools",
+                "copy",
+                "easycv"
             ],
             "module": "modelscope.trainers.easycv.trainer"
         },
+        "('TRAINERS', 'default', 'efficient-diffusion-tuning')": {
+            "filepath": "TEMPLATE_PATH/trainers/multi_modal/efficient_diffusion_tuning/efficient_diffusion_tuning_trainer.py",
+            "imports": [
+                "typing",
+                "torch"
+            ],
+            "module": "modelscope.trainers.multi_modal.efficient_diffusion_tuning.efficient_diffusion_tuning_trainer"
+        },
         "('TRAINERS', 'default', 'face-detection-scrfd')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/face_detection_scrfd_trainer.py",
             "imports": [
                 "typing",
-                "time",
+                "copy",
                 "os",
-                "copy"
+                "time"
             ],
             "module": "modelscope.trainers.cv.face_detection_scrfd_trainer"
         },
         "('TRAINERS', 'default', 'faq-question-answering-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/faq_question_answering_trainer.py",
             "imports": [
                 "numpy",
-                "typing",
-                "functools",
                 "distutils",
+                "dataclasses",
                 "contextlib",
+                "typing",
                 "torch",
-                "dataclasses",
+                "functools",
                 "collections"
             ],
             "module": "modelscope.trainers.nlp.faq_question_answering_trainer"
         },
         "('TRAINERS', 'default', 'image-classification')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/image_classifition_trainer.py",
             "imports": [
                 "numpy",
-                "typing",
                 "time",
-                "os",
+                "typing",
+                "torch",
                 "copy",
-                "torch"
+                "os"
             ],
             "module": "modelscope.trainers.cv.image_classifition_trainer"
         },
         "('TRAINERS', 'default', 'image-classification-team')": {
             "filepath": "TEMPLATE_PATH/trainers/multi_modal/team/team_trainer.py",
             "imports": [
                 "numpy",
                 "typing",
                 "sklearn",
-                "os",
                 "torch",
-                "collections"
+                "collections",
+                "os"
             ],
             "module": "modelscope.trainers.multi_modal.team.team_trainer"
         },
         "('TRAINERS', 'default', 'image-fewshot-detection')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/image_defrcn_fewshot_detection_trainer.py",
             "imports": [
                 "typing",
-                "detectron2",
-                "os",
                 "torch",
-                "collections"
+                "collections",
+                "os",
+                "detectron2"
             ],
             "module": "modelscope.trainers.cv.image_defrcn_fewshot_detection_trainer"
         },
         "('TRAINERS', 'default', 'image-inpainting')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/image_inpainting_trainer.py",
             "imports": [
-                "time",
                 "collections",
-                "torch"
+                "torch",
+                "time"
             ],
             "module": "modelscope.trainers.cv.image_inpainting_trainer"
         },
         "('TRAINERS', 'default', 'image-instance-segmentation')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/image_instance_segmentation_trainer.py",
             "imports": [],
             "module": "modelscope.trainers.cv.image_instance_segmentation_trainer"
@@ -9495,44 +9731,44 @@
             ],
             "module": "modelscope.trainers.cv.image_portrait_enhancement_trainer"
         },
         "('TRAINERS', 'default', 'mgeo-ranking-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/multi_modal/mgeo_ranking_trainer.py",
             "imports": [
                 "typing",
-                "dataclasses",
-                "torch"
+                "torch",
+                "dataclasses"
             ],
             "module": "modelscope.trainers.multi_modal.mgeo_ranking_trainer"
         },
         "('TRAINERS', 'default', 'movie-scene-segmentation')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/movie_scene_segmentation_trainer.py",
             "imports": [],
             "module": "modelscope.trainers.cv.movie_scene_segmentation_trainer"
         },
         "('TRAINERS', 'default', 'mplug')": {
             "filepath": "TEMPLATE_PATH/trainers/multi_modal/mplug/mplug_trainer.py",
             "imports": [
-                "typing",
                 "collections",
+                "typing",
                 "torch"
             ],
             "module": "modelscope.trainers.multi_modal.mplug.mplug_trainer"
         },
         "('TRAINERS', 'default', 'nerf-recon-acc')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/nerf_recon_acc_trainer.py",
             "imports": [
                 "numpy",
-                "typing",
                 "time",
-                "random",
+                "glob",
+                "typing",
+                "torch",
                 "os",
                 "cv2",
-                "torch",
-                "glob",
+                "random",
                 "tqdm",
                 "datetime"
             ],
             "module": "modelscope.trainers.cv.nerf_recon_acc_trainer"
         },
         "('TRAINERS', 'default', 'nlp-base-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp_trainer.py",
@@ -9544,64 +9780,64 @@
             ],
             "module": "modelscope.trainers.nlp_trainer"
         },
         "('TRAINERS', 'default', 'nlp-gpt-moe-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/gpt_moe_trainer.py",
             "imports": [
                 "typing",
-                "megatron_util",
-                "os",
                 "torch",
-                "collections"
+                "collections",
+                "os",
+                "megatron_util"
             ],
             "module": "modelscope.trainers.nlp.gpt_moe_trainer"
         },
         "('TRAINERS', 'default', 'nlp-gpt3-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/gpt3_trainer.py",
             "imports": [
                 "typing",
-                "os",
                 "copy",
+                "os",
                 "torch"
             ],
             "module": "modelscope.trainers.nlp.gpt3_trainer"
         },
         "('TRAINERS', 'default', 'nlp-plug-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/plug_trainer.py",
             "imports": [
-                "typing",
-                "megatron_util",
                 "deepspeed",
+                "typing",
+                "torch",
                 "os",
-                "torch"
+                "megatron_util"
             ],
             "module": "modelscope.trainers.nlp.plug_trainer"
         },
         "('TRAINERS', 'default', 'nlp-sentence-embedding-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/sentence_embedding_trainer.py",
             "imports": [
                 "numpy",
-                "typing",
-                "transformers",
                 "time",
+                "dataclasses",
+                "typing",
                 "torch",
                 "tqdm",
-                "dataclasses"
+                "transformers"
             ],
             "module": "modelscope.trainers.nlp.sentence_embedding_trainer"
         },
         "('TRAINERS', 'default', 'nlp-text-ranking-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/text_ranking_trainer.py",
             "imports": [
                 "numpy",
-                "typing",
                 "time",
+                "dataclasses",
+                "typing",
                 "torch",
-                "tqdm",
-                "dataclasses"
+                "tqdm"
             ],
             "module": "modelscope.trainers.nlp.text_ranking_trainer"
         },
         "('TRAINERS', 'default', 'nlp-veco-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp_trainer.py",
             "imports": [
                 "numpy",
@@ -9611,46 +9847,46 @@
             ],
             "module": "modelscope.trainers.nlp_trainer"
         },
         "('TRAINERS', 'default', 'ocr-detection-db')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/ocr_detection_db_trainer.py",
             "imports": [
                 "numpy",
-                "typing",
                 "time",
-                "os",
-                "copy",
+                "typing",
                 "torch",
+                "copy",
+                "os",
                 "easydict",
-                "math",
                 "tqdm",
+                "math",
                 "datetime"
             ],
             "module": "modelscope.trainers.cv.ocr_detection_db_trainer"
         },
         "('TRAINERS', 'default', 'ocr-recognition')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/ocr_recognition_trainer.py",
             "imports": [
-                "time",
                 "collections",
-                "torch"
+                "torch",
+                "time"
             ],
             "module": "modelscope.trainers.cv.ocr_recognition_trainer"
         },
         "('TRAINERS', 'default', 'ofa')": {
             "filepath": "TEMPLATE_PATH/trainers/multi_modal/ofa/ofa_trainer.py",
             "imports": [
                 "typing",
-                "functools",
                 "shutil",
-                "os",
-                "tempfile",
                 "torch",
-                "json",
-                "math"
+                "functools",
+                "tempfile",
+                "os",
+                "math",
+                "json"
             ],
             "module": "modelscope.trainers.multi_modal.ofa.ofa_trainer"
         },
         "('TRAINERS', 'default', 'referring-video-object-segmentation')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/referring_video_object_segmentation_trainer.py",
             "imports": [
                 "os",
@@ -9658,162 +9894,162 @@
             ],
             "module": "modelscope.trainers.cv.referring_video_object_segmentation_trainer"
         },
         "('TRAINERS', 'default', 'siamese-uie-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/siamese_uie_trainer.py",
             "imports": [
                 "numpy",
-                "typing",
                 "time",
-                "random",
-                "os",
+                "typing",
                 "torch",
-                "json",
+                "collections",
+                "os",
+                "random",
                 "math",
-                "collections"
+                "json"
             ],
             "module": "modelscope.trainers.nlp.siamese_uie_trainer"
         },
         "('TRAINERS', 'default', 'speech-asr-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/audio/asr_trainer.py",
             "imports": [
                 "typing",
                 "shutil",
-                "os",
+                "funasr",
                 "tempfile",
-                "json",
-                "funasr"
+                "os",
+                "json"
             ],
             "module": "modelscope.trainers.audio.asr_trainer"
         },
         "('TRAINERS', 'default', 'speech-kantts-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/audio/tts_trainer.py",
             "imports": [
                 "typing",
                 "shutil",
-                "os",
                 "tempfile",
                 "zipfile",
+                "os",
                 "json"
             ],
             "module": "modelscope.trainers.audio.tts_trainer"
         },
         "('TRAINERS', 'default', 'speech-separation')": {
             "filepath": "TEMPLATE_PATH/trainers/audio/separation_trainer.py",
             "imports": [
                 "numpy",
-                "typing",
                 "csv",
-                "os",
+                "torchaudio",
+                "typing",
                 "torch",
+                "os",
                 "speechbrain",
-                "tqdm",
-                "torchaudio"
+                "tqdm"
             ],
             "module": "modelscope.trainers.audio.separation_trainer"
         },
         "('TRAINERS', 'default', 'speech_dfsmn_kws_char_farfield')": {
             "filepath": "TEMPLATE_PATH/trainers/audio/kws_farfield_trainer.py",
             "imports": [
                 "numpy",
+                "glob",
                 "typing",
-                "os",
                 "torch",
-                "math",
+                "os",
                 "pickle",
-                "glob",
+                "math",
                 "datetime"
             ],
             "module": "modelscope.trainers.audio.kws_farfield_trainer"
         },
         "('TRAINERS', 'default', 'speech_frcrn_ans_cirm_16k')": {
             "filepath": "TEMPLATE_PATH/trainers/audio/ans_trainer.py",
             "imports": [],
             "module": "modelscope.trainers.audio.ans_trainer"
         },
         "('TRAINERS', 'default', 'speech_kws_fsmn_char_ctc_nearfield')": {
             "filepath": "TEMPLATE_PATH/trainers/audio/kws_nearfield_trainer.py",
             "imports": [
                 "typing",
-                "os",
-                "copy",
+                "re",
                 "torch",
+                "copy",
+                "os",
                 "tensorboardX",
                 "yaml",
-                "re",
                 "datetime"
             ],
             "module": "modelscope.trainers.audio.kws_nearfield_trainer"
         },
         "('TRAINERS', 'default', 'table-question-answering-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/table_question_answering_trainer.py",
             "imports": [
                 "numpy",
-                "typing",
                 "time",
-                "os",
+                "typing",
                 "torch",
-                "json",
-                "tqdm"
+                "os",
+                "tqdm",
+                "json"
             ],
             "module": "modelscope.trainers.nlp.table_question_answering_trainer"
         },
         "('TRAINERS', 'default', 'text-generation-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/text_generation_trainer.py",
             "imports": [
                 "collections",
                 "torch"
             ],
             "module": "modelscope.trainers.nlp.text_generation_trainer"
         },
         "('TRAINERS', 'default', 'tinynas-damoyolo')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/image_detection_damoyolo_trainer.py",
             "imports": [
-                "typing",
                 "time",
+                "typing",
+                "torch",
                 "os",
                 "easydict",
-                "torch",
                 "math",
                 "datetime"
             ],
             "module": "modelscope.trainers.cv.image_detection_damoyolo_trainer"
         },
         "('TRAINERS', 'default', 'trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/trainer.py",
             "imports": [
-                "typing",
-                "functools",
                 "distutils",
                 "inspect",
+                "typing",
+                "torch",
+                "functools",
+                "collections",
                 "os",
                 "copy",
-                "torch",
-                "json",
-                "collections"
+                "json"
             ],
             "module": "modelscope.trainers.trainer"
         },
         "('TRAINERS', 'default', 'vision-efficient-tuning')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/vision_efficient_tuning_trainer.py",
             "imports": [
                 "typing",
                 "torch"
             ],
             "module": "modelscope.trainers.cv.vision_efficient_tuning_trainer"
         },
         "('TRANSFORMER', 'default', 'PETRDNTransformer')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py",
             "imports": [
+                "warnings",
+                "mmcv",
+                "copy",
                 "mmdet",
                 "typing",
                 "math",
-                "mmcv",
-                "warnings",
-                "copy",
                 "torch"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.petr_transformer"
         },
         "('TRANSFORMER_LAYER', 'default', 'KernelUpdator')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_updator.py",
             "imports": [
@@ -9821,126 +10057,126 @@
                 "torch"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.head.kernel_updator"
         },
         "('TRANSFORMER_LAYER', 'default', 'PETRTransformerDecoderLayer')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py",
             "imports": [
+                "warnings",
+                "mmcv",
+                "copy",
                 "mmdet",
                 "typing",
                 "math",
-                "mmcv",
-                "warnings",
-                "copy",
                 "torch"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.petr_transformer"
         },
         "('TRANSFORMER_LAYER_SEQUENCE', 'default', 'PETRTransformerDecoder')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py",
             "imports": [
+                "warnings",
+                "mmcv",
+                "copy",
                 "mmdet",
                 "typing",
                 "math",
-                "mmcv",
-                "warnings",
-                "copy",
                 "torch"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.petr_transformer"
         },
         "('TRANSFORMER_LAYER_SEQUENCE', 'default', 'PETRTransformerEncoder')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py",
             "imports": [
+                "warnings",
+                "mmcv",
+                "copy",
                 "mmdet",
                 "typing",
                 "math",
-                "mmcv",
-                "warnings",
-                "copy",
                 "torch"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.petr_transformer"
         }
     },
-    "md5": "5445a25ad017c0cdc7daa23251dfbed1",
+    "md5": "ffb6080a88345fc20edce316e4ce8b50",
     "modelscope_path": "TEMPLATE_PATH",
     "requirements": {
         "modelscope.exporters.base": [
             "abc",
             "typing",
             "os"
         ],
         "modelscope.exporters.builder": [],
         "modelscope.exporters.cv.cartoon_translation_exporter": [
-            "typing",
             "tensorflow",
+            "typing",
             "os",
             "packaging"
         ],
         "modelscope.exporters.cv.face_detection_scrfd_exporter": [
             "numpy",
             "typing",
+            "torch",
             "functools",
             "os",
-            "torch",
             "onnx"
         ],
         "modelscope.exporters.cv.object_detection_damoyolo_exporter": [
             "numpy",
             "typing",
+            "torch",
             "functools",
             "os",
-            "torch",
             "onnx"
         ],
         "modelscope.exporters.nlp.csanmt_for_translation_exporter": [
             "typing",
             "tensorflow",
             "os"
         ],
         "modelscope.exporters.nlp.model_for_token_classification_exporter": [
-            "typing",
             "collections",
+            "typing",
             "torch"
         ],
         "modelscope.exporters.nlp.sbert_for_sequence_classification_exporter": [
-            "typing",
             "collections",
+            "typing",
             "torch"
         ],
         "modelscope.exporters.nlp.sbert_for_zero_shot_classification_exporter": [
-            "typing",
-            "collections"
+            "collections",
+            "typing"
         ],
         "modelscope.exporters.tf_model_exporter": [
-            "typing",
             "tensorflow",
+            "typing",
             "os"
         ],
         "modelscope.exporters.torch_model_exporter": [
-            "typing",
             "contextlib",
-            "os",
+            "typing",
+            "itertools",
             "torch",
-            "itertools"
+            "os"
         ],
         "modelscope.metrics.accuracy_metric": [
             "numpy",
             "typing"
         ],
         "modelscope.metrics.action_detection_evaluator": [
+            "logging",
             "numpy",
             "scipy",
-            "detectron2",
-            "os",
+            "collections",
             "copy",
             "pandas",
-            "collections",
-            "logging"
+            "os",
+            "detectron2"
         ],
         "modelscope.metrics.audio_noise_metric": [
             "typing"
         ],
         "modelscope.metrics.base": [
             "abc",
             "typing"
@@ -9954,79 +10190,81 @@
             "typing"
         ],
         "modelscope.metrics.ciderD.ciderD": [
             "__future__"
         ],
         "modelscope.metrics.ciderD.ciderD_scorer": [
             "numpy",
-            "os",
-            "copy",
-            "math",
-            "__future__",
             "pdb",
+            "__future__",
             "collections",
+            "copy",
+            "os",
+            "math",
             "six"
         ],
         "modelscope.metrics.image_color_enhance_metric": [
             "numpy",
             "typing",
             "cv2"
         ],
         "modelscope.metrics.image_colorization_metric": [
             "numpy",
-            "typing",
             "scipy",
-            "torch"
+            "typing",
+            "torch",
+            "torchvision",
+            "cv2"
         ],
         "modelscope.metrics.image_denoise_metric": [
             "numpy",
             "typing",
             "cv2",
             "torch"
         ],
         "modelscope.metrics.image_inpainting_metric": [
             "numpy",
             "typing",
-            "scipy",
-            "torch"
+            "torch",
+            "scipy"
         ],
         "modelscope.metrics.image_instance_segmentation_metric": [
             "numpy",
             "typing",
-            "pycocotools",
-            "os",
             "tempfile",
-            "collections"
+            "collections",
+            "os",
+            "pycocotools"
         ],
         "modelscope.metrics.image_portrait_enhancement_metric": [
             "numpy",
             "typing",
             "cv2"
         ],
         "modelscope.metrics.image_quality_assessment_degradation_metric": [
             "numpy",
-            "typing",
             "scipy",
-            "cv2",
+            "sys",
+            "typing",
+            "torch",
             "tempfile",
+            "collections",
             "os",
-            "torch",
-            "sys",
-            "tqdm",
-            "collections"
+            "cv2",
+            "tqdm"
         ],
         "modelscope.metrics.image_quality_assessment_mos_metric": [
             "numpy",
-            "typing",
             "scipy",
-            "cv2",
+            "sys",
+            "typing",
+            "torch",
             "tempfile",
             "os",
-            "torch",
-            "sys",
+            "cv2",
             "tqdm"
         ],
         "modelscope.metrics.inbatch_recall_metric": [
             "numpy",
             "typing",
             "torch"
         ],
@@ -10045,16 +10283,16 @@
         ],
         "modelscope.metrics.ned_metric": [
             "numpy",
             "typing"
         ],
         "modelscope.metrics.ocr_recognition_metric": [
             "numpy",
-            "typing",
             "edit_distance",
+            "typing",
             "torch"
         ],
         "modelscope.metrics.ppl_metric": [
             "numpy",
             "typing",
             "math",
             "torch"
@@ -10062,52 +10300,52 @@
         "modelscope.metrics.prediction_saving_wrapper": [
             "numpy",
             "typing",
             "sklearn"
         ],
         "modelscope.metrics.referring_video_object_segmentation_metric": [
             "numpy",
-            "typing",
             "pycocotools",
+            "typing",
             "torch",
             "tqdm"
         ],
         "modelscope.metrics.sequence_classification_metric": [
             "numpy",
             "typing",
             "sklearn"
         ],
         "modelscope.metrics.text_generation_metric": [
-            "nltk",
             "typing",
+            "nltk",
             "rouge"
         ],
         "modelscope.metrics.text_ranking_metric": [
             "numpy",
             "typing"
         ],
         "modelscope.metrics.token_classification_metric": [
             "numpy",
             "typing",
             "importlib"
         ],
         "modelscope.metrics.video_frame_interpolation_metric": [
-            "lpips",
-            "typing",
             "numpy",
+            "typing",
             "torch",
+            "lpips",
             "math"
         ],
         "modelscope.metrics.video_stabilization_metric": [
             "numpy",
+            "sys",
             "typing",
-            "cv2",
-            "os",
             "tempfile",
-            "sys",
+            "os",
+            "cv2",
             "tqdm"
         ],
         "modelscope.metrics.video_summarization_metric": [
             "numpy",
             "typing"
         ],
         "modelscope.metrics.video_super_resolution_metric.matlab_functions": [
@@ -10136,41 +10374,41 @@
             "torch"
         ],
         "modelscope.models.audio.aec.layers.deep_fsmn": [
             "numpy",
             "torch"
         ],
         "modelscope.models.audio.aec.layers.layer_base": [
+            "numpy",
             "abc",
             "re",
-            "numpy",
             "torch"
         ],
         "modelscope.models.audio.aec.layers.uni_deep_fsmn": [
             "numpy",
             "torch"
         ],
         "modelscope.models.audio.aec.network.loss": [
             "torch"
         ],
         "modelscope.models.audio.aec.network.modulation_loss": [
             "math",
-            "torchaudio",
-            "torch"
+            "torch",
+            "torchaudio"
         ],
         "modelscope.models.audio.aec.network.se_net": [
             "torch"
         ],
         "modelscope.models.audio.ans.complex_nn": [
             "torch"
         ],
         "modelscope.models.audio.ans.conv_stft": [
             "numpy",
-            "scipy",
-            "torch"
+            "torch",
+            "scipy"
         ],
         "modelscope.models.audio.ans.denoise_net": [
             "torch"
         ],
         "modelscope.models.audio.ans.frcrn": [
             "typing",
             "os",
@@ -10179,17 +10417,17 @@
         "modelscope.models.audio.ans.layers.activations": [
             "torch"
         ],
         "modelscope.models.audio.ans.layers.affine_transform": [
             "torch"
         ],
         "modelscope.models.audio.ans.layers.layer_base": [
+            "numpy",
             "abc",
             "torch",
-            "numpy",
             "six"
         ],
         "modelscope.models.audio.ans.layers.uni_deep_fsmn": [
             "numpy",
             "torch"
         ],
         "modelscope.models.audio.ans.se_module_complex": [
@@ -10199,18 +10437,18 @@
             "torch"
         ],
         "modelscope.models.audio.asr.generic_automatic_speech_recognition": [
             "typing",
             "os"
         ],
         "modelscope.models.audio.asr.wenet_automatic_speech_recognition": [
-            "typing",
-            "json",
             "wenetruntime",
-            "os"
+            "typing",
+            "os",
+            "json"
         ],
         "modelscope.models.audio.itn.generic_inverse_text_processing": [
             "typing",
             "os"
         ],
         "modelscope.models.audio.kws.farfield.fsmn": [
             "numpy",
@@ -10221,17 +10459,17 @@
         ],
         "modelscope.models.audio.kws.farfield.model": [
             "typing",
             "os",
             "tempfile"
         ],
         "modelscope.models.audio.kws.farfield.model_def": [
+            "struct",
             "math",
-            "enum",
-            "struct"
+            "enum"
         ],
         "modelscope.models.audio.kws.generic_key_word_spotting": [
             "typing",
             "os"
         ],
         "modelscope.models.audio.kws.nearfield.cmvn": [
             "numpy",
@@ -10240,74 +10478,84 @@
         ],
         "modelscope.models.audio.kws.nearfield.fsmn": [
             "numpy",
             "typing",
             "torch"
         ],
         "modelscope.models.audio.kws.nearfield.model": [
+            "sys",
             "typing",
-            "os",
-            "tempfile",
             "torch",
-            "sys"
+            "tempfile",
+            "os"
         ],
         "modelscope.models.audio.punc.generic_punctuation": [
             "typing",
             "os"
         ],
         "modelscope.models.audio.separation.layer_norm": [
-            "__future__",
-            "torch"
+            "torch",
+            "__future__"
         ],
         "modelscope.models.audio.separation.mossformer": [
             "typing",
-            "os",
             "copy",
+            "os",
             "torch"
         ],
         "modelscope.models.audio.separation.mossformer_block": [
             "torch"
         ],
         "modelscope.models.audio.separation.mossformer_conv_module": [
             "torch"
         ],
+        "modelscope.models.audio.sv.DTDNN": [
+            "torchaudio",
+            "typing",
+            "torch",
+            "collections",
+            "os"
+        ],
+        "modelscope.models.audio.sv.DTDNN_layers": [
+            "torch"
+        ],
         "modelscope.models.audio.sv.ecapa_tdnn": [
+            "torchaudio",
             "typing",
-            "os",
             "torch",
-            "math",
-            "torchaudio"
+            "os",
+            "math"
         ],
         "modelscope.models.audio.sv.generic_speaker_verification": [
             "typing",
             "os"
         ],
         "modelscope.models.audio.tts.sambert_hifi": [
             "numpy",
-            "__future__",
-            "shutil",
-            "os",
-            "zipfile",
-            "json",
             "wave",
             "yaml",
+            "shutil",
+            "__future__",
             "matplotlib",
+            "zipfile",
+            "os",
+            "json",
             "datetime"
         ],
         "modelscope.models.audio.tts.voice": [
             "numpy",
             "time",
-            "os",
+            "yaml",
             "torch",
+            "collections",
+            "os",
             "threading",
-            "json",
-            "pickle",
-            "yaml",
             "kantts",
-            "collections"
+            "pickle",
+            "json"
         ],
         "modelscope.models.base.base_head": [
             "abc",
             "typing"
         ],
         "modelscope.models.base.base_model": [
             "abc",
@@ -10315,118 +10563,118 @@
             "os"
         ],
         "modelscope.models.base.base_torch_head": [
             "typing",
             "torch"
         ],
         "modelscope.models.base.base_torch_model": [
+            "packaging",
             "typing",
+            "torch",
             "functools",
-            "os",
             "copy",
-            "torch",
-            "packaging"
+            "os"
         ],
         "modelscope.models.builder": [],
         "modelscope.models.cv.abnormal_object_detection.mmdet_model": [
             "numpy",
             "os",
             "torch"
         ],
         "modelscope.models.cv.abnormal_object_detection.mmdet_ms.roi_head.mask_scoring_roi_head": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.abnormal_object_detection.mmdet_ms.roi_head.roi_extractors.single_level_roi_extractor": [
             "mmcv",
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.action_detection.action_detection_onnx": [
             "numpy",
-            "subprocess",
-            "shutil",
             "urllib",
-            "cv2",
-            "os",
+            "shutil",
             "tempfile",
-            "onnxruntime",
-            "uuid"
+            "os",
+            "cv2",
+            "subprocess",
+            "uuid",
+            "onnxruntime"
         ],
         "modelscope.models.cv.action_detection.modules.action_detection_pytorch": [
-            "typing",
-            "detectron2",
+            "logging",
             "fvcore",
+            "typing",
             "torch",
-            "logging"
+            "detectron2"
         ],
         "modelscope.models.cv.action_detection.modules.resnet": [
-            "detectron2",
-            "torch"
+            "torch",
+            "detectron2"
         ],
         "modelscope.models.cv.action_recognition.models": [
             "torch"
         ],
         "modelscope.models.cv.action_recognition.s3dg": [
             "torch"
         ],
         "modelscope.models.cv.action_recognition.tada_convnext": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.action_recognition.temporal_patch_shift_transformer": [
             "numpy",
             "operator",
-            "functools",
-            "torch",
             "abc",
-            "einops",
+            "torch",
+            "functools",
             "torchvision",
-            "timm"
+            "timm",
+            "einops"
         ],
         "modelscope.models.cv.animal_recognition.resnet": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.animal_recognition.splat": [
             "torch"
         ],
         "modelscope.models.cv.bad_image_detecting.bad_image_detecting": [
             "numpy",
             "typing",
-            "os",
             "torch",
+            "os",
             "torchvision"
         ],
         "modelscope.models.cv.body_2d_keypoints.hrnet_basic_modules": [
             "torch"
         ],
         "modelscope.models.cv.body_2d_keypoints.hrnet_v2": [
             "numpy",
             "os",
             "torch"
         ],
         "modelscope.models.cv.body_2d_keypoints.w48": [],
         "modelscope.models.cv.body_3d_keypoints.cannonical_pose.body_3d_pose": [
+            "logging",
             "numpy",
             "typing",
-            "os",
             "torch",
-            "logging"
+            "os"
         ],
         "modelscope.models.cv.body_3d_keypoints.cannonical_pose.canonical_pose_modules": [
             "torch"
         ],
         "modelscope.models.cv.body_3d_keypoints.hdformer.backbone": [
             "torch"
         ],
         "modelscope.models.cv.body_3d_keypoints.hdformer.block": [
             "math",
-            "einops",
-            "torch"
+            "torch",
+            "einops"
         ],
         "modelscope.models.cv.body_3d_keypoints.hdformer.directed_graph": [
             "numpy",
             "sys",
             "typing"
         ],
         "modelscope.models.cv.body_3d_keypoints.hdformer.hdformer": [
@@ -10448,34 +10696,34 @@
             "numpy",
             "os",
             "easydict"
         ],
         "modelscope.models.cv.cartoon.facelib.face_detector": [
             "numpy",
             "tensorflow",
-            "time",
-            "cv2"
+            "cv2",
+            "time"
         ],
         "modelscope.models.cv.cartoon.facelib.face_landmark": [
             "numpy",
             "tensorflow",
             "cv2"
         ],
         "modelscope.models.cv.cartoon.facelib.facer": [
             "numpy",
-            "time",
-            "cv2"
+            "cv2",
+            "time"
         ],
         "modelscope.models.cv.cartoon.loss": [
             "numpy",
+            "joblib",
             "scipy",
-            "os",
             "skimage",
-            "tensorflow",
-            "joblib"
+            "os",
+            "tensorflow"
         ],
         "modelscope.models.cv.cartoon.model_tf": [
             "typing",
             "tensorflow"
         ],
         "modelscope.models.cv.cartoon.mtcnn_pytorch.src.align_trans": [
             "numpy",
@@ -10485,42 +10733,42 @@
             "numpy"
         ],
         "modelscope.models.cv.cartoon.network": [
             "tensorflow"
         ],
         "modelscope.models.cv.cartoon.utils": [
             "numpy",
-            "tensorflow",
-            "random",
+            "os",
             "cv2",
-            "os"
+            "random",
+            "tensorflow"
         ],
         "modelscope.models.cv.cmdssl_video_embedding.c3d": [
             "torch"
         ],
         "modelscope.models.cv.cmdssl_video_embedding.resnet2p1d": [
             "torch"
         ],
         "modelscope.models.cv.cmdssl_video_embedding.resnet3d": [
             "torch"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.annotator": [
             "numpy",
-            "mmseg",
             "mmcv",
-            "cv2",
-            "os",
             "torch",
+            "os",
+            "cv2",
+            "mmseg",
             "einops"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.midas.api": [
-            "torchvision",
-            "cv2",
             "os",
-            "torch"
+            "cv2",
+            "torch",
+            "torchvision"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.midas.midas.base_model": [
             "torch"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.midas.midas.blocks": [
             "torch"
         ],
@@ -10536,144 +10784,144 @@
         "modelscope.models.cv.controllable_image_generation.annotator.midas.midas.transforms": [
             "numpy",
             "math",
             "cv2"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.midas.midas.vit": [
             "timm",
+            "torch",
             "math",
-            "types",
-            "torch"
+            "types"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.midas.utils": [
             "numpy",
+            "cv2",
             "sys",
             "re",
-            "cv2",
             "torch"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.mlsd.mbv2_mlsd_large": [
             "sys",
             "os",
             "torch"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.mlsd.utils": [
             "numpy",
-            "cv2",
             "os",
+            "cv2",
             "torch"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.openpose.body": [
             "numpy",
-            "scipy",
             "time",
-            "cv2",
+            "scipy",
             "torch",
-            "math",
             "torchvision",
+            "cv2",
+            "math",
             "matplotlib"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.openpose.hand": [
             "numpy",
-            "scipy",
             "time",
-            "cv2",
+            "scipy",
             "torch",
+            "skimage",
+            "cv2",
             "math",
             "json",
-            "skimage",
             "matplotlib"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.openpose.model": [
             "collections",
             "torch"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.openpose.util": [
             "numpy",
-            "math",
             "cv2",
+            "math",
             "matplotlib"
         ],
         "modelscope.models.cv.controllable_image_generation.controlnet": [
             "numpy",
+            "sys",
             "typing",
-            "random",
-            "cv2",
-            "tempfile",
-            "os",
             "torch",
-            "sys",
             "control_ldm",
+            "tempfile",
+            "PIL",
+            "os",
+            "cv2",
+            "random",
             "math",
-            "einops",
-            "PIL"
+            "einops"
         ],
         "modelscope.models.cv.crowd_counting.cc_model": [
             "typing",
             "os",
             "torch"
         ],
         "modelscope.models.cv.crowd_counting.hrnet_aspp_relu": [
+            "logging",
             "numpy",
-            "functools",
-            "os",
             "torch",
-            "logging"
+            "functools",
+            "os"
         ],
         "modelscope.models.cv.easycv_base": [
             "easycv"
         ],
         "modelscope.models.cv.face_2d_keypoints.face_2d_keypoints_align": [
             "easycv"
         ],
         "modelscope.models.cv.face_attribute_recognition.fair_face.face_attribute_recognition": [
             "numpy",
-            "cv2",
-            "os",
             "torch",
             "PIL",
+            "os",
+            "cv2",
             "torchvision"
         ],
         "modelscope.models.cv.face_detection.mogface.models.detectors": [
             "numpy",
-            "cv2",
             "os",
+            "cv2",
             "torch"
         ],
         "modelscope.models.cv.face_detection.mogface.models.mogface": [
             "torch"
         ],
         "modelscope.models.cv.face_detection.mogface.models.mogprednet": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.face_detection.mogface.models.resnet": [
             "torch"
         ],
         "modelscope.models.cv.face_detection.mogface.models.utils": [
             "numpy",
-            "math",
             "itertools",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.face_detection.mtcnn.models.box_utils": [
             "numpy",
             "PIL"
         ],
         "modelscope.models.cv.face_detection.mtcnn.models.detector": [
             "numpy",
             "os",
-            "PIL",
-            "torch"
+            "torch",
+            "PIL"
         ],
         "modelscope.models.cv.face_detection.mtcnn.models.first_stage": [
             "numpy",
             "math",
-            "PIL",
-            "torch"
+            "torch",
+            "PIL"
         ],
         "modelscope.models.cv.face_detection.mtcnn.models.get_nets": [
             "numpy",
             "collections",
             "torch"
         ],
         "modelscope.models.cv.face_detection.peppa_pig_face.LK.lk": [
@@ -10695,122 +10943,132 @@
         ],
         "modelscope.models.cv.face_detection.retinaface.detection": [
             "numpy",
             "cv2",
             "torch"
         ],
         "modelscope.models.cv.face_detection.retinaface.models.net": [
-            "time",
             "torchvision",
-            "torch"
+            "torch",
+            "time"
         ],
         "modelscope.models.cv.face_detection.retinaface.models.retinaface": [
-            "torchvision",
             "collections",
+            "torchvision",
             "torch"
         ],
         "modelscope.models.cv.face_detection.retinaface.utils": [
             "numpy",
             "math",
             "itertools",
             "torch"
         ],
+        "modelscope.models.cv.face_detection.scrfd.damofd_detect": [
+            "typing",
+            "copy",
+            "os",
+            "torch"
+        ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.core.bbox.transforms": [
             "numpy",
             "torch"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.core.post_processing.bbox_nms": [
             "torch"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.auto_augment": [
             "numpy",
-            "mmdet",
             "mmcv",
+            "copy",
             "cv2",
-            "copy"
+            "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.formating": [
             "numpy",
-            "mmdet",
             "mmcv",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.loading": [
             "numpy",
             "pycocotools",
             "os",
             "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.transforms": [
-            "mmcv",
             "numpy",
+            "mmcv",
             "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.retinaface": [
             "numpy",
             "mmdet"
         ],
+        "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.backbones.master_net": [
+            "torch",
+            "mmdet"
+        ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.backbones.mobilenet": [
             "mmcv",
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.backbones.resnet": [
             "mmcv",
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.dense_heads.scrfd_head": [
             "numpy",
-            "mmdet",
             "mmcv",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.detectors.base": [
             "numpy",
-            "abc",
-            "mmcv",
             "collections",
+            "mmcv",
             "mmdet",
+            "abc",
             "torch"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.detectors.scrfd": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.detectors.single_stage": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.detectors.tinymog": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.preprocessor": [
             "numpy",
             "typing",
             "PIL"
         ],
         "modelscope.models.cv.face_detection.scrfd.scrfd_detect": [
             "numpy",
             "typing",
-            "os",
+            "torch",
             "copy",
-            "torch"
+            "os"
         ],
         "modelscope.models.cv.face_detection.scrfd.tinymog_detect": [
             "typing",
-            "os",
             "copy",
+            "os",
             "torch"
         ],
         "modelscope.models.cv.face_detection.ulfd_slim.detection": [
             "numpy",
-            "cv2",
             "os",
+            "cv2",
             "torch"
         ],
         "modelscope.models.cv.face_detection.ulfd_slim.vision.box_utils": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.face_detection.ulfd_slim.vision.mb_tiny": [
@@ -10830,50 +11088,50 @@
             "numpy",
             "typing",
             "collections",
             "torch"
         ],
         "modelscope.models.cv.face_detection.ulfd_slim.vision.transforms": [
             "numpy",
-            "torch",
+            "cv2",
             "types",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.face_emotion.efficient.model": [
             "torch"
         ],
         "modelscope.models.cv.face_emotion.efficient.utils": [
-            "math",
-            "functools",
-            "re",
             "collections",
-            "torch"
+            "re",
+            "math",
+            "torch",
+            "functools"
         ],
         "modelscope.models.cv.face_emotion.emotion_infer": [
             "torchvision",
-            "PIL",
-            "torch"
+            "torch",
+            "PIL"
         ],
         "modelscope.models.cv.face_emotion.emotion_model": [
             "sys",
             "os",
             "torch"
         ],
         "modelscope.models.cv.face_emotion.face_alignment.face": [
             "numpy",
             "tensorflow",
-            "cv2",
-            "os"
+            "os",
+            "cv2"
         ],
         "modelscope.models.cv.face_emotion.face_alignment.face_align": [
             "numpy",
-            "cv2",
-            "os",
             "sys",
-            "PIL"
+            "PIL",
+            "os",
+            "cv2"
         ],
         "modelscope.models.cv.face_generation.op.conv2d_gradfix": [
             "contextlib",
             "warnings",
             "torch"
         ],
         "modelscope.models.cv.face_generation.op.fused_act": [
@@ -10882,35 +11140,35 @@
         ],
         "modelscope.models.cv.face_generation.op.upfirdn2d": [
             "collections",
             "os",
             "torch"
         ],
         "modelscope.models.cv.face_generation.stylegan2": [
+            "operator",
+            "torch",
             "functools",
             "random",
-            "torch",
-            "math",
-            "operator"
+            "math"
         ],
         "modelscope.models.cv.face_human_hand_detection.det_infer": [
             "numpy",
             "cv2",
             "torch"
         ],
         "modelscope.models.cv.face_human_hand_detection.ghost_pan": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.face_human_hand_detection.nanodet_plus_head": [
             "numpy",
-            "cv2",
             "torch",
-            "math",
-            "torchvision"
+            "torchvision",
+            "cv2",
+            "math"
         ],
         "modelscope.models.cv.face_human_hand_detection.one_stage_detector": [
             "torch"
         ],
         "modelscope.models.cv.face_human_hand_detection.shufflenetv2": [
             "torch"
         ],
@@ -10937,131 +11195,146 @@
             "torch"
         ],
         "modelscope.models.cv.face_recognition.torchkit.backbone.model_resnet": [
             "torch"
         ],
         "modelscope.models.cv.face_recognition.torchkit.rts_backbone": [
             "collections",
-            "math",
             "os",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.face_reconstruction.models.bfm": [
             "numpy",
             "os",
-            "scipy",
-            "torch"
+            "torch",
+            "scipy"
         ],
-        "modelscope.models.cv.face_reconstruction.models.facelandmark.large_base_lmks_infer": [
-            "numpy",
+        "modelscope.models.cv.face_reconstruction.models.de_retouching_module": [
             "torch"
         ],
-        "modelscope.models.cv.face_reconstruction.models.facelandmark.large_model_infer": [
+        "modelscope.models.cv.face_reconstruction.models.facelandmark.large_base_lmks_infer": [
             "numpy",
-            "cv2",
-            "os",
             "torch"
         ],
         "modelscope.models.cv.face_reconstruction.models.facelandmark.nets.large_base_lmks_net": [
             "torch"
         ],
         "modelscope.models.cv.face_reconstruction.models.facelandmark.nets.large_eyeball_net": [
             "torch"
         ],
         "modelscope.models.cv.face_reconstruction.models.facerecon_model": [
             "numpy",
-            "cv2",
+            "torch",
+            "collections",
             "os",
-            "torch"
+            "cv2"
         ],
         "modelscope.models.cv.face_reconstruction.models.losses": [
             "numpy",
             "kornia",
-            "clip",
             "torch"
         ],
         "modelscope.models.cv.face_reconstruction.models.networks": [
+            "kornia",
             "typing",
             "os",
-            "kornia",
             "torch"
         ],
         "modelscope.models.cv.face_reconstruction.models.nv_diffrast": [
             "numpy",
+            "warnings",
             "typing",
             "torch",
-            "nvdiffrast",
-            "warnings"
+            "nvdiffrast"
         ],
         "modelscope.models.cv.face_reconstruction.models.opt": [],
+        "modelscope.models.cv.face_reconstruction.models.pix2pix.networks": [
+            "torch",
+            "functools"
+        ],
+        "modelscope.models.cv.face_reconstruction.models.pix2pix.pix2pix_model": [
+            "torch"
+        ],
+        "modelscope.models.cv.face_reconstruction.models.pix2pix.pix2pix_options": [],
+        "modelscope.models.cv.face_reconstruction.models.renderer": [
+            "numpy",
+            "imageio",
+            "torch",
+            "skimage"
+        ],
+        "modelscope.models.cv.face_reconstruction.models.unet": [
+            "warnings",
+            "torch"
+        ],
         "modelscope.models.cv.face_reconstruction.utils": [
+            "argparse",
             "numpy",
-            "numba",
-            "scipy",
-            "cv2",
-            "os",
             "array",
+            "scipy",
             "torch",
-            "math",
             "PIL",
-            "argparse"
+            "os",
+            "cv2",
+            "numba",
+            "math"
         ],
         "modelscope.models.cv.facial_expression_recognition.fer.facial_expression_recognition": [
             "numpy",
-            "cv2",
-            "os",
             "torch",
-            "PIL"
+            "PIL",
+            "os",
+            "cv2"
         ],
         "modelscope.models.cv.facial_expression_recognition.fer.transforms": [
             "numpy",
-            "types",
             "numbers",
-            "PIL",
-            "torch"
+            "types",
+            "torch",
+            "PIL"
         ],
         "modelscope.models.cv.facial_expression_recognition.fer.vgg": [
             "torch"
         ],
         "modelscope.models.cv.facial_landmark_confidence.flc.facial_landmark_confidence": [
             "numpy",
-            "cv2",
-            "os",
             "torch",
-            "PIL"
+            "PIL",
+            "os",
+            "cv2"
         ],
         "modelscope.models.cv.facial_landmark_confidence.flc.manual_landmark_net": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.hand_2d_keypoints.hand_2d_keypoints": [
             "easycv"
         ],
         "modelscope.models.cv.hand_static.hand_model": [
             "numpy",
-            "cv2",
-            "os",
-            "torch",
             "sys",
+            "torch",
             "PIL",
+            "os",
+            "cv2",
             "torchvision"
         ],
         "modelscope.models.cv.hand_static.networks": [
-            "torchvision",
             "os",
-            "torch"
+            "torch",
+            "torchvision"
         ],
         "modelscope.models.cv.human_reconstruction.Reconstruction": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
             "PIL",
             "skimage",
+            "os",
+            "cv2",
             "torchvision"
         ],
         "modelscope.models.cv.human_reconstruction.models.Embedding": [
             "torch"
         ],
         "modelscope.models.cv.human_reconstruction.models.PixToMesh": [
             "torch"
@@ -11083,16 +11356,16 @@
         "modelscope.models.cv.human_reconstruction.models.human_segmenter": [
             "numpy",
             "tensorflow",
             "cv2"
         ],
         "modelscope.models.cv.human_reconstruction.models.networks": [
             "numpy",
-            "functools",
-            "torch"
+            "torch",
+            "functools"
         ],
         "modelscope.models.cv.human_reconstruction.utils": [
             "numpy",
             "mcubes",
             "os",
             "torch"
         ],
@@ -11107,131 +11380,131 @@
         "modelscope.models.cv.image_binary_quant_classification.bnext": [
             "numpy",
             "torch"
         ],
         "modelscope.models.cv.image_body_reshaping.image_body_reshaping": [
             "numpy",
             "typing",
-            "cv2",
+            "torch",
             "os",
-            "torch"
+            "cv2"
         ],
         "modelscope.models.cv.image_body_reshaping.model": [
             "torch"
         ],
         "modelscope.models.cv.image_body_reshaping.person_info": [
             "numpy",
-            "cv2",
             "copy",
+            "cv2",
             "torch"
         ],
         "modelscope.models.cv.image_body_reshaping.pose_estimator.body": [
             "numpy",
             "scipy",
-            "cv2",
             "torch",
+            "cv2",
             "math"
         ],
         "modelscope.models.cv.image_body_reshaping.pose_estimator.model": [
             "collections",
             "torch"
         ],
         "modelscope.models.cv.image_body_reshaping.pose_estimator.util": [
             "numpy"
         ],
         "modelscope.models.cv.image_body_reshaping.slim_utils": [
             "numpy",
-            "math",
+            "os",
+            "cv2",
             "numba",
             "random",
-            "cv2",
-            "os",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.image_classification.backbones.beit_v2": [
+            "warnings",
+            "mmcv",
             "typing",
-            "collections",
+            "itertools",
+            "torch",
             "functools",
-            "mmcv",
+            "collections",
             "os",
-            "torch",
             "math",
-            "itertools",
             "einops",
-            "warnings",
             "mmcls"
         ],
         "modelscope.models.cv.image_classification.backbones.nextvit": [
+            "warnings",
+            "mmcv",
             "typing",
-            "collections",
+            "itertools",
+            "torch",
             "functools",
-            "mmcv",
+            "collections",
             "os",
-            "torch",
             "math",
-            "itertools",
             "einops",
-            "warnings",
             "mmcls"
         ],
         "modelscope.models.cv.image_classification.mmcls_model": [
             "os"
         ],
         "modelscope.models.cv.image_classification.resnet50_cc": [
+            "torch",
             "collections",
             "os",
-            "torch",
-            "math",
-            "torchvision"
+            "torchvision",
+            "math"
         ],
         "modelscope.models.cv.image_classification.utils": [
             "numpy",
-            "mmcls",
-            "math",
-            "itertools",
             "collections",
+            "math",
             "os",
-            "torch"
+            "itertools",
+            "torch",
+            "mmcls"
         ],
         "modelscope.models.cv.image_color_enhance.adaint.adaint": [
+            "numbers",
             "typing",
-            "os",
             "torch",
-            "numbers",
-            "torchvision"
+            "torchvision",
+            "os"
         ],
         "modelscope.models.cv.image_color_enhance.csrnet": [
             "math",
-            "functools",
-            "torch"
+            "torch",
+            "functools"
         ],
         "modelscope.models.cv.image_color_enhance.deeplpf.deeplpf_image_color_enhance": [
             "typing",
             "os",
             "torch"
         ],
         "modelscope.models.cv.image_color_enhance.deeplpf.deeplpfnet": [
             "math",
-            "matplotlib",
-            "torch"
+            "torch",
+            "matplotlib"
         ],
         "modelscope.models.cv.image_color_enhance.image_color_enhance": [
             "typing",
             "os",
             "torch"
         ],
         "modelscope.models.cv.image_colorization.ddcolor.ddcolor": [
             "torch"
         ],
         "modelscope.models.cv.image_colorization.ddcolor.ddcolor_for_image_colorization": [
             "numpy",
             "typing",
-            "os",
+            "torch",
             "copy",
-            "torch"
+            "os"
         ],
         "modelscope.models.cv.image_colorization.ddcolor.loss": [
             "torch"
         ],
         "modelscope.models.cv.image_colorization.ddcolor.utils.convnext": [
             "timm",
             "torch"
@@ -11242,31 +11515,31 @@
         ],
         "modelscope.models.cv.image_colorization.ddcolor.utils.transformer_utils": [
             "typing",
             "torch"
         ],
         "modelscope.models.cv.image_colorization.ddcolor.utils.unet": [
             "collections",
-            "enum",
-            "torch"
+            "torch",
+            "enum"
         ],
         "modelscope.models.cv.image_colorization.ddcolor.utils.vgg": [
-            "torchvision",
             "collections",
             "os",
-            "torch"
+            "torch",
+            "torchvision"
         ],
         "modelscope.models.cv.image_colorization.unet.unet": [
             "numpy",
             "torch"
         ],
         "modelscope.models.cv.image_colorization.unet.utils": [
-            "functools",
+            "torch",
             "enum",
-            "torch"
+            "functools"
         ],
         "modelscope.models.cv.image_debanding.rrdb.rrdb_image_debanding": [
             "typing",
             "os",
             "torch"
         ],
         "modelscope.models.cv.image_deblur.nafnet_for_image_deblur": [
@@ -11276,102 +11549,102 @@
         ],
         "modelscope.models.cv.image_defrcn_fewshot.defrcn_for_fewshot": [
             "typing",
             "os",
             "torch"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.evaluation.coco_evaluation": [
+            "logging",
             "numpy",
-            "pycocotools",
             "tabulate",
-            "contextlib",
-            "detectron2",
-            "os",
-            "copy",
-            "torch",
             "fvcore",
-            "json",
+            "contextlib",
             "itertools",
+            "torch",
             "collections",
             "io",
-            "logging"
+            "copy",
+            "os",
+            "detectron2",
+            "pycocotools",
+            "json"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.evaluation.evaluator": [
+            "logging",
             "time",
-            "detectron2",
             "torch",
-            "datetime",
-            "logging"
+            "detectron2",
+            "datetime"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.evaluation.pascal_voc_evaluation": [
             "numpy",
-            "os",
             "collections",
+            "os",
             "detectron2",
             "tempfile"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.models.calibration_layer": [
-            "torch",
             "sklearn",
-            "detectron2",
-            "cv2"
+            "cv2",
+            "torch",
+            "detectron2"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.models.defrcn": [
             "typing",
-            "detectron2",
             "os",
-            "torch"
+            "torch",
+            "detectron2"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.models.fast_rcnn": [
             "numpy",
-            "detectron2",
             "fvcore",
-            "torch"
+            "torch",
+            "detectron2"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.models.gdl": [
             "torch"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.models.resnet": [
             "torchvision",
             "torch"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.models.roi_heads": [
-            "detectron2",
-            "torch"
+            "torch",
+            "detectron2"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.utils.coco_register": [
-            "pycocotools",
             "io",
-            "contextlib",
-            "detectron2",
             "os",
-            "fvcore"
+            "detectron2",
+            "fvcore",
+            "contextlib",
+            "pycocotools"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.utils.configuration_mapper": [
             "detectron2"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.utils.model_surgery_op": [
             "argparse",
             "os",
             "torch"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.utils.register_data": [],
         "modelscope.models.cv.image_defrcn_fewshot.utils.requirements_check": [
-            "importlib_metadata",
             "packaging",
-            "importlib",
             "sys",
-            "collections"
+            "collections",
+            "importlib_metadata",
+            "importlib"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.utils.voc_register": [
             "numpy",
-            "xml",
-            "detectron2",
             "os",
-            "fvcore"
+            "detectron2",
+            "fvcore",
+            "xml"
         ],
         "modelscope.models.cv.image_denoise.nafnet.NAFNet_arch": [
             "numpy",
             "torch"
         ],
         "modelscope.models.cv.image_denoise.nafnet.arch_util": [
             "torch"
@@ -11386,20 +11659,20 @@
         ],
         "modelscope.models.cv.image_depth_estimation.networks.newcrf_layers": [
             "numpy",
             "timm",
             "torch"
         ],
         "modelscope.models.cv.image_depth_estimation.networks.newcrf_utils": [
+            "collections",
+            "warnings",
             "importlib",
+            "os",
             "torchvision",
             "pkgutil",
-            "warnings",
-            "collections",
-            "os",
             "torch"
         ],
         "modelscope.models.cv.image_depth_estimation.networks.swin_transformer": [
             "numpy",
             "timm",
             "torch"
         ],
@@ -11429,42 +11702,42 @@
         "modelscope.models.cv.image_depth_estimation_bts.networks.utils": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.image_driving_perception.image_driving_percetion_model": [
             "numpy",
             "typing",
-            "cv2",
+            "torch",
             "os",
-            "torch"
+            "cv2"
         ],
         "modelscope.models.cv.image_driving_perception.preprocessor": [
             "numpy",
             "typing",
             "cv2",
             "torch"
         ],
         "modelscope.models.cv.image_driving_perception.utils": [
             "numpy",
             "torchvision",
-            "time",
-            "torch"
+            "torch",
+            "time"
         ],
         "modelscope.models.cv.image_face_fusion.facegan.gan_wrap": [
             "numpy",
-            "cv2",
-            "os",
             "torch",
             "PIL",
+            "os",
+            "cv2",
             "torchvision"
         ],
         "modelscope.models.cv.image_face_fusion.facegan.model": [
             "math",
-            "random",
-            "torch"
+            "torch",
+            "random"
         ],
         "modelscope.models.cv.image_face_fusion.facegan.op.conv2d_gradfix": [
             "contextlib",
             "warnings",
             "torch"
         ],
         "modelscope.models.cv.image_face_fusion.facegan.op.fused_act": [
@@ -11480,32 +11753,32 @@
         ],
         "modelscope.models.cv.image_face_fusion.facelib.matlab_cp2tform": [
             "numpy"
         ],
         "modelscope.models.cv.image_face_fusion.image_face_fusion": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
-            "torchvision",
             "PIL",
-            "collections"
+            "collections",
+            "os",
+            "cv2",
+            "torchvision"
         ],
         "modelscope.models.cv.image_face_fusion.network.aad_layer": [
             "torch"
         ],
         "modelscope.models.cv.image_face_fusion.network.aei_flow_net": [
             "torch"
         ],
         "modelscope.models.cv.image_face_fusion.network.bfm": [
             "numpy",
             "os",
-            "scipy",
-            "torch"
+            "torch",
+            "scipy"
         ],
         "modelscope.models.cv.image_face_fusion.network.dense_motion": [
             "torch"
         ],
         "modelscope.models.cv.image_face_fusion.network.facerecon_model": [
             "typing",
             "os",
@@ -11533,17 +11806,17 @@
         "modelscope.models.cv.image_human_parsing.m2fp_net": [
             "typing",
             "os",
             "torch"
         ],
         "modelscope.models.cv.image_human_parsing.parsing_utils": [
             "numpy",
-            "PIL",
             "copy",
-            "torch"
+            "torch",
+            "PIL"
         ],
         "modelscope.models.cv.image_inpainting.base": [
             "typing",
             "torch"
         ],
         "modelscope.models.cv.image_inpainting.default": [
             "bisect",
@@ -11555,16 +11828,16 @@
             "torch"
         ],
         "modelscope.models.cv.image_inpainting.modules.ade20k.base": [
             "os",
             "torch"
         ],
         "modelscope.models.cv.image_inpainting.modules.ade20k.resnet": [
-            "math",
             "os",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.image_inpainting.modules.adversarial": [
             "typing",
             "torch"
         ],
         "modelscope.models.cv.image_inpainting.modules.feature_matching": [
@@ -11581,25 +11854,25 @@
             "torch"
         ],
         "modelscope.models.cv.image_inpainting.modules.perceptual": [
             "torchvision",
             "torch"
         ],
         "modelscope.models.cv.image_inpainting.modules.pix2pixhd": [
+            "logging",
             "numpy",
-            "functools",
             "collections",
-            "logging",
-            "torch"
+            "torch",
+            "functools"
         ],
         "modelscope.models.cv.image_inpainting.refinement": [
             "numpy",
             "kornia",
-            "cv2",
             "torch",
+            "cv2",
             "tqdm"
         ],
         "modelscope.models.cv.image_instance_segmentation.backbones.swin_transformer": [
             "numpy",
             "timm",
             "torch"
         ],
@@ -11621,27 +11894,27 @@
         ],
         "modelscope.models.cv.image_instance_segmentation.maskdino.maskdino_encoder": [
             "numpy",
             "typing",
             "torch"
         ],
         "modelscope.models.cv.image_instance_segmentation.maskdino.ms_deform_attn": [
-            "math",
-            "__future__",
-            "mmcv",
             "warnings",
-            "torch"
+            "mmcv",
+            "math",
+            "torch",
+            "__future__"
         ],
         "modelscope.models.cv.image_instance_segmentation.maskdino.position_encoding": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.image_instance_segmentation.maskdino.utils": [
-            "math",
             "copy",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.image_instance_segmentation.maskdino_model": [
             "typing",
             "os",
             "torch"
         ],
@@ -11652,122 +11925,122 @@
         "modelscope.models.cv.image_instance_segmentation.model": [
             "typing",
             "os",
             "torch"
         ],
         "modelscope.models.cv.image_instance_segmentation.postprocess_utils": [
             "numpy",
-            "pycocotools",
-            "cv2",
+            "itertools",
             "torch",
-            "itertools"
+            "cv2",
+            "pycocotools"
         ],
         "modelscope.models.cv.image_matching.config.default": [
             "yacs"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.backbone.resnet_fpn": [
             "torch"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.loftr": [
-            "einops",
-            "torch"
+            "torch",
+            "einops"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.loftr_module.fine_preprocess": [
-            "einops",
-            "torch"
+            "torch",
+            "einops"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.loftr_module.linear_attention": [
             "torch"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.loftr_module.quadtree_attention": [
             "timm",
             "torch"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.loftr_module.transformer": [
-            "copy",
             "torch",
+            "copy",
+            "timm",
             "math",
-            "einops",
-            "timm"
+            "einops"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.utils.coarse_matching": [
-            "einops",
-            "torch"
+            "torch",
+            "einops"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.utils.fine_matching": [
             "kornia",
             "math",
             "torch"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.utils.position_encoding": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.image_matching.quadtree_attention_model": [
             "numpy",
-            "cv2",
+            "torch",
             "os",
-            "pathlib",
-            "torch"
+            "cv2",
+            "pathlib"
         ],
         "modelscope.models.cv.image_matching.utils.misc": [
             "yacs"
         ],
         "modelscope.models.cv.image_mvs_depth_estimation.cas_mvsnet": [
             "torch"
         ],
         "modelscope.models.cv.image_mvs_depth_estimation.casmvs_model": [
             "numpy",
-            "cv2",
+            "torch",
             "os",
-            "easydict",
-            "torch"
+            "cv2",
+            "easydict"
         ],
         "modelscope.models.cv.image_mvs_depth_estimation.colmap2mvsnet": [
             "numpy",
-            "functools",
-            "shutil",
-            "cv2",
-            "os",
             "multiprocessing",
+            "shutil",
             "__future__",
+            "functools",
             "collections",
+            "os",
+            "cv2",
             "struct"
         ],
         "modelscope.models.cv.image_mvs_depth_estimation.depth_filter": [
             "numpy",
             "plyfile",
-            "cv2",
+            "PIL",
             "os",
-            "PIL"
+            "cv2"
         ],
         "modelscope.models.cv.image_mvs_depth_estimation.general_eval_dataset": [
             "numpy",
+            "torch",
+            "os",
+            "cv2",
             "sys",
-            "PIL",
             "re",
-            "cv2",
-            "os",
-            "torch"
+            "PIL"
         ],
         "modelscope.models.cv.image_mvs_depth_estimation.module": [
             "torch"
         ],
         "modelscope.models.cv.image_mvs_depth_estimation.utils": [
             "numpy",
             "torchvision",
-            "random",
-            "torch"
+            "torch",
+            "random"
         ],
         "modelscope.models.cv.image_paintbyexample.model": [
             "typing",
-            "paint_ldm",
-            "os",
             "torch",
-            "omegaconf"
+            "omegaconf",
+            "os",
+            "paint_ldm"
         ],
         "modelscope.models.cv.image_panoptic_segmentation.panseg_model": [
             "os",
             "torch"
         ],
         "modelscope.models.cv.image_panoptic_segmentation.r50_panseg_model": [
             "easycv"
@@ -11775,119 +12048,119 @@
         "modelscope.models.cv.image_portrait_enhancement.align_faces": [
             "numpy",
             "cv2",
             "skimage"
         ],
         "modelscope.models.cv.image_portrait_enhancement.eqface.fqa": [
             "numpy",
-            "cv2",
             "os",
+            "cv2",
             "torch"
         ],
         "modelscope.models.cv.image_portrait_enhancement.eqface.model_resnet": [
             "torch"
         ],
         "modelscope.models.cv.image_portrait_enhancement.gpen": [
+            "operator",
+            "itertools",
+            "torch",
             "functools",
             "random",
-            "torch",
-            "math",
-            "itertools",
-            "operator"
+            "math"
         ],
         "modelscope.models.cv.image_portrait_enhancement.image_portrait_enhancement": [
             "typing",
-            "math",
             "os",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.image_portrait_enhancement.losses.helpers": [
             "collections",
             "torch"
         ],
         "modelscope.models.cv.image_portrait_enhancement.losses.losses": [
             "torch"
         ],
         "modelscope.models.cv.image_portrait_enhancement.losses.model_irse": [
             "torch"
         ],
         "modelscope.models.cv.image_portrait_enhancement.retinaface.detection": [
             "numpy",
-            "cv2",
             "os",
+            "cv2",
             "torch"
         ],
         "modelscope.models.cv.image_portrait_enhancement.retinaface.models.net": [
-            "time",
             "torchvision",
-            "torch"
+            "torch",
+            "time"
         ],
         "modelscope.models.cv.image_portrait_enhancement.retinaface.models.retinaface": [
-            "torchvision",
             "collections",
+            "torchvision",
             "torch"
         ],
         "modelscope.models.cv.image_portrait_enhancement.retinaface.utils": [
             "numpy",
             "math",
             "itertools",
             "torch"
         ],
         "modelscope.models.cv.image_probing_model.backbone": [
             "numpy",
             "operator",
-            "functools",
-            "torch",
             "sys",
-            "torchvision",
-            "math",
+            "torch",
             "PIL",
-            "collections"
+            "functools",
+            "collections",
+            "torchvision",
+            "math"
         ],
         "modelscope.models.cv.image_probing_model.model": [
             "typing",
-            "json",
             "os",
-            "torch"
+            "torch",
+            "json"
         ],
         "modelscope.models.cv.image_probing_model.utils": [
             "re",
             "torch"
         ],
         "modelscope.models.cv.image_quality_assessment_degradation.degradation_model": [
             "numpy",
-            "time",
-            "json",
-            "torchvision",
             "collections",
+            "torchvision",
             "cv2",
+            "json",
+            "time",
             "torch"
         ],
         "modelscope.models.cv.image_quality_assessment_degradation.image_quality_assessment_degradation": [
             "typing",
             "os",
             "torch"
         ],
         "modelscope.models.cv.image_quality_assessment_man.image_quality_assessment_man": [
             "typing",
             "os",
             "torch"
         ],
         "modelscope.models.cv.image_quality_assessment_man.maniqa": [
             "timm",
-            "einops",
-            "torch"
+            "torch",
+            "einops"
         ],
         "modelscope.models.cv.image_quality_assessment_man.swin": [
+            "collections",
+            "warnings",
             "math",
             "itertools",
-            "einops",
-            "warnings",
-            "collections",
-            "torch"
+            "torch",
+            "einops"
         ],
         "modelscope.models.cv.image_quality_assessment_mos.backbones.resnet": [
             "os",
             "torch"
         ],
         "modelscope.models.cv.image_quality_assessment_mos.censeo_ivqa_model": [
             "torch"
@@ -11897,186 +12170,186 @@
         ],
         "modelscope.models.cv.image_quality_assessment_mos.image_quality_assessment_mos": [
             "typing",
             "os",
             "torch"
         ],
         "modelscope.models.cv.image_reid_person.pass_model": [
+            "torch",
             "os",
-            "enum",
-            "torch"
+            "enum"
         ],
         "modelscope.models.cv.image_reid_person.transreid_model": [
-            "itertools",
             "collections",
-            "functools",
-            "torch"
+            "itertools",
+            "torch",
+            "functools"
         ],
         "modelscope.models.cv.image_restoration.demoire_models.nets": [
             "torch"
         ],
         "modelscope.models.cv.image_restoration.image_restoration_model": [
             "numpy",
-            "cv2",
             "os",
+            "cv2",
             "torch"
         ],
         "modelscope.models.cv.image_semantic_segmentation.ddpm_seg.data_util": [],
         "modelscope.models.cv.image_semantic_segmentation.ddpm_seg.feature_extractors": [
             "typing",
             "torch"
         ],
         "modelscope.models.cv.image_semantic_segmentation.ddpm_seg.pixel_classifier": [
             "numpy",
-            "os",
             "torch",
             "PIL",
-            "collections"
+            "collections",
+            "os"
         ],
         "modelscope.models.cv.image_semantic_segmentation.ddpm_seg.utils": [
             "numpy",
+            "torch",
             "random",
-            "PIL",
-            "torch"
+            "PIL"
         ],
         "modelscope.models.cv.image_semantic_segmentation.ddpm_segmentation_model": [
             "typing",
             "os",
-            "ddpm_guided_diffusion",
-            "torch"
+            "torch",
+            "ddpm_guided_diffusion"
         ],
         "modelscope.models.cv.image_semantic_segmentation.pan_merge.base_panoptic_fusion_head": [
             "abc",
             "mmcv",
             "mmdet"
         ],
         "modelscope.models.cv.image_semantic_segmentation.pan_merge.maskformer_semantic_head": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.image_semantic_segmentation.segformer": [
             "easycv"
         ],
         "modelscope.models.cv.image_semantic_segmentation.semantic_seg_model": [
             "numpy",
             "os",
             "torch"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.backbone.adapter_modules": [
-            "functools",
-            "timm",
             "logging",
             "mmdet",
-            "torch"
+            "timm",
+            "torch",
+            "functools"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.backbone.base.beit": [
-            "math",
-            "functools",
             "mmcv",
-            "timm",
             "mmdet",
-            "torch"
+            "timm",
+            "math",
+            "torch",
+            "functools"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.backbone.beit_adapter": [
+            "logging",
+            "mmdet",
             "torch",
-            "math",
             "timm",
-            "logging",
-            "mmdet"
+            "math"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.decode_heads.base_decode_head": [
-            "mmcv",
             "abc",
-            "mmdet",
-            "torch"
+            "mmcv",
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.decode_heads.mask2former_head_from_mmseg": [
-            "mmcv",
-            "mmdet",
             "copy",
-            "torch"
+            "mmcv",
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.segmentors.base_segmentor": [
             "numpy",
-            "abc",
-            "mmcv",
             "warnings",
             "collections",
+            "mmcv",
+            "abc",
             "torch"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.segmentors.encoder_decoder_mask2former": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.utils.builder": [
             "mmcv"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.utils.data_process_func": [
             "mmcv",
             "mmdet"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.utils.seg_func": [
             "warnings",
             "torch"
         ],
         "modelscope.models.cv.image_skychange.preprocessor": [
             "numpy",
-            "typing",
-            "cv2",
-            "torch",
-            "json",
             "numbers",
             "pdb",
-            "torchvision"
+            "typing",
+            "torch",
+            "torchvision",
+            "cv2",
+            "json"
         ],
         "modelscope.models.cv.image_skychange.ptsemseg.BlockModules": [
             "torch"
         ],
         "modelscope.models.cv.image_skychange.ptsemseg.hrnet_backnone": [
-            "numpy",
-            "os",
             "logging",
+            "os",
+            "numpy",
             "torch"
         ],
         "modelscope.models.cv.image_skychange.ptsemseg.hrnet_super_and_ocr": [
             "numpy",
             "torch",
             "__future__"
         ],
         "modelscope.models.cv.image_skychange.ptsemseg.unet": [
             "torch"
         ],
         "modelscope.models.cv.image_skychange.skychange": [
             "numpy",
-            "cv2",
-            "os",
-            "torch",
-            "torchvision",
-            "json",
             "numbers",
-            "PIL",
             "pdb",
-            "collections"
+            "torch",
+            "PIL",
+            "collections",
+            "os",
+            "cv2",
+            "torchvision",
+            "json"
         ],
         "modelscope.models.cv.image_skychange.skychange_model": [
-            "typing",
             "time",
-            "cv2",
-            "os",
+            "pdb",
+            "typing",
             "torch",
+            "collections",
+            "os",
+            "cv2",
             "math",
-            "json",
-            "pdb",
-            "collections"
+            "json"
         ],
         "modelscope.models.cv.image_to_image_generation.data.transforms": [
             "torchvision",
             "math",
-            "random",
-            "PIL"
+            "PIL",
+            "random"
         ],
         "modelscope.models.cv.image_to_image_generation.model": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.image_to_image_generation.models.autoencoder": [
             "math",
@@ -12093,16 +12366,16 @@
         "modelscope.models.cv.image_to_image_generation.ops.losses": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.image_to_image_translation.data.transforms": [
             "torchvision",
             "math",
-            "random",
-            "PIL"
+            "PIL",
+            "random"
         ],
         "modelscope.models.cv.image_to_image_translation.model_translation": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.image_to_image_translation.models.autoencoder": [
             "math",
@@ -12110,67 +12383,67 @@
         ],
         "modelscope.models.cv.image_to_image_translation.models.clip": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.apps": [
             "numpy",
-            "os",
             "torch",
             "PIL",
+            "os",
             "torchvision",
             "artist"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.degradation": [
             "numpy",
-            "math",
+            "os",
+            "cv2",
             "scipy",
             "random",
-            "cv2",
-            "os",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.diffusion": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.losses": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.metrics": [
             "numpy",
-            "scipy",
-            "torch"
+            "torch",
+            "scipy"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.random_color": [
             "colorsys",
             "random"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.random_mask": [
             "numpy",
             "cv2"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.svd": [
             "torch"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.utils": [
             "numpy",
+            "multiprocessing",
+            "base64",
             "hashlib",
-            "os",
-            "cv2",
             "torch",
-            "zipfile",
-            "base64",
-            "multiprocessing",
-            "math",
-            "json",
             "PIL",
+            "binascii",
             "io",
-            "binascii"
+            "os",
+            "zipfile",
+            "cv2",
+            "math",
+            "json"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.backbone.resnet_DA": [
             "torchvision",
             "torch"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.backbone.vit_horizon_pry_image": [
             "numpy",
@@ -12180,28 +12453,28 @@
         "modelscope.models.cv.indoor_layout_estimation.networks.misc.fourier": [
             "numpy",
             "PIL",
             "scipy"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.misc.panostretch": [
             "numpy",
-            "functools",
-            "scipy"
+            "scipy",
+            "functools"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.misc.post_proc": [
             "numpy",
             "sklearn",
             "scipy"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.modality.layout": [
             "numpy",
+            "shapely",
             "scipy",
             "torch",
-            "math",
-            "shapely"
+            "math"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.panovit": [
             "numpy",
             "torch"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.utils": [
             "numpy",
@@ -12210,21 +12483,21 @@
         "modelscope.models.cv.indoor_layout_estimation.panovit": [
             "numpy",
             "yacs",
             "os",
             "torch"
         ],
         "modelscope.models.cv.language_guided_video_summarization.summarizer": [
+            "argparse",
             "numpy",
+            "videofeatures_clipit",
+            "bmt_clipit",
             "typing",
-            "os",
             "torch",
-            "bmt_clipit",
-            "videofeatures_clipit",
-            "argparse"
+            "os"
         ],
         "modelscope.models.cv.language_guided_video_summarization.transformer.layers": [
             "torch"
         ],
         "modelscope.models.cv.language_guided_video_summarization.transformer.models": [
             "numpy",
             "torch"
@@ -12239,17 +12512,17 @@
         "modelscope.models.cv.motion_generation.model": [],
         "modelscope.models.cv.motion_generation.modules.cfg_sampler": [
             "copy",
             "torch"
         ],
         "modelscope.models.cv.motion_generation.modules.gaussian_diffusion": [
             "numpy",
-            "math",
-            "enum",
             "copy",
+            "enum",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.motion_generation.modules.mdm": [
             "numpy",
             "clip",
             "torch"
         ],
@@ -12258,290 +12531,290 @@
             "torch"
         ],
         "modelscope.models.cv.motion_generation.modules.rotation2xyz": [
             "torch"
         ],
         "modelscope.models.cv.motion_generation.modules.smpl": [
             "numpy",
-            "smplx",
-            "contextlib",
             "os",
-            "torch"
+            "contextlib",
+            "torch",
+            "smplx"
         ],
         "modelscope.models.cv.movie_scene_segmentation.get_model": [],
         "modelscope.models.cv.movie_scene_segmentation.model": [
             "numpy",
             "typing",
-            "os",
             "torch",
-            "math",
-            "shotdetect_scenedetect_lgss",
-            "einops",
             "PIL",
-            "torchvision"
+            "os",
+            "torchvision",
+            "shotdetect_scenedetect_lgss",
+            "math",
+            "einops"
         ],
         "modelscope.models.cv.movie_scene_segmentation.utils.head": [
             "torch"
         ],
         "modelscope.models.cv.movie_scene_segmentation.utils.save_op": [
             "numpy",
-            "subprocess",
-            "tqdm",
+            "os",
             "cv2",
-            "os"
+            "subprocess",
+            "tqdm"
         ],
         "modelscope.models.cv.movie_scene_segmentation.utils.shot_encoder": [
             "typing",
             "torch"
         ],
         "modelscope.models.cv.movie_scene_segmentation.utils.trn": [
-            "transformers",
-            "torch"
+            "torch",
+            "transformers"
         ],
         "modelscope.models.cv.nerf_recon_acc.dataloader.nerf_dataset": [
             "numpy",
-            "os",
             "torch",
-            "json",
-            "math",
             "PIL",
-            "torchvision"
+            "os",
+            "torchvision",
+            "math",
+            "json"
         ],
         "modelscope.models.cv.nerf_recon_acc.dataloader.read_write_model": [
-            "numpy",
             "argparse",
-            "struct",
+            "numpy",
             "collections",
-            "os"
+            "os",
+            "struct"
         ],
         "modelscope.models.cv.nerf_recon_acc.nerf_preprocess": [
             "numpy",
+            "glob",
             "typing",
-            "subprocess",
-            "cv2",
             "os",
-            "glob",
+            "cv2",
+            "subprocess",
             "tensorflow"
         ],
         "modelscope.models.cv.nerf_recon_acc.nerf_recon_acc": [
             "numpy",
             "time",
-            "cv2",
-            "os",
-            "torch",
             "glob",
+            "torch",
+            "os",
+            "cv2",
             "tqdm"
         ],
         "modelscope.models.cv.nerf_recon_acc.network.nerf": [
             "numpy",
-            "tinycudann",
             "nerfacc",
-            "torch"
+            "torch",
+            "tinycudann"
         ],
         "modelscope.models.cv.nerf_recon_acc.network.segmenter": [
             "numpy",
             "tensorflow"
         ],
         "modelscope.models.cv.nerf_recon_acc.network.utils": [
             "numpy",
+            "collections",
             "mcubes",
-            "tinycudann",
             "gc",
-            "collections",
-            "torch"
+            "torch",
+            "tinycudann"
         ],
         "modelscope.models.cv.object_detection.dino": [
             "easycv"
         ],
         "modelscope.models.cv.object_detection.mmdet_model": [
             "numpy",
             "os",
             "torch"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.backbones.vit": [
-            "functools",
+            "mmdet",
             "torch",
-            "math",
+            "functools",
             "timm",
-            "mmdet"
+            "math"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.dense_heads.anchor_head": [
             "mmdet"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.dense_heads.rpn_head": [
-            "mmcv",
-            "mmdet",
             "copy",
-            "torch"
+            "mmcv",
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.necks.fpn": [
             "mmcv",
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.roi_heads.bbox_heads.convfc_bbox_head": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.roi_heads.mask_heads.fcn_mask_head": [
             "numpy",
-            "mmcv",
-            "torch",
             "warnings",
-            "mmdet"
+            "mmcv",
+            "mmdet",
+            "torch"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.utils.checkpoint": [
-            "collections",
+            "warnings",
             "mmcv",
             "time",
-            "os",
-            "tempfile",
             "torch",
-            "importlib",
+            "tempfile",
+            "collections",
+            "io",
+            "os",
             "torchvision",
-            "pkgutil",
-            "warnings",
-            "io"
+            "importlib",
+            "pkgutil"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.utils.convModule_norm": [
             "mmcv"
         ],
         "modelscope.models.cv.object_detection.yolox_pai": [
             "easycv"
         ],
         "modelscope.models.cv.object_detection_3d.depe.depe_detect": [
             "numpy",
             "typing",
             "os",
             "torch"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.core.bbox.assigners.hungarian_assigner_3d": [
+            "torch",
             "scipy",
-            "mmdet",
-            "torch"
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.core.bbox.coders.nms_free_coder": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.core.bbox.match_costs.match_cost": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.core.bbox.util": [
             "numpy",
-            "mmdet3d",
-            "torch"
+            "torch",
+            "mmdet3d"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.nuscenes_dataset": [
             "numpy",
             "mmdet3d",
             "mmdet"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.pipelines.loading": [
-            "mmcv",
             "numpy",
+            "mmcv",
             "mmdet"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.pipelines.transform_3d": [
             "numpy",
-            "mmdet",
-            "PIL",
             "mmcv",
-            "mmdet3d",
             "copy",
-            "torch"
+            "mmdet3d",
+            "mmdet",
+            "torch",
+            "PIL"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.backbones.vovnet": [
-            "mmcv",
             "collections",
-            "mmdet",
-            "torch"
+            "mmcv",
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.dense_heads.depth_net": [
             "mmcv",
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.dense_heads.petrv2_dednhead": [
             "numpy",
             "mmcv",
-            "copy",
+            "mmdet",
             "torch",
-            "math",
+            "copy",
             "mmdet3d",
-            "mmdet"
+            "math"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.detectors.petr3d": [
             "numpy",
             "mmcv",
+            "mmdet",
             "torch",
-            "mmdet3d",
-            "mmdet"
+            "mmdet3d"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.necks.cp_fpn": [
             "mmcv",
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.petr_transformer": [
+            "warnings",
+            "mmcv",
+            "copy",
             "mmdet",
             "typing",
             "math",
-            "mmcv",
-            "warnings",
-            "copy",
             "torch"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.positional_encoding": [
             "mmcv",
             "math",
             "torch"
         ],
         "modelscope.models.cv.object_detection_3d.depe.result_vis": [
+            "argparse",
             "numpy",
-            "cv2",
+            "pyquaternion",
             "os",
-            "json",
-            "pickle",
-            "argparse",
+            "cv2",
             "mmdet3d",
-            "pyquaternion"
+            "pickle",
+            "json"
         ],
         "modelscope.models.cv.ocr_detection.model": [
             "numpy",
             "typing",
             "os",
             "torch"
         ],
         "modelscope.models.cv.ocr_detection.modules.dbnet": [
-            "sys",
-            "math",
             "collections",
             "os",
+            "sys",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.ocr_detection.modules.seg_detector_loss": [
             "sys",
             "torch"
         ],
         "modelscope.models.cv.ocr_detection.preprocessor": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
+            "PIL",
             "torch",
-            "math",
-            "PIL"
+            "os",
+            "cv2",
+            "math"
         ],
         "modelscope.models.cv.ocr_detection.utils": [
             "numpy",
-            "cv2",
             "pyclipper",
-            "shapely"
+            "shapely",
+            "cv2"
         ],
         "modelscope.models.cv.ocr_recognition.model": [
             "os",
             "torch"
         ],
         "modelscope.models.cv.ocr_recognition.modules.convnext": [
             "torch"
@@ -12549,85 +12822,91 @@
         "modelscope.models.cv.ocr_recognition.modules.convnextvit": [
             "torch"
         ],
         "modelscope.models.cv.ocr_recognition.modules.crnn": [
             "torch"
         ],
         "modelscope.models.cv.ocr_recognition.modules.timm_tinyc": [
-            "math",
-            "functools",
-            "itertools",
-            "collections",
             "logging",
+            "collections",
+            "math",
             "copy",
-            "torch"
+            "itertools",
+            "torch",
+            "functools"
         ],
         "modelscope.models.cv.ocr_recognition.modules.vitstr": [
-            "functools",
-            "copy",
+            "logging",
             "torch",
             "__future__",
-            "logging"
+            "functools",
+            "copy"
         ],
         "modelscope.models.cv.ocr_recognition.preprocessor": [
             "numpy",
-            "cv2",
-            "os",
             "torch",
-            "PIL"
+            "PIL",
+            "os",
+            "cv2"
         ],
         "modelscope.models.cv.open_vocabulary_detection_vild.vild": [
             "numpy",
-            "typing",
-            "clip",
             "scipy",
-            "os",
+            "typing",
             "torch",
+            "os",
+            "clip",
             "tensorflow"
         ],
         "modelscope.models.cv.panorama_depth_estimation.networks.equi": [
             "numpy",
             "collections",
-            "__future__",
-            "torch"
+            "torch",
+            "__future__"
         ],
         "modelscope.models.cv.panorama_depth_estimation.networks.layers": [
             "numpy",
             "torch"
         ],
         "modelscope.models.cv.panorama_depth_estimation.networks.mobilenet": [
             "torch"
         ],
         "modelscope.models.cv.panorama_depth_estimation.networks.resnet": [
             "torch"
         ],
         "modelscope.models.cv.panorama_depth_estimation.networks.unifuse": [
             "numpy",
             "collections",
-            "__future__",
-            "torch"
+            "torch",
+            "__future__"
         ],
         "modelscope.models.cv.panorama_depth_estimation.networks.util": [
             "numpy",
             "cv2",
             "scipy"
         ],
         "modelscope.models.cv.panorama_depth_estimation.unifuse_model": [
             "numpy",
-            "torchvision",
             "os",
-            "torch"
+            "torch",
+            "torchvision"
+        ],
+        "modelscope.models.cv.pedestrian_attribute_recognition.model": [
+            "numpy",
+            "os",
+            "torch",
+            "torchvision"
         ],
         "modelscope.models.cv.pointcloud_sceneflow_estimation.common": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.pointcloud_sceneflow_estimation.pointnet2_utils": [
-            "pointnet2_cuda",
             "typing",
+            "pointnet2_cuda",
             "torch"
         ],
         "modelscope.models.cv.pointcloud_sceneflow_estimation.rcp_model": [
             "numpy",
             "os",
             "torch"
         ],
@@ -12651,73 +12930,73 @@
         ],
         "modelscope.models.cv.product_segmentation.net": [
             "torch"
         ],
         "modelscope.models.cv.product_segmentation.seg_infer": [
             "numpy",
             "cv2",
-            "PIL",
-            "torch"
+            "torch",
+            "PIL"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.model": [
             "typing",
             "os",
             "torch"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.backbone": [
             "torchvision",
-            "einops",
-            "torch"
+            "torch",
+            "einops"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.criterion": [
             "torch"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.matcher": [
-            "scipy",
-            "torch"
+            "torch",
+            "scipy"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.misc": [
+            "pickle",
             "typing",
             "torchvision",
-            "pickle",
             "torch"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.mttr": [
-            "einops",
-            "torch"
+            "torch",
+            "einops"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.multimodal_transformer": [
             "typing",
-            "transformers",
-            "os",
-            "copy",
             "torch",
-            "einops"
+            "copy",
+            "os",
+            "einops",
+            "transformers"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.position_encoding_2d": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.postprocessing": [
             "numpy",
             "pycocotools",
-            "einops",
-            "torch"
+            "torch",
+            "einops"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.segmentation": [
             "typing",
             "torch"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.swin_transformer": [
             "numpy",
-            "functools",
-            "einops",
             "operator",
             "timm",
-            "torch"
+            "torch",
+            "einops",
+            "functools"
         ],
         "modelscope.models.cv.robust_image_classification.easyrobust_model": [
             "os",
             "torch"
         ],
         "modelscope.models.cv.salient_detection.models.backbone.Res2Net_v1b": [
             "math",
@@ -12732,61 +13011,61 @@
         "modelscope.models.cv.salient_detection.models.u2net": [
             "torch"
         ],
         "modelscope.models.cv.salient_detection.models.utils": [
             "torch"
         ],
         "modelscope.models.cv.salient_detection.salient_model": [
-            "cv2",
-            "os",
             "torch",
             "PIL",
+            "os",
+            "cv2",
             "torchvision"
         ],
         "modelscope.models.cv.shop_segmentation.common": [
             "warnings",
             "torch"
         ],
         "modelscope.models.cv.shop_segmentation.head_fpn": [
             "numpy",
             "timm",
             "mmcv",
             "torch"
         ],
         "modelscope.models.cv.shop_segmentation.models": [
+            "collections",
             "timm",
             "math",
-            "collections",
             "torch"
         ],
         "modelscope.models.cv.shop_segmentation.neck_fpn": [
-            "mmcv",
             "timm",
+            "mmcv",
             "torch"
         ],
         "modelscope.models.cv.shop_segmentation.shop_seg_base": [
             "torch"
         ],
         "modelscope.models.cv.shop_segmentation.shop_seg_model": [
             "numpy",
             "typing",
-            "os",
             "torch",
-            "json",
-            "PIL"
+            "PIL",
+            "os",
+            "json"
         ],
         "modelscope.models.cv.shop_segmentation.utils": [
+            "regex",
             "typing",
+            "torch",
             "functools",
             "os",
-            "regex",
-            "html",
             "ftfy",
             "gzip",
-            "torch"
+            "html"
         ],
         "modelscope.models.cv.skin_retouching.detection_model.detection_module": [
             "torch"
         ],
         "modelscope.models.cv.skin_retouching.detection_model.detection_unet_in": [
             "torch"
         ],
@@ -12808,52 +13087,52 @@
         "modelscope.models.cv.skin_retouching.retinaface.network": [
             "typing",
             "torchvision",
             "torch"
         ],
         "modelscope.models.cv.skin_retouching.retinaface.predict_single": [
             "numpy",
-            "typing",
             "albumentations",
+            "typing",
             "torch",
             "torchvision"
         ],
         "modelscope.models.cv.skin_retouching.retinaface.prior_box": [
-            "math",
             "itertools",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.skin_retouching.retinaface.utils": [
             "numpy",
-            "typing",
-            "re",
             "cv2",
             "pathlib",
+            "typing",
+            "re",
             "torch"
         ],
         "modelscope.models.cv.skin_retouching.unet_deploy": [
             "warnings",
             "torch"
         ],
         "modelscope.models.cv.skin_retouching.utils": [
             "numpy",
-            "typing",
-            "einops",
-            "time",
             "cv2",
-            "torch"
+            "time",
+            "typing",
+            "torch",
+            "einops"
         ],
         "modelscope.models.cv.skin_retouching.weights_init": [
             "torch"
         ],
         "modelscope.models.cv.stream_yolo.data.data_augment": [
             "numpy",
             "math",
-            "random",
-            "cv2"
+            "cv2",
+            "random"
         ],
         "modelscope.models.cv.stream_yolo.exp.base_exp": [
             "abc",
             "torch"
         ],
         "modelscope.models.cv.stream_yolo.exp.build": [
             "sys",
@@ -12861,17 +13140,17 @@
         ],
         "modelscope.models.cv.stream_yolo.exp.default.streamyolo": [
             "sys",
             "os",
             "torch"
         ],
         "modelscope.models.cv.stream_yolo.exp.yolox_base": [
-            "random",
             "os",
-            "torch"
+            "torch",
+            "random"
         ],
         "modelscope.models.cv.stream_yolo.models.darknet": [
             "torch"
         ],
         "modelscope.models.cv.stream_yolo.models.dfp_pafpn": [
             "torch"
         ],
@@ -12881,37 +13160,37 @@
         "modelscope.models.cv.stream_yolo.models.streamyolo": [
             "torch"
         ],
         "modelscope.models.cv.stream_yolo.models.tal_head": [
             "torch"
         ],
         "modelscope.models.cv.stream_yolo.realtime_video_detector": [
+            "argparse",
+            "logging",
             "numpy",
             "time",
-            "cv2",
-            "os",
             "torch",
-            "json",
+            "os",
+            "cv2",
             "tqdm",
-            "argparse",
-            "logging"
+            "json"
         ],
         "modelscope.models.cv.stream_yolo.utils.boxes": [
             "torchvision",
             "torch"
         ],
         "modelscope.models.cv.stream_yolo.utils.format": [
             "math"
         ],
         "modelscope.models.cv.super_resolution.arch_util": [
-            "torchvision",
+            "collections",
+            "warnings",
             "math",
+            "torchvision",
             "itertools",
-            "warnings",
-            "collections",
             "torch"
         ],
         "modelscope.models.cv.super_resolution.ecb": [
             "torch"
         ],
         "modelscope.models.cv.super_resolution.ecbsr_model": [
             "typing",
@@ -12919,65 +13198,65 @@
             "torch"
         ],
         "modelscope.models.cv.super_resolution.rrdbnet_arch": [
             "torch"
         ],
         "modelscope.models.cv.table_recognition.lineless_table_process": [
             "numpy",
-            "cv2",
             "shapely",
+            "cv2",
             "torch"
         ],
         "modelscope.models.cv.table_recognition.model_lore": [
             "numpy",
             "typing",
-            "os",
-            "copy",
             "torch",
+            "copy",
+            "os",
             "math"
         ],
         "modelscope.models.cv.table_recognition.modules.lore_detector": [
             "numpy",
-            "math",
-            "os",
             "copy",
+            "os",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.table_recognition.modules.lore_processor": [
             "numpy",
-            "math",
-            "os",
             "copy",
+            "os",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.text_driven_segmentation.clip": [
+            "warnings",
+            "urllib",
             "typing",
             "pkg_resources",
-            "urllib",
-            "hashlib",
-            "os",
             "torch",
-            "torchvision",
             "PIL",
+            "os",
+            "torchvision",
             "tqdm",
-            "warnings"
+            "hashlib"
         ],
         "modelscope.models.cv.text_driven_segmentation.lseg_base": [
             "torch"
         ],
         "modelscope.models.cv.text_driven_segmentation.lseg_blocks": [
             "torch"
         ],
         "modelscope.models.cv.text_driven_segmentation.lseg_model": [
             "numpy",
             "typing",
-            "os",
             "torch",
-            "json",
-            "PIL"
+            "PIL",
+            "os",
+            "json"
         ],
         "modelscope.models.cv.text_driven_segmentation.lseg_net": [
             "numpy",
             "torch"
         ],
         "modelscope.models.cv.text_driven_segmentation.lseg_vit": [
             "timm",
@@ -12988,20 +13267,20 @@
         "modelscope.models.cv.text_driven_segmentation.model": [
             "numpy",
             "typing",
             "collections",
             "torch"
         ],
         "modelscope.models.cv.text_driven_segmentation.simple_tokenizer": [
-            "ftfy",
-            "gzip",
-            "functools",
             "os",
             "regex",
-            "html"
+            "ftfy",
+            "gzip",
+            "html",
+            "functools"
         ],
         "modelscope.models.cv.tinynas_classfication.basic_blocks": [
             "numpy",
             "uuid",
             "torch"
         ],
         "modelscope.models.cv.tinynas_classfication.global_utils": [],
@@ -13029,35 +13308,35 @@
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.apis.detector_evaluater": [
             "os",
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.apis.detector_inference": [
-            "os",
             "tqdm",
+            "os",
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.augmentations.box_level_augs.box_level_augs": [
             "numpy",
             "random"
         ],
         "modelscope.models.cv.tinynas_detection.damo.augmentations.box_level_augs.color_augs": [
-            "random",
-            "torch"
+            "torch",
+            "random"
         ],
         "modelscope.models.cv.tinynas_detection.damo.augmentations.box_level_augs.gaussian_maps": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.augmentations.box_level_augs.geometric_augs": [
-            "torchvision",
-            "random",
             "copy",
-            "torch"
+            "torchvision",
+            "torch",
+            "random"
         ],
         "modelscope.models.cv.tinynas_detection.damo.augmentations.scale_aware_aug": [
             "copy"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.backbones.darknet": [
             "torch"
         ],
@@ -13084,100 +13363,100 @@
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.core.repvgg_block": [
             "numpy",
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.core.utils": [
-            "functools",
-            "torch"
+            "torch",
+            "functools"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.core.weight_init": [
             "numpy",
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.heads.gfocal_v2_tiny": [
             "numpy",
-            "functools",
-            "torch"
+            "torch",
+            "functools"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.heads.zero_head": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.losses.distill_loss": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.losses.gfocal_loss": [
-            "functools",
-            "torch"
+            "torch",
+            "functools"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.necks.giraffe_config": [
             "collections",
             "networkx"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.necks.giraffe_fpn": [
             "numpy",
             "typing",
-            "functools",
             "torch",
-            "math",
+            "functools",
             "collections",
-            "timm"
+            "timm",
+            "math"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.necks.giraffe_fpn_btn": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.detectors.detector": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.structures.bounding_box": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.structures.boxlist_ops": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.structures.image_list": [
-            "__future__",
-            "torch"
+            "torch",
+            "__future__"
         ],
         "modelscope.models.cv.tinynas_detection.damo.utils.boxes": [
             "numpy",
             "torchvision",
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.utils.model_utils": [
-            "math",
+            "copy",
             "thop",
             "time",
-            "copy",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.utils.scheduler": [
             "math"
         ],
         "modelscope.models.cv.tinynas_detection.detector": [
             "pickle",
-            "torchvision",
             "os",
-            "torch"
+            "torch",
+            "torchvision"
         ],
         "modelscope.models.cv.tinynas_detection.tinynas_damoyolo": [],
         "modelscope.models.cv.tinynas_detection.tinynas_detector": [],
         "modelscope.models.cv.tinynas_detection.utils": [
-            "os",
             "importlib",
+            "os",
+            "easydict",
             "sys",
             "shutil",
-            "tempfile",
-            "easydict"
+            "tempfile"
         ],
         "modelscope.models.cv.video_deinterlace.UNet_for_video_deinterlace": [
             "typing",
-            "os",
             "copy",
+            "os",
             "torch"
         ],
         "modelscope.models.cv.video_deinterlace.deinterlace_arch": [
             "torch"
         ],
         "modelscope.models.cv.video_deinterlace.models.archs": [
             "numpy",
@@ -13198,145 +13477,145 @@
         ],
         "modelscope.models.cv.video_depth_estimation.configs.default_config": [
             "yacs",
             "os"
         ],
         "modelscope.models.cv.video_depth_estimation.dro_model": [
             "numpy",
-            "cv2",
-            "os",
-            "torch",
             "glob",
+            "torch",
+            "os",
+            "cv2",
             "tqdm"
         ],
         "modelscope.models.cv.video_depth_estimation.geometry.camera": [
-            "functools",
-            "torch"
+            "torch",
+            "functools"
         ],
         "modelscope.models.cv.video_depth_estimation.geometry.camera_utils": [
             "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.geometry.pose": [
             "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.geometry.pose_utils": [
             "numpy",
             "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.models.model_checkpoint": [
             "numpy",
-            "re",
             "os",
+            "re",
             "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.models.model_utils": [],
         "modelscope.models.cv.video_depth_estimation.models.model_wrapper": [
             "numpy",
-            "random",
             "torch",
+            "collections",
             "importlib",
-            "collections"
+            "random"
         ],
         "modelscope.models.cv.video_depth_estimation.models.sfm_model_mf": [
-            "random",
-            "torch"
+            "torch",
+            "random"
         ],
         "modelscope.models.cv.video_depth_estimation.models.sup_model_mf": [],
         "modelscope.models.cv.video_depth_estimation.networks.depth_pose.depth_pose_net": [
-            "functools",
-            "torch"
+            "torch",
+            "functools"
         ],
         "modelscope.models.cv.video_depth_estimation.networks.layers.resnet.depth_decoder": [
             "numpy",
             "collections",
-            "__future__",
-            "torch"
+            "torch",
+            "__future__"
         ],
         "modelscope.models.cv.video_depth_estimation.networks.layers.resnet.layers": [
-            "__future__",
-            "torch"
+            "torch",
+            "__future__"
         ],
         "modelscope.models.cv.video_depth_estimation.networks.layers.resnet.pose_decoder": [
-            "__future__",
             "collections",
-            "torch"
+            "torch",
+            "__future__"
         ],
         "modelscope.models.cv.video_depth_estimation.networks.layers.resnet.resnet_encoder": [
             "numpy",
             "torchvision",
-            "__future__",
-            "torch"
+            "torch",
+            "__future__"
         ],
         "modelscope.models.cv.video_depth_estimation.networks.optim.extractor": [
             "torchvision",
             "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.networks.optim.update": [
             "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.augmentations": [
             "numpy",
-            "random",
-            "cv2",
             "PIL",
-            "torchvision"
+            "torchvision",
+            "cv2",
+            "random"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.config": [
-            "datetime",
             "yacs",
             "os",
-            "torch"
+            "torch",
+            "datetime"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.depth": [
             "numpy",
             "torchvision",
-            "matplotlib",
-            "torch"
+            "torch",
+            "matplotlib"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.horovod": [
             "horovod"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.image": [
             "numpy",
+            "torch",
+            "PIL",
             "functools",
-            "cv2",
             "os",
-            "torch",
-            "PIL"
+            "cv2"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.image_gt": [
             "torch",
-            "functools",
+            "cv2",
             "PIL",
-            "cv2"
+            "functools"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.load": [
-            "collections",
+            "logging",
+            "warnings",
             "inspect",
-            "os",
             "torch",
+            "collections",
             "importlib",
-            "warnings",
-            "logging"
+            "os"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.misc": [
             "termcolor"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.types": [
             "numpy",
             "yacs",
             "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.VFINet_arch": [
             "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.VFINet_for_video_frame_interpolation": [
             "typing",
-            "os",
             "copy",
+            "os",
             "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.flow_model.corr": [
             "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.flow_model.extractor": [
             "torch"
@@ -13360,34 +13639,34 @@
             "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.interp_model.refinenet_arch": [
             "numpy",
             "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.interp_model.transformer_layers": [
+            "timm",
             "sys",
             "math",
-            "functools",
-            "timm",
-            "torch"
+            "torch",
+            "functools"
         ],
         "modelscope.models.cv.video_frame_interpolation.utils.scene_change_detection": [
             "numpy",
             "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.utils.utils": [
             "numpy",
-            "scipy",
-            "torch"
+            "torch",
+            "scipy"
         ],
         "modelscope.models.cv.video_human_matting.model": [
             "numpy",
             "typing",
-            "os",
             "torch",
+            "os",
             "torchvision"
         ],
         "modelscope.models.cv.video_human_matting.models.decoder": [
             "typing",
             "torch"
         ],
         "modelscope.models.cv.video_human_matting.models.deep_guided_filter": [
@@ -13401,89 +13680,89 @@
         ],
         "modelscope.models.cv.video_human_matting.models.matting": [
             "typing",
             "torch"
         ],
         "modelscope.models.cv.video_inpainting.inpainting": [
             "numpy",
+            "os",
+            "cv2",
             "torchvision",
-            "PIL",
             "time",
-            "cv2",
-            "os",
-            "torch"
+            "torch",
+            "PIL"
         ],
         "modelscope.models.cv.video_inpainting.inpainting_model": [
             "numpy",
             "torchvision",
             "math",
             "torch"
         ],
         "modelscope.models.cv.video_instance_segmentation.head.kernel_frame_iter_head": [
             "mmcv",
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_instance_segmentation.head.kernel_head": [
             "mmcv",
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_instance_segmentation.head.kernel_iter_head": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_instance_segmentation.head.kernel_update_head": [
             "numpy",
-            "mmdet",
             "mmcv",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_instance_segmentation.head.kernel_updator": [
             "mmcv",
             "torch"
         ],
         "modelscope.models.cv.video_instance_segmentation.neck.msdeformattn_decoder": [
             "mmcv",
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_instance_segmentation.track.kernel_update_head": [
             "numpy",
-            "mmdet",
             "mmcv",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_instance_segmentation.track.mask_hungarian_assigner": [
             "numpy",
+            "torch",
             "scipy",
-            "mmdet",
-            "torch"
+            "mmdet"
         ],
         "modelscope.models.cv.video_instance_segmentation.utils": [
             "numpy",
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_instance_segmentation.video_knet": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_multi_object_tracking.models.common": [
             "torch"
         ],
         "modelscope.models.cv.video_multi_object_tracking.models.decode": [
             "torch"
         ],
         "modelscope.models.cv.video_multi_object_tracking.models.model": [
             "torch"
         ],
         "modelscope.models.cv.video_multi_object_tracking.models.yolo": [
-            "math",
             "copy",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.video_multi_object_tracking.tracker.basetrack": [
             "numpy",
             "collections"
         ],
         "modelscope.models.cv.video_multi_object_tracking.tracker.matching": [
@@ -13526,16 +13805,16 @@
             "torch"
         ],
         "modelscope.models.cv.video_object_segmentation.inference_memory_bank": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.video_object_segmentation.mod_resnet": [
-            "math",
             "collections",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.video_object_segmentation.model": [
             "typing",
             "os",
             "torch"
         ],
@@ -13544,91 +13823,91 @@
             "torch"
         ],
         "modelscope.models.cv.video_object_segmentation.network": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.backbone.swin_checkpoint": [
+            "collections",
             "importlib",
+            "os",
             "torchvision",
             "pkgutil",
-            "collections",
-            "os",
             "torch"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.backbone.swin_transformer": [
             "numpy",
             "timm",
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.head.kernel_head": [
             "mmcv",
             "torch"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.head.kernel_iter_head": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.head.kernel_update_head": [
             "numpy",
-            "mmdet",
             "mmcv",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.head.kernel_updator": [
             "mmcv",
             "torch"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.head.mask": [
             "numpy",
-            "pycocotools",
-            "__future__",
             "cv2",
-            "torch"
+            "pycocotools",
+            "torch",
+            "__future__"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.head.semantic_fpn_wrapper": [
             "mmcv",
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.head.track_heads": [
             "numpy",
             "mmcv",
             "torch"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.neck.fpn": [
             "mmcv",
             "torch"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.track.quasi_dense_embed_tracker": [
             "mmcv",
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.video_k_net": [
             "numpy",
-            "mmdet",
             "mmcv",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.visualizer": [
             "numpy",
             "hashlib",
             "cv2"
         ],
         "modelscope.models.cv.video_single_object_tracking.config.ostrack": [
             "easydict"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.layers.attn": [
             "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.layers.attn_blocks": [
-            "math",
             "timm",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.layers.head": [
             "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.layers.patch_embed": [
             "timm",
@@ -13642,53 +13921,53 @@
             "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.ostrack.utils": [
             "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.ostrack.vit_ce": [
             "timm",
-            "functools",
-            "torch"
+            "torch",
+            "functools"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.procontext.procontext": [
             "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.procontext.utils": [
             "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.procontext.vit_ce": [
             "timm",
-            "functools",
-            "torch"
+            "torch",
+            "functools"
         ],
         "modelscope.models.cv.video_single_object_tracking.tracker.ostrack": [
             "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.tracker.procontext": [
             "copy",
             "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.utils.utils": [
             "numpy",
+            "cv2",
             "typing",
             "math",
-            "cv2",
             "torch"
         ],
         "modelscope.models.cv.video_stabilization.DUT.DUT_raft": [
             "numpy",
             "sys",
             "cv2",
             "torch"
         ],
         "modelscope.models.cv.video_stabilization.DUT.MotionPro": [
             "numpy",
-            "cv2",
-            "os",
             "torch",
+            "os",
+            "cv2",
             "math"
         ],
         "modelscope.models.cv.video_stabilization.DUT.RAFT.corr": [
             "alt_cuda_corr",
             "torch"
         ],
         "modelscope.models.cv.video_stabilization.DUT.RAFT.extractor": [
@@ -13703,37 +13982,37 @@
         ],
         "modelscope.models.cv.video_stabilization.DUT.Smoother": [
             "numpy",
             "math",
             "torch"
         ],
         "modelscope.models.cv.video_stabilization.DUT.config": [
-            "__future__",
-            "easydict"
+            "easydict",
+            "__future__"
         ],
         "modelscope.models.cv.video_stabilization.DUT.rf_det_module": [
             "torch"
         ],
         "modelscope.models.cv.video_stabilization.DUT.rf_det_so": [
             "torch"
         ],
         "modelscope.models.cv.video_stabilization.DUTRAFTStabilizer": [
             "numpy",
+            "sys",
             "typing",
-            "cv2",
+            "torch",
             "tempfile",
             "os",
-            "torch",
-            "sys",
+            "cv2",
             "math"
         ],
         "modelscope.models.cv.video_stabilization.utils.IterativeSmooth": [
             "numpy",
-            "math",
             "os",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.video_stabilization.utils.MedianFilter": [
             "numpy",
             "math",
             "cv2",
             "torch"
@@ -13742,41 +14021,41 @@
             "numpy",
             "math",
             "cv2",
             "torch"
         ],
         "modelscope.models.cv.video_stabilization.utils.RAFTUtils": [
             "numpy",
-            "scipy",
-            "torch"
+            "torch",
+            "scipy"
         ],
         "modelscope.models.cv.video_stabilization.utils.WarpUtils": [
             "numpy",
             "tqdm",
             "torch"
         ],
         "modelscope.models.cv.video_stabilization.utils.image_utils": [
-            "skimage",
-            "torch"
+            "torch",
+            "skimage"
         ],
         "modelscope.models.cv.video_stabilization.utils.math_utils": [
             "numpy",
             "torch"
         ],
         "modelscope.models.cv.video_streaming_perception.longshortnet.exp.longshortnet_base": [],
         "modelscope.models.cv.video_streaming_perception.longshortnet.longshortnet": [
+            "argparse",
+            "logging",
             "numpy",
             "time",
-            "cv2",
-            "os",
             "torch",
-            "json",
+            "os",
+            "cv2",
             "tqdm",
-            "argparse",
-            "logging"
+            "json"
         ],
         "modelscope.models.cv.video_streaming_perception.longshortnet.models.dfp_pafpn_long": [
             "collections",
             "torch"
         ],
         "modelscope.models.cv.video_streaming_perception.longshortnet.models.dfp_pafpn_short": [
             "collections",
@@ -13813,391 +14092,399 @@
             "torch"
         ],
         "modelscope.models.cv.video_super_resolution.common": [
             "torch"
         ],
         "modelscope.models.cv.video_super_resolution.msrresnet_lite_model": [
             "typing",
-            "functools",
             "os",
-            "torch"
+            "torch",
+            "functools"
         ],
         "modelscope.models.cv.video_super_resolution.real_basicvsr_for_video_super_resolution": [
             "typing",
             "os",
             "torch"
         ],
         "modelscope.models.cv.video_super_resolution.real_basicvsr_net": [
             "torch"
         ],
         "modelscope.models.cv.vidt.backbone": [
             "numpy",
-            "math",
-            "timm",
             "os",
+            "timm",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.vidt.deformable_transformer": [
-            "math",
             "warnings",
-            "timm",
             "copy",
+            "timm",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.vidt.fpn_fusion": [
             "torch"
         ],
         "modelscope.models.cv.vidt.head": [
-            "math",
             "copy",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.vidt.model": [
             "os",
             "torch"
         ],
         "modelscope.models.cv.virual_tryon.sdafnet": [
             "numpy",
-            "random",
-            "torch"
+            "torch",
+            "random"
         ],
         "modelscope.models.cv.vision_efficient_tuning.backbone": [
-            "functools",
-            "torch"
+            "torch",
+            "functools"
         ],
         "modelscope.models.cv.vision_efficient_tuning.head": [
             "torch"
         ],
         "modelscope.models.cv.vision_efficient_tuning.model": [
             "typing",
             "torch"
         ],
         "modelscope.models.cv.vision_efficient_tuning.petl": [
+            "collections",
             "torchvision",
             "math",
-            "collections",
             "torch"
         ],
         "modelscope.models.cv.vision_efficient_tuning.timm_helpers": [
             "typing",
-            "math",
             "itertools",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.vision_efficient_tuning.timm_vision_transformer": [
-            "functools",
-            "torch",
-            "math",
+            "logging",
             "itertools",
+            "torch",
+            "functools",
             "collections",
-            "logging"
+            "math"
         ],
         "modelscope.models.cv.vision_efficient_tuning.timm_weight_init": [
             "warnings",
             "math",
             "torch"
         ],
         "modelscope.models.cv.vision_efficient_tuning.vision_efficient_tuning": [
             "collections",
             "os",
             "torch"
         ],
         "modelscope.models.cv.vision_middleware.backbone": [
             "numpy",
             "typing",
-            "os",
             "torch",
-            "math",
-            "collections"
+            "collections",
+            "os",
+            "math"
         ],
         "modelscope.models.cv.vision_middleware.head": [
-            "mmcv",
-            "abc",
             "numpy",
+            "abc",
+            "mmcv",
             "torch"
         ],
         "modelscope.models.cv.vision_middleware.model": [
             "typing",
-            "json",
             "os",
-            "torch"
+            "torch",
+            "json"
         ],
         "modelscope.models.cv.vision_middleware.vim": [
             "math",
-            "einops",
-            "torch"
+            "torch",
+            "einops"
         ],
         "modelscope.models.cv.vop_retrieval.backbone": [
             "numpy",
+            "warnings",
+            "urllib",
             "typing",
+            "torch",
             "collections",
-            "urllib",
-            "hashlib",
             "os",
-            "torch",
             "tqdm",
-            "warnings"
+            "hashlib"
         ],
         "modelscope.models.cv.vop_retrieval.basic_utils": [
             "numpy",
+            "ujson",
             "shutil",
-            "random",
-            "cv2",
-            "os",
             "torch",
+            "PIL",
+            "collections",
             "zipfile",
+            "os",
+            "cv2",
+            "random",
             "torchvision",
-            "pickle",
-            "PIL",
-            "ujson",
-            "collections"
+            "pickle"
         ],
         "modelscope.models.cv.vop_retrieval.model": [
             "os",
             "torch"
         ],
         "modelscope.models.cv.vop_retrieval.model_se": [
             "os",
             "torch"
         ],
         "modelscope.models.cv.vop_retrieval.tokenization_clip": [
+            "os",
+            "regex",
             "ftfy",
             "gzip",
-            "functools",
             "torch",
-            "os",
-            "regex",
-            "html"
+            "html",
+            "functools"
         ],
         "modelscope.models.multi_modal.clip.bert_tokenizer": [
-            "unicodedata",
-            "__future__",
-            "re",
             "collections",
             "os",
-            "six"
+            "six",
+            "unicodedata",
+            "re",
+            "__future__"
         ],
         "modelscope.models.multi_modal.clip.configuration_bert": [
-            "__future__",
-            "logging"
+            "logging",
+            "__future__"
         ],
         "modelscope.models.multi_modal.clip.model": [
             "numpy",
             "typing",
-            "os",
             "torch",
-            "json",
-            "collections"
+            "collections",
+            "os",
+            "json"
         ],
         "modelscope.models.multi_modal.clip.modeling_bert": [
-            "os",
-            "torch",
+            "logging",
             "sys",
-            "json",
-            "math",
+            "torch",
             "__future__",
             "io",
-            "logging"
+            "os",
+            "math",
+            "json"
         ],
         "modelscope.models.multi_modal.diffusion.diffusion": [
             "math",
             "torch"
         ],
         "modelscope.models.multi_modal.diffusion.model": [
             "numpy",
             "typing",
-            "os",
             "torch",
+            "os",
             "json"
         ],
         "modelscope.models.multi_modal.diffusion.structbert": [
             "numpy",
+            "copy",
             "json",
-            "math",
             "six",
-            "__future__",
-            "copy",
-            "torch"
+            "math",
+            "torch",
+            "__future__"
         ],
         "modelscope.models.multi_modal.diffusion.tokenizer": [
-            "unicodedata",
             "collections",
-            "__future__",
-            "six"
+            "unicodedata",
+            "six",
+            "__future__"
         ],
         "modelscope.models.multi_modal.diffusion.unet_generator": [
             "math",
             "torch"
         ],
         "modelscope.models.multi_modal.diffusion.unet_upsampler_1024": [
             "math",
             "torch"
         ],
         "modelscope.models.multi_modal.diffusion.unet_upsampler_256": [
             "math",
-            "functools",
-            "torch"
+            "torch",
+            "functools"
         ],
         "modelscope.models.multi_modal.dpm_solver_pytorch": [
             "math",
             "torch"
         ],
+        "modelscope.models.multi_modal.efficient_diffusion_tuning.efficient_stable_diffusion": [
+            "diffusers",
+            "typing",
+            "torch",
+            "functools",
+            "os",
+            "transformers"
+        ],
         "modelscope.models.multi_modal.gemm.gemm_base": [
             "numpy",
             "typing",
-            "os",
             "torch",
-            "json",
-            "collections"
+            "collections",
+            "os",
+            "json"
         ],
         "modelscope.models.multi_modal.gemm.gemm_model": [
             "numpy",
             "typing",
-            "os",
             "torch",
-            "json",
             "PIL",
-            "torchvision"
+            "os",
+            "torchvision",
+            "json"
         ],
         "modelscope.models.multi_modal.gemm.tokenizer": [
+            "os",
+            "regex",
             "ftfy",
             "gzip",
-            "functools",
             "torch",
-            "os",
-            "regex",
-            "html"
+            "html",
+            "functools"
         ],
         "modelscope.models.multi_modal.guided_diffusion.gaussian_diffusion": [
             "numpy",
+            "torch",
             "math",
-            "enum",
-            "torch"
+            "enum"
         ],
         "modelscope.models.multi_modal.guided_diffusion.respace": [
             "numpy",
             "torch"
         ],
         "modelscope.models.multi_modal.guided_diffusion.script": [],
         "modelscope.models.multi_modal.guided_diffusion.unet": [
             "numpy",
             "abc",
-            "transformers",
             "math",
-            "torch"
+            "torch",
+            "transformers"
         ],
         "modelscope.models.multi_modal.mgeo.backbone": [
+            "warnings",
+            "dataclasses",
             "typing",
-            "transformers",
-            "random",
-            "os",
             "torch",
+            "os",
+            "random",
             "math",
-            "dataclasses",
-            "warnings"
+            "transformers"
         ],
         "modelscope.models.multi_modal.mgeo.text_classification": [
             "torch"
         ],
         "modelscope.models.multi_modal.mgeo.text_ranking": [
             "torch"
         ],
         "modelscope.models.multi_modal.mgeo.token_classification": [
             "torch"
         ],
         "modelscope.models.multi_modal.mmr.dataloaders.rawvideo_util": [
             "numpy",
-            "cv2",
             "torch",
             "PIL",
-            "torchvision"
+            "torchvision",
+            "cv2"
         ],
         "modelscope.models.multi_modal.mmr.models.clip_for_mm_video_embedding": [
             "numpy",
-            "typing",
             "decord",
-            "random",
+            "typing",
             "urllib",
-            "os",
-            "tempfile",
             "torch",
-            "json",
             "PIL",
-            "uuid"
+            "tempfile",
+            "os",
+            "uuid",
+            "random",
+            "json"
         ],
         "modelscope.models.multi_modal.mmr.models.dynamic_inverted_softmax": [
             "numpy"
         ],
         "modelscope.models.multi_modal.mmr.models.modeling": [
             "platform",
-            "os",
-            "torch",
             "types",
-            "collections"
+            "torch",
+            "collections",
+            "os"
         ],
         "modelscope.models.multi_modal.mmr.models.module_clip": [
+            "warnings",
+            "urllib",
             "typing",
+            "torch",
             "collections",
-            "urllib",
-            "hashlib",
             "os",
-            "torch",
             "tqdm",
-            "warnings"
+            "hashlib"
         ],
         "modelscope.models.multi_modal.mmr.models.module_cross": [
+            "logging",
             "torch",
-            "json",
             "__future__",
             "collections",
-            "logging"
+            "json"
         ],
         "modelscope.models.multi_modal.mmr.models.tokenization_clip": [
-            "ftfy",
-            "gzip",
-            "functools",
             "os",
             "regex",
-            "html"
+            "ftfy",
+            "gzip",
+            "html",
+            "functools"
         ],
         "modelscope.models.multi_modal.mmr.models.until_module": [
-            "numpy",
-            "math",
             "logging",
-            "torch"
+            "torch",
+            "math",
+            "numpy"
         ],
         "modelscope.models.multi_modal.mplug.clip.clip": [
-            "typing",
             "collections",
+            "typing",
             "torch"
         ],
         "modelscope.models.multi_modal.mplug.configuration_mplug": [
-            "yaml",
             "typing",
-            "transformers",
-            "os"
+            "os",
+            "yaml",
+            "transformers"
         ],
         "modelscope.models.multi_modal.mplug.modeling_mplug": [
             "typing",
-            "transformers",
-            "os",
             "torch",
-            "math"
+            "os",
+            "math",
+            "transformers"
         ],
         "modelscope.models.multi_modal.mplug.mvit": [
             "numpy",
-            "functools",
             "collections",
             "timm",
+            "torch",
             "fairscale",
-            "torch"
+            "functools"
         ],
         "modelscope.models.multi_modal.mplug.predictor": [
-            "__future__",
-            "torch"
+            "torch",
+            "__future__"
         ],
         "modelscope.models.multi_modal.mplug_for_all_tasks": [
             "typing",
             "os"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.clip": [
             "math",
@@ -14210,318 +14497,318 @@
         "modelscope.models.multi_modal.multi_stage_diffusion.gaussian_diffusion": [
             "math",
             "torch"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.model": [
             "numpy",
             "typing",
-            "os",
             "torch",
-            "json",
+            "PIL",
+            "os",
             "math",
-            "PIL"
+            "json"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.prior": [
             "math",
             "torch"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.tokenizer": [
+            "regex",
             "ftfy",
-            "gzip",
             "transformers",
+            "gzip",
+            "torch",
             "html",
-            "functools",
-            "regex",
-            "torch"
+            "functools"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.upsampler": [
             "math",
             "torch"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.xglm": [
             "math",
             "torch"
         ],
         "modelscope.models.multi_modal.ofa.configuration_mmspeech": [
-            "transformers",
-            "warnings"
+            "warnings",
+            "transformers"
         ],
         "modelscope.models.multi_modal.ofa.configuration_ofa": [
-            "transformers",
-            "warnings"
+            "warnings",
+            "transformers"
         ],
         "modelscope.models.multi_modal.ofa.generate.incremental_decoding_utils": [
             "typing",
             "uuid",
             "torch"
         ],
         "modelscope.models.multi_modal.ofa.generate.multihead_attention": [
+            "fairseq",
             "typing",
             "math",
-            "fairseq",
             "torch"
         ],
         "modelscope.models.multi_modal.ofa.generate.ngram_repeat_block": [
-            "typing",
-            "math",
             "fairseq",
             "warnings",
+            "typing",
+            "math",
             "torch"
         ],
         "modelscope.models.multi_modal.ofa.generate.search": [
             "typing",
             "math",
             "torch"
         ],
         "modelscope.models.multi_modal.ofa.generate.sequence_generator": [
+            "sys",
             "typing",
             "math",
-            "sys",
             "torch"
         ],
         "modelscope.models.multi_modal.ofa.generate.token_generation_constraints": [
-            "typing",
             "collections",
+            "typing",
             "torch"
         ],
         "modelscope.models.multi_modal.ofa.generate.utils": [
+            "collections",
             "torch_xla",
-            "itertools",
             "amp_C",
-            "collections",
+            "itertools",
             "torch"
         ],
         "modelscope.models.multi_modal.ofa.modeling_mmspeech": [
             "numpy",
+            "fairseq",
+            "apex",
+            "dataclasses",
+            "packaging",
             "typing",
-            "transformers",
             "torch",
-            "packaging",
             "math",
-            "fairseq",
-            "apex",
-            "dataclasses"
+            "transformers"
         ],
         "modelscope.models.multi_modal.ofa.modeling_ofa": [
+            "apex",
+            "dataclasses",
+            "packaging",
             "typing",
-            "transformers",
-            "random",
             "torch",
-            "packaging",
+            "random",
             "math",
-            "apex",
-            "dataclasses"
+            "transformers"
         ],
         "modelscope.models.multi_modal.ofa.resnet": [
             "torch"
         ],
         "modelscope.models.multi_modal.ofa.tokenization_ofa": [
-            "typing",
-            "transformers",
             "collections",
-            "os"
+            "typing",
+            "os",
+            "transformers"
         ],
         "modelscope.models.multi_modal.ofa.tokenization_ofa_fast": [
             "typing",
-            "transformers",
+            "tokenizers",
             "json",
-            "tokenizers"
+            "transformers"
         ],
         "modelscope.models.multi_modal.ofa.utils.constant": [],
         "modelscope.models.multi_modal.ofa.utils.utils": [
             "typing",
             "torch"
         ],
         "modelscope.models.multi_modal.ofa.vit": [
-            "torch",
             "collections",
+            "torch",
             "fairseq"
         ],
         "modelscope.models.multi_modal.ofa_for_all_tasks": [
             "typing",
+            "re",
+            "string",
+            "torch",
             "functools",
             "os",
-            "torch",
-            "json",
             "math",
-            "re",
-            "string"
+            "json"
         ],
         "modelscope.models.multi_modal.ofa_for_text_to_image_synthesis_model": [
             "numpy",
+            "taming",
             "typing",
             "pkg_resources",
-            "os",
             "torch",
-            "taming",
-            "json",
             "PIL",
-            "torchvision"
+            "os",
+            "torchvision",
+            "json"
         ],
         "modelscope.models.multi_modal.rleg.model": [
-            "json",
             "os",
-            "torch"
+            "torch",
+            "json"
         ],
         "modelscope.models.multi_modal.rleg.rleg": [
             "typing",
             "torchvision",
             "torch"
         ],
         "modelscope.models.multi_modal.soonet.blocks": [
             "math",
             "torch"
         ],
         "modelscope.models.multi_modal.soonet.clip": [
             "numpy",
-            "typing",
             "warnings",
             "collections",
+            "typing",
             "torch"
         ],
         "modelscope.models.multi_modal.soonet.model": [
             "os",
             "torch"
         ],
         "modelscope.models.multi_modal.soonet.swin_transformer": [
             "numpy",
             "torch"
         ],
         "modelscope.models.multi_modal.soonet.tokenizer": [
+            "regex",
             "ftfy",
             "gzip",
+            "torch",
             "html",
-            "functools",
-            "regex",
-            "torch"
+            "functools"
         ],
         "modelscope.models.multi_modal.soonet.utils": [
             "numpy",
             "tqdm",
             "copy",
             "decord"
         ],
         "modelscope.models.multi_modal.team.team_model": [
             "numpy",
             "typing",
-            "cv2",
             "torch",
             "PIL",
+            "tokenizers",
             "torchvision",
-            "tokenizers"
+            "cv2"
         ],
         "modelscope.models.multi_modal.team.utils": [
             "numpy",
-            "typing",
-            "transformers",
             "collections",
-            "torch"
+            "typing",
+            "torch",
+            "transformers"
         ],
         "modelscope.models.multi_modal.video_synthesis.autoencoder": [
             "numpy",
             "torch"
         ],
         "modelscope.models.multi_modal.video_synthesis.diffusion": [
             "torch"
         ],
         "modelscope.models.multi_modal.video_synthesis.text_to_video_synthesis_model": [
             "typing",
-            "open_clip",
-            "os",
             "torch",
-            "einops"
+            "os",
+            "einops",
+            "open_clip"
         ],
         "modelscope.models.multi_modal.video_synthesis.unet_sd": [
             "math",
-            "einops",
-            "torch"
+            "torch",
+            "einops"
         ],
         "modelscope.models.multi_modal.vldoc.conv_fpn_trans": [
-            "random",
-            "torch",
             "apex",
+            "torch",
             "collections",
+            "random",
             "timm"
         ],
         "modelscope.models.multi_modal.vldoc.convnext": [
             "timm",
             "os",
             "torch"
         ],
         "modelscope.models.multi_modal.vldoc.model": [
-            "os",
-            "copy",
-            "torch",
+            "logging",
             "sys",
-            "math",
-            "json",
             "re",
+            "torch",
+            "copy",
+            "os",
             "torchvision",
-            "logging"
+            "math",
+            "json"
         ],
         "modelscope.models.multi_modal.vldoc.modeling_layout_roberta": [
-            "transformers",
+            "os",
+            "packaging",
             "math",
             "torch",
-            "os",
-            "packaging"
+            "transformers"
         ],
         "modelscope.models.multi_modal.vldoc.processing": [
             "numpy",
             "typing",
-            "cv2",
             "torch",
-            "torchvision",
             "PIL",
             "collections",
+            "torchvision",
+            "cv2",
             "timm"
         ],
         "modelscope.models.multi_modal.vldoc.tokenization": [
-            "transformers",
-            "os"
+            "os",
+            "transformers"
         ],
         "modelscope.models.multi_modal.vldoc.transformer_local": [
             "copy",
             "torch"
         ],
         "modelscope.models.nlp.T5.backbone": [
+            "warnings",
             "typing",
-            "transformers",
-            "os",
-            "copy",
             "torch",
+            "copy",
+            "os",
             "math",
-            "warnings"
+            "transformers"
         ],
         "modelscope.models.nlp.T5.configuration": [
             "typing",
             "transformers"
         ],
         "modelscope.models.nlp.T5.text2text_generation": [
+            "warnings",
             "typing",
-            "transformers",
-            "copy",
             "torch",
-            "warnings"
+            "copy",
+            "transformers"
         ],
         "modelscope.models.nlp.bart.text_error_correction": [
             "typing",
             "os",
             "torch"
         ],
         "modelscope.models.nlp.bert.backbone": [
-            "transformers",
             "math",
             "torch",
-            "packaging"
+            "packaging",
+            "transformers"
         ],
         "modelscope.models.nlp.bert.configuration": [
+            "collections",
             "typing",
-            "transformers",
-            "collections"
+            "transformers"
         ],
         "modelscope.models.nlp.bert.document_segmentation": [
             "typing",
             "torch"
         ],
         "modelscope.models.nlp.bert.fill_mask": [],
         "modelscope.models.nlp.bert.sentence_embedding": [
@@ -14536,14 +14823,36 @@
         "modelscope.models.nlp.bert.token_classification": [],
         "modelscope.models.nlp.bert.word_alignment": [
             "torch"
         ],
         "modelscope.models.nlp.bloom.backbone": [
             "transformers"
         ],
+        "modelscope.models.nlp.canmt.canmt_model": [
+            "numpy",
+            "fairseq",
+            "typing",
+            "math",
+            "torch"
+        ],
+        "modelscope.models.nlp.canmt.canmt_translation": [
+            "numpy",
+            "typing",
+            "torch",
+            "os",
+            "math"
+        ],
+        "modelscope.models.nlp.canmt.sequence_generator": [
+            "numpy",
+            "fairseq",
+            "typing",
+            "sys",
+            "math",
+            "torch"
+        ],
         "modelscope.models.nlp.codegeex.codegeex": [
             "math",
             "torch"
         ],
         "modelscope.models.nlp.codegeex.codegeex_for_code_generation": [
             "typing",
             "copy",
@@ -14556,55 +14865,55 @@
         ],
         "modelscope.models.nlp.codegeex.inference": [
             "typing",
             "torch"
         ],
         "modelscope.models.nlp.codegeex.tokenizer": [
             "typing",
-            "transformers",
-            "torch"
+            "torch",
+            "transformers"
         ],
         "modelscope.models.nlp.csanmt.translation": [
+            "collections",
             "typing",
             "tensorflow",
-            "collections",
             "math"
         ],
         "modelscope.models.nlp.deberta_v2.backbone": [
-            "typing",
-            "transformers",
             "collections",
-            "torch"
+            "typing",
+            "torch",
+            "transformers"
         ],
         "modelscope.models.nlp.deberta_v2.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.deberta_v2.fill_mask": [
             "typing",
-            "transformers",
-            "torch"
+            "torch",
+            "transformers"
         ],
         "modelscope.models.nlp.deberta_v2.tokenization": [
+            "os",
             "unicodedata",
             "typing",
-            "transformers",
             "sentencepiece",
-            "os"
+            "transformers"
         ],
         "modelscope.models.nlp.deberta_v2.tokenization_fast": [
             "typing",
-            "transformers",
+            "shutil",
             "os",
-            "shutil"
+            "transformers"
         ],
         "modelscope.models.nlp.dgds.backbone": [
-            "__future__",
-            "transformers",
             "os",
-            "torch"
+            "torch",
+            "__future__",
+            "transformers"
         ],
         "modelscope.models.nlp.dgds.document_grounded_dialog_generate": [
             "typing",
             "os",
             "torch"
         ],
         "modelscope.models.nlp.dgds.document_grounded_dialog_rerank": [
@@ -14613,111 +14922,148 @@
             "torch"
         ],
         "modelscope.models.nlp.dgds.document_grounded_dialog_retrieval": [
             "typing",
             "os",
             "torch"
         ],
+        "modelscope.models.nlp.fid_T5.text_generation": [
+            "io",
+            "os",
+            "torch",
+            "transformers"
+        ],
         "modelscope.models.nlp.fid_plug.backbone": [
             "numpy",
+            "dataclasses",
             "typing",
-            "transformers",
-            "os",
-            "copy",
             "torch",
+            "copy",
+            "os",
             "math",
-            "dataclasses"
+            "transformers"
         ],
         "modelscope.models.nlp.fid_plug.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.fid_plug.text_generation": [
-            "transformers",
             "io",
             "os",
+            "torch",
+            "transformers"
+        ],
+        "modelscope.models.nlp.glm_130b.generation.strategies": [
+            "numpy",
+            "torch",
+            "SwissArmyTransformer"
+        ],
+        "modelscope.models.nlp.glm_130b.initialize": [
+            "argparse",
+            "torch",
+            "time",
+            "SwissArmyTransformer"
+        ],
+        "modelscope.models.nlp.glm_130b.quantization.functional": [
             "torch"
         ],
+        "modelscope.models.nlp.glm_130b.quantization.layers": [
+            "torch",
+            "SwissArmyTransformer"
+        ],
+        "modelscope.models.nlp.glm_130b.text_generation": [
+            "time",
+            "sys",
+            "typing",
+            "re",
+            "torch",
+            "functools",
+            "SwissArmyTransformer",
+            "copy",
+            "os",
+            "random",
+            "stat"
+        ],
         "modelscope.models.nlp.gpt2.backbone": [
             "transformers"
         ],
         "modelscope.models.nlp.gpt3.backbone": [
             "typing",
-            "transformers",
+            "torch",
             "os",
             "addict",
-            "torch",
-            "math"
+            "math",
+            "transformers"
         ],
         "modelscope.models.nlp.gpt3.configuration": [
-            "transformers",
-            "torch"
+            "torch",
+            "transformers"
         ],
         "modelscope.models.nlp.gpt3.distributed_gpt3": [
             "typing",
-            "transformers",
-            "megatron_util",
-            "os",
             "torch",
+            "collections",
+            "os",
+            "megatron_util",
             "math",
-            "collections"
+            "transformers"
         ],
         "modelscope.models.nlp.gpt3.text_generation": [
-            "typing",
-            "transformers",
             "collections",
-            "torch"
+            "typing",
+            "torch",
+            "transformers"
         ],
         "modelscope.models.nlp.gpt3.tokenizer": [
             "typing",
             "tokenizers"
         ],
         "modelscope.models.nlp.gpt_moe.backbone": [
             "typing",
-            "transformers",
+            "torch",
             "os",
             "addict",
-            "torch",
-            "math"
+            "math",
+            "transformers"
         ],
         "modelscope.models.nlp.gpt_moe.checkpointing": [
-            "torch",
             "os",
-            "megatron_util"
+            "megatron_util",
+            "torch"
         ],
         "modelscope.models.nlp.gpt_moe.configuration": [
-            "transformers",
-            "torch"
+            "torch",
+            "transformers"
         ],
         "modelscope.models.nlp.gpt_moe.distributed_gpt_moe": [
-            "transformers",
             "math",
+            "megatron_util",
             "torch",
-            "megatron_util"
+            "transformers"
         ],
         "modelscope.models.nlp.gpt_moe.moe.experts": [
             "copy",
             "torch"
         ],
         "modelscope.models.nlp.gpt_moe.moe.layer": [
             "typing",
-            "torch",
-            "megatron_util"
+            "megatron_util",
+            "torch"
         ],
         "modelscope.models.nlp.gpt_moe.moe.mappings": [
-            "torch",
-            "megatron_util"
+            "megatron_util",
+            "torch"
         ],
         "modelscope.models.nlp.gpt_moe.moe.sharded_moe": [
+            "apex",
+            "scipy",
             "typing",
+            "torch",
             "tutel",
             "megatron_util",
-            "scipy",
-            "torch",
-            "math",
-            "apex"
+            "math"
         ],
         "modelscope.models.nlp.gpt_moe.moe.utils": [
             "typing",
             "torch"
         ],
         "modelscope.models.nlp.gpt_moe.text_generation": [
             "typing",
@@ -14727,21 +15073,21 @@
             "tokenizers"
         ],
         "modelscope.models.nlp.gpt_neo.backbone": [
             "transformers"
         ],
         "modelscope.models.nlp.heads.crf_head": [
             "typing",
-            "transformers",
-            "torch"
+            "torch",
+            "transformers"
         ],
         "modelscope.models.nlp.heads.fill_mask_head": [
             "typing",
-            "transformers",
-            "torch"
+            "torch",
+            "transformers"
         ],
         "modelscope.models.nlp.heads.infromation_extraction_head": [
             "torch"
         ],
         "modelscope.models.nlp.heads.text_classification_head": [
             "typing",
             "torch"
@@ -14756,477 +15102,512 @@
         ],
         "modelscope.models.nlp.heads.token_classification_head": [
             "typing",
             "torch"
         ],
         "modelscope.models.nlp.heads.torch_pretrain_head": [
             "typing",
-            "transformers",
-            "torch"
+            "torch",
+            "transformers"
         ],
         "modelscope.models.nlp.hf_transformers.backbone": [
             "transformers"
         ],
+        "modelscope.models.nlp.llama.backbone": [
+            "typing",
+            "math",
+            "torch",
+            "transformers"
+        ],
+        "modelscope.models.nlp.llama.configuration": [
+            "transformers"
+        ],
+        "modelscope.models.nlp.llama.convert_llama_weights_to_hf": [
+            "argparse",
+            "shutil",
+            "gc",
+            "torch",
+            "os",
+            "math",
+            "json"
+        ],
+        "modelscope.models.nlp.llama.text_generation": [
+            "typing",
+            "torch"
+        ],
+        "modelscope.models.nlp.llama.tokenization": [
+            "typing",
+            "shutil",
+            "sentencepiece",
+            "os",
+            "transformers"
+        ],
+        "modelscope.models.nlp.llama.tokenization_fast": [
+            "typing",
+            "shutil",
+            "os",
+            "transformers"
+        ],
         "modelscope.models.nlp.lstm.backbone": [
             "torch"
         ],
         "modelscope.models.nlp.lstm.token_classification": [],
         "modelscope.models.nlp.megatron_bert.backbone": [
-            "transformers",
             "math",
-            "torch"
+            "torch",
+            "transformers"
         ],
         "modelscope.models.nlp.megatron_bert.configuration": [
+            "collections",
             "typing",
-            "transformers",
-            "collections"
+            "transformers"
         ],
         "modelscope.models.nlp.megatron_bert.fill_mask": [
-            "transformers",
-            "torch"
+            "torch",
+            "transformers"
         ],
         "modelscope.models.nlp.mglm.arguments": [
+            "argparse",
             "deepspeed",
-            "os",
             "torch",
-            "json",
-            "argparse"
+            "os",
+            "json"
         ],
         "modelscope.models.nlp.mglm.blocklm_utils": [
             "numpy",
-            "math",
+            "copy",
             "megatron_util",
             "scipy",
             "random",
-            "copy",
+            "math",
             "torch"
         ],
         "modelscope.models.nlp.mglm.configure_data": [
             "numpy",
+            "itertools",
+            "torch",
+            "copy",
+            "os",
             "megatron_util",
             "random",
-            "os",
-            "copy",
-            "torch",
-            "itertools",
             "bisect"
         ],
         "modelscope.models.nlp.mglm.data_utils.corpora": [
-            "queue",
-            "random",
-            "os",
             "multiprocessing",
             "torch",
+            "collections",
+            "os",
             "json",
+            "random",
             "tqdm",
-            "collections"
+            "queue"
         ],
         "modelscope.models.nlp.mglm.data_utils.datasets": [
-            "nltk",
-            "csv",
             "time",
-            "torch",
-            "math",
-            "json",
-            "itertools",
+            "nltk",
+            "os",
+            "bisect",
             "tqdm",
-            "operator",
+            "math",
             "numpy",
-            "random",
-            "os",
+            "operator",
+            "csv",
+            "itertools",
+            "torch",
             "pandas",
-            "bisect"
+            "json",
+            "random"
         ],
         "modelscope.models.nlp.mglm.data_utils.extraction": [
-            "nltk",
+            "glob",
             "os",
             "json",
-            "glob"
+            "nltk"
         ],
         "modelscope.models.nlp.mglm.data_utils.file_utils": [
-            "functools",
-            "shutil",
-            "pathlib",
+            "logging",
             "sys",
-            "json",
+            "functools",
+            "os",
             "tqdm",
+            "boto3",
+            "shutil",
+            "urllib",
             "__future__",
-            "botocore",
-            "logging",
+            "tempfile",
+            "io",
             "requests",
-            "urllib",
+            "botocore",
             "hashlib",
-            "os",
-            "tempfile",
-            "boto3",
-            "io"
+            "json",
+            "pathlib"
         ],
         "modelscope.models.nlp.mglm.data_utils.lazy_loader": [
             "numpy",
-            "itertools",
-            "pickle",
             "mmap",
-            "time",
             "os",
+            "time",
+            "pickle",
+            "itertools",
             "torch"
         ],
         "modelscope.models.nlp.mglm.data_utils.samplers": [
             "numpy",
+            "os",
             "sys",
             "math",
-            "os",
             "torch"
         ],
         "modelscope.models.nlp.mglm.data_utils.sp_tokenizer": [
             "os"
         ],
         "modelscope.models.nlp.mglm.data_utils.tokenization": [
-            "nltk",
-            "csv",
-            "random",
-            "os",
             "regex",
-            "torch",
+            "csv",
             "itertools",
+            "torch",
             "sentencepiece",
-            "collections"
+            "collections",
+            "nltk",
+            "os",
+            "random"
         ],
         "modelscope.models.nlp.mglm.data_utils.tokenization_gpt2": [
-            "functools",
-            "os",
+            "logging",
             "regex",
             "sys",
-            "json",
             "__future__",
+            "functools",
             "io",
-            "logging"
+            "os",
+            "json"
         ],
         "modelscope.models.nlp.mglm.data_utils.wordpiece": [
-            "os",
-            "unicodedata",
+            "logging",
             "__future__",
             "collections",
             "io",
-            "logging"
+            "os",
+            "unicodedata"
         ],
         "modelscope.models.nlp.mglm.generation_utils": [
+            "collections",
             "abc",
             "typing",
-            "collections",
             "torch"
         ],
         "modelscope.models.nlp.mglm.mglm_for_text_summarization": [
             "numpy",
             "typing",
-            "megatron_util",
-            "random",
+            "torch",
             "os",
-            "torch"
+            "megatron_util",
+            "random"
         ],
         "modelscope.models.nlp.mglm.model.distributed": [
-            "torch",
-            "megatron_util"
+            "megatron_util",
+            "torch"
         ],
         "modelscope.models.nlp.mglm.model.downstream": [
             "torch"
         ],
         "modelscope.models.nlp.mglm.model.modeling_bert": [
-            "megatron_util",
-            "shutil",
+            "logging",
+            "apex",
             "tarfile",
-            "os",
-            "copy",
-            "tempfile",
+            "shutil",
             "torch",
-            "math",
-            "json",
-            "data_utils",
             "__future__",
-            "logging",
-            "apex"
+            "tempfile",
+            "data_utils",
+            "copy",
+            "os",
+            "megatron_util",
+            "math",
+            "json"
         ],
         "modelscope.models.nlp.mglm.model.modeling_glm": [
-            "torch",
-            "megatron_util"
+            "megatron_util",
+            "torch"
         ],
         "modelscope.models.nlp.mglm.model.prompt": [
-            "random",
-            "torch"
+            "torch",
+            "random"
         ],
         "modelscope.models.nlp.mglm.model.transformer": [
-            "math",
-            "megatron_util",
-            "apex",
             "deepspeed",
+            "apex",
+            "megatron_util",
+            "math",
             "torch"
         ],
         "modelscope.models.nlp.mglm.process_grid": [
-            "sys",
-            "json",
-            "glob",
             "statistics",
-            "os"
+            "os",
+            "glob",
+            "sys",
+            "json"
         ],
         "modelscope.models.nlp.mglm.run_test": [
             "sys",
             "test"
         ],
         "modelscope.models.nlp.mglm.tasks.data_utils": [
             "numpy",
             "typing",
-            "megatron_util",
-            "copy",
+            "re",
             "torch",
-            "json",
+            "copy",
+            "megatron_util",
             "pickle",
-            "re"
+            "json"
         ],
         "modelscope.models.nlp.mglm.tasks.eval_utils": [
-            "typing",
-            "utils",
-            "megatron_util",
             "time",
-            "random",
-            "os",
+            "typing",
             "sklearn",
+            "finetune_glm",
             "torch",
-            "tasks",
             "collections",
-            "datetime",
-            "finetune_glm"
+            "tasks",
+            "os",
+            "megatron_util",
+            "random",
+            "utils",
+            "datetime"
         ],
         "modelscope.models.nlp.mglm.tasks.language_model.dataset": [
             "numpy",
-            "utils",
-            "torch",
-            "json",
-            "math",
             "itertools",
+            "torch",
             "tasks",
-            "bisect"
+            "bisect",
+            "utils",
+            "math",
+            "json"
         ],
         "modelscope.models.nlp.mglm.tasks.language_model.detokenizer": [
             "re"
         ],
         "modelscope.models.nlp.mglm.tasks.language_model.finetune": [
-            "pretrain_glm",
+            "tasks",
             "math",
-            "functools",
             "megatron_util",
-            "tasks",
+            "pretrain_glm",
             "finetune_glm",
-            "torch"
+            "torch",
+            "functools"
         ],
         "modelscope.models.nlp.mglm.tasks.seq2seq.dataset": [
             "numpy",
-            "utils",
-            "random",
-            "os",
             "torch",
-            "json",
-            "tqdm",
             "data_utils",
-            "tasks"
+            "tasks",
+            "os",
+            "random",
+            "tqdm",
+            "utils",
+            "json"
         ],
         "modelscope.models.nlp.mglm.tasks.seq2seq.evaluate": [
             "megatron_util",
-            "generation_utils",
             "rouge_score",
             "random",
+            "generation_utils",
             "string",
-            "datetime",
-            "torch"
+            "torch",
+            "datetime"
         ],
         "modelscope.models.nlp.mglm.tasks.seq2seq.finetune": [
-            "pretrain_glm",
             "collections",
-            "functools",
-            "megatron_util",
             "tasks",
+            "megatron_util",
+            "pretrain_glm",
             "finetune_glm",
-            "torch"
+            "torch",
+            "functools"
         ],
         "modelscope.models.nlp.mglm.tasks.superglue.dataset": [
-            "csv",
-            "torch",
-            "abc",
-            "json",
-            "glob",
+            "re",
+            "data_utils",
+            "copy",
+            "os",
             "tqdm",
+            "utils",
             "numpy",
+            "csv",
+            "glob",
+            "abc",
             "typing",
-            "utils",
-            "random",
-            "os",
-            "copy",
+            "torch",
+            "collections",
             "pandas",
-            "data_utils",
-            "re",
-            "collections"
+            "json",
+            "random"
         ],
         "modelscope.models.nlp.mglm.tasks.superglue.evaluate": [
-            "typing",
-            "functools",
+            "collections",
             "tasks",
-            "__future__",
+            "typing",
             "re",
-            "collections",
-            "string"
+            "string",
+            "__future__",
+            "functools"
         ],
         "modelscope.models.nlp.mglm.tasks.superglue.finetune": [
             "collections",
-            "finetune_glm",
-            "tasks"
+            "tasks",
+            "finetune_glm"
         ],
         "modelscope.models.nlp.mglm.tasks.superglue.pvp": [
             "numpy",
-            "typing",
-            "utils",
-            "random",
-            "copy",
             "abc",
-            "math",
-            "tasks",
+            "typing",
+            "string",
             "collections",
-            "string"
+            "tasks",
+            "copy",
+            "random",
+            "utils",
+            "math"
         ],
         "modelscope.models.nlp.mglm.test.test_block": [
             "numpy",
+            "argparse",
             "random",
-            "blocklm_utils",
-            "argparse"
+            "blocklm_utils"
         ],
         "modelscope.models.nlp.mglm.test.test_rel_shift": [
             "numpy",
+            "torch",
             "learning_rates",
-            "matplotlib",
-            "torch"
+            "matplotlib"
         ],
         "modelscope.models.nlp.mglm.train_utils": [
-            "apex",
             "deepspeed",
-            "torch",
-            "megatron_util"
+            "apex",
+            "megatron_util",
+            "torch"
         ],
         "modelscope.models.nlp.mglm.utils": [
             "numpy",
+            "time",
+            "torch",
             "subprocess",
+            "os",
             "megatron_util",
-            "time",
             "random",
-            "os",
-            "torch",
             "json"
         ],
         "modelscope.models.nlp.palm_v2.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.palm_v2.dureader_eval": [
+            "argparse",
             "numpy",
+            "sys",
+            "re",
             "collections",
             "copy",
             "zipfile",
-            "sys",
             "math",
             "json",
-            "re",
-            "argparse",
             "rouge"
         ],
         "modelscope.models.nlp.palm_v2.text_generation": [
             "numpy",
+            "dataclasses",
             "typing",
-            "transformers",
-            "subprocess",
-            "os",
-            "copy",
             "torch",
+            "codecs",
+            "copy",
+            "os",
+            "subprocess",
             "math",
             "json",
-            "dataclasses",
-            "codecs"
+            "transformers"
         ],
         "modelscope.models.nlp.peer.backbone": [
+            "dataclasses",
             "typing",
-            "transformers",
             "torch",
             "math",
-            "dataclasses"
+            "transformers"
         ],
         "modelscope.models.nlp.peer.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.peer.sas_utils": [
-            "nltk",
-            "random",
             "numpy",
-            "torch"
+            "nltk",
+            "torch",
+            "random"
         ],
         "modelscope.models.nlp.peer.text_classification": [
             "copy",
             "torch"
         ],
         "modelscope.models.nlp.plug.AnnealingLR": [
             "math",
             "torch"
         ],
         "modelscope.models.nlp.plug.backbone": [
-            "megatron_util",
+            "logging",
             "torch",
-            "math",
             "__future__",
-            "logging"
+            "megatron_util",
+            "math"
         ],
         "modelscope.models.nlp.plug.configuration": [
-            "transformers",
+            "copy",
             "json",
-            "copy"
+            "transformers"
         ],
         "modelscope.models.nlp.plug.distributed_plug": [
             "typing",
-            "torch",
-            "megatron_util"
+            "megatron_util",
+            "torch"
         ],
         "modelscope.models.nlp.plug.generator": [
             "torch"
         ],
         "modelscope.models.nlp.plug_mental.adv_utils": [
             "torch"
         ],
         "modelscope.models.nlp.plug_mental.backbone": [
+            "packaging",
+            "dataclasses",
             "typing",
-            "transformers",
             "torch",
-            "packaging",
             "math",
-            "dataclasses"
+            "transformers"
         ],
         "modelscope.models.nlp.plug_mental.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.plug_mental.text_classification": [
             "torch"
         ],
         "modelscope.models.nlp.ponet.backbone": [
-            "transformers",
             "distutils",
-            "torch",
             "packaging",
-            "math"
+            "torch",
+            "math",
+            "transformers"
         ],
         "modelscope.models.nlp.ponet.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.ponet.document_segmentation": [
             "typing",
             "torch"
         ],
         "modelscope.models.nlp.ponet.fill_mask": [
-            "transformers",
-            "torch"
+            "torch",
+            "transformers"
         ],
         "modelscope.models.nlp.ponet.tokenization": [
             "typing",
             "transformers"
         ],
         "modelscope.models.nlp.space.configuration": [],
         "modelscope.models.nlp.space.dialog_intent_prediction": [
@@ -15235,16 +15616,16 @@
         ],
         "modelscope.models.nlp.space.dialog_modeling": [
             "typing",
             "os"
         ],
         "modelscope.models.nlp.space.dialog_state_tracking": [
             "typing",
-            "transformers",
-            "torch"
+            "torch",
+            "transformers"
         ],
         "modelscope.models.nlp.space.model.gen_unified_transformer": [
             "torch"
         ],
         "modelscope.models.nlp.space.model.generator": [
             "numpy",
             "math",
@@ -15278,66 +15659,66 @@
             "torch"
         ],
         "modelscope.models.nlp.space.modules.transformer_block": [
             "torch"
         ],
         "modelscope.models.nlp.space_T_cn.backbone": [
             "numpy",
-            "shutil",
             "tarfile",
-            "os",
-            "copy",
-            "tempfile",
+            "shutil",
             "torch",
-            "math",
-            "__future__"
+            "__future__",
+            "tempfile",
+            "copy",
+            "os",
+            "math"
         ],
         "modelscope.models.nlp.space_T_cn.configuration": [
-            "__future__",
-            "json",
             "logging",
-            "copy"
+            "copy",
+            "json",
+            "__future__"
         ],
         "modelscope.models.nlp.space_T_cn.table_question_answering": [
             "numpy",
             "typing",
-            "transformers",
+            "torch",
             "os",
-            "torch"
+            "transformers"
         ],
         "modelscope.models.nlp.space_T_en.text_to_sql": [
             "typing",
             "text2sql_lgesql",
             "os",
             "torch"
         ],
         "modelscope.models.nlp.structbert.adv_utils": [
             "torch"
         ],
         "modelscope.models.nlp.structbert.backbone": [
+            "packaging",
+            "dataclasses",
             "typing",
-            "transformers",
             "torch",
-            "packaging",
             "math",
-            "dataclasses"
+            "transformers"
         ],
         "modelscope.models.nlp.structbert.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.structbert.faq_question_answering": [
             "typing",
-            "os",
             "torch",
-            "math",
-            "collections"
+            "collections",
+            "os",
+            "math"
         ],
         "modelscope.models.nlp.structbert.fill_mask": [
-            "transformers",
-            "torch"
+            "torch",
+            "transformers"
         ],
         "modelscope.models.nlp.structbert.text_classification": [
             "torch"
         ],
         "modelscope.models.nlp.structbert.token_classification": [
             "torch"
         ],
@@ -15351,28 +15732,29 @@
             "torch"
         ],
         "modelscope.models.nlp.task_models.information_extraction": [
             "numpy",
             "typing"
         ],
         "modelscope.models.nlp.task_models.task_model": [
-            "typing",
-            "os",
-            "torch",
             "abc",
+            "typing",
             "re",
-            "collections"
+            "torch",
+            "collections",
+            "os"
         ],
         "modelscope.models.nlp.task_models.text_classification": [
             "numpy",
             "typing"
         ],
         "modelscope.models.nlp.task_models.text_generation": [
             "numpy",
             "typing",
+            "torch",
             "transformers"
         ],
         "modelscope.models.nlp.task_models.text_ranking": [
             "numpy",
             "typing"
         ],
         "modelscope.models.nlp.task_models.token_classification": [
@@ -15380,292 +15762,292 @@
             "torch"
         ],
         "modelscope.models.nlp.unite.configuration_unite": [
             "enum"
         ],
         "modelscope.models.nlp.unite.modeling_unite": [
             "numpy",
+            "warnings",
+            "dataclasses",
+            "packaging",
             "typing",
-            "transformers",
             "torch",
-            "packaging",
             "math",
-            "dataclasses",
-            "warnings"
+            "transformers"
         ],
         "modelscope.models.nlp.use.transformer": [
             "math",
             "torch"
         ],
         "modelscope.models.nlp.use.user_satisfaction_estimation": [
             "numpy",
             "typing",
-            "transformers",
+            "torch",
             "os",
-            "torch"
+            "transformers"
         ],
         "modelscope.models.nlp.veco.backbone": [
             "transformers"
         ],
         "modelscope.models.nlp.veco.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.veco.fill_mask": [
             "transformers"
         ],
         "modelscope.models.nlp.veco.text_classification": [
             "transformers"
         ],
         "modelscope.models.nlp.veco.token_classification": [
-            "transformers",
-            "torch"
+            "torch",
+            "transformers"
         ],
         "modelscope.models.nlp.xlm_roberta.backbone": [
-            "transformers",
             "math",
             "torch",
-            "packaging"
+            "packaging",
+            "transformers"
         ],
         "modelscope.models.nlp.xlm_roberta.configuration": [
+            "collections",
             "typing",
-            "transformers",
-            "collections"
+            "transformers"
         ],
         "modelscope.models.science.unifold.config": [
             "typing",
-            "ml_collections",
-            "copy"
+            "copy",
+            "ml_collections"
         ],
         "modelscope.models.science.unifold.data.data_ops": [
             "numpy",
-            "typing",
-            "functools",
+            "operator",
             "unicore",
-            "torch",
+            "typing",
             "itertools",
-            "operator"
+            "torch",
+            "functools"
         ],
         "modelscope.models.science.unifold.data.msa_pairing": [
             "numpy",
-            "typing",
             "scipy",
-            "pandas",
-            "collections"
+            "typing",
+            "collections",
+            "pandas"
         ],
         "modelscope.models.science.unifold.data.process": [
             "numpy",
             "typing",
             "torch"
         ],
         "modelscope.models.science.unifold.data.process_multimer": [
             "numpy",
             "typing",
             "collections"
         ],
         "modelscope.models.science.unifold.data.protein": [
             "numpy",
-            "typing",
-            "Bio",
             "dataclasses",
-            "io"
+            "typing",
+            "io",
+            "Bio"
         ],
         "modelscope.models.science.unifold.data.residue_constants": [
             "numpy",
-            "typing",
-            "functools",
-            "unicore",
             "collections",
-            "os"
+            "unicore",
+            "os",
+            "typing",
+            "functools"
         ],
         "modelscope.models.science.unifold.data.utils": [
             "numpy",
+            "scipy",
             "typing",
             "functools",
-            "scipy",
             "copy",
+            "pickle",
             "gzip",
-            "json",
-            "pickle"
+            "json"
         ],
         "modelscope.models.science.unifold.dataset": [
+            "logging",
             "numpy",
-            "typing",
             "unicore",
-            "ml_collections",
-            "os",
-            "copy",
+            "typing",
             "torch",
-            "json",
-            "logging"
+            "copy",
+            "os",
+            "ml_collections",
+            "json"
         ],
         "modelscope.models.science.unifold.model": [
-            "typing",
             "argparse",
+            "typing",
             "os",
             "torch"
         ],
         "modelscope.models.science.unifold.modules.alphafold": [
             "unicore",
             "torch"
         ],
         "modelscope.models.science.unifold.modules.attentions": [
             "typing",
-            "functools",
             "unicore",
-            "torch"
+            "torch",
+            "functools"
         ],
         "modelscope.models.science.unifold.modules.auxillary_heads": [
             "typing",
             "unicore",
             "torch"
         ],
         "modelscope.models.science.unifold.modules.common": [
             "typing",
-            "functools",
             "unicore",
-            "torch"
+            "torch",
+            "functools"
         ],
         "modelscope.models.science.unifold.modules.confidence": [
             "typing",
             "torch"
         ],
         "modelscope.models.science.unifold.modules.embedders": [
             "typing",
             "unicore",
             "torch"
         ],
         "modelscope.models.science.unifold.modules.evoformer": [
             "typing",
-            "functools",
             "unicore",
-            "torch"
+            "torch",
+            "functools"
         ],
         "modelscope.models.science.unifold.modules.featurization": [
             "typing",
             "unicore",
             "torch"
         ],
         "modelscope.models.science.unifold.modules.frame": [
             "numpy",
             "typing",
-            "__future__",
-            "torch"
+            "torch",
+            "__future__"
         ],
         "modelscope.models.science.unifold.modules.structure_module": [
             "typing",
-            "math",
             "unicore",
+            "math",
             "torch"
         ],
         "modelscope.models.science.unifold.modules.template": [
-            "typing",
-            "functools",
             "unicore",
+            "typing",
             "torch",
+            "functools",
             "math"
         ],
         "modelscope.models.science.unifold.modules.triangle_multiplication": [
             "typing",
-            "functools",
             "unicore",
-            "torch"
+            "torch",
+            "functools"
         ],
         "modelscope.models.science.unifold.msa.mmcif": [
-            "typing",
-            "functools",
-            "Bio",
-            "dataclasses",
             "collections",
+            "absl",
             "io",
-            "absl"
+            "dataclasses",
+            "typing",
+            "Bio",
+            "functools"
         ],
         "modelscope.models.science.unifold.msa.msa_identifiers": [
+            "typing",
             "re",
-            "dataclasses",
-            "typing"
+            "dataclasses"
         ],
         "modelscope.models.science.unifold.msa.parsers": [
+            "collections",
+            "dataclasses",
             "typing",
-            "itertools",
             "re",
-            "dataclasses",
-            "collections",
+            "itertools",
             "string"
         ],
         "modelscope.models.science.unifold.msa.pipeline": [
             "numpy",
             "typing",
             "os",
             "absl"
         ],
         "modelscope.models.science.unifold.msa.templates": [
             "numpy",
-            "typing",
-            "functools",
-            "os",
+            "dataclasses",
             "abc",
             "glob",
             "re",
-            "dataclasses",
-            "datetime",
-            "absl"
+            "typing",
+            "functools",
+            "absl",
+            "os",
+            "datetime"
         ],
         "modelscope.models.science.unifold.msa.tools.hhblits": [
+            "glob",
             "typing",
-            "subprocess",
+            "absl",
             "os",
-            "glob",
-            "absl"
+            "subprocess"
         ],
         "modelscope.models.science.unifold.msa.tools.hhsearch": [
+            "glob",
             "typing",
-            "subprocess",
+            "absl",
             "os",
-            "glob",
-            "absl"
+            "subprocess"
         ],
         "modelscope.models.science.unifold.msa.tools.hmmbuild": [
             "re",
-            "subprocess",
             "os",
+            "subprocess",
             "absl"
         ],
         "modelscope.models.science.unifold.msa.tools.hmmsearch": [
             "typing",
-            "subprocess",
             "os",
+            "subprocess",
             "absl"
         ],
         "modelscope.models.science.unifold.msa.tools.jackhmmer": [
+            "glob",
             "typing",
-            "subprocess",
-            "concurrent",
             "urllib",
+            "absl",
+            "concurrent",
             "os",
-            "glob",
-            "absl"
+            "subprocess"
         ],
         "modelscope.models.science.unifold.msa.tools.kalign": [
             "typing",
-            "subprocess",
             "os",
+            "subprocess",
             "absl"
         ],
         "modelscope.models.science.unifold.msa.tools.utils": [
-            "typing",
+            "absl",
+            "time",
             "contextlib",
+            "typing",
             "shutil",
-            "time",
-            "tempfile",
-            "absl"
+            "tempfile"
         ],
         "modelscope.models.science.unifold.msa.utils": [
             "typing",
-            "json",
             "os",
+            "json",
             "absl"
         ],
         "modelscope.msdatasets.audio.asr_dataset": [],
         "modelscope.msdatasets.auth.auth_config": [
             "typing",
             "http"
         ],
@@ -15680,69 +16062,68 @@
         "modelscope.msdatasets.data_loader.data_loader": [
             "abc",
             "typing",
             "datasets"
         ],
         "modelscope.msdatasets.data_loader.data_loader_manager": [
             "abc",
-            "enum",
             "os",
+            "enum",
             "datasets"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.audio.asr_dataset": [
             "os"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.audio.kws_farfield_dataset": [
             "numpy",
-            "queue",
-            "os",
             "torch",
+            "os",
             "threading",
-            "math"
+            "math",
+            "queue"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.audio.kws_nearfield_dataset": [
-            "random",
-            "torch"
+            "torch",
+            "random"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.audio.kws_nearfield_processor": [
             "numpy",
-            "json",
+            "torchaudio",
             "kaldiio",
+            "torch",
             "random",
-            "logging",
-            "torchaudio",
-            "torch"
+            "json"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.bad_image_detecting.bad_image_detecting_dataset": [],
         "modelscope.msdatasets.dataset_cls.custom_datasets.builder": [],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.build": [
             "bisect",
-            "math",
             "copy",
+            "math",
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.collate_batch": [],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.datasets.coco": [
             "numpy",
             "torchvision",
             "cv2",
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.datasets.mosaic_wrapper": [
             "numpy",
-            "random",
-            "cv2",
             "torch",
+            "cv2",
+            "random",
             "math"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.evaluation.coco.coco_eval": [
-            "os",
             "collections",
-            "tempfile",
-            "torch"
+            "os",
+            "torch",
+            "tempfile"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.samplers.distributed": [
             "math",
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.samplers.grouped_batch_sampler": [
             "itertools",
@@ -15751,16 +16132,16 @@
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.samplers.iteration_based_batch_sampler": [
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.transforms.build": [],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.transforms.transforms": [
             "numpy",
             "torchvision",
-            "random",
             "cv2",
+            "random",
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.easycv_base": [
             "os"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.face_2d_keypoins.face_2d_keypoints_dataset": [
             "easycv"
@@ -15780,24 +16161,24 @@
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_colorization.image_colorization_dataset": [
             "numpy",
             "cv2",
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_inpainting.aug": [
-            "albumentations",
-            "imgaug"
+            "imgaug",
+            "albumentations"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_inpainting.image_inpainting_dataset": [
             "numpy",
             "albumentations",
-            "cv2",
             "enum",
+            "glob",
             "os",
-            "glob"
+            "cv2"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_instance_segmentation_coco_dataset": [
             "numpy",
             "pycocotools",
             "os"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_portrait_enhancement.data_utils": [
@@ -15812,33 +16193,33 @@
             "torchvision"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_quality_assmessment_mos.image_quality_assessment_mos_dataset": [],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_semantic_segmentation.segmentation_dataset": [
             "easycv"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.language_guided_video_summarization_dataset": [
-            "numpy",
             "h5py",
-            "os",
+            "numpy",
             "torch",
+            "os",
             "json"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.mgeo_ranking_dataset": [
-            "typing",
-            "json",
             "random",
-            "torch"
+            "typing",
+            "torch",
+            "json"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.movie_scene_segmentation.movie_scene_segmentation_dataset": [
-            "random",
-            "os",
-            "copy",
             "torch",
-            "json",
-            "torchvision"
+            "copy",
+            "os",
+            "torchvision",
+            "random",
+            "json"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.movie_scene_segmentation.sampler": [
             "numpy",
             "random"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.object_detection.detection_dataset": [
             "easycv"
@@ -15846,116 +16227,116 @@
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.augmenter": [
             "imgaug"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.data_loader": [
             "numpy",
             "imgaug",
             "torch",
-            "math",
-            "bisect"
+            "bisect",
+            "math"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.image_dataset": [
+            "logging",
             "numpy",
+            "glob",
+            "torch",
             "functools",
-            "cv2",
             "os",
-            "torch",
-            "math",
-            "glob",
+            "cv2",
             "bisect",
-            "logging"
+            "math"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.measures.iou_evaluator": [
             "numpy",
             "collections",
             "shapely"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.measures.quad_measurer": [
             "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.augment_data": [
             "numpy",
+            "imgaug",
             "math",
-            "cv2",
-            "imgaug"
+            "cv2"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.data_process": [],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.make_border_map": [
             "numpy",
-            "cv2",
             "pyclipper",
-            "shapely"
+            "shapely",
+            "cv2"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.make_icdar_data": [
             "numpy",
             "collections",
             "cv2",
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.make_seg_detection_data": [
             "numpy",
-            "cv2",
             "pyclipper",
-            "shapely"
+            "shapely",
+            "cv2"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.normalize_image": [
             "numpy",
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.random_crop_data": [
             "numpy",
             "cv2"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_recognition_dataset": [
             "numpy",
-            "cv2",
-            "os",
             "torch",
-            "json",
             "PIL",
+            "os",
+            "cv2",
             "lmdb",
+            "json",
             "six"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.reds_image_deblurring_dataset": [
             "numpy",
             "cv2"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.referring_video_object_segmentation.referring_video_object_segmentation_dataset": [
-            "numpy",
             "h5py",
-            "pycocotools",
-            "os",
-            "torch",
-            "json",
+            "numpy",
+            "tqdm",
             "glob",
+            "torch",
+            "torchvision",
             "pandas",
-            "tqdm",
-            "torchvision"
+            "os",
+            "pycocotools",
+            "json"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.referring_video_object_segmentation.transformers": [
             "torchvision",
+            "torch",
             "random",
-            "PIL",
-            "torch"
+            "PIL"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.sidd_image_denoising.data_utils": [
             "cv2",
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.sidd_image_denoising.sidd_image_denoising_dataset": [
             "numpy",
             "cv2"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.sidd_image_denoising.transforms": [
             "random"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.text_ranking_dataset": [
             "typing",
-            "random",
-            "torch"
+            "torch",
+            "random"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.torch_custom_dataset": [
             "typing",
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.veco_dataset": [
             "numpy",
@@ -15969,517 +16350,517 @@
         "modelscope.msdatasets.dataset_cls.custom_datasets.video_frame_interpolation.video_frame_interpolation_dataset": [
             "numpy",
             "cv2",
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.video_stabilization.video_stabilization_dataset": [],
         "modelscope.msdatasets.dataset_cls.custom_datasets.video_summarization_dataset": [
-            "numpy",
             "h5py",
-            "os",
+            "numpy",
             "torch",
+            "os",
             "json"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.video_super_resolution.video_super_resolution_dataset": [
             "numpy",
             "collections",
             "cv2",
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.dataset": [
-            "PIL",
             "copy",
             "os",
+            "PIL",
             "datasets"
         ],
         "modelscope.msdatasets.download.dataset_builder": [
             "typing",
-            "datasets",
-            "os",
             "pyarrow",
-            "pandas"
+            "os",
+            "pandas",
+            "datasets"
         ],
         "modelscope.msdatasets.download.download_config": [
             "typing",
             "datasets"
         ],
         "modelscope.msdatasets.download.download_manager": [
             "datasets"
         ],
         "modelscope.msdatasets.meta.data_meta_config": [],
         "modelscope.msdatasets.meta.data_meta_manager": [
-            "datasets",
             "shutil",
+            "collections",
             "os",
-            "json",
-            "collections"
+            "datasets",
+            "json"
         ],
         "modelscope.msdatasets.ms_dataset": [
             "numpy",
+            "warnings",
             "typing",
-            "datasets",
             "os",
-            "tensorflow",
-            "warnings"
+            "datasets"
         ],
         "modelscope.msdatasets.task_datasets.gopro_image_deblurring_dataset": [],
         "modelscope.msdatasets.task_datasets.reds_image_deblurring_dataset": [],
         "modelscope.msdatasets.task_datasets.sidd_image_denoising": [],
         "modelscope.msdatasets.task_datasets.torch_base_dataset": [],
         "modelscope.msdatasets.task_datasets.video_summarization_dataset": [],
         "modelscope.msdatasets.utils.dataset_utils": [
-            "typing",
             "collections",
+            "typing",
             "os"
         ],
         "modelscope.msdatasets.utils.delete_utils": [],
         "modelscope.msdatasets.utils.oss_utils": [
-            "datasets",
-            "os",
             "multiprocessing",
+            "__future__",
+            "os",
             "oss2",
-            "__future__"
+            "datasets"
         ],
         "modelscope.msdatasets.utils.upload_utils": [
-            "multiprocessing",
+            "tqdm",
             "os",
-            "tqdm"
+            "multiprocessing"
         ],
         "modelscope.pipelines.audio.ans_dfsmn_pipeline": [
             "numpy",
-            "typing",
-            "os",
-            "librosa",
-            "soundfile",
             "sys",
+            "typing",
             "torch",
             "collections",
-            "io"
+            "io",
+            "os",
+            "soundfile",
+            "librosa"
         ],
         "modelscope.pipelines.audio.ans_pipeline": [
             "numpy",
             "typing",
-            "librosa",
-            "soundfile",
             "torch",
-            "io"
+            "io",
+            "soundfile",
+            "librosa"
         ],
         "modelscope.pipelines.audio.asr_inference_pipeline": [
-            "yaml",
             "typing",
-            "json",
-            "os"
+            "os",
+            "yaml",
+            "json"
         ],
         "modelscope.pipelines.audio.asr_wenet_inference_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.audio.inverse_text_processing_pipeline": [
-            "yaml",
             "typing",
             "shutil",
-            "os"
+            "os",
+            "yaml"
         ],
         "modelscope.pipelines.audio.kws_farfield_pipeline": [
             "numpy",
+            "wave",
             "typing",
             "soundfile",
-            "wave",
             "io"
         ],
         "modelscope.pipelines.audio.kws_kwsbp_pipeline": [
             "typing",
-            "json",
-            "os"
+            "os",
+            "json"
         ],
         "modelscope.pipelines.audio.linear_aec_pipeline": [
             "numpy",
-            "typing",
             "scipy",
-            "os",
+            "typing",
             "torch",
             "importlib",
+            "os",
             "yaml"
         ],
         "modelscope.pipelines.audio.lm_infer_pipeline": [
             "typing",
             "os"
         ],
         "modelscope.pipelines.audio.punctuation_processing_pipeline": [
-            "yaml",
             "typing",
             "shutil",
-            "os"
+            "os",
+            "yaml"
         ],
         "modelscope.pipelines.audio.separation_pipeline": [
             "numpy",
             "typing",
-            "soundfile",
             "torch",
-            "io"
+            "io",
+            "soundfile"
         ],
         "modelscope.pipelines.audio.speaker_diarization_pipeline": [
             "numpy",
             "typing",
             "shutil",
+            "yaml",
             "os",
-            "json",
-            "yaml"
+            "json"
         ],
         "modelscope.pipelines.audio.speaker_verification_light_pipeline": [
-            "soundfile",
             "typing",
             "io",
-            "torch"
+            "torch",
+            "soundfile"
         ],
         "modelscope.pipelines.audio.speaker_verification_pipeline": [
-            "yaml",
             "typing",
             "shutil",
-            "os"
+            "os",
+            "yaml"
         ],
         "modelscope.pipelines.audio.text_to_speech_pipeline": [
             "numpy",
             "typing"
         ],
         "modelscope.pipelines.audio.timestamp_pipeline": [
             "typing",
-            "os",
-            "json",
+            "yaml",
             "funasr",
-            "yaml"
+            "os",
+            "json"
         ],
         "modelscope.pipelines.audio.voice_activity_detection_pipeline": [
             "typing",
-            "os",
-            "json",
+            "yaml",
             "funasr",
-            "yaml"
+            "os",
+            "json"
         ],
         "modelscope.pipelines.base": [
             "numpy",
+            "multiprocessing",
+            "packaging",
+            "abc",
             "typing",
+            "torch",
             "functools",
-            "random",
             "os",
-            "multiprocessing",
-            "torch",
-            "abc",
             "threading",
-            "packaging"
+            "random"
         ],
         "modelscope.pipelines.builder": [
             "typing",
             "os"
         ],
         "modelscope.pipelines.cv.action_detection_pipeline": [
             "typing",
-            "math",
-            "os"
+            "os",
+            "math"
         ],
         "modelscope.pipelines.cv.action_recognition_pipeline": [
             "typing",
-            "math",
             "os",
+            "math",
             "torch"
         ],
         "modelscope.pipelines.cv.animal_recognition_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
             "PIL",
+            "os",
+            "cv2",
             "torchvision"
         ],
         "modelscope.pipelines.cv.arc_face_recognition_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
-            "PIL"
+            "PIL",
+            "os",
+            "cv2"
         ],
         "modelscope.pipelines.cv.bad_image_detecting_pipeline": [
             "numpy",
             "typing",
             "torch"
         ],
         "modelscope.pipelines.cv.body_2d_keypoints_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
-            "json",
             "PIL",
-            "torchvision"
+            "os",
+            "cv2",
+            "torchvision",
+            "json"
         ],
         "modelscope.pipelines.cv.body_3d_keypoints_pipeline": [
             "numpy",
             "typing",
+            "torch",
+            "datetime",
+            "tempfile",
             "mpl_toolkits",
-            "cv2",
             "os",
-            "tempfile",
-            "torch",
-            "matplotlib",
-            "datetime"
+            "cv2",
+            "matplotlib"
         ],
         "modelscope.pipelines.cv.card_detection_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.cmdssl_video_embedding_pipeline": [
             "numpy",
             "typing",
-            "decord",
-            "os",
             "torch",
             "PIL",
-            "torchvision"
+            "os",
+            "torchvision",
+            "decord"
         ],
         "modelscope.pipelines.cv.content_check_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
             "PIL",
+            "os",
+            "cv2",
             "torchvision"
         ],
         "modelscope.pipelines.cv.controllable_image_generation_pipeline": [
             "numpy",
+            "glob",
             "typing",
+            "torch",
+            "tempfile",
+            "os",
             "subprocess",
             "cv2",
-            "os",
-            "tempfile",
-            "torch",
-            "math",
-            "glob"
+            "math"
         ],
         "modelscope.pipelines.cv.crowd_counting_pipeline": [
             "numpy",
             "typing",
             "torch",
-            "math",
             "PIL",
-            "torchvision"
+            "torchvision",
+            "math"
         ],
         "modelscope.pipelines.cv.ddcolor_image_colorization_pipeline": [
             "numpy",
             "typing",
-            "cv2",
             "torch",
-            "torchvision"
+            "torchvision",
+            "cv2"
         ],
         "modelscope.pipelines.cv.ddpm_semantic_segmentation_pipeline": [
             "typing",
             "torchvision",
             "torch"
         ],
         "modelscope.pipelines.cv.easycv_pipelines.base": [
             "numpy",
+            "glob",
             "typing",
+            "PIL",
             "os",
-            "easycv",
-            "glob",
-            "PIL"
+            "easycv"
         ],
         "modelscope.pipelines.cv.easycv_pipelines.detection_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.easycv_pipelines.face_2d_keypoints_pipeline": [
             "numpy",
             "typing",
-            "cv2",
             "copy",
+            "cv2",
             "math"
         ],
         "modelscope.pipelines.cv.easycv_pipelines.human_wholebody_keypoint_pipeline": [
             "typing",
             "os"
         ],
         "modelscope.pipelines.cv.easycv_pipelines.segmentation_pipeline": [
             "numpy",
             "typing"
         ],
         "modelscope.pipelines.cv.face_attribute_recognition_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
-            "PIL"
+            "PIL",
+            "os",
+            "cv2"
         ],
         "modelscope.pipelines.cv.face_detection_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
-            "PIL"
+            "PIL",
+            "os",
+            "cv2"
         ],
         "modelscope.pipelines.cv.face_emotion_pipeline": [
             "numpy",
             "typing"
         ],
         "modelscope.pipelines.cv.face_human_hand_detection_pipeline": [
             "numpy",
             "typing"
         ],
         "modelscope.pipelines.cv.face_image_generation_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
-            "PIL"
+            "PIL",
+            "os",
+            "cv2"
         ],
         "modelscope.pipelines.cv.face_liveness_ir_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
-            "torch",
             "PIL",
+            "torch",
+            "os",
+            "cv2",
             "onnxruntime"
         ],
         "modelscope.pipelines.cv.face_liveness_xc_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
-            "torch",
             "PIL",
+            "torch",
+            "os",
+            "cv2",
             "onnxruntime"
         ],
         "modelscope.pipelines.cv.face_processing_base_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
-            "PIL"
+            "PIL",
+            "os",
+            "cv2"
         ],
         "modelscope.pipelines.cv.face_quality_assessment_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
-            "torch",
             "PIL",
+            "torch",
+            "os",
+            "cv2",
             "onnxruntime"
         ],
         "modelscope.pipelines.cv.face_recognition_onnx_fm_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
-            "torch",
             "PIL",
+            "torch",
+            "os",
+            "cv2",
             "onnxruntime"
         ],
         "modelscope.pipelines.cv.face_recognition_onnx_ir_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
-            "torch",
             "PIL",
+            "torch",
+            "os",
+            "cv2",
             "onnxruntime"
         ],
         "modelscope.pipelines.cv.face_recognition_ood_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
-            "PIL"
+            "PIL",
+            "os",
+            "cv2"
         ],
         "modelscope.pipelines.cv.face_recognition_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
-            "PIL"
+            "PIL",
+            "os",
+            "cv2"
         ],
         "modelscope.pipelines.cv.face_reconstruction_pipeline": [
             "numpy",
-            "typing",
             "scipy",
+            "typing",
             "shutil",
-            "cv2",
-            "os",
-            "torch",
+            "face_alignment",
             "PIL",
-            "tensorflow",
-            "face_alignment"
+            "torch",
+            "io",
+            "os",
+            "cv2",
+            "tensorflow"
         ],
         "modelscope.pipelines.cv.facial_expression_recognition_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
-            "PIL"
+            "PIL",
+            "os",
+            "cv2"
         ],
         "modelscope.pipelines.cv.facial_landmark_confidence_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
-            "PIL"
+            "PIL",
+            "os",
+            "cv2"
         ],
         "modelscope.pipelines.cv.general_recognition_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
             "PIL",
+            "os",
+            "cv2",
             "torchvision"
         ],
         "modelscope.pipelines.cv.hand_2d_keypoints_pipeline": [
             "os"
         ],
         "modelscope.pipelines.cv.hand_static_pipeline": [
             "numpy",
             "typing"
         ],
         "modelscope.pipelines.cv.hicossl_video_embedding_pipeline": [
             "typing",
-            "math",
             "os",
+            "math",
             "torch"
         ],
         "modelscope.pipelines.cv.human_reconstruction_pipeline": [
             "numpy",
+            "trimesh",
             "typing",
             "shutil",
-            "trimesh",
-            "os",
-            "torch"
+            "torch",
+            "os"
         ],
         "modelscope.pipelines.cv.image_body_reshaping_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.image_bts_depth_estimation_pipeline": [
             "numpy",
-            "typing",
             "albumentations",
-            "cv2",
-            "torch"
+            "typing",
+            "torch",
+            "cv2"
         ],
         "modelscope.pipelines.cv.image_cartoon_pipeline": [
             "numpy",
             "typing",
-            "cv2",
             "os",
+            "cv2",
             "tensorflow"
         ],
         "modelscope.pipelines.cv.image_classification_pipeline": [
             "numpy",
             "typing",
             "torch"
         ],
@@ -16487,18 +16868,18 @@
             "typing",
             "torchvision",
             "torch"
         ],
         "modelscope.pipelines.cv.image_colorization_pipeline": [
             "numpy",
             "typing",
-            "cv2",
             "torch",
             "PIL",
-            "torchvision"
+            "torchvision",
+            "cv2"
         ],
         "modelscope.pipelines.cv.image_debanding_pipeline": [
             "typing",
             "torchvision",
             "torch"
         ],
         "modelscope.pipelines.cv.image_deblur_pipeline": [
@@ -16516,267 +16897,267 @@
             "typing",
             "torchvision",
             "torch"
         ],
         "modelscope.pipelines.cv.image_depth_estimation_pipeline": [
             "numpy",
             "typing",
-            "cv2",
             "torch",
-            "PIL"
+            "PIL",
+            "cv2"
         ],
         "modelscope.pipelines.cv.image_detection_pipeline": [
             "numpy",
             "typing"
         ],
         "modelscope.pipelines.cv.image_driving_perception_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os"
+            "os",
+            "cv2"
         ],
         "modelscope.pipelines.cv.image_face_fusion_pipeline": [
             "numpy",
             "typing"
         ],
         "modelscope.pipelines.cv.image_human_parsing_pipeline": [
             "numpy",
             "typing",
             "torchvision",
             "torch"
         ],
         "modelscope.pipelines.cv.image_inpainting_pipeline": [
             "numpy",
             "typing",
-            "cv2",
             "torch",
-            "PIL"
+            "PIL",
+            "cv2"
         ],
         "modelscope.pipelines.cv.image_inpainting_sdv2_pipeline": [
             "numpy",
-            "typing",
             "diffusers",
-            "cv2",
+            "sys",
+            "typing",
+            "torch",
             "tempfile",
             "os",
-            "torch",
-            "sys",
+            "cv2",
             "math"
         ],
         "modelscope.pipelines.cv.image_instance_segmentation_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
-            "PIL"
+            "PIL",
+            "os",
+            "cv2"
         ],
         "modelscope.pipelines.cv.image_matching_pipeline": [
             "numpy",
             "typing",
-            "cv2",
             "torch",
-            "PIL"
+            "PIL",
+            "cv2"
         ],
         "modelscope.pipelines.cv.image_matting_pipeline": [
             "numpy",
             "typing",
-            "cv2",
             "os",
+            "cv2",
             "tensorflow"
         ],
         "modelscope.pipelines.cv.image_mvs_depth_estimation_pipeline": [
             "typing",
             "shutil",
             "os",
             "tempfile"
         ],
         "modelscope.pipelines.cv.image_open_vocabulary_detection_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
-            "PIL"
+            "PIL",
+            "os",
+            "cv2"
         ],
         "modelscope.pipelines.cv.image_paintbyexample_pipeline": [
             "numpy",
             "typing",
-            "cv2",
             "torch",
             "PIL",
-            "einops",
-            "torchvision"
+            "torchvision",
+            "cv2",
+            "einops"
         ],
         "modelscope.pipelines.cv.image_panoptic_segmentation_pipeline": [
             "numpy",
             "typing",
-            "cv2",
             "torch",
-            "PIL"
+            "PIL",
+            "cv2"
         ],
         "modelscope.pipelines.cv.image_portrait_enhancement_pipeline": [
             "numpy",
-            "typing",
             "scipy",
-            "cv2",
+            "typing",
             "torch",
-            "math",
-            "PIL"
+            "PIL",
+            "cv2",
+            "math"
         ],
         "modelscope.pipelines.cv.image_quality_assessment_degradation_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "tempfile",
             "torch",
-            "math",
-            "torchvision"
+            "tempfile",
+            "torchvision",
+            "cv2",
+            "math"
         ],
         "modelscope.pipelines.cv.image_quality_assessment_man_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "tempfile",
             "torch",
-            "math",
-            "torchvision"
+            "tempfile",
+            "torchvision",
+            "cv2",
+            "math"
         ],
         "modelscope.pipelines.cv.image_quality_assessment_mos_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "tempfile",
             "torch",
-            "math",
-            "torchvision"
+            "tempfile",
+            "torchvision",
+            "cv2",
+            "math"
         ],
         "modelscope.pipelines.cv.image_reid_person_pipeline": [
             "typing",
-            "os",
             "torch",
-            "math",
             "PIL",
-            "torchvision"
+            "os",
+            "torchvision",
+            "math"
         ],
         "modelscope.pipelines.cv.image_restoration_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.image_salient_detection_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.image_semantic_segmentation_pipeline": [
             "numpy",
             "typing",
-            "cv2",
             "torch",
-            "PIL"
+            "PIL",
+            "cv2"
         ],
         "modelscope.pipelines.cv.image_skychange_pipeline": [
             "numpy",
-            "typing",
             "time",
-            "cv2",
+            "pdb",
+            "typing",
             "PIL",
-            "pdb"
+            "cv2"
         ],
         "modelscope.pipelines.cv.image_structured_model_probing_pipeline": [
             "numpy",
-            "typing",
             "mmcv",
-            "os",
+            "typing",
             "torch",
-            "math",
-            "torchvision"
+            "os",
+            "torchvision",
+            "math"
         ],
         "modelscope.pipelines.cv.image_style_transfer_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os"
+            "os",
+            "cv2"
         ],
         "modelscope.pipelines.cv.image_super_resolution_pipeline": [
             "numpy",
             "typing",
-            "cv2",
             "torch",
-            "PIL"
+            "PIL",
+            "cv2"
         ],
         "modelscope.pipelines.cv.image_to_image_generate_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
             "PIL",
+            "os",
+            "cv2",
             "torchvision"
         ],
         "modelscope.pipelines.cv.image_to_image_translation_pipeline": [
             "numpy",
+            "sys",
             "typing",
-            "cv2",
-            "os",
             "torch",
-            "sys",
             "PIL",
-            "torchvision",
-            "io"
+            "io",
+            "os",
+            "cv2",
+            "torchvision"
         ],
         "modelscope.pipelines.cv.indoor_layout_estimation_pipeline": [
             "numpy",
             "typing",
             "cv2"
         ],
         "modelscope.pipelines.cv.language_guided_video_summarization_pipeline": [
             "numpy",
             "typing",
-            "clip",
             "shutil",
-            "random",
-            "cv2",
+            "torch",
+            "PIL",
             "tempfile",
             "os",
-            "torch",
-            "PIL"
+            "cv2",
+            "random",
+            "clip"
         ],
         "modelscope.pipelines.cv.license_plate_detection_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
+            "PIL",
             "torch",
-            "math",
-            "PIL"
+            "os",
+            "cv2",
+            "math"
         ],
         "modelscope.pipelines.cv.lineless_table_recognition_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
+            "PIL",
             "torch",
-            "math",
-            "PIL"
+            "os",
+            "cv2",
+            "math"
         ],
         "modelscope.pipelines.cv.live_category_pipeline": [
             "numpy",
             "typing",
-            "decord",
-            "os",
             "torch",
             "PIL",
-            "torchvision"
+            "os",
+            "torchvision",
+            "decord"
         ],
         "modelscope.pipelines.cv.mask_face_recognition_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
             "PIL",
-            "collections"
+            "collections",
+            "os",
+            "cv2"
         ],
         "modelscope.pipelines.cv.maskdino_instance_segmentation_pipeline": [
             "typing",
             "torchvision",
             "torch"
         ],
         "modelscope.pipelines.cv.mobile_image_super_resolution_pipeline": [
@@ -16790,17 +17171,17 @@
             "numpy",
             "typing",
             "os"
         ],
         "modelscope.pipelines.cv.motion_generation_pipeline": [
             "numpy",
             "typing",
-            "os",
+            "torch",
             "tempfile",
-            "torch"
+            "os"
         ],
         "modelscope.pipelines.cv.movie_scene_segmentation_pipeline": [
             "typing",
             "torch"
         ],
         "modelscope.pipelines.cv.mtcnn_face_detection_pipeline": [
             "typing",
@@ -16809,458 +17190,476 @@
         ],
         "modelscope.pipelines.cv.nerf_recon_acc_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.object_detection_3d_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
-            "tempfile",
             "torch",
-            "PIL"
+            "PIL",
+            "tempfile",
+            "os",
+            "cv2"
         ],
         "modelscope.pipelines.cv.ocr_detection_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
-            "math",
+            "os",
+            "cv2",
+            "tf_slim",
             "tensorflow",
-            "tf_slim"
+            "math"
         ],
         "modelscope.pipelines.cv.ocr_recognition_pipeline": [],
         "modelscope.pipelines.cv.ocr_utils.model_convnext_transformer": [
             "torch"
         ],
         "modelscope.pipelines.cv.ocr_utils.model_dla34": [
             "numpy",
-            "math",
             "os",
+            "math",
             "torch"
         ],
         "modelscope.pipelines.cv.ocr_utils.model_resnet18_half": [
             "os",
             "torch"
         ],
         "modelscope.pipelines.cv.ocr_utils.model_resnet_mutex_v4_linewithchar": [
-            "tf_slim",
-            "tensorflow"
+            "tensorflow",
+            "tf_slim"
         ],
         "modelscope.pipelines.cv.ocr_utils.model_vlpt": [
             "sys",
-            "math",
             "os",
+            "math",
             "torch"
         ],
         "modelscope.pipelines.cv.ocr_utils.ocr_modules.convnext": [
             "torch"
         ],
         "modelscope.pipelines.cv.ocr_utils.ocr_modules.timm_tinyc": [
-            "math",
-            "functools",
-            "itertools",
-            "collections",
             "logging",
+            "collections",
+            "math",
             "copy",
-            "torch"
+            "itertools",
+            "torch",
+            "functools"
         ],
         "modelscope.pipelines.cv.ocr_utils.ocr_modules.vitstr": [
-            "functools",
-            "copy",
+            "logging",
             "torch",
             "__future__",
-            "logging"
+            "functools",
+            "copy"
         ],
         "modelscope.pipelines.cv.ocr_utils.ops": [
             "numpy",
+            "sys",
             "shutil",
-            "cv2",
+            "absl",
             "os",
-            "sys",
-            "math",
-            "tensorflow",
             "uuid",
-            "absl"
+            "cv2",
+            "tensorflow",
+            "math"
         ],
         "modelscope.pipelines.cv.ocr_utils.resnet18_v1": [
-            "tf_slim",
-            "tensorflow"
+            "tensorflow",
+            "tf_slim"
         ],
         "modelscope.pipelines.cv.ocr_utils.resnet_utils": [
-            "tf_slim",
             "collections",
-            "tensorflow"
+            "tensorflow",
+            "tf_slim"
         ],
         "modelscope.pipelines.cv.ocr_utils.table_process": [
             "numpy",
-            "math",
-            "random",
-            "cv2",
             "copy",
+            "cv2",
+            "random",
+            "math",
             "torch"
         ],
         "modelscope.pipelines.cv.ocr_utils.utils": [
             "numpy",
-            "cv2",
             "pyclipper",
-            "shapely"
+            "shapely",
+            "cv2"
         ],
         "modelscope.pipelines.cv.panorama_depth_estimation_pipeline": [
             "numpy",
             "typing",
-            "cv2",
             "torch",
-            "PIL"
+            "PIL",
+            "cv2"
+        ],
+        "modelscope.pipelines.cv.pedestrian_attribute_recognition_pipeline": [
+            "numpy",
+            "typing",
+            "torch",
+            "PIL",
+            "os",
+            "cv2",
+            "torchvision",
+            "json"
         ],
         "modelscope.pipelines.cv.pointcloud_sceneflow_estimation_pipeline": [
             "numpy",
             "typing",
             "plyfile",
             "torch"
         ],
         "modelscope.pipelines.cv.product_retrieval_embedding_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
             "PIL",
+            "os",
+            "cv2",
             "torchvision"
         ],
         "modelscope.pipelines.cv.product_segmentation_pipeline": [
             "numpy",
             "typing"
         ],
         "modelscope.pipelines.cv.realtime_video_object_detection_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
-            "json",
             "PIL",
-            "torchvision"
+            "os",
+            "cv2",
+            "torchvision",
+            "json"
         ],
         "modelscope.pipelines.cv.referring_video_object_segmentation_pipeline": [
             "numpy",
             "typing",
-            "moviepy",
-            "tempfile",
             "torch",
-            "einops",
             "PIL",
+            "tempfile",
+            "torchvision",
+            "moviepy",
             "tqdm",
-            "torchvision"
+            "einops"
         ],
         "modelscope.pipelines.cv.retina_face_detection_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
-            "PIL"
+            "PIL",
+            "os",
+            "cv2"
         ],
         "modelscope.pipelines.cv.shop_segmentation_pipleline": [
             "typing"
         ],
         "modelscope.pipelines.cv.skin_retouching_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
-            "torch",
             "PIL",
-            "tensorflow",
-            "torchvision"
+            "torch",
+            "os",
+            "cv2",
+            "torchvision",
+            "tensorflow"
         ],
         "modelscope.pipelines.cv.table_recognition_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
+            "PIL",
             "torch",
-            "math",
-            "PIL"
+            "os",
+            "cv2",
+            "math"
         ],
         "modelscope.pipelines.cv.tbs_detection_pipeline": [
             "numpy",
             "typing",
             "colorsys",
-            "cv2",
-            "os",
             "torch",
-            "PIL"
+            "PIL",
+            "os",
+            "cv2"
         ],
         "modelscope.pipelines.cv.tbs_detection_utils.utils": [
             "numpy",
             "colorsys",
-            "os",
             "torch",
+            "__future__",
             "PIL",
+            "os",
             "pandas",
-            "__future__",
             "torchvision",
             "matplotlib"
         ],
         "modelscope.pipelines.cv.text_driven_segmentation_pipleline": [
             "typing"
         ],
         "modelscope.pipelines.cv.tinynas_classification_pipeline": [
             "typing",
-            "os",
             "torch",
-            "math",
-            "torchvision"
+            "os",
+            "torchvision",
+            "math"
         ],
         "modelscope.pipelines.cv.tinynas_detection_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.ulfd_face_detection_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
-            "PIL"
+            "PIL",
+            "os",
+            "cv2"
         ],
         "modelscope.pipelines.cv.video_category_pipeline": [
             "numpy",
             "typing",
-            "decord",
-            "os",
             "torch",
-            "json",
             "PIL",
-            "torchvision"
+            "os",
+            "torchvision",
+            "json",
+            "decord"
         ],
         "modelscope.pipelines.cv.video_colorization_pipeline": [
             "numpy",
             "typing",
-            "subprocess",
-            "cv2",
+            "PIL",
+            "torch",
             "tempfile",
             "os",
-            "torch",
-            "PIL",
+            "cv2",
+            "subprocess",
             "torchvision"
         ],
         "modelscope.pipelines.cv.video_deinterlace_pipeline": [
             "numpy",
             "typing",
-            "subprocess",
-            "cv2",
+            "torch",
             "tempfile",
             "os",
-            "torch",
-            "math",
-            "torchvision"
+            "cv2",
+            "subprocess",
+            "torchvision",
+            "math"
         ],
         "modelscope.pipelines.cv.video_depth_estimation_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.video_frame_interpolation_pipeline": [
             "numpy",
+            "glob",
             "typing",
+            "torch",
+            "tempfile",
+            "os",
             "subprocess",
             "cv2",
-            "os",
-            "tempfile",
-            "torch",
-            "math",
-            "glob",
-            "torchvision"
+            "torchvision",
+            "math"
         ],
         "modelscope.pipelines.cv.video_human_matting_pipeline": [
             "numpy",
             "typing",
-            "moviepy",
-            "cv2",
+            "torch",
             "os",
-            "torch"
+            "cv2",
+            "moviepy"
         ],
         "modelscope.pipelines.cv.video_inpainting_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.video_instance_segmentation_pipeline": [
             "numpy",
-            "typing",
             "mmcv",
-            "cv2",
-            "os",
+            "typing",
             "torch",
+            "os",
+            "cv2",
             "tqdm"
         ],
         "modelscope.pipelines.cv.video_multi_object_tracking_pipeline": [
             "typing",
             "os",
             "torch"
         ],
         "modelscope.pipelines.cv.video_object_segmentation_pipeline": [
             "numpy",
             "typing",
-            "os",
             "torch",
             "PIL",
+            "os",
             "torchvision"
         ],
         "modelscope.pipelines.cv.video_panoptic_segmentation_pipeline": [
             "numpy",
-            "typing",
             "mmcv",
-            "cv2",
-            "os",
+            "typing",
             "torch",
+            "os",
+            "cv2",
             "tqdm"
         ],
         "modelscope.pipelines.cv.video_single_object_tracking_pipeline": [
             "typing",
-            "cv2",
-            "os"
+            "os",
+            "cv2"
         ],
         "modelscope.pipelines.cv.video_stabilization_pipeline": [
             "numpy",
+            "glob",
             "typing",
+            "torch",
+            "tempfile",
+            "os",
             "subprocess",
             "cv2",
-            "os",
-            "tempfile",
-            "torch",
-            "math",
-            "glob"
+            "math"
         ],
         "modelscope.pipelines.cv.video_summarization_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
+            "os",
+            "cv2",
             "tqdm"
         ],
         "modelscope.pipelines.cv.video_super_resolution_pipeline": [
             "numpy",
             "typing",
-            "subprocess",
-            "cv2",
+            "torch",
             "tempfile",
             "os",
-            "torch",
-            "math",
-            "torchvision"
+            "cv2",
+            "subprocess",
+            "torchvision",
+            "math"
         ],
         "modelscope.pipelines.cv.vidt_pipeline": [
             "typing",
             "torchvision",
             "torch"
         ],
         "modelscope.pipelines.cv.virtual_try_on_pipeline": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
-            "PIL"
+            "PIL",
+            "os",
+            "cv2"
         ],
         "modelscope.pipelines.cv.vision_efficient_tuning_pipeline": [
             "numpy",
             "typing",
             "torchvision",
             "torch"
         ],
         "modelscope.pipelines.cv.vision_middleware_pipeline": [
             "numpy",
-            "typing",
             "mmcv",
-            "os",
+            "typing",
             "torch",
-            "math",
-            "torchvision"
+            "os",
+            "torchvision",
+            "math"
         ],
         "modelscope.pipelines.cv.vop_retrieval_pipeline": [
             "numpy",
             "typing",
-            "random",
-            "os",
             "torch",
+            "collections",
+            "os",
             "gzip",
-            "math",
+            "random",
             "pickle",
             "tqdm",
-            "collections"
+            "math"
         ],
         "modelscope.pipelines.cv.vop_retrieval_se_pipeline": [
             "numpy",
             "typing",
-            "os",
             "torch",
+            "os",
             "gzip"
         ],
         "modelscope.pipelines.multi_modal.asr_pipeline": [
             "typing",
             "torch"
         ],
         "modelscope.pipelines.multi_modal.diffusers_wrapped.diffusers_pipeline": [
             "typing",
             "os"
         ],
         "modelscope.pipelines.multi_modal.diffusers_wrapped.stable_diffusion.chinese_stable_diffusion_pipeline": [
             "numpy",
-            "typing",
-            "transformers",
             "diffusers",
-            "cv2",
+            "typing",
             "torch",
-            "PIL"
+            "PIL",
+            "cv2",
+            "transformers"
         ],
         "modelscope.pipelines.multi_modal.diffusers_wrapped.stable_diffusion.stable_diffusion_pipeline": [
             "numpy",
-            "typing",
             "diffusers",
-            "cv2",
+            "typing",
             "torch",
-            "PIL"
+            "PIL",
+            "cv2"
         ],
         "modelscope.pipelines.multi_modal.disco_guided_diffusion_pipeline.disco_guided_diffusion": [
             "numpy",
-            "clip",
             "gc",
-            "cv2",
-            "os",
             "torch",
+            "PIL",
             "importlib",
+            "os",
+            "cv2",
+            "torchvision",
+            "clip",
             "math",
-            "json",
-            "PIL",
-            "torchvision"
+            "json"
         ],
         "modelscope.pipelines.multi_modal.disco_guided_diffusion_pipeline.utils": [
             "numpy",
-            "math",
             "warnings",
             "fractions",
+            "math",
             "torch"
         ],
         "modelscope.pipelines.multi_modal.document_vl_embedding_pipeline": [
             "typing",
             "torch"
         ],
+        "modelscope.pipelines.multi_modal.efficient_diffusion_tuning_pipeline": [
+            "numpy",
+            "typing",
+            "torch",
+            "PIL",
+            "torchvision",
+            "cv2"
+        ],
         "modelscope.pipelines.multi_modal.generative_multi_modal_embedding_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.multi_modal.gridvlp_pipeline": [
             "numpy",
-            "typing",
-            "transformers",
-            "traceback",
             "time",
-            "os",
+            "typing",
             "torch",
+            "PIL",
+            "os",
+            "traceback",
             "json",
-            "PIL"
+            "transformers"
         ],
         "modelscope.pipelines.multi_modal.image_captioning_pipeline": [
             "typing",
             "torch"
         ],
         "modelscope.pipelines.multi_modal.image_text_retrieval_pipeline": [
             "typing",
@@ -17277,16 +17676,16 @@
         "modelscope.pipelines.multi_modal.ocr_recognition_pipeline": [
             "typing",
             "torch"
         ],
         "modelscope.pipelines.multi_modal.soonet_video_temporal_grounding_pipeline": [
             "numpy",
             "typing",
-            "os",
             "torch",
+            "os",
             "torchvision"
         ],
         "modelscope.pipelines.multi_modal.sudoku_pipeline": [
             "typing",
             "torch"
         ],
         "modelscope.pipelines.multi_modal.team_multi_modal_similarity_pipeline": [
@@ -17298,17 +17697,17 @@
         ],
         "modelscope.pipelines.multi_modal.text_to_image_synthesis_pipeline": [
             "typing",
             "torch"
         ],
         "modelscope.pipelines.multi_modal.text_to_video_synthesis_pipeline": [
             "typing",
-            "cv2",
-            "tempfile",
             "torch",
+            "tempfile",
+            "cv2",
             "einops"
         ],
         "modelscope.pipelines.multi_modal.video_captioning_pipeline": [
             "typing",
             "torch"
         ],
         "modelscope.pipelines.multi_modal.video_multi_modal_embedding_pipeline": [
@@ -17329,20 +17728,26 @@
         "modelscope.pipelines.multi_modal.visual_question_answering_pipeline": [
             "typing",
             "torch"
         ],
         "modelscope.pipelines.nlp.automatic_post_editing_pipeline": [
             "numpy",
             "typing",
-            "sacremoses",
+            "jieba",
+            "sentencepiece",
             "os",
-            "html",
             "tensorflow",
-            "sentencepiece",
-            "jieba"
+            "sacremoses",
+            "html"
+        ],
+        "modelscope.pipelines.nlp.canmt_translation_pipeline": [
+            "typing",
+            "os",
+            "sacremoses",
+            "torch"
         ],
         "modelscope.pipelines.nlp.codegeex_code_generation_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.nlp.codegeex_code_translation_pipeline": [
             "typing"
         ],
@@ -17373,90 +17778,93 @@
             "torch"
         ],
         "modelscope.pipelines.nlp.document_grounded_dialog_generate_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.nlp.document_grounded_dialog_rerank_pipeline": [
             "numpy",
-            "typing",
-            "transformers",
-            "random",
             "time",
-            "os",
-            "torch",
-            "sys",
-            "pprint",
             "ujson",
+            "sys",
+            "typing",
             "re",
-            "collections"
+            "torch",
+            "collections",
+            "os",
+            "random",
+            "pprint",
+            "transformers"
         ],
         "modelscope.pipelines.nlp.document_grounded_dialog_retrieval_pipeline": [
             "numpy",
             "typing",
             "os",
-            "json",
-            "faiss"
+            "faiss",
+            "json"
         ],
         "modelscope.pipelines.nlp.document_segmentation_pipeline": [
             "numpy",
             "typing",
-            "transformers",
-            "datasets",
+            "re",
             "torch",
-            "re"
+            "datasets",
+            "transformers"
         ],
         "modelscope.pipelines.nlp.extractive_summarization_pipeline": [
             "numpy",
             "typing",
-            "datasets",
+            "re",
             "torch",
-            "re"
+            "datasets"
         ],
         "modelscope.pipelines.nlp.faq_question_answering_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.nlp.fasttext_text_classification_pipeline": [
             "numpy",
             "typing",
+            "sentencepiece",
             "fasttext",
-            "os",
-            "sentencepiece"
+            "os"
         ],
         "modelscope.pipelines.nlp.feature_extraction_pipeline": [
             "typing",
             "os",
             "torch"
         ],
         "modelscope.pipelines.nlp.fid_dialogue_pipeline": [
-            "re",
             "typing",
+            "re",
             "torch"
         ],
         "modelscope.pipelines.nlp.fill_mask_pipeline": [
             "numpy",
             "typing"
         ],
+        "modelscope.pipelines.nlp.glm130b_text_generation_pipeline": [
+            "typing"
+        ],
         "modelscope.pipelines.nlp.information_extraction_pipeline": [
             "typing",
             "torch"
         ],
         "modelscope.pipelines.nlp.interactive_translation_pipeline": [
             "numpy",
             "typing",
-            "sacremoses",
             "os",
-            "tensorflow",
             "subword_nmt",
+            "tensorflow",
+            "sacremoses",
             "jieba"
         ],
         "modelscope.pipelines.nlp.language_identification_pipline": [
             "numpy",
             "typing",
-            "os",
             "re",
+            "os",
             "tensorflow"
         ],
         "modelscope.pipelines.nlp.mglm_text_summarization_pipeline": [
             "typing",
             "os"
         ],
         "modelscope.pipelines.nlp.named_entity_recognition_pipeline": [
@@ -17464,36 +17872,36 @@
         ],
         "modelscope.pipelines.nlp.sentence_embedding_pipeline": [
             "numpy",
             "typing",
             "torch"
         ],
         "modelscope.pipelines.nlp.siamese_uie_pipeline": [
-            "typing",
-            "scipy",
+            "logging",
             "time",
-            "os",
-            "pathlib",
+            "scipy",
+            "typing",
             "torch",
+            "os",
             "copy",
-            "json",
-            "math",
+            "pathlib",
             "tqdm",
-            "logging"
+            "math",
+            "json"
         ],
         "modelscope.pipelines.nlp.summarization_pipeline": [
             "typing",
             "torch"
         ],
         "modelscope.pipelines.nlp.table_question_answering_pipeline": [
             "typing",
-            "transformers",
-            "os",
             "torch",
-            "json"
+            "os",
+            "json",
+            "transformers"
         ],
         "modelscope.pipelines.nlp.text_classification_pipeline": [
             "numpy",
             "typing",
             "torch"
         ],
         "modelscope.pipelines.nlp.text_error_correction_pipeline": [
@@ -17512,34 +17920,34 @@
         "modelscope.pipelines.nlp.token_classification_pipeline": [
             "numpy",
             "typing",
             "torch"
         ],
         "modelscope.pipelines.nlp.translation_evaluation_pipeline": [
             "numpy",
-            "typing",
-            "os",
             "enum",
-            "torch"
+            "typing",
+            "torch",
+            "os"
         ],
         "modelscope.pipelines.nlp.translation_pipeline": [
             "numpy",
             "typing",
-            "sacremoses",
             "os",
-            "tensorflow",
             "subword_nmt",
+            "tensorflow",
+            "sacremoses",
             "jieba"
         ],
         "modelscope.pipelines.nlp.translation_quality_estimation_pipeline": [
             "typing",
-            "transformers",
-            "os",
             "torch",
-            "io"
+            "io",
+            "os",
+            "transformers"
         ],
         "modelscope.pipelines.nlp.user_satisfaction_estimation_pipeline": [
             "numpy",
             "typing",
             "torch"
         ],
         "modelscope.pipelines.nlp.word_alignment_pipeline": [
@@ -17547,238 +17955,246 @@
             "typing"
         ],
         "modelscope.pipelines.nlp.word_segmentation_pipeline": [
             "typing",
             "torch"
         ],
         "modelscope.pipelines.nlp.zero_shot_classification_pipeline": [
-            "scipy",
             "typing",
-            "torch"
+            "torch",
+            "scipy"
         ],
         "modelscope.pipelines.science.protein_structure_pipeline": [
             "numpy",
-            "typing",
             "unicore",
             "time",
-            "os",
+            "typing",
             "torch",
+            "os",
             "json"
         ],
         "modelscope.pipelines.util": [
             "typing",
             "os"
         ],
         "modelscope.preprocessors.asr": [
             "typing",
             "os"
         ],
         "modelscope.preprocessors.audio": [
             "numpy",
-            "typing",
             "scipy",
-            "os",
+            "typing",
             "torch",
-            "io"
+            "io",
+            "os"
         ],
         "modelscope.preprocessors.base": [
             "abc",
             "typing",
             "os"
         ],
         "modelscope.preprocessors.builder": [],
         "modelscope.preprocessors.common": [
             "numpy",
-            "typing",
             "time",
+            "typing",
             "torch",
             "collections"
         ],
         "modelscope.preprocessors.cv.action_detection_mapper": [
             "numpy",
-            "decord",
+            "copy",
+            "detectron2",
             "scipy",
             "random",
-            "detectron2",
-            "copy",
+            "decord",
             "torch"
         ],
         "modelscope.preprocessors.cv.bad_image_detecting_preprocessor": [
             "numpy",
             "typing",
             "torch",
-            "math",
             "PIL",
-            "torchvision"
+            "torchvision",
+            "math"
         ],
         "modelscope.preprocessors.cv.controllable_image_generation": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
-            "math",
             "PIL",
-            "torchvision"
+            "os",
+            "cv2",
+            "torchvision",
+            "math"
         ],
         "modelscope.preprocessors.cv.cv2_transforms": [
             "numpy",
-            "math",
             "collections",
+            "cv2",
             "numbers",
             "random",
-            "cv2",
+            "math",
             "torch"
         ],
         "modelscope.preprocessors.cv.image_classification_preprocessor": [
             "numpy",
             "typing",
-            "cv2",
-            "os",
             "torch",
             "PIL",
+            "os",
+            "cv2",
             "torchvision"
         ],
         "modelscope.preprocessors.cv.image_quality_assessment_man": [
             "numpy",
             "typing",
             "torch",
-            "math",
             "PIL",
-            "torchvision"
+            "torchvision",
+            "math"
         ],
         "modelscope.preprocessors.cv.image_quality_assessment_mos": [
             "numpy",
             "typing",
+            "torchvision",
             "cv2",
-            "math",
-            "torchvision"
+            "math"
         ],
         "modelscope.preprocessors.cv.image_restoration_preprocessor": [
             "numpy",
             "typing",
             "torch",
-            "math",
             "PIL",
-            "torchvision"
+            "torchvision",
+            "math"
         ],
         "modelscope.preprocessors.cv.mmcls_preprocessor": [
             "numpy",
             "typing",
             "os"
         ],
         "modelscope.preprocessors.cv.timer": [
             "time"
         ],
         "modelscope.preprocessors.cv.util": [
+            "collections",
             "sys",
             "shutil",
-            "collections",
             "os"
         ],
         "modelscope.preprocessors.cv.video_stabilization": [
             "numpy",
             "cv2",
             "torch"
         ],
         "modelscope.preprocessors.cv.video_super_resolution": [
             "collections",
-            "cv2",
-            "os"
+            "os",
+            "cv2"
         ],
         "modelscope.preprocessors.image": [
             "numpy",
             "typing",
-            "cv2",
             "PIL",
-            "io"
+            "io",
+            "cv2"
         ],
         "modelscope.preprocessors.kws": [
-            "yaml",
             "typing",
-            "os"
+            "os",
+            "yaml"
         ],
         "modelscope.preprocessors.movie_scene_segmentation.transforms": [
             "numpy",
+            "numbers",
             "typing",
-            "random",
-            "os",
             "torch",
-            "numbers",
             "PIL",
-            "torchvision"
+            "os",
+            "torchvision",
+            "random"
         ],
         "modelscope.preprocessors.multi_modal": [
             "numpy",
             "typing",
-            "decord",
-            "os",
             "torch",
-            "json",
             "PIL",
-            "timm",
+            "io",
+            "os",
             "torchvision",
-            "io"
+            "json",
+            "timm",
+            "decord"
         ],
         "modelscope.preprocessors.nlp.bert_seq_cls_tokenizer": [
             "typing",
             "transformers"
         ],
+        "modelscope.preprocessors.nlp.canmt_translation": [
+            "typing",
+            "torch",
+            "os",
+            "subword_nmt",
+            "sacremoses",
+            "jieba"
+        ],
         "modelscope.preprocessors.nlp.dialog_classification_use_preprocessor": [
             "typing",
-            "transformers",
-            "torch"
+            "torch",
+            "transformers"
         ],
         "modelscope.preprocessors.nlp.document_grounded_dialog_generate_preprocessor": [
             "typing",
-            "transformers",
             "os",
-            "torch"
+            "torch",
+            "transformers"
         ],
         "modelscope.preprocessors.nlp.document_grounded_dialog_rerank_preprocessor": [
             "typing",
-            "transformers",
-            "os",
+            "torch",
             "copy",
-            "torch"
+            "os",
+            "transformers"
         ],
         "modelscope.preprocessors.nlp.document_grounded_dialog_retrieval_preprocessor": [
             "typing",
-            "transformers",
             "os",
-            "torch"
+            "torch",
+            "transformers"
         ],
         "modelscope.preprocessors.nlp.document_segmentation_preprocessor": [
             "typing"
         ],
         "modelscope.preprocessors.nlp.faq_question_answering_preprocessor": [
             "typing",
             "torch"
         ],
         "modelscope.preprocessors.nlp.feature_extraction_preprocessor": [
             "numpy",
             "typing"
         ],
         "modelscope.preprocessors.nlp.fill_mask_preprocessor": [
             "numpy",
+            "abc",
             "typing",
-            "os",
+            "re",
             "torch",
-            "abc",
-            "re"
+            "os"
         ],
         "modelscope.preprocessors.nlp.mgeo_ranking_preprocessor": [
             "typing",
-            "transformers",
-            "torch"
+            "torch",
+            "transformers"
         ],
         "modelscope.preprocessors.nlp.mglm_summarization_preprocessor": [
-            "re",
+            "typing",
             "os",
-            "typing"
+            "re"
         ],
         "modelscope.preprocessors.nlp.relation_extraction_preprocessor": [
             "typing",
             "transformers"
         ],
         "modelscope.preprocessors.nlp.sentence_embedding_preprocessor": [
             "typing"
@@ -17790,58 +18206,58 @@
         "modelscope.preprocessors.nlp.space.args": [
             "argparse",
             "json"
         ],
         "modelscope.preprocessors.nlp.space.batch": [],
         "modelscope.preprocessors.nlp.space.data_loader": [
             "numpy",
-            "math",
-            "os"
+            "os",
+            "math"
         ],
         "modelscope.preprocessors.nlp.space.dialog_intent_prediction_preprocessor": [
             "typing",
-            "json",
-            "os"
+            "os",
+            "json"
         ],
         "modelscope.preprocessors.nlp.space.dialog_modeling_preprocessor": [
             "typing",
             "os"
         ],
         "modelscope.preprocessors.nlp.space.dialog_state_tracking_preprocessor": [
             "typing"
         ],
         "modelscope.preprocessors.nlp.space.dst_processors": [
+            "logging",
             "numpy",
-            "json",
             "tqdm",
             "re",
-            "logging",
+            "json",
             "six"
         ],
         "modelscope.preprocessors.nlp.space.fields.gen_field": [
             "numpy",
-            "random",
-            "asyncio",
-            "os",
-            "json",
             "itertools",
-            "collections"
+            "collections",
+            "os",
+            "asyncio",
+            "random",
+            "json"
         ],
         "modelscope.preprocessors.nlp.space.fields.intent_field": [
             "numpy",
-            "random",
-            "time",
-            "os",
             "multiprocessing",
-            "json",
-            "itertools",
+            "time",
             "glob",
-            "tqdm",
             "re",
-            "collections"
+            "itertools",
+            "collections",
+            "os",
+            "random",
+            "tqdm",
+            "json"
         ],
         "modelscope.preprocessors.nlp.space.lazy_dataset": [
             "json"
         ],
         "modelscope.preprocessors.nlp.space.preprocess": [
             "glob",
             "os"
@@ -17849,53 +18265,53 @@
         "modelscope.preprocessors.nlp.space.sampler": [
             "numpy"
         ],
         "modelscope.preprocessors.nlp.space.tensorlistdataset": [
             "torch"
         ],
         "modelscope.preprocessors.nlp.space.tokenizer": [
-            "functools",
-            "os",
+            "logging",
             "regex",
-            "unicodedata",
             "sys",
-            "json",
             "__future__",
+            "functools",
             "collections",
-            "logging"
+            "os",
+            "unicodedata",
+            "json"
         ],
         "modelscope.preprocessors.nlp.space_T_cn.fields.database": [
-            "json",
+            "sqlite3",
             "tqdm",
-            "sqlite3"
+            "json"
         ],
         "modelscope.preprocessors.nlp.space_T_cn.fields.schema_link": [
             "re"
         ],
         "modelscope.preprocessors.nlp.space_T_cn.fields.struct": [],
         "modelscope.preprocessors.nlp.space_T_cn.table_question_answering_preprocessor": [
             "typing",
-            "transformers",
             "os",
-            "torch"
+            "torch",
+            "transformers"
         ],
         "modelscope.preprocessors.nlp.space_T_en.conversational_text_to_sql_preprocessor": [
             "typing",
+            "torch",
             "text2sql_lgesql",
             "os",
-            "torch",
             "json"
         ],
         "modelscope.preprocessors.nlp.space_T_en.fields.common_utils": [
-            "nltk",
-            "text2sql_lgesql",
             "numpy",
-            "os",
+            "sqlite3",
             "itertools",
-            "sqlite3"
+            "nltk",
+            "text2sql_lgesql",
+            "os"
         ],
         "modelscope.preprocessors.nlp.space_T_en.fields.parse": [],
         "modelscope.preprocessors.nlp.space_T_en.fields.preprocess_dataset": [
             "text2sql_lgesql"
         ],
         "modelscope.preprocessors.nlp.space_T_en.fields.process_dataset": [
             "pickle",
@@ -17903,19 +18319,24 @@
             "text2sql_lgesql",
             "os"
         ],
         "modelscope.preprocessors.nlp.text_classification_preprocessor": [
             "numpy",
             "typing"
         ],
+        "modelscope.preprocessors.nlp.text_clean": [
+            "sys",
+            "re",
+            "codecs"
+        ],
         "modelscope.preprocessors.nlp.text_error_correction": [
             "typing",
-            "transformers",
             "os",
-            "torch"
+            "torch",
+            "transformers"
         ],
         "modelscope.preprocessors.nlp.text_generation_preprocessor": [
             "numpy",
             "typing",
             "os",
             "torch"
         ],
@@ -17932,78 +18353,78 @@
             "typing"
         ],
         "modelscope.preprocessors.nlp.token_classification_viet_preprocessor": [
             "typing",
             "torch"
         ],
         "modelscope.preprocessors.nlp.transformers_tokenizer": [
-            "transformers",
             "collections",
+            "os",
             "json",
-            "os"
+            "transformers"
         ],
         "modelscope.preprocessors.nlp.translation_evaluation_preprocessor": [
             "typing",
             "transformers"
         ],
         "modelscope.preprocessors.nlp.utils": [
             "numpy",
             "typing",
-            "transformers",
+            "collections",
             "os",
             "json",
-            "collections"
+            "transformers"
         ],
         "modelscope.preprocessors.nlp.word_alignment_preprocessor": [
             "numpy",
             "typing",
-            "os",
+            "itertools",
             "torch",
-            "itertools"
+            "os"
         ],
         "modelscope.preprocessors.nlp.zero_shot_classification_preprocessor": [
             "typing"
         ],
         "modelscope.preprocessors.ofa.asr": [
+            "fairseq",
             "typing",
-            "random",
+            "torch",
+            "soundfile",
             "os",
+            "random",
             "pathlib",
-            "soundfile",
-            "torch",
-            "librosa",
-            "fairseq"
+            "librosa"
         ],
         "modelscope.preprocessors.ofa.base": [
             "numpy",
-            "os",
+            "torchaudio",
+            "re",
             "torch",
-            "json",
+            "string",
             "PIL",
-            "re",
             "io",
-            "torchaudio",
-            "string"
+            "os",
+            "json"
         ],
         "modelscope.preprocessors.ofa.image_captioning": [
             "typing",
             "torchvision",
             "torch"
         ],
         "modelscope.preprocessors.ofa.image_classification": [
             "typing",
-            "functools",
-            "torch",
             "PIL",
+            "torch",
+            "functools",
             "torchvision",
             "timm"
         ],
         "modelscope.preprocessors.ofa.ocr_recognition": [
-            "typing",
             "unicodedata2",
+            "typing",
             "torch",
             "zhconv",
             "torchvision"
         ],
         "modelscope.preprocessors.ofa.sudoku": [
             "numpy",
             "typing",
@@ -18011,18 +18432,18 @@
         ],
         "modelscope.preprocessors.ofa.summarization": [
             "typing",
             "torch"
         ],
         "modelscope.preprocessors.ofa.text2sql": [
             "typing",
-            "random",
-            "os",
+            "re",
             "torch",
-            "re"
+            "os",
+            "random"
         ],
         "modelscope.preprocessors.ofa.text_classification": [
             "typing",
             "torch"
         ],
         "modelscope.preprocessors.ofa.text_to_image_synthesis": [
             "typing",
@@ -18030,339 +18451,340 @@
         ],
         "modelscope.preprocessors.ofa.utils.audio_helper": [
             "numpy",
             "typing",
             "torch"
         ],
         "modelscope.preprocessors.ofa.utils.bridge_content_encoder": [
-            "typing",
-            "functools",
-            "rapidfuzz",
             "difflib",
-            "sqlite3"
+            "rapidfuzz",
+            "sqlite3",
+            "typing",
+            "functools"
         ],
         "modelscope.preprocessors.ofa.utils.collate": [
             "numpy",
             "typing",
             "torch"
         ],
         "modelscope.preprocessors.ofa.utils.constant": [],
         "modelscope.preprocessors.ofa.utils.get_tables": [
-            "traceback",
+            "sqlite3",
             "sys",
-            "sqlite3"
+            "traceback"
         ],
         "modelscope.preprocessors.ofa.utils.random_help": [
-            "torch_xla",
-            "torch"
+            "torch",
+            "torch_xla"
         ],
         "modelscope.preprocessors.ofa.utils.text2phone": [],
         "modelscope.preprocessors.ofa.utils.transforms": [
             "numpy",
             "torchvision",
-            "PIL",
             "random",
-            "torch"
+            "torch",
+            "PIL"
         ],
         "modelscope.preprocessors.ofa.utils.vision_helper": [
             "numpy",
             "cv2"
         ],
         "modelscope.preprocessors.ofa.visual_entailment": [
             "typing",
             "torchvision",
-            "PIL",
-            "torch"
+            "torch",
+            "PIL"
         ],
         "modelscope.preprocessors.ofa.visual_grounding": [
             "numpy",
             "typing",
             "torch",
             "PIL",
             "torchvision"
         ],
         "modelscope.preprocessors.ofa.visual_question_answering": [
             "typing",
             "torchvision",
-            "PIL",
-            "torch"
+            "torch",
+            "PIL"
         ],
         "modelscope.preprocessors.science.uni_fold": [
+            "logging",
             "time",
-            "pathlib",
-            "torch",
+            "re",
+            "os",
             "gzip",
-            "json",
             "pickle",
             "tqdm",
-            "logging",
-            "requests",
             "numpy",
-            "typing",
-            "random",
             "tarfile",
-            "hashlib",
-            "os",
             "ipdb",
             "unittest",
-            "re"
+            "typing",
+            "torch",
+            "requests",
+            "random",
+            "json",
+            "pathlib",
+            "hashlib"
         ],
         "modelscope.preprocessors.tts": [
-            "kantts",
             "typing",
-            "os"
+            "os",
+            "kantts"
         ],
         "modelscope.preprocessors.video": [
             "numpy",
-            "decord",
-            "random",
             "urllib",
-            "os",
-            "tempfile",
             "torch",
+            "tempfile",
+            "os",
+            "uuid",
             "torchvision",
+            "random",
             "math",
-            "uuid"
+            "decord"
         ],
         "modelscope.trainers.audio.ans_trainer": [],
         "modelscope.trainers.audio.asr_trainer": [
             "typing",
             "shutil",
-            "os",
+            "funasr",
             "tempfile",
-            "json",
-            "funasr"
+            "os",
+            "json"
         ],
         "modelscope.trainers.audio.kws_farfield_trainer": [
             "numpy",
+            "glob",
             "typing",
-            "os",
             "torch",
-            "math",
+            "os",
             "pickle",
-            "glob",
+            "math",
             "datetime"
         ],
         "modelscope.trainers.audio.kws_nearfield_trainer": [
             "typing",
-            "os",
-            "copy",
+            "re",
             "torch",
+            "copy",
+            "os",
             "tensorboardX",
             "yaml",
-            "re",
             "datetime"
         ],
         "modelscope.trainers.audio.kws_utils.batch_utils": [
             "numpy",
+            "sys",
             "typing",
-            "os",
             "torch",
-            "sys",
-            "math",
             "collections",
+            "os",
+            "math",
             "datetime"
         ],
         "modelscope.trainers.audio.kws_utils.det_utils": [
             "numpy",
-            "os",
+            "kaldiio",
+            "glob",
             "torch",
+            "os",
             "threading",
             "json",
-            "glob",
-            "kaldiio",
             "matplotlib"
         ],
         "modelscope.trainers.audio.kws_utils.file_utils": [
             "re"
         ],
         "modelscope.trainers.audio.kws_utils.model_utils": [
             "numpy",
+            "glob",
             "shutil",
-            "os",
+            "re",
             "torch",
-            "glob",
-            "yaml",
-            "re"
+            "os",
+            "yaml"
         ],
         "modelscope.trainers.audio.kws_utils.runtime_utils": [
-            "shutil",
-            "os",
             "sys",
-            "json",
-            "stat",
+            "shutil",
             "re",
             "collections",
-            "codecs"
+            "os",
+            "json",
+            "codecs",
+            "stat"
         ],
         "modelscope.trainers.audio.separation_trainer": [
             "numpy",
-            "typing",
             "csv",
-            "os",
+            "torchaudio",
+            "typing",
             "torch",
+            "os",
             "speechbrain",
-            "tqdm",
-            "torchaudio"
+            "tqdm"
         ],
         "modelscope.trainers.audio.tts_trainer": [
             "typing",
             "shutil",
-            "os",
             "tempfile",
             "zipfile",
+            "os",
             "json"
         ],
         "modelscope.trainers.base": [
             "abc",
             "typing",
-            "time",
-            "os"
+            "os",
+            "time"
         ],
         "modelscope.trainers.builder": [],
         "modelscope.trainers.cv.action_detection_trainer": [
+            "fvcore",
             "typing",
-            "detectron2",
+            "torch",
             "os",
-            "fvcore",
-            "torch"
+            "detectron2"
         ],
         "modelscope.trainers.cv.card_detection_scrfd_trainer": [],
         "modelscope.trainers.cv.cartoon_translation_trainer": [
             "numpy",
+            "packaging",
             "typing",
             "os",
-            "packaging",
             "tqdm",
             "tensorflow"
         ],
         "modelscope.trainers.cv.face_detection_scrfd_trainer": [
             "typing",
-            "time",
+            "copy",
             "os",
-            "copy"
+            "time"
         ],
         "modelscope.trainers.cv.image_classifition_trainer": [
             "numpy",
-            "typing",
             "time",
-            "os",
+            "typing",
+            "torch",
             "copy",
-            "torch"
+            "os"
         ],
         "modelscope.trainers.cv.image_defrcn_fewshot_detection_trainer": [
             "typing",
-            "detectron2",
-            "os",
             "torch",
-            "collections"
+            "collections",
+            "os",
+            "detectron2"
         ],
         "modelscope.trainers.cv.image_detection_damoyolo_trainer": [
-            "typing",
             "time",
+            "typing",
+            "torch",
             "os",
             "easydict",
-            "torch",
             "math",
             "datetime"
         ],
         "modelscope.trainers.cv.image_inpainting_trainer": [
-            "time",
             "collections",
-            "torch"
+            "torch",
+            "time"
         ],
         "modelscope.trainers.cv.image_instance_segmentation_trainer": [],
         "modelscope.trainers.cv.image_portrait_enhancement_trainer": [
             "collections",
             "torch"
         ],
         "modelscope.trainers.cv.movie_scene_segmentation_trainer": [],
         "modelscope.trainers.cv.nerf_recon_acc_trainer": [
             "numpy",
-            "typing",
             "time",
-            "random",
+            "glob",
+            "typing",
+            "torch",
             "os",
             "cv2",
-            "torch",
-            "glob",
+            "random",
             "tqdm",
             "datetime"
         ],
         "modelscope.trainers.cv.ocr_detection_db_trainer": [
             "numpy",
-            "typing",
             "time",
-            "os",
-            "copy",
+            "typing",
             "torch",
+            "copy",
+            "os",
             "easydict",
-            "math",
             "tqdm",
+            "math",
             "datetime"
         ],
         "modelscope.trainers.cv.ocr_recognition_trainer": [
-            "time",
             "collections",
-            "torch"
+            "torch",
+            "time"
         ],
         "modelscope.trainers.cv.referring_video_object_segmentation_trainer": [
             "os",
             "torch"
         ],
         "modelscope.trainers.cv.vision_efficient_tuning_trainer": [
             "typing",
             "torch"
         ],
         "modelscope.trainers.default_config": [
             "typing"
         ],
         "modelscope.trainers.easycv.trainer": [
             "typing",
-            "functools",
-            "easycv",
             "torch",
-            "copy"
+            "functools",
+            "copy",
+            "easycv"
         ],
         "modelscope.trainers.easycv.utils.hooks": [],
         "modelscope.trainers.easycv.utils.metric": [
             "numpy",
             "typing",
             "itertools",
             "torch"
         ],
         "modelscope.trainers.easycv.utils.register_util": [
-            "inspect",
-            "logging"
+            "logging",
+            "inspect"
         ],
         "modelscope.trainers.hooks.builder": [],
         "modelscope.trainers.hooks.checkpoint_hook": [
             "numpy",
-            "random",
-            "os",
             "packaging",
+            "time",
+            "re",
             "torch",
-            "re"
+            "os",
+            "random"
         ],
         "modelscope.trainers.hooks.clip_clamp_logit_scale_hook": [
             "torch"
         ],
         "modelscope.trainers.hooks.compression.sparsity_hook": [
             "os"
         ],
         "modelscope.trainers.hooks.compression.utils": [
             "torch"
         ],
         "modelscope.trainers.hooks.ddp_hook": [],
         "modelscope.trainers.hooks.deepspeed_hook": [
-            "megatron_util",
             "deepspeed",
             "shutil",
+            "torch",
             "os",
-            "torch"
+            "megatron_util"
         ],
         "modelscope.trainers.hooks.early_stop_hook": [
             "numpy"
         ],
         "modelscope.trainers.hooks.evaluation_hook": [
             "collections"
         ],
@@ -18371,319 +18793,323 @@
         ],
         "modelscope.trainers.hooks.iter_timer_hook": [
             "time"
         ],
         "modelscope.trainers.hooks.logger.base": [
             "numpy",
             "abc",
-            "numbers",
-            "torch"
+            "torch",
+            "numbers"
         ],
         "modelscope.trainers.hooks.logger.tensorboard_hook": [
             "numpy",
             "os",
             "torch"
         ],
         "modelscope.trainers.hooks.logger.text_logger_hook": [
-            "os",
             "torch",
-            "json",
             "collections",
+            "os",
+            "json",
             "datetime"
         ],
         "modelscope.trainers.hooks.lr_scheduler_hook": [],
         "modelscope.trainers.hooks.megatron_hook": [
-            "torch",
-            "os",
             "copy",
+            "os",
+            "torch",
             "megatron_util"
         ],
         "modelscope.trainers.hooks.optimizer.apex_optimizer_hook": [
-            "torch",
             "logging",
+            "torch",
             "packaging"
         ],
         "modelscope.trainers.hooks.optimizer.base": [
             "logging",
             "torch"
         ],
         "modelscope.trainers.hooks.optimizer.torch_optimizer_hook": [
             "logging"
         ],
         "modelscope.trainers.hooks.priority": [
             "typing",
             "enum"
         ],
         "modelscope.trainers.lrscheduler.builder": [
-            "torch",
             "inspect",
+            "torch",
             "packaging"
         ],
         "modelscope.trainers.lrscheduler.warmup.base": [
             "torch"
         ],
         "modelscope.trainers.lrscheduler.warmup.warmup": [],
         "modelscope.trainers.multi_modal.clip.clip_trainer": [
             "typing",
-            "math",
             "os",
+            "math",
             "torch"
         ],
         "modelscope.trainers.multi_modal.clip.clip_trainer_utils": [
-            "functools",
             "inspect",
-            "os",
             "torch",
+            "functools",
+            "os",
             "math"
         ],
-        "modelscope.trainers.multi_modal.mgeo_ranking_trainer": [
+        "modelscope.trainers.multi_modal.efficient_diffusion_tuning.efficient_diffusion_tuning_trainer": [
             "typing",
-            "dataclasses",
             "torch"
         ],
-        "modelscope.trainers.multi_modal.mplug.mplug_trainer": [
+        "modelscope.trainers.multi_modal.mgeo_ranking_trainer": [
             "typing",
+            "torch",
+            "dataclasses"
+        ],
+        "modelscope.trainers.multi_modal.mplug.mplug_trainer": [
             "collections",
+            "typing",
             "torch"
         ],
         "modelscope.trainers.multi_modal.ofa.ofa_trainer": [
             "typing",
-            "functools",
             "shutil",
-            "os",
-            "tempfile",
             "torch",
-            "json",
-            "math"
+            "functools",
+            "tempfile",
+            "os",
+            "math",
+            "json"
         ],
         "modelscope.trainers.multi_modal.ofa.ofa_trainer_utils": [
             "numpy",
-            "transformers",
-            "math",
-            "shutil",
             "os",
-            "torch"
+            "shutil",
+            "math",
+            "torch",
+            "transformers"
         ],
         "modelscope.trainers.multi_modal.team.team_trainer": [
             "numpy",
             "typing",
             "sklearn",
-            "os",
             "torch",
-            "collections"
+            "collections",
+            "os"
         ],
         "modelscope.trainers.multi_modal.team.team_trainer_utils": [
             "torchvision",
-            "PIL",
-            "torch"
+            "torch",
+            "PIL"
         ],
         "modelscope.trainers.nlp.csanmt_translation_trainer": [
-            "typing",
             "tensorflow",
-            "time",
-            "os"
+            "typing",
+            "os",
+            "time"
         ],
         "modelscope.trainers.nlp.document_grounded_dialog_generate_trainer": [
+            "re",
             "transformers",
             "sacrebleu",
-            "os",
+            "string",
             "torch",
-            "json",
-            "tqdm",
-            "re",
             "collections",
-            "rouge",
-            "string"
+            "os",
+            "tqdm",
+            "json",
+            "rouge"
         ],
         "modelscope.trainers.nlp.document_grounded_dialog_rerank_trainer": [
             "numpy",
-            "typing",
-            "transformers",
             "time",
-            "random",
+            "typing",
+            "torch",
             "os",
-            "torch"
+            "random",
+            "transformers"
         ],
         "modelscope.trainers.nlp.document_grounded_dialog_retrieval_trainer": [
             "numpy",
-            "transformers",
-            "os",
             "torch",
-            "json",
+            "os",
+            "faiss",
             "tqdm",
-            "faiss"
+            "json",
+            "transformers"
         ],
         "modelscope.trainers.nlp.faq_question_answering_trainer": [
             "numpy",
-            "typing",
-            "functools",
             "distutils",
+            "dataclasses",
             "contextlib",
+            "typing",
             "torch",
-            "dataclasses",
+            "functools",
             "collections"
         ],
         "modelscope.trainers.nlp.gpt3_trainer": [
             "typing",
-            "os",
             "copy",
+            "os",
             "torch"
         ],
         "modelscope.trainers.nlp.gpt_moe_trainer": [
             "typing",
-            "megatron_util",
-            "os",
             "torch",
-            "collections"
+            "collections",
+            "os",
+            "megatron_util"
         ],
         "modelscope.trainers.nlp.plug_trainer": [
-            "typing",
-            "megatron_util",
             "deepspeed",
+            "typing",
+            "torch",
             "os",
-            "torch"
+            "megatron_util"
         ],
         "modelscope.trainers.nlp.sentence_embedding_trainer": [
             "numpy",
-            "typing",
-            "transformers",
             "time",
+            "dataclasses",
+            "typing",
             "torch",
             "tqdm",
-            "dataclasses"
+            "transformers"
         ],
         "modelscope.trainers.nlp.sequence_classification_trainer": [
             "numpy",
             "typing",
             "time"
         ],
         "modelscope.trainers.nlp.siamese_uie_trainer": [
             "numpy",
-            "typing",
             "time",
-            "random",
-            "os",
+            "typing",
             "torch",
-            "json",
+            "collections",
+            "os",
+            "random",
             "math",
-            "collections"
+            "json"
         ],
         "modelscope.trainers.nlp.space.dialog_intent_trainer": [
             "numpy",
             "typing",
             "os"
         ],
         "modelscope.trainers.nlp.space.dialog_modeling_trainer": [
             "numpy",
             "typing",
-            "time",
-            "os"
+            "os",
+            "time"
         ],
         "modelscope.trainers.nlp.space.eval": [
             "numpy",
-            "nltk",
             "sklearn",
+            "collections",
+            "nltk",
             "math",
-            "json",
-            "collections"
+            "json"
         ],
         "modelscope.trainers.nlp.space.metrics.metrics_tracker": [
-            "math",
-            "collections"
+            "collections",
+            "math"
         ],
         "modelscope.trainers.nlp.space.trainer.gen_trainer": [
             "numpy",
-            "transformers",
             "time",
-            "os",
             "torch",
-            "json",
+            "collections",
+            "os",
             "tqdm",
-            "collections"
+            "json",
+            "transformers"
         ],
         "modelscope.trainers.nlp.space.trainer.intent_trainer": [
             "numpy",
-            "transformers",
             "time",
-            "os",
             "torch",
-            "json",
+            "collections",
+            "os",
             "tqdm",
-            "collections"
+            "json",
+            "transformers"
         ],
         "modelscope.trainers.nlp.table_question_answering_trainer": [
             "numpy",
-            "typing",
             "time",
-            "os",
+            "typing",
             "torch",
-            "json",
-            "tqdm"
+            "os",
+            "tqdm",
+            "json"
         ],
         "modelscope.trainers.nlp.text_generation_trainer": [
             "collections",
             "torch"
         ],
         "modelscope.trainers.nlp.text_ranking_trainer": [
             "numpy",
-            "typing",
             "time",
+            "dataclasses",
+            "typing",
             "torch",
-            "tqdm",
-            "dataclasses"
+            "tqdm"
         ],
         "modelscope.trainers.nlp_trainer": [
             "numpy",
             "typing",
             "os",
             "torch"
         ],
         "modelscope.trainers.optimizer.builder": [
             "typing",
-            "inspect",
-            "torch"
+            "torch",
+            "inspect"
         ],
         "modelscope.trainers.optimizer.child_tuning_adamw_optimizer": [
             "numpy",
             "typing",
+            "types",
             "torch",
-            "math",
-            "types"
+            "math"
         ],
         "modelscope.trainers.parallel.builder": [
             "torch"
         ],
         "modelscope.trainers.parallel.utils": [],
         "modelscope.trainers.trainer": [
-            "typing",
-            "functools",
             "distutils",
             "inspect",
+            "typing",
+            "torch",
+            "functools",
+            "collections",
             "os",
             "copy",
-            "torch",
-            "json",
-            "collections"
+            "json"
         ],
         "modelscope.trainers.training_args": [
+            "argparse",
+            "dataclasses",
             "typing",
-            "functools",
             "re",
-            "dataclasses",
-            "argparse"
+            "functools"
         ],
         "modelscope.trainers.utils.inference": [
+            "logging",
             "shutil",
-            "os",
             "torch",
-            "pickle",
-            "tqdm",
             "collections",
-            "logging"
+            "os",
+            "pickle",
+            "tqdm"
         ],
         "modelscope.trainers.utils.log_buffer": [
             "numpy",
             "collections"
         ]
     },
-    "version": "1.4.2"
+    "version": "1.5.0"
 }
```

### Comparing `modelscope-1.4.2/modelscope/utils/ast_utils.py` & `modelscope-1.5.0/modelscope/utils/ast_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -609,14 +609,15 @@
     new_files = list(set(files_mtime) - set(origin_files_mtime))
     removed_files = list(set(origin_files_mtime) - set(files_mtime))
     updated_files = []
     for file in origin_files_mtime:
         if file not in removed_files and \
                 (origin_files_mtime[file] != files_mtime[file]):
             updated_files.append(file)
+    removed_files.extend(updated_files)
     updated_files.extend(new_files)
 
     # remove deleted index
     if len(removed_files) > 0:
         remove_index_keys = []
         remove_requirement_keys = []
         for key in index[INDEX_KEY]:
```

### Comparing `modelscope-1.4.2/modelscope/utils/audio/audio_utils.py` & `modelscope-1.5.0/modelscope/utils/audio/audio_utils.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,19 +1,24 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import os
 import re
+import shutil
 import struct
 import sys
 import tempfile
 from typing import Union
 from urllib.parse import urlparse
 
 import numpy as np
 
 from modelscope.fileio.file import HTTPStorage
+from modelscope.utils.hub import snapshot_download
+from modelscope.utils.logger import get_logger
+
+logger = get_logger()
 
 SEGMENT_LENGTH_TRAIN = 16000
 SUPPORT_AUDIO_TYPE_SETS = ('.flac', '.mp3', '.ogg', '.opus', '.wav', '.pcm')
 
 
 class TtsTrainType(object):
     TRAIN_TYPE_SAMBERT = 'train-type-sambert'
@@ -311,7 +316,41 @@
                 storage = HTTPStorage()
                 wav_bytes = storage.read(url)
                 audio_scp = wav_bytes
             else:
                 raise ValueError("Can't download from {}.".format(url))
         audio_scps.append(audio_scp)
     return audio_scps
+
+
+def update_local_model(model_config, model_path, extra_args):
+    if 'update_model' in extra_args:
+        if extra_args['update_model'] == 'latest':
+            model_revision = None
+        else:
+            model_revision = extra_args['update_model']
+        if model_config.__contains__('model'):
+            model_name = model_config['model']
+            if isinstance(model_path, str) and os.path.exists(model_path):
+                try:
+                    logger.info(
+                        'Download the model to local path {0} ...'.format(
+                            model_path))
+                    src_path = snapshot_download(
+                        model_name, revision=model_revision)
+                    # cp to model_path
+                    if src_path == model_path:
+                        logger.warning('src_path is the same with model_path')
+                        return
+                    for filename in os.listdir(src_path):
+                        src_file = os.path.join(src_path, filename)
+                        dst_file = os.path.join(model_path, filename)
+                        if os.path.isfile(src_file):
+                            shutil.copy2(src_file, model_path)
+                        elif os.path.isdir(src_file):
+                            if os.path.exists(dst_file):
+                                shutil.rmtree(dst_file)
+                            shutil.copytree(src_file, dst_file)
+                except Exception as e:
+                    logger.warning(str(e))
+        else:
+            logger.warning('Can not find model name in configuration')
```

### Comparing `modelscope-1.4.2/modelscope/utils/audio/tts_exceptions.py` & `modelscope-1.5.0/modelscope/utils/audio/tts_exceptions.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/checkpoint.py` & `modelscope-1.5.0/modelscope/utils/checkpoint.py`

 * *Files 4% similar despite different names*

```diff
@@ -96,14 +96,17 @@
                 checkpoint['optimizer'][name] = optim.state_dict()
 
         # save lr_scheduler state dict in the checkpoint
         if lr_scheduler is not None and hasattr(lr_scheduler, 'state_dict'):
             checkpoint['lr_scheduler'] = lr_scheduler.state_dict()
 
     if with_model:
+        if isinstance(model, torch.nn.parallel.DistributedDataParallel):
+            model = model.module
+
         _weights = weights_to_cpu(model.state_dict())
         if not with_meta:
             checkpoint = _weights
         else:
             checkpoint['state_dict'] = _weights
 
     with io.BytesIO() as f:
@@ -505,28 +508,63 @@
                 name = '.'.join([prefix, name]) if len(name) > 0 else prefix
 
             if name in module_keys:
                 retrieved_modules.append(module)
 
         return retrieved_modules
 
+    def _tie_or_clone_weights(output_embeddings,
+                              input_embeddings,
+                              torchscript=False):
+        if torchscript:
+            output_embeddings.weight = nn.Parameter(
+                input_embeddings.weight.clone())
+        else:
+            output_embeddings.weight = input_embeddings.weight
+
+        if getattr(output_embeddings, 'bias', None) is not None:
+            output_embeddings.bias.data = nn.functional.pad(
+                output_embeddings.bias.data,
+                (
+                    0,
+                    output_embeddings.weight.shape[0]
+                    - output_embeddings.bias.shape[0],
+                ),
+                'constant',
+                0,
+            )
+
+        if hasattr(output_embeddings, 'out_features') and hasattr(
+                input_embeddings, 'num_embeddings'):
+            output_embeddings.out_features = input_embeddings.num_embeddings
+
+    def tie_weights(model, tie_word_embeddings=False):
+        if tie_word_embeddings:
+            output_embeddings = model.head.get_output_embeddings()
+            if output_embeddings is not None:
+                input_embeddings = model.encoder.get_input_embeddings()
+                _tie_or_clone_weights(output_embeddings, input_embeddings)
+
     # TODO Sharded ckpt
     ckpt_file = os.path.join(model_local_dir, ModelFile.TORCH_MODEL_BIN_FILE)
     state_dict = torch.load(ckpt_file, map_location='cpu')
     if default_dtype is not None:
         torch.set_default_dtype(default_dtype)
 
     missing_keys, unexpected_keys, mismatched_keys, error_msgs = _load_checkpoint(
         model_to_load,
         state_dict,
         load_state_fn=load_state_fn,
         ignore_mismatched_sizes=True,
         _fast_init=True,
     )
 
+    if getattr(kwargs.get('head'), 'tie_word_embeddings', False):
+        tie_weights(model_to_load, kwargs.get('head').tie_word_embeddings)
+
     return {
         'model': model_to_load,
         'missing_keys': missing_keys,
         'unexpected_keys': unexpected_keys,
         'mismatched_keys': mismatched_keys,
         'error_msgs': error_msgs,
     }
```

### Comparing `modelscope-1.4.2/modelscope/utils/chinese_utils.py` & `modelscope-1.5.0/modelscope/utils/chinese_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -33,15 +33,15 @@
         for char in string
     ]).split())
 
 
 def _is_chinese_str(string: str) -> bool:
     return all(
         _is_chinese_char(cp) or cp in CHINESE_PUNCTUATION
-        or cp in ENGLISH_PUNCTUATION or cp for cp in string)
+        or cp in ENGLISH_PUNCTUATION for cp in string)
 
 
 def _is_chinese_char(cp: str) -> bool:
     """Checks whether CP is the codepoint of a CJK character."""
     cp = ord(cp)
     if ((cp >= 0x4E00 and cp <= 0x9FFF) or (cp >= 0x3400 and cp <= 0x4DBF)
             or (cp >= 0x20000 and cp <= 0x2A6DF)
```

### Comparing `modelscope-1.4.2/modelscope/utils/config.py` & `modelscope-1.5.0/modelscope/utils/config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/config_ds.py` & `modelscope-1.5.0/modelscope/utils/config_ds.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/constant.py` & `modelscope-1.5.0/modelscope/utils/constant.py`

 * *Files 1% similar despite different names*

```diff
@@ -35,14 +35,15 @@
     human_object_interaction = 'human-object-interaction'
     face_image_generation = 'face-image-generation'
     body_2d_keypoints = 'body-2d-keypoints'
     body_3d_keypoints = 'body-3d-keypoints'
     hand_2d_keypoints = 'hand-2d-keypoints'
     general_recognition = 'general-recognition'
     human_wholebody_keypoint = 'human-wholebody-keypoint'
+    pedestrian_attribute_recognition = 'pedestrian-attribute-recognition'
 
     image_classification = 'image-classification'
     image_multilabel_classification = 'image-multilabel-classification'
     image_classification_imagenet = 'image-classification-imagenet'
     image_classification_dailylife = 'image-classification-dailylife'
 
     image_object_detection = 'image-object-detection'
@@ -172,14 +173,15 @@
     sentence_similarity = 'sentence-similarity'
     text_classification = 'text-classification'
     sentence_embedding = 'sentence-embedding'
     text_ranking = 'text-ranking'
     relation_extraction = 'relation-extraction'
     zero_shot = 'zero-shot'
     translation = 'translation'
+    competency_aware_translation = 'competency-aware-translation'
     token_classification = 'token-classification'
     transformer_crf = 'transformer-crf'
     conversational = 'conversational'
     text_generation = 'text-generation'
     fid_dialogue = 'fid-dialogue'
     text2text_generation = 'text2text-generation'
     task_oriented_conversation = 'task-oriented-conversation'
@@ -240,14 +242,15 @@
     video_multi_modal_embedding = 'video-multi-modal-embedding'
     image_text_retrieval = 'image-text-retrieval'
     document_vl_embedding = 'document-vl-embedding'
     video_captioning = 'video-captioning'
     video_question_answering = 'video-question-answering'
     video_temporal_grounding = 'video-temporal-grounding'
     text_to_video_synthesis = 'text-to-video-synthesis'
+    efficient_diffusion_tuning = 'efficient-diffusion-tuning'
 
 
 class ScienceTasks(object):
     protein_structure = 'protein-structure'
 
 
 class TasksIODescriptions(object):
@@ -260,14 +263,15 @@
     speech_to_text = 'speech_to_text',
     speech_to_speech = 'speech_to_speech'
     speeches_to_speech = 'speeches_to_speech',
     visual_grounding = 'visual_grounding',
     visual_question_answering = 'visual_question_answering',
     visual_entailment = 'visual_entailment',
     generative_multi_modal_embedding = 'generative_multi_modal_embedding'
+    efficient_diffusion_tuning = 'efficient_diffusion_tuning'
 
 
 class Tasks(CVTasks, NLPTasks, AudioTasks, MultiModalTasks, ScienceTasks):
     """ Names for tasks supported by modelscope.
 
     Holds the standard task name to use for identifying different tasks.
     This should be used to register models, pipelines, trainers.
```

### Comparing `modelscope-1.4.2/modelscope/utils/cv/image_utils.py` & `modelscope-1.5.0/modelscope/utils/cv/image_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -84,26 +84,55 @@
 def realtime_object_detection_bbox_vis(image, bboxes):
     for bbox in bboxes:
         cv2.rectangle(image, (bbox[0], bbox[1]), (bbox[2], bbox[3]),
                       (255, 0, 0), 2)
     return image
 
 
+def draw_attribute(image, box, labels):
+    cv2.rectangle(image, (int(box[0]), int(box[1])),
+                  (int(box[2]), int(box[3])), (0, 0, 255), 2)
+    title = [
+        'gender      : ', 'age         : ', 'orient      : ', 'hat         : ',
+        'glass       : ', 'hand_bag    : ', 'shoulder_bag: ', 'back_pack   : ',
+        'upper_wear  : ', 'lower_wear  : ', 'upper_color : ', 'lower_color : '
+    ]
+
+    clr = (np.random.randint(0, 255), np.random.randint(0, 255),
+           np.random.randint(0, 255))
+
+    point = (int(box[0] + 5), int(box[1] + 20))
+    for idx, lb in enumerate(labels):
+        sz = title[idx] + lb
+        cv2.putText(image, f'{sz}', (point[0], point[1] + idx * 20),
+                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, clr, 1)
+
+
 def draw_keypoints(output, original_image):
     poses = np.array(output[OutputKeys.KEYPOINTS])
     scores = np.array(output[OutputKeys.SCORES])
     boxes = np.array(output[OutputKeys.BOXES])
     assert len(poses) == len(scores) and len(poses) == len(boxes)
     image = cv2.imread(original_image, -1)
     for i in range(len(poses)):
         draw_box(image, np.array(boxes[i]))
         draw_joints(image, np.array(poses[i]), np.array(scores[i]))
     return image
 
 
+def draw_pedestrian_attribute(output, original_image):
+    labels = np.array(output[OutputKeys.LABELS])
+    boxes = np.array(output[OutputKeys.BOXES])
+    assert len(labels) == len(boxes)
+    image = cv2.imread(original_image, -1)
+    for i in range(len(boxes)):
+        draw_attribute(image, np.array(boxes[i]), labels[i])
+    return image
+
+
 def draw_106face_keypoints(in_path,
                            keypoints,
                            boxes,
                            scale=4.0,
                            save_path=None):
     face_contour_point_index = [
         0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,
```

### Comparing `modelscope-1.4.2/modelscope/utils/cv/motion_utils/motion_process.py` & `modelscope-1.5.0/modelscope/utils/cv/motion_utils/motion_process.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/cv/motion_utils/plot_script.py` & `modelscope-1.5.0/modelscope/utils/cv/motion_utils/plot_script.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/cv/motion_utils/rotation_conversions.py` & `modelscope-1.5.0/modelscope/utils/cv/motion_utils/rotation_conversions.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/data_collators.py` & `modelscope-1.5.0/modelscope/utils/data_collators.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/data_utils.py` & `modelscope-1.5.0/modelscope/utils/data_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/demo_utils.py` & `modelscope-1.5.0/modelscope/utils/demo_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/device.py` & `modelscope-1.5.0/modelscope/utils/device.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/error.py` & `modelscope-1.5.0/modelscope/utils/error.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/file_utils.py` & `modelscope-1.5.0/modelscope/utils/file_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/hub.py` & `modelscope-1.5.0/modelscope/utils/hub.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/import_utils.py` & `modelscope-1.5.0/modelscope/utils/import_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/logger.py` & `modelscope-1.5.0/modelscope/utils/logger.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/metric.py` & `modelscope-1.5.0/modelscope/utils/metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/model_tag.py` & `modelscope-1.5.0/modelscope/utils/model_tag.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/nlp/distributed.py` & `modelscope-1.5.0/modelscope/utils/nlp/distributed.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/nlp/load_checkpoint.py` & `modelscope-1.5.0/modelscope/utils/nlp/load_checkpoint.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/nlp/space/args.py` & `modelscope-1.5.0/modelscope/utils/nlp/space/args.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/nlp/space/clean_dataset.py` & `modelscope-1.5.0/modelscope/utils/nlp/space/clean_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/nlp/space/criterions.py` & `modelscope-1.5.0/modelscope/utils/nlp/space/criterions.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/nlp/space/db_ops.py` & `modelscope-1.5.0/modelscope/utils/nlp/space/db_ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/nlp/space/ontology.py` & `modelscope-1.5.0/modelscope/utils/nlp/space/ontology.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/nlp/space/utils.py` & `modelscope-1.5.0/modelscope/utils/nlp/space/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/nlp/space/utils_dst.py` & `modelscope-1.5.0/modelscope/utils/nlp/space/utils_dst.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/nlp/space_T_en/utils.py` & `modelscope-1.5.0/modelscope/utils/nlp/space_T_en/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/nlp/utils.py` & `modelscope-1.5.0/modelscope/utils/nlp/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/plugins.py` & `modelscope-1.5.0/modelscope/utils/plugins.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,13 +1,16 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 # This file is adapted from the AllenNLP library at https://github.com/allenai/allennlp
+# Part of the implementation is borrowed from wimglenn/johnnydep
+
 import copy
 import importlib
 import os
 import pkgutil
+import shutil
 import sys
 import venv
 from contextlib import contextmanager
 from fnmatch import fnmatch
 from pathlib import Path
 from typing import Any, Iterable, List, Optional, Set, Union
 
@@ -244,14 +247,19 @@
                     # skip directly importing private subpackages
                     continue
                 if name.startswith('test'):
                     # skip tests
                     continue
                 subpackage = f'{package_name}.{name}'
                 import_module_and_submodules(subpackage, exclude=exclude)
+        except SystemExit as e:
+            # this case is specific for easy_cv's tools/predict.py exit
+            logger.warning(
+                f'{package_name} not imported: {str(e)}, but should continue')
+            pass
         except Exception as e:
             logger.warning(f'{package_name} not imported: {str(e)}')
             if len(package_name.split('.')) == 1:
                 raise ModuleNotFoundError('Package not installed')
 
 
 def install_module_from_requirements(requirement_path, ):
@@ -259,22 +267,32 @@
     Args:
         requirement_path: The path of requirement file
 
     Returns:
 
     """
 
-    install_args = ['-r', requirement_path]
-    status_code, _, args = PluginsManager.pip_command(
-        'install',
-        install_args,
-    )
-    if status_code != 0:
-        raise ImportError(
-            f'Failed to install requirements from {requirement_path}')
+    install_list = []
+    with open(requirement_path, 'r', encoding='utf-8') as f:
+        requirements = f.read().splitlines()
+        for req in requirements:
+            if req == '':
+                continue
+            installed, _ = PluginsManager.check_plugin_installed(req)
+            if not installed:
+                install_list.append(req)
+
+    if len(install_list) > 0:
+        status_code, _, args = PluginsManager.pip_command(
+            'install',
+            install_list,
+        )
+        if status_code != 0:
+            raise ImportError(
+                f'Failed to install requirements from {requirement_path}')
 
 
 def import_module_from_file(module_name, file_path):
     spec = importlib.util.spec_from_file_location(module_name, file_path)
     module = importlib.util.module_from_spec(spec)
     spec.loader.exec_module(module)
     return module
@@ -294,67 +312,306 @@
     import sys
     sys.path.insert(0, model_dir)
     for file in file_dirs:
         module_name = Path(file).stem
         import_module_from_file(module_name, file)
 
 
-def install_modelscope_if_need():
-    plugin_installed, version = PluginsManager.check_plugin_installed(
-        'modelscope')
-    if not plugin_installed:
-        status_code, _, args = PluginsManager.pip_command(
-            'install',
-            ['modelscope'],
-        )
-        if status_code != 0:
-            raise ImportError('Failed to install package modelscope')
-
-
 def install_requirements_by_names(plugins: List[str]):
     plugins_manager = PluginsManager()
     uninstalled_plugins = []
     for plugin in plugins:
         plugin_installed, version = plugins_manager.check_plugin_installed(
             plugin)
         if not plugin_installed:
             uninstalled_plugins.append(plugin)
     status, _ = plugins_manager.install_plugins(uninstalled_plugins)
     if status != 0:
         raise EnvironmentError(
             f'The required packages {",".join(uninstalled_plugins)} are not installed.',
             f'Please run the command `modelscope plugin install {" ".join(uninstalled_plugins)}` to install them.'
         )
-    install_modelscope_if_need()
 
 
 def install_requirements_by_files(requirements: List[str]):
     for requirement in requirements:
         install_module_from_requirements(requirement)
-    install_modelscope_if_need()
 
 
 def register_plugins_repo(plugins: List[str]) -> None:
     """ Try to install and import plugins from repo"""
     if plugins is not None:
         install_requirements_by_names(plugins)
-        import_plugins(plugins)
+        modules = []
+        for plugin in plugins:
+            modules.extend(get_modules_from_package(plugin))
+        import_plugins(modules)
 
 
 def register_modelhub_repo(model_dir, allow_remote=False) -> None:
     """ Try to install and import remote model from modelhub"""
     if allow_remote:
         try:
             import_module_from_model_dir(model_dir)
         except KeyError:
             logger.warning(
                 'Multi component keys in the hub are registered in same file')
             pass
 
 
+DEFAULT_INDEX = 'https://pypi.org/simple/'
+
+
+def get_modules_from_package(package):
+    """ to get the modules from a installed package
+
+    Args:
+        package: The distribution name or package name
+
+    Returns:
+
+    """
+    from zipfile import ZipFile
+    from tempfile import mkdtemp
+    from subprocess import check_output, STDOUT
+    from glob import glob
+    import hashlib
+    from urllib.parse import urlparse
+    from urllib import request as urllib2
+    from pip._internal.utils.packaging import get_requirement
+    req = get_requirement(package)
+    package = req.name
+
+    def urlretrieve(url, filename, data=None, auth=None):
+        if auth is not None:
+            # https://docs.python.org/2.7/howto/urllib2.html#id6
+            password_mgr = urllib2.HTTPPasswordMgrWithDefaultRealm()
+
+            # Add the username and password.
+            # If we knew the realm, we could use it instead of None.
+            username, password = auth
+            top_level_url = urlparse(url).netloc
+            password_mgr.add_password(None, top_level_url, username, password)
+
+            handler = urllib2.HTTPBasicAuthHandler(password_mgr)
+
+            # create "opener" (OpenerDirector instance)
+            opener = urllib2.build_opener(handler)
+        else:
+            opener = urllib2.build_opener()
+
+        res = opener.open(url, data=data)
+
+        headers = res.info()
+
+        with open(filename, 'wb') as fp:
+            fp.write(res.read())
+
+        return filename, headers
+
+    def compute_checksum(target, algorithm='sha256', blocksize=2**13):
+        hashtype = getattr(hashlib, algorithm)
+        hash_ = hashtype()
+        logger.debug('computing checksum', target=target, algorithm=algorithm)
+        with open(target, 'rb') as f:
+            for chunk in iter(lambda: f.read(blocksize), b''):
+                hash_.update(chunk)
+        result = hash_.hexdigest()
+        logger.debug('computed checksum', result=result)
+        return result
+
+    def _get_pip_version():
+        # try to get pip version without actually importing pip
+        # setuptools gets upset if you import pip before importing setuptools..
+        try:
+            import importlib.metadata  # Python 3.8+
+            return importlib.metadata.version('pip')
+        except Exception:
+            pass
+        import pip
+        return pip.__version__
+
+    def _download_dist(url, scratch_file, index_url, extra_index_url):
+        auth = None
+        if index_url:
+            parsed = urlparse(index_url)
+            if parsed.username and parsed.password and parsed.hostname == urlparse(
+                    url).hostname:
+                # handling private PyPI credentials in index_url
+                auth = (parsed.username, parsed.password)
+        if extra_index_url:
+            parsed = urlparse(extra_index_url)
+            if parsed.username and parsed.password and parsed.hostname == urlparse(
+                    url).hostname:
+                # handling private PyPI credentials in extra_index_url
+                auth = (parsed.username, parsed.password)
+        target, _headers = urlretrieve(url, scratch_file, auth=auth)
+        return target, _headers
+
+    def _get_wheel_args(index_url, env, extra_index_url):
+        args = [
+            sys.executable,
+            '-m',
+            'pip',
+            'wheel',
+            '-vvv',  # --verbose x3
+            '--no-deps',
+            '--no-cache-dir',
+            '--disable-pip-version-check',
+        ]
+        if index_url is not None:
+            args += ['--index-url', index_url]
+            if index_url != DEFAULT_INDEX:
+                hostname = urlparse(index_url).hostname
+                if hostname:
+                    args += ['--trusted-host', hostname]
+        if extra_index_url is not None:
+            args += [
+                '--extra-index-url', extra_index_url, '--trusted-host',
+                urlparse(extra_index_url).hostname
+            ]
+        if env is None:
+            pip_version = _get_pip_version()
+        else:
+            pip_version = dict(env)['pip_version']
+            args[0] = dict(env)['python_executable']
+        pip_major, pip_minor = pip_version.split('.')[0:2]
+        pip_major = int(pip_major)
+        pip_minor = int(pip_minor)
+        if pip_major >= 10:
+            args.append('--progress-bar=off')
+        if (20, 3) <= (pip_major, pip_minor) < (21, 1):
+            # See https://github.com/pypa/pip/issues/9139#issuecomment-735443177
+            args.append('--use-deprecated=legacy-resolver')
+        return args
+
+    def get(dist_name,
+            index_url=None,
+            env=None,
+            extra_index_url=None,
+            tmpdir=None,
+            ignore_errors=False):
+        args = _get_wheel_args(index_url, env, extra_index_url) + [dist_name]
+        scratch_dir = mkdtemp(dir=tmpdir)
+        logger.debug(
+            'wheeling and dealing',
+            scratch_dir=os.path.abspath(scratch_dir),
+            args=' '.join(args))
+        try:
+            out = check_output(
+                args, stderr=STDOUT, cwd=scratch_dir).decode('utf-8')
+        except ChildProcessError as err:
+            out = getattr(err, 'output', b'').decode('utf-8')
+            logger.warning(out)
+            if not ignore_errors:
+                raise
+        logger.debug('wheel command completed ok', dist_name=dist_name)
+        links = []
+        local_links = []
+        lines = out.splitlines()
+        for i, line in enumerate(lines):
+            line = line.strip()
+            if line.startswith('Downloading from URL '):
+                parts = line.split()
+                link = parts[3]
+                links.append(link)
+            elif line.startswith('Downloading '):
+                parts = line.split()
+                last = parts[-1]
+                if len(parts) == 3 and last.startswith('(') and last.endswith(
+                        ')'):
+                    link = parts[-2]
+                elif len(parts) == 4 and parts[-2].startswith(
+                        '(') and last.endswith(')'):
+                    link = parts[-3]
+                    if not urlparse(link).scheme:
+                        # newest pip versions have changed to not log the full url
+                        # in the download event. it is becoming more and more annoying
+                        # to preserve compatibility across a wide range of pip versions
+                        next_line = lines[i + 1].strip()
+                        if next_line.startswith(
+                                'Added ') and ' to build tracker' in next_line:
+                            link = next_line.split(
+                                ' to build tracker')[0].split()[-1]
+                else:
+                    link = last
+                links.append(link)
+            elif line.startswith(
+                    'Source in ') and 'which satisfies requirement' in line:
+                link = line.split()[-1]
+                links.append(link)
+            elif line.startswith('Added ') and ' from file://' in line:
+                [link] = [x for x in line.split() if x.startswith('file://')]
+                local_links.append(link)
+        if not links:
+            # prefer http scheme over file
+            links += local_links
+        links = list(dict.fromkeys(links))  # order-preserving dedupe
+        if not links:
+            logger.warning('could not find download link', out=out)
+            raise Exception('failed to collect dist')
+        if len(links) == 2:
+            # sometimes we collect the same link, once with a url fragment/checksum and once without
+            first, second = links
+            if first.startswith(second):
+                del links[1]
+            elif second.startswith(first):
+                del links[0]
+        if len(links) > 1:
+            logger.debug('more than 1 link collected', out=out, links=links)
+            # Since PEP 517, maybe an sdist will also need to collect other distributions
+            # for the build system, even with --no-deps specified. pendulum==1.4.4 is one
+            # example, which uses poetry and doesn't publish any python37 wheel to PyPI.
+            # However, the dist itself should still be the first one downloaded.
+        link = links[0]
+        whls = glob(os.path.join(os.path.abspath(scratch_dir), '*.whl'))
+        try:
+            [whl] = whls
+        except ValueError:
+            if ignore_errors:
+                whl = ''
+            else:
+                raise
+        url, _sep, checksum = link.partition('#')
+        url = url.replace(
+            '/%2Bf/', '/+f/'
+        )  # some versions of pip did not unquote this fragment in the log
+        if not checksum.startswith('md5=') and not checksum.startswith(
+                'sha256='):
+            # PyPI gives you the checksum in url fragment, as a convenience. But not all indices are so kind.
+            algorithm = 'md5'
+            if os.path.basename(whl).lower() == url.rsplit('/', 1)[-1].lower():
+                target = whl
+            else:
+                scratch_file = os.path.join(scratch_dir, os.path.basename(url))
+                target, _headers = _download_dist(url, scratch_file, index_url,
+                                                  extra_index_url)
+            checksum = compute_checksum(target=target, algorithm=algorithm)
+            checksum = '='.join([algorithm, checksum])
+        result = {'path': whl, 'url': url, 'checksum': checksum}
+        return result
+
+    def discover_import_names(whl_file):
+        logger.debug('finding import names')
+        zipfile = ZipFile(file=whl_file)
+        namelist = zipfile.namelist()
+        [top_level_fname
+         ] = [x for x in namelist if x.endswith('top_level.txt')]
+        all_names = zipfile.read(top_level_fname).decode(
+            'utf-8').strip().splitlines()
+        public_names = [n for n in all_names if not n.startswith('_')]
+        return public_names
+
+    tmpdir = mkdtemp()
+    data = get(package, tmpdir=tmpdir)
+    import_names = discover_import_names(data['path'])
+    shutil.rmtree(tmpdir)
+    return import_names
+
+
 class PluginsManager(object):
 
     def __init__(self,
                  cache_dir=MODELSCOPE_FILE_DIR,
                  plugins_file=PLUGINS_FILENAME):
         cache_dir = os.getenv('MODELSCOPE_CACHE', cache_dir)
         plugins_file = os.getenv('MODELSCOPE_PLUGINS_FILE', plugins_file)
@@ -366,19 +623,43 @@
 
     @file_path.setter
     def file_path(self, value):
         self._file_path = value
 
     @staticmethod
     def check_plugin_installed(package):
+        """ Check if the plugin is installed, and if the version is valid
+
+        Args:
+            package: the package name need to be installed
+
+        Returns:
+
+        """
+
+        if package.split('.')[-1] == 'whl':
+            return False, ''
+
+        from pip._internal.utils.packaging import get_requirement, specifiers
+        req = get_requirement(package)
+
         try:
             importlib.reload(pkg_resources)
-            package_meta_info = pkg_resources.working_set.by_key[package]
+            package_meta_info = pkg_resources.working_set.by_key[req.name]
             version = package_meta_info.version
+
+            # To test if the package is installed
             installed = True
+
+            # If installed, test if the version is correct
+            for spec in req.specifier:
+                installed_valid_version = spec.contains(version)
+                if not installed_valid_version:
+                    installed = False
+                    break
         except KeyError:
             version = ''
             installed = False
 
         return installed, version
 
     @staticmethod
@@ -394,18 +675,27 @@
               such as ['-r', 'requirements']
 
         Returns:
 
         """
         from pip._internal.commands import create_command
         importlib.reload(pkg_resources)
+        if command == 'install':
+            command_args.append('-f')
+            command_args.append(
+                'https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html'
+            )
         command = create_command(command)
         options, args = command.parse_args(command_args)
 
         status_code = command.main(command_args)
+
+        # reload the pkg_resources in order to get the latest pkgs information
+        importlib.reload(pkg_resources)
+
         return status_code, options, args
 
     def install_plugins(self,
                         install_args: List[str],
                         index_url: Optional[str] = None,
                         force_update=False) -> Any:
         """Install packages via pip
@@ -718,7 +1008,8 @@
                 'running the cmd: {} failed, with message: {}'.format(
                     cmd, result))
         return result
 
 
 if __name__ == '__main__':
     install_requirements_by_files(['adaseq'])
+    import_name = get_modules_from_package('pai-easycv')
```

### Comparing `modelscope-1.4.2/modelscope/utils/registry.py` & `modelscope-1.5.0/modelscope/utils/registry.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/regress_test_utils.py` & `modelscope-1.5.0/modelscope/utils/regress_test_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/service_utils.py` & `modelscope-1.5.0/modelscope/utils/service_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/task_utils.py` & `modelscope-1.5.0/modelscope/utils/task_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/tensor_utils.py` & `modelscope-1.5.0/modelscope/utils/tensor_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/test_utils.py` & `modelscope-1.5.0/modelscope/utils/test_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -361,16 +361,19 @@
               num_gpus,
               assert_callback=None,
               save_all_ranks=False,
               *args,
               **kwargs):
         from .torch_utils import _find_free_port
         ip = socket.gethostbyname(socket.gethostname())
-        dist_start_cmd = '%s -m torch.distributed.launch --nproc_per_node=%d --master_addr=\'%s\' --master_port=%s' % (
-            sys.executable, num_gpus, ip, _find_free_port())
+        if 'dist_start_cmd' in kwargs:
+            dist_start_cmd = kwargs.pop('dist_start_cmd')
+        else:
+            dist_start_cmd = '%s -m torch.distributed.launch --nproc_per_node=%d ' \
+                             '--master_addr=\'%s\' --master_port=%s' % (sys.executable, num_gpus, ip, _find_free_port())
 
         return self._start(
             dist_start_cmd=dist_start_cmd,
             func=func,
             num_gpus=num_gpus,
             assert_callback=assert_callback,
             save_all_ranks=save_all_ranks,
```

### Comparing `modelscope-1.4.2/modelscope/utils/timer.py` & `modelscope-1.5.0/modelscope/utils/timer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/torch_utils.py` & `modelscope-1.5.0/modelscope/utils/torch_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -11,16 +11,14 @@
 
 import numpy as np
 import torch
 import torch.multiprocessing as mp
 from packaging import version
 from torch import distributed as dist
 
-from modelscope.utils.megatron_utils import is_megatron_initialized
-
 
 def _find_free_port() -> str:
     # Copied from https://github.com/facebookresearch/detectron2/blob/main/detectron2/engine/launch.py # noqa: E501
     sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
     # Binding to port 0 will cause the OS to find an available port for us
     sock.bind(('', 0))
     port = sock.getsockname()[1]
@@ -128,14 +126,15 @@
     Args:
         group: The parallel group, default None, for the global group
 
     Returns:
         A tuple of the current rank and world_size of the group
     """
     if is_dist():
+        from modelscope.utils.megatron_utils import is_megatron_initialized
         if group is None and is_megatron_initialized():
             from megatron_util import mpu
             group = mpu.get_data_parallel_group()
         rank = dist.get_rank(group)
         world_size = dist.get_world_size(group)
     else:
         rank = 0
```

### Comparing `modelscope-1.4.2/modelscope/utils/trie.py` & `modelscope-1.5.0/modelscope/utils/trie.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope/utils/type_assert.py` & `modelscope-1.5.0/modelscope/utils/type_assert.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.4.2/modelscope.egg-info/PKG-INFO` & `modelscope-1.5.0/modelscope.egg-info/PKG-INFO`

 * *Files 19% similar despite different names*

```diff
@@ -1,303 +1,15 @@
 Metadata-Version: 2.1
 Name: modelscope
-Version: 1.4.2
+Version: 1.5.0
 Summary: UNKNOWN
 Home-page: https://github.com/modelscope/modelscope
 Author: Alibaba ModelScope team
 Author-email: modelscope@list.alibaba-inc.com
 License: Apache License 2.0
-Description: 
-        <p align="center">
-            <br>
-            <img src="https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif" width="400"/>
-            <br>
-        <p>
-        
-        <div align="center">
-        
-        [![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/modelscope/)
-        <!-- [![Documentation Status](https://readthedocs.org/projects/easy-cv/badge/?version=latest)](https://easy-cv.readthedocs.io/en/latest/) -->
-        [![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)
-        [![open issues](https://isitmaintained.com/badge/open/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/issues)
-        [![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/pull/)
-        [![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/modelscope)](https://GitHub.com/modelscope/modelscope/commit/)
-        [![Leaderboard](https://img.shields.io/badge/ModelScope-Check%20Your%20Contribution-orange)](https://opensource.alibaba.com/contribution_leaderboard/details?projectValue=modelscope)
-        
-        <!-- [![GitHub contributors](https://img.shields.io/github/contributors/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/graphs/contributors/) -->
-        <!-- [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) -->
-        
-        <h4 align="center">
-            <p>
-                <b>English</b> |
-                <a href="https://github.com/modelscope/modelscope/blob/master/README_zh.md">中文</a>
-            <p>
-        </h4>
-        
-        
-        </div>
-        
-        # Introduction
-        
-        [ModelScope]( https://www.modelscope.cn) is built upon the notion of “Model-as-a-Service” (MaaS). It seeks to bring together most advanced machine learning models from the AI community, and streamlines the process of leveraging AI models in real-world applications. The core ModelScope library open-sourced in this repository provides the interfaces and implementations that allow developers to perform  model inference, training and evaluation.
-        
-        
-        In particular, with rich layers of API-abstraction, the ModelScope library offers unified experience to explore state-of-the-art models spanning across domains such as CV, NLP, Speech, Multi-Modality, and Scientific-computation. Model contributors of different areas can integrate models into the ModelScope ecosystem through the layered-APIs, allowing easy and unified access to their models. Once integrated, model inference, fine-tuning, and evaluations can be done with only a few lines of codes. In the meantime, flexibilities are also provided so that different components in the model applications can be customized wherever necessary.
-        
-        Apart from harboring implementations of a wide range of different models, ModelScope library also enables the necessary interactions with ModelScope backend services, particularly with the Model-Hub and Dataset-Hub. Such interactions facilitate management of  various entities (models and datasets) to be performed seamlessly under-the-hood, including entity lookup, version control, cache management, and many others.
-        
-        # Models and Online Accessibility
-        
-        Hundreds of models are made publicly available on [ModelScope]( https://www.modelscope.cn)  (700+ and counting), covering the latest development in areas such as NLP, CV, Audio, Multi-modality, and AI for Science, etc. Many of these models represent the SOTA in their specific fields, and made their open-sourced debut on ModelScope. Users can visit ModelScope([modelscope.cn](http://www.modelscope.cn)) and experience first-hand how these models perform via online experience, with just a few clicks. Immediate developer-experience is also possible through the ModelScope Notebook, which is backed by ready-to-use CPU/GPU development environment in the cloud - only one click away on [ModelScope](https://www.modelscope.cn).
-        
-        
-        <p align="center">
-            <br>
-            <img src="data/resource/inference.gif" width="1024"/>
-            <br>
-        <p>
-        
-        Some representative examples include:
-        
-        NLP:
-        
-        * [nlp_gpt3_text-generation_2.7B](https://modelscope.cn/models/damo/nlp_gpt3_text-generation_2.7B)
-        
-        * [ChatYuan-large](https://modelscope.cn/models/ClueAI/ChatYuan-large)
-        
-        * [mengzi-t5-base](https://modelscope.cn/models/langboat/mengzi-t5-base)
-        
-        * [nlp_csanmt_translation_en2zh](https://modelscope.cn/models/damo/nlp_csanmt_translation_en2zh)
-        
-        * [nlp_raner_named-entity-recognition_chinese-base-news](https://modelscope.cn/models/damo/nlp_raner_named-entity-recognition_chinese-base-news)
-        
-        * [nlp_structbert_word-segmentation_chinese-base](https://modelscope.cn/models/damo/nlp_structbert_word-segmentation_chinese-base)
-        
-        * [Erlangshen-RoBERTa-330M-Sentiment](https://modelscope.cn/models/fengshenbang/Erlangshen-RoBERTa-330M-Sentiment)
-        
-        * [nlp_convai_text2sql_pretrain_cn](https://modelscope.cn/models/damo/nlp_convai_text2sql_pretrain_cn)
-        
-        Multi-Modal:
-        
-        * [multi-modal_clip-vit-base-patch16_zh](https://modelscope.cn/models/damo/multi-modal_clip-vit-base-patch16_zh)
-        
-        * [ofa_pretrain_base_zh](https://modelscope.cn/models/damo/ofa_pretrain_base_zh)
-        
-        * [Taiyi-Stable-Diffusion-1B-Chinese-v0.1](https://modelscope.cn/models/fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-v0.1)
-        
-        * [mplug_visual-question-answering_coco_large_en](https://modelscope.cn/models/damo/mplug_visual-question-answering_coco_large_en)
-        
-        CV:
-        
-        * [cv_controlnet_controllable-image-generation_nine-annotators](https://modelscope.cn/models/dienstag/cv_controlnet_controllable-image-generation_nine-annotators/summary)
-        
-        * [cv_tinynas_object-detection_damoyolo](https://modelscope.cn/models/damo/cv_tinynas_object-detection_damoyolo)
-        
-        * [cv_unet_person-image-cartoon_compound-models](https://modelscope.cn/models/damo/cv_unet_person-image-cartoon_compound-models)
-        
-        * [cv_convnextTiny_ocr-recognition-general_damo](https://modelscope.cn/models/damo/cv_convnextTiny_ocr-recognition-general_damo)
-        
-        * [cv_resnet18_human-detection](https://modelscope.cn/models/damo/cv_resnet18_human-detection)
-        
-        * [cv_resnet50_face-detection_retinaface](https://modelscope.cn/models/damo/cv_resnet50_face-detection_retinaface)
-        
-        * [cv_unet_image-matting](https://modelscope.cn/models/damo/cv_unet_image-matting)
-        
-        * [cv_F3Net_product-segmentation](https://modelscope.cn/models/damo/cv_F3Net_product-segmentation)
-        
-        * [cv_resnest101_general_recognition](https://modelscope.cn/models/damo/cv_resnest101_general_recognition)
-        
-        
-        Audio:
-        
-        * [speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch)
-        
-        * [speech_sambert-hifigan_tts_zh-cn_16k](https://modelscope.cn/models/damo/speech_sambert-hifigan_tts_zh-cn_16k)
-        
-        * [speech_charctc_kws_phone-xiaoyun](https://modelscope.cn/models/damo/speech_charctc_kws_phone-xiaoyun)
-        
-        * [u2pp_conformer-asr-cn-16k-online](https://modelscope.cn/models/wenet/u2pp_conformer-asr-cn-16k-online)
-        
-        * [speech_frcrn_ans_cirm_16k](https://modelscope.cn/models/damo/speech_frcrn_ans_cirm_16k)
-        
-        * [speech_dfsmn_aec_psm_16k](https://modelscope.cn/models/damo/speech_dfsmn_aec_psm_16k)
-        
-        
-        
-        AI for Science:
-        
-        * [uni-fold-monomer](https://modelscope.cn/models/DPTech/uni-fold-monomer/summary)
-        
-        * [uni-fold-multimer](https://modelscope.cn/models/DPTech/uni-fold-multimer/summary)
-        
-        **Note:** Most models on ModelScope are public and can be downloaded without account registration on modelscope website([www.modelscope.cn](www.modelscope.cn)), please refer to instructions for [model download](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD), for dowloading models with api provided by modelscope library or git.
-        
-        # QuickTour
-        
-        We provide unified interface for inference using `pipeline`, fine-tuning and evaluation using `Trainer` for different tasks.
-        
-        For any given task with any type of input (image, text, audio, video...), inference pipeline can be implemented with only a few lines of code, which will automatically load the underlying model to get inference result, as is exemplified below:
-        
-        ```python
-        >>> from modelscope.pipelines import pipeline
-        >>> word_segmentation = pipeline('word-segmentation',model='damo/nlp_structbert_word-segmentation_chinese-base')
-        >>> word_segmentation('今天天气不错，适合出去游玩')
-        {'output': '今天 天气 不错 ， 适合 出去 游玩'}
-        ```
-        
-        Given an image, portrait matting (aka. background-removal) can be accomplished with the following code snippet:
-        
-        ![image](data/resource/portrait_input.png)
-        
-        ```python
-        >>> import cv2
-        >>> from modelscope.pipelines import pipeline
-        
-        >>> portrait_matting = pipeline('portrait-matting')
-        >>> result = portrait_matting('https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/image_matting.png')
-        >>> cv2.imwrite('result.png', result['output_img'])
-        ```
-        
-        The output image with the background removed is:
-        ![image](data/resource/portrait_output.png)
-        
-        
-        Fine-tuning and evaluation can also be done with a few more lines of code to set up training dataset and trainer, with the heavy-lifting work of training and evaluation a model encapsulated in the implementation of  `traner.train()` and
-        `trainer.evaluate()`  interfaces.
-        
-        For example, the gpt3 base model (1.3B) can be fine-tuned with the chinese-poetry dataset, resulting in a model that can be used for chinese-poetry generation.
-        
-        ```python
-        >>> from modelscope.metainfo import Trainers
-        >>> from modelscope.msdatasets import MsDataset
-        >>> from modelscope.trainers import build_trainer
-        
-        >>> train_dataset = MsDataset.load('chinese-poetry-collection', split='train'). remap_columns({'text1': 'src_txt'})
-        >>> eval_dataset = MsDataset.load('chinese-poetry-collection', split='test').remap_columns({'text1': 'src_txt'})
-        >>> max_epochs = 10
-        >>> tmp_dir = './gpt3_poetry'
-        
-        >>> kwargs = dict(
-             model='damo/nlp_gpt3_text-generation_1.3B',
-             train_dataset=train_dataset,
-             eval_dataset=eval_dataset,
-             max_epochs=max_epochs,
-             work_dir=tmp_dir)
-        
-        >>> trainer = build_trainer(name=Trainers.gpt3_trainer, default_args=kwargs)
-        >>> trainer.train()
-        ```
-        
-        # Why should I use ModelScope library
-        
-        1. A unified and concise user interface is abstracted for different tasks and different models. Model inferences and training can be implemented by as few as 3 and 10 lines of code, respectively. It is convenient for users to explore models in different fields in the ModelScope community. All models integrated into ModelScope are ready to use, which makes it easy to get started with AI, in both educational and industrial settings.
-        
-        2. ModelScope offers a model-centric development and application experience. It streamlines the support for model training, inference, export and deployment, and facilitates users to build their own MLOps based on the ModelScope ecosystem.
-        
-        3. For the model inference and training process, a modular design is put in place, and a wealth of functional module implementations are provided, which is convenient for users to customize their own model inference, training and other processes.
-        
-        4. For distributed model training, especially for large models, it provides rich training strategy support, including data parallel, model parallel, hybrid parallel and so on.
-        
-        # Installation
-        
-        ## Docker
-        
-        ModelScope Library currently supports popular deep learning framework for model training and inference, including PyTorch, TensorFlow and ONNX. All releases are tested and run on Python 3.7+, Pytorch 1.8+, Tensorflow1.15 or Tensorflow2.0+.
-        
-        To allow out-of-box usage for all the models on ModelScope, official docker images are provided for all releases. Based on the docker image, developers can skip all environment installation and configuration and use it directly. Currently, the latest version of the CPU image and GPU image can be obtained from:
-        
-        CPU docker image
-        ```shell
-        registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py37-torch1.11.0-tf1.15.5-1.3.0
-        ```
-        
-        GPU docker image
-        ```shell
-        registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.3.0-py37-torch1.11.0-tf1.15.5-1.3.0
-        ```
-        
-        ## Setup Local Python Environment
-        
-        One can also set up local ModelScope environment using pip and conda.  We suggest [anaconda](https://docs.anaconda.com/anaconda/install/) for creating local python environment:
-        
-        ```shell
-        conda create -n modelscope python=3.7
-        conda activate modelscope
-        ```
-        
-        PyTorch or TensorFlow can be installed separately according to each model's requirements.
-        * Install pytorch [doc](https://pytorch.org/get-started/locally/)
-        * Install tensorflow [doc](https://www.tensorflow.org/install/pip)
-        
-        After installing the necessary machine-learning framework, you can install modelscope library as follows:
-        
-        If you only want to play around with the modelscope framework, of trying out model/dataset download, you can install the core modelscope components:
-        ```shell
-        pip install modelscope
-        ```
-        
-        If you want to use multi-modal models:
-        ```shell
-        pip install modelscope[multi-modal]
-        ```
-        
-        If you want to use nlp models:
-        ```shell
-        pip install modelscope[nlp] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
-        ```
-        
-        If you want to use cv models:
-        ```shell
-        pip install modelscope[cv] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
-        ```
-        
-        If you want to use audio models:
-        ```shell
-        pip install modelscope[audio] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
-        ```
-        
-        If you want to use science models:
-        ```shell
-        pip install modelscope[science] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
-        ```
-        
-        `Notes`:
-        1. Currently, some audio-task models only support python3.7, tensorflow1.15.4 Linux environments. Most other models can be installed and used on Windows and Mac (x86).
-        
-        2. Some models in the audio field use the third-party library SoundFile for wav file processing. On the Linux system, users need to manually install libsndfile of SoundFile([doc link](https://github.com/bastibe/python-soundfile#installation)). On Windows and MacOS, it will be installed automatically without user operation. For example, on Ubuntu, you can use following commands:
-            ```shell
-            sudo apt-get update
-            sudo apt-get install libsndfile1
-            ```
-        
-        3. Some models in computer vision need mmcv-full, you can refer to mmcv [installation guide](https://github.com/open-mmlab/mmcv#installation), a minimal installation is as follows:
-        
-            ```shell
-            pip uninstall mmcv # if you have installed mmcv, uninstall it
-            pip install -U openmim
-            mim install mmcv-full
-            ```
-        
-        
-        
-        # Learn More
-        
-        We  provide additional documentations including:
-        * [More detailed Installation Guide](https://modelscope.cn/docs/%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85)
-        * [Introduction to tasks](https://modelscope.cn/docs/%E4%BB%BB%E5%8A%A1%E7%9A%84%E4%BB%8B%E7%BB%8D)
-        * [Use pipeline for model inference](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86Pipeline)
-        * [Finetuning example](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83Train)
-        * [Preprocessing of data](https://modelscope.cn/docs/%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86)
-        * [Evaluation](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0)
-        * [Contribute your own model to ModelScope](https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)
-        
-        # License
-        
-        This project is licensed under the [Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE).
-        
 Keywords: python,nlp,science,cv,speech,multi-modal
 Platform: UNKNOWN
 Classifier: Development Status :: 4 - Beta
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.7
@@ -311,7 +23,297 @@
 Provides-Extra: nlp
 Provides-Extra: science
 Provides-Extra: audio_asr
 Provides-Extra: audio_kws
 Provides-Extra: audio_signal
 Provides-Extra: audio_tts
 Provides-Extra: all
+
+
+<p align="center">
+    <br>
+    <img src="https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif" width="400"/>
+    <br>
+<p>
+
+<div align="center">
+
+[![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/modelscope/)
+<!-- [![Documentation Status](https://readthedocs.org/projects/easy-cv/badge/?version=latest)](https://easy-cv.readthedocs.io/en/latest/) -->
+[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)
+[![open issues](https://isitmaintained.com/badge/open/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/issues)
+[![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/pull/)
+[![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/modelscope)](https://GitHub.com/modelscope/modelscope/commit/)
+[![Leaderboard](https://img.shields.io/badge/ModelScope-Check%20Your%20Contribution-orange)](https://opensource.alibaba.com/contribution_leaderboard/details?projectValue=modelscope)
+
+<!-- [![GitHub contributors](https://img.shields.io/github/contributors/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/graphs/contributors/) -->
+<!-- [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) -->
+
+<h4 align="center">
+    <p>
+        <b>English</b> |
+        <a href="https://github.com/modelscope/modelscope/blob/master/README_zh.md">中文</a>
+    <p>
+</h4>
+
+
+</div>
+
+# Introduction
+
+[ModelScope]( https://www.modelscope.cn) is built upon the notion of “Model-as-a-Service” (MaaS). It seeks to bring together most advanced machine learning models from the AI community, and streamlines the process of leveraging AI models in real-world applications. The core ModelScope library open-sourced in this repository provides the interfaces and implementations that allow developers to perform  model inference, training and evaluation.
+
+
+In particular, with rich layers of API-abstraction, the ModelScope library offers unified experience to explore state-of-the-art models spanning across domains such as CV, NLP, Speech, Multi-Modality, and Scientific-computation. Model contributors of different areas can integrate models into the ModelScope ecosystem through the layered-APIs, allowing easy and unified access to their models. Once integrated, model inference, fine-tuning, and evaluations can be done with only a few lines of codes. In the meantime, flexibilities are also provided so that different components in the model applications can be customized wherever necessary.
+
+Apart from harboring implementations of a wide range of different models, ModelScope library also enables the necessary interactions with ModelScope backend services, particularly with the Model-Hub and Dataset-Hub. Such interactions facilitate management of  various entities (models and datasets) to be performed seamlessly under-the-hood, including entity lookup, version control, cache management, and many others.
+
+# Models and Online Accessibility
+
+Hundreds of models are made publicly available on [ModelScope]( https://www.modelscope.cn)  (700+ and counting), covering the latest development in areas such as NLP, CV, Audio, Multi-modality, and AI for Science, etc. Many of these models represent the SOTA in their specific fields, and made their open-sourced debut on ModelScope. Users can visit ModelScope([modelscope.cn](http://www.modelscope.cn)) and experience first-hand how these models perform via online experience, with just a few clicks. Immediate developer-experience is also possible through the ModelScope Notebook, which is backed by ready-to-use CPU/GPU development environment in the cloud - only one click away on [ModelScope](https://www.modelscope.cn).
+
+
+<p align="center">
+    <br>
+    <img src="data/resource/inference.gif" width="1024"/>
+    <br>
+<p>
+
+Some representative examples include:
+
+NLP:
+
+* [nlp_gpt3_text-generation_2.7B](https://modelscope.cn/models/damo/nlp_gpt3_text-generation_2.7B)
+
+* [ChatYuan-large](https://modelscope.cn/models/ClueAI/ChatYuan-large)
+
+* [mengzi-t5-base](https://modelscope.cn/models/langboat/mengzi-t5-base)
+
+* [nlp_csanmt_translation_en2zh](https://modelscope.cn/models/damo/nlp_csanmt_translation_en2zh)
+
+* [nlp_raner_named-entity-recognition_chinese-base-news](https://modelscope.cn/models/damo/nlp_raner_named-entity-recognition_chinese-base-news)
+
+* [nlp_structbert_word-segmentation_chinese-base](https://modelscope.cn/models/damo/nlp_structbert_word-segmentation_chinese-base)
+
+* [Erlangshen-RoBERTa-330M-Sentiment](https://modelscope.cn/models/fengshenbang/Erlangshen-RoBERTa-330M-Sentiment)
+
+* [nlp_convai_text2sql_pretrain_cn](https://modelscope.cn/models/damo/nlp_convai_text2sql_pretrain_cn)
+
+Multi-Modal:
+
+* [multi-modal_clip-vit-base-patch16_zh](https://modelscope.cn/models/damo/multi-modal_clip-vit-base-patch16_zh)
+
+* [ofa_pretrain_base_zh](https://modelscope.cn/models/damo/ofa_pretrain_base_zh)
+
+* [Taiyi-Stable-Diffusion-1B-Chinese-v0.1](https://modelscope.cn/models/fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-v0.1)
+
+* [mplug_visual-question-answering_coco_large_en](https://modelscope.cn/models/damo/mplug_visual-question-answering_coco_large_en)
+
+CV:
+
+* [cv_controlnet_controllable-image-generation_nine-annotators](https://modelscope.cn/models/dienstag/cv_controlnet_controllable-image-generation_nine-annotators/summary)
+
+* [cv_tinynas_object-detection_damoyolo](https://modelscope.cn/models/damo/cv_tinynas_object-detection_damoyolo)
+
+* [cv_unet_person-image-cartoon_compound-models](https://modelscope.cn/models/damo/cv_unet_person-image-cartoon_compound-models)
+
+* [cv_convnextTiny_ocr-recognition-general_damo](https://modelscope.cn/models/damo/cv_convnextTiny_ocr-recognition-general_damo)
+
+* [cv_resnet18_human-detection](https://modelscope.cn/models/damo/cv_resnet18_human-detection)
+
+* [cv_resnet50_face-detection_retinaface](https://modelscope.cn/models/damo/cv_resnet50_face-detection_retinaface)
+
+* [cv_unet_image-matting](https://modelscope.cn/models/damo/cv_unet_image-matting)
+
+* [cv_F3Net_product-segmentation](https://modelscope.cn/models/damo/cv_F3Net_product-segmentation)
+
+* [cv_resnest101_general_recognition](https://modelscope.cn/models/damo/cv_resnest101_general_recognition)
+
+
+Audio:
+
+* [speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch)
+
+* [speech_sambert-hifigan_tts_zh-cn_16k](https://modelscope.cn/models/damo/speech_sambert-hifigan_tts_zh-cn_16k)
+
+* [speech_charctc_kws_phone-xiaoyun](https://modelscope.cn/models/damo/speech_charctc_kws_phone-xiaoyun)
+
+* [u2pp_conformer-asr-cn-16k-online](https://modelscope.cn/models/wenet/u2pp_conformer-asr-cn-16k-online)
+
+* [speech_frcrn_ans_cirm_16k](https://modelscope.cn/models/damo/speech_frcrn_ans_cirm_16k)
+
+* [speech_dfsmn_aec_psm_16k](https://modelscope.cn/models/damo/speech_dfsmn_aec_psm_16k)
+
+
+
+AI for Science:
+
+* [uni-fold-monomer](https://modelscope.cn/models/DPTech/uni-fold-monomer/summary)
+
+* [uni-fold-multimer](https://modelscope.cn/models/DPTech/uni-fold-multimer/summary)
+
+**Note:** Most models on ModelScope are public and can be downloaded without account registration on modelscope website([www.modelscope.cn](www.modelscope.cn)), please refer to instructions for [model download](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD), for dowloading models with api provided by modelscope library or git.
+
+# QuickTour
+
+We provide unified interface for inference using `pipeline`, fine-tuning and evaluation using `Trainer` for different tasks.
+
+For any given task with any type of input (image, text, audio, video...), inference pipeline can be implemented with only a few lines of code, which will automatically load the underlying model to get inference result, as is exemplified below:
+
+```python
+>>> from modelscope.pipelines import pipeline
+>>> word_segmentation = pipeline('word-segmentation',model='damo/nlp_structbert_word-segmentation_chinese-base')
+>>> word_segmentation('今天天气不错，适合出去游玩')
+{'output': '今天 天气 不错 ， 适合 出去 游玩'}
+```
+
+Given an image, portrait matting (aka. background-removal) can be accomplished with the following code snippet:
+
+![image](data/resource/portrait_input.png)
+
+```python
+>>> import cv2
+>>> from modelscope.pipelines import pipeline
+
+>>> portrait_matting = pipeline('portrait-matting')
+>>> result = portrait_matting('https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/image_matting.png')
+>>> cv2.imwrite('result.png', result['output_img'])
+```
+
+The output image with the background removed is:
+![image](data/resource/portrait_output.png)
+
+
+Fine-tuning and evaluation can also be done with a few more lines of code to set up training dataset and trainer, with the heavy-lifting work of training and evaluation a model encapsulated in the implementation of  `traner.train()` and
+`trainer.evaluate()`  interfaces.
+
+For example, the gpt3 base model (1.3B) can be fine-tuned with the chinese-poetry dataset, resulting in a model that can be used for chinese-poetry generation.
+
+```python
+>>> from modelscope.metainfo import Trainers
+>>> from modelscope.msdatasets import MsDataset
+>>> from modelscope.trainers import build_trainer
+
+>>> train_dataset = MsDataset.load('chinese-poetry-collection', split='train'). remap_columns({'text1': 'src_txt'})
+>>> eval_dataset = MsDataset.load('chinese-poetry-collection', split='test').remap_columns({'text1': 'src_txt'})
+>>> max_epochs = 10
+>>> tmp_dir = './gpt3_poetry'
+
+>>> kwargs = dict(
+     model='damo/nlp_gpt3_text-generation_1.3B',
+     train_dataset=train_dataset,
+     eval_dataset=eval_dataset,
+     max_epochs=max_epochs,
+     work_dir=tmp_dir)
+
+>>> trainer = build_trainer(name=Trainers.gpt3_trainer, default_args=kwargs)
+>>> trainer.train()
+```
+
+# Why should I use ModelScope library
+
+1. A unified and concise user interface is abstracted for different tasks and different models. Model inferences and training can be implemented by as few as 3 and 10 lines of code, respectively. It is convenient for users to explore models in different fields in the ModelScope community. All models integrated into ModelScope are ready to use, which makes it easy to get started with AI, in both educational and industrial settings.
+
+2. ModelScope offers a model-centric development and application experience. It streamlines the support for model training, inference, export and deployment, and facilitates users to build their own MLOps based on the ModelScope ecosystem.
+
+3. For the model inference and training process, a modular design is put in place, and a wealth of functional module implementations are provided, which is convenient for users to customize their own model inference, training and other processes.
+
+4. For distributed model training, especially for large models, it provides rich training strategy support, including data parallel, model parallel, hybrid parallel and so on.
+
+# Installation
+
+## Docker
+
+ModelScope Library currently supports popular deep learning framework for model training and inference, including PyTorch, TensorFlow and ONNX. All releases are tested and run on Python 3.7+, Pytorch 1.8+, Tensorflow1.15 or Tensorflow2.0+.
+
+To allow out-of-box usage for all the models on ModelScope, official docker images are provided for all releases. Based on the docker image, developers can skip all environment installation and configuration and use it directly. Currently, the latest version of the CPU image and GPU image can be obtained from:
+
+CPU docker image
+```shell
+registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py37-torch1.11.0-tf1.15.5-1.3.0
+```
+
+GPU docker image
+```shell
+registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.3.0-py37-torch1.11.0-tf1.15.5-1.3.0
+```
+
+## Setup Local Python Environment
+
+One can also set up local ModelScope environment using pip and conda.  We suggest [anaconda](https://docs.anaconda.com/anaconda/install/) for creating local python environment:
+
+```shell
+conda create -n modelscope python=3.7
+conda activate modelscope
+```
+
+PyTorch or TensorFlow can be installed separately according to each model's requirements.
+* Install pytorch [doc](https://pytorch.org/get-started/locally/)
+* Install tensorflow [doc](https://www.tensorflow.org/install/pip)
+
+After installing the necessary machine-learning framework, you can install modelscope library as follows:
+
+If you only want to play around with the modelscope framework, of trying out model/dataset download, you can install the core modelscope components:
+```shell
+pip install modelscope
+```
+
+If you want to use multi-modal models:
+```shell
+pip install modelscope[multi-modal]
+```
+
+If you want to use nlp models:
+```shell
+pip install modelscope[nlp] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
+```
+
+If you want to use cv models:
+```shell
+pip install modelscope[cv] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
+```
+
+If you want to use audio models:
+```shell
+pip install modelscope[audio] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
+```
+
+If you want to use science models:
+```shell
+pip install modelscope[science] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
+```
+
+`Notes`:
+1. Currently, some audio-task models only support python3.7, tensorflow1.15.4 Linux environments. Most other models can be installed and used on Windows and Mac (x86).
+
+2. Some models in the audio field use the third-party library SoundFile for wav file processing. On the Linux system, users need to manually install libsndfile of SoundFile([doc link](https://github.com/bastibe/python-soundfile#installation)). On Windows and MacOS, it will be installed automatically without user operation. For example, on Ubuntu, you can use following commands:
+    ```shell
+    sudo apt-get update
+    sudo apt-get install libsndfile1
+    ```
+
+3. Some models in computer vision need mmcv-full, you can refer to mmcv [installation guide](https://github.com/open-mmlab/mmcv#installation), a minimal installation is as follows:
+
+    ```shell
+    pip uninstall mmcv # if you have installed mmcv, uninstall it
+    pip install -U openmim
+    mim install mmcv-full
+    ```
+
+
+
+# Learn More
+
+We  provide additional documentations including:
+* [More detailed Installation Guide](https://modelscope.cn/docs/%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85)
+* [Introduction to tasks](https://modelscope.cn/docs/%E4%BB%BB%E5%8A%A1%E7%9A%84%E4%BB%8B%E7%BB%8D)
+* [Use pipeline for model inference](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86Pipeline)
+* [Finetuning example](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83Train)
+* [Preprocessing of data](https://modelscope.cn/docs/%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86)
+* [Evaluation](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0)
+* [Contribute your own model to ModelScope](https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)
+
+# License
+
+This project is licensed under the [Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE).
+
+
```

#### html2text {}

```diff
@@ -1,11 +1,20 @@
-Metadata-Version: 2.1 Name: modelscope Version: 1.4.2 Summary: UNKNOWN Home-
+Metadata-Version: 2.1 Name: modelscope Version: 1.5.0 Summary: UNKNOWN Home-
 page: https://github.com/modelscope/modelscope Author: Alibaba ModelScope team
 Author-email: modelscope@list.alibaba-inc.com License: Apache License 2.0
-Description:
+Keywords: python,nlp,science,cv,speech,multi-modal Platform: UNKNOWN
+Classifier: Development Status :: 4 - Beta Classifier: License :: OSI Approved
+:: Apache Software License Classifier: Operating System :: OS Independent
+Classifier: Programming Language :: Python :: 3 Classifier: Programming
+Language :: Python :: 3.7 Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9 Classifier: Programming
+Language :: Python :: 3.10 Description-Content-Type: text/markdown Provides-
+Extra: audio Provides-Extra: cv Provides-Extra: multi-modal Provides-Extra: nlp
+Provides-Extra: science Provides-Extra: audio_asr Provides-Extra: audio_kws
+Provides-Extra: audio_signal Provides-Extra: audio_tts Provides-Extra: all
 
        [https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif]
  [![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/
   modelscope/)  [![license](https://img.shields.io/github/license/modelscope/
 modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)
        [![open issues](https://isitmaintained.com/badge/open/modelscope/
   modelscope.svg)](https://github.com/modelscope/modelscope/issues) [![GitHub
@@ -199,18 +208,8 @@
 (https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83Train)
 * [Preprocessing of data](https://modelscope.cn/docs/
 %E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86) * [Evaluation](https://
 modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0) * [Contribute
 your own model to ModelScope](https://modelscope.cn/docs/
 ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)
 # License This project is licensed under the [Apache License (Version 2.0)]
-(https://github.com/modelscope/modelscope/blob/master/LICENSE). Keywords:
-python,nlp,science,cv,speech,multi-modal Platform: UNKNOWN Classifier:
-Development Status :: 4 - Beta Classifier: License :: OSI Approved :: Apache
-Software License Classifier: Operating System :: OS Independent Classifier:
-Programming Language :: Python :: 3 Classifier: Programming Language :: Python
-:: 3.7 Classifier: Programming Language :: Python :: 3.8 Classifier:
-Programming Language :: Python :: 3.9 Classifier: Programming Language ::
-Python :: 3.10 Description-Content-Type: text/markdown Provides-Extra: audio
-Provides-Extra: cv Provides-Extra: multi-modal Provides-Extra: nlp Provides-
-Extra: science Provides-Extra: audio_asr Provides-Extra: audio_kws Provides-
-Extra: audio_signal Provides-Extra: audio_tts Provides-Extra: all
+(https://github.com/modelscope/modelscope/blob/master/LICENSE).
```

### Comparing `modelscope-1.4.2/modelscope.egg-info/SOURCES.txt` & `modelscope-1.5.0/modelscope.egg-info/SOURCES.txt`

 * *Files 2% similar despite different names*

```diff
@@ -35,23 +35,25 @@
 modelscope/exporters/nlp/sbert_for_zero_shot_classification_exporter.py
 modelscope/fileio/__init__.py
 modelscope/fileio/file.py
 modelscope/fileio/io.py
 modelscope/fileio/format/__init__.py
 modelscope/fileio/format/base.py
 modelscope/fileio/format/json.py
+modelscope/fileio/format/jsonplus.py
 modelscope/fileio/format/yaml.py
 modelscope/hub/__init__.py
 modelscope/hub/api.py
 modelscope/hub/check_model.py
 modelscope/hub/constants.py
 modelscope/hub/deploy.py
 modelscope/hub/errors.py
 modelscope/hub/file_download.py
 modelscope/hub/git.py
+modelscope/hub/push_to_hub.py
 modelscope/hub/repository.py
 modelscope/hub/snapshot_download.py
 modelscope/hub/utils/__init__.py
 modelscope/hub/utils/caching.py
 modelscope/hub/utils/utils.py
 modelscope/metrics/__init__.py
 modelscope/metrics/accuracy_metric.py
@@ -137,14 +139,16 @@
 modelscope/models/audio/punc/__init__.py
 modelscope/models/audio/punc/generic_punctuation.py
 modelscope/models/audio/separation/__init__.py
 modelscope/models/audio/separation/layer_norm.py
 modelscope/models/audio/separation/mossformer.py
 modelscope/models/audio/separation/mossformer_block.py
 modelscope/models/audio/separation/mossformer_conv_module.py
+modelscope/models/audio/sv/DTDNN.py
+modelscope/models/audio/sv/DTDNN_layers.py
 modelscope/models/audio/sv/__init__.py
 modelscope/models/audio/sv/ecapa_tdnn.py
 modelscope/models/audio/sv/generic_speaker_verification.py
 modelscope/models/audio/tts/__init__.py
 modelscope/models/audio/tts/sambert_hifi.py
 modelscope/models/audio/tts/voice.py
 modelscope/models/base/__init__.py
@@ -265,14 +269,15 @@
 modelscope/models/cv/face_detection/retinaface/__init__.py
 modelscope/models/cv/face_detection/retinaface/detection.py
 modelscope/models/cv/face_detection/retinaface/utils.py
 modelscope/models/cv/face_detection/retinaface/models/__init__.py
 modelscope/models/cv/face_detection/retinaface/models/net.py
 modelscope/models/cv/face_detection/retinaface/models/retinaface.py
 modelscope/models/cv/face_detection/scrfd/__init__.py
+modelscope/models/cv/face_detection/scrfd/damofd_detect.py
 modelscope/models/cv/face_detection/scrfd/preprocessor.py
 modelscope/models/cv/face_detection/scrfd/scrfd_detect.py
 modelscope/models/cv/face_detection/scrfd/tinymog_detect.py
 modelscope/models/cv/face_detection/scrfd/mmdet_patch/__init__.py
 modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/__init__.py
 modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/__init__.py
 modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/transforms.py
@@ -283,14 +288,15 @@
 modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/__init__.py
 modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py
 modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py
 modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py
 modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py
 modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/__init__.py
 modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/__init__.py
+modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/master_net.py
 modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py
 modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py
 modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/__init__.py
 modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py
 modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/__init__.py
 modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/base.py
 modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py
@@ -340,25 +346,31 @@
 modelscope/models/cv/face_recognition/torchkit/backbone/facemask_backbone.py
 modelscope/models/cv/face_recognition/torchkit/backbone/model_irse.py
 modelscope/models/cv/face_recognition/torchkit/backbone/model_resnet.py
 modelscope/models/cv/face_reconstruction/__init__.py
 modelscope/models/cv/face_reconstruction/utils.py
 modelscope/models/cv/face_reconstruction/models/__init__.py
 modelscope/models/cv/face_reconstruction/models/bfm.py
+modelscope/models/cv/face_reconstruction/models/de_retouching_module.py
 modelscope/models/cv/face_reconstruction/models/facerecon_model.py
 modelscope/models/cv/face_reconstruction/models/losses.py
 modelscope/models/cv/face_reconstruction/models/networks.py
 modelscope/models/cv/face_reconstruction/models/nv_diffrast.py
 modelscope/models/cv/face_reconstruction/models/opt.py
+modelscope/models/cv/face_reconstruction/models/renderer.py
+modelscope/models/cv/face_reconstruction/models/unet.py
 modelscope/models/cv/face_reconstruction/models/facelandmark/__init__.py
 modelscope/models/cv/face_reconstruction/models/facelandmark/large_base_lmks_infer.py
-modelscope/models/cv/face_reconstruction/models/facelandmark/large_model_infer.py
 modelscope/models/cv/face_reconstruction/models/facelandmark/nets/__init__.py
 modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_base_lmks_net.py
 modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_eyeball_net.py
+modelscope/models/cv/face_reconstruction/models/pix2pix/__init__.py
+modelscope/models/cv/face_reconstruction/models/pix2pix/networks.py
+modelscope/models/cv/face_reconstruction/models/pix2pix/pix2pix_model.py
+modelscope/models/cv/face_reconstruction/models/pix2pix/pix2pix_options.py
 modelscope/models/cv/facial_expression_recognition/__init__.py
 modelscope/models/cv/facial_expression_recognition/fer/__init__.py
 modelscope/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py
 modelscope/models/cv/facial_expression_recognition/fer/transforms.py
 modelscope/models/cv/facial_expression_recognition/fer/vgg.py
 modelscope/models/cv/facial_landmark_confidence/__init__.py
 modelscope/models/cv/facial_landmark_confidence/flc/__init__.py
@@ -795,14 +807,16 @@
 modelscope/models/cv/panorama_depth_estimation/networks/__init__.py
 modelscope/models/cv/panorama_depth_estimation/networks/equi.py
 modelscope/models/cv/panorama_depth_estimation/networks/layers.py
 modelscope/models/cv/panorama_depth_estimation/networks/mobilenet.py
 modelscope/models/cv/panorama_depth_estimation/networks/resnet.py
 modelscope/models/cv/panorama_depth_estimation/networks/unifuse.py
 modelscope/models/cv/panorama_depth_estimation/networks/util.py
+modelscope/models/cv/pedestrian_attribute_recognition/__init__.py
+modelscope/models/cv/pedestrian_attribute_recognition/model.py
 modelscope/models/cv/pointcloud_sceneflow_estimation/__init__.py
 modelscope/models/cv/pointcloud_sceneflow_estimation/common.py
 modelscope/models/cv/pointcloud_sceneflow_estimation/pointnet2_utils.py
 modelscope/models/cv/pointcloud_sceneflow_estimation/rcp_model.py
 modelscope/models/cv/pointcloud_sceneflow_estimation/sf_rcp.py
 modelscope/models/cv/product_retrieval_embedding/__init__.py
 modelscope/models/cv/product_retrieval_embedding/item_detection.py
@@ -1199,14 +1213,16 @@
 modelscope/models/multi_modal/diffusion/diffusion.py
 modelscope/models/multi_modal/diffusion/model.py
 modelscope/models/multi_modal/diffusion/structbert.py
 modelscope/models/multi_modal/diffusion/tokenizer.py
 modelscope/models/multi_modal/diffusion/unet_generator.py
 modelscope/models/multi_modal/diffusion/unet_upsampler_1024.py
 modelscope/models/multi_modal/diffusion/unet_upsampler_256.py
+modelscope/models/multi_modal/efficient_diffusion_tuning/__init__.py
+modelscope/models/multi_modal/efficient_diffusion_tuning/efficient_stable_diffusion.py
 modelscope/models/multi_modal/gemm/__init__.py
 modelscope/models/multi_modal/gemm/gemm_base.py
 modelscope/models/multi_modal/gemm/gemm_model.py
 modelscope/models/multi_modal/gemm/tokenizer.py
 modelscope/models/multi_modal/guided_diffusion/__init__.py
 modelscope/models/multi_modal/guided_diffusion/gaussian_diffusion.py
 modelscope/models/multi_modal/guided_diffusion/respace.py
@@ -1307,14 +1323,18 @@
 modelscope/models/nlp/bert/siamese_uie.py
 modelscope/models/nlp/bert/text_classification.py
 modelscope/models/nlp/bert/text_ranking.py
 modelscope/models/nlp/bert/token_classification.py
 modelscope/models/nlp/bert/word_alignment.py
 modelscope/models/nlp/bloom/__init__.py
 modelscope/models/nlp/bloom/backbone.py
+modelscope/models/nlp/canmt/__init__.py
+modelscope/models/nlp/canmt/canmt_model.py
+modelscope/models/nlp/canmt/canmt_translation.py
+modelscope/models/nlp/canmt/sequence_generator.py
 modelscope/models/nlp/codegeex/__init__.py
 modelscope/models/nlp/codegeex/codegeex.py
 modelscope/models/nlp/codegeex/codegeex_for_code_generation.py
 modelscope/models/nlp/codegeex/codegeex_for_code_translation.py
 modelscope/models/nlp/codegeex/inference.py
 modelscope/models/nlp/codegeex/tokenizer.py
 modelscope/models/nlp/csanmt/__init__.py
@@ -1326,18 +1346,29 @@
 modelscope/models/nlp/deberta_v2/tokenization.py
 modelscope/models/nlp/deberta_v2/tokenization_fast.py
 modelscope/models/nlp/dgds/__init__.py
 modelscope/models/nlp/dgds/backbone.py
 modelscope/models/nlp/dgds/document_grounded_dialog_generate.py
 modelscope/models/nlp/dgds/document_grounded_dialog_rerank.py
 modelscope/models/nlp/dgds/document_grounded_dialog_retrieval.py
+modelscope/models/nlp/fid_T5/__init__.py
+modelscope/models/nlp/fid_T5/text_generation.py
 modelscope/models/nlp/fid_plug/__init__.py
 modelscope/models/nlp/fid_plug/backbone.py
 modelscope/models/nlp/fid_plug/configuration.py
 modelscope/models/nlp/fid_plug/text_generation.py
+modelscope/models/nlp/glm_130b/__init__.py
+modelscope/models/nlp/glm_130b/initialize.py
+modelscope/models/nlp/glm_130b/text_generation.py
+modelscope/models/nlp/glm_130b/generation/__init__.py
+modelscope/models/nlp/glm_130b/generation/strategies.py
+modelscope/models/nlp/glm_130b/kernels/__init__.py
+modelscope/models/nlp/glm_130b/quantization/__init__.py
+modelscope/models/nlp/glm_130b/quantization/functional.py
+modelscope/models/nlp/glm_130b/quantization/layers.py
 modelscope/models/nlp/gpt2/__init__.py
 modelscope/models/nlp/gpt2/backbone.py
 modelscope/models/nlp/gpt3/__init__.py
 modelscope/models/nlp/gpt3/backbone.py
 modelscope/models/nlp/gpt3/configuration.py
 modelscope/models/nlp/gpt3/distributed_gpt3.py
 modelscope/models/nlp/gpt3/text_generation.py
@@ -1364,14 +1395,21 @@
 modelscope/models/nlp/heads/text_classification_head.py
 modelscope/models/nlp/heads/text_generation_head.py
 modelscope/models/nlp/heads/text_ranking_head.py
 modelscope/models/nlp/heads/token_classification_head.py
 modelscope/models/nlp/heads/torch_pretrain_head.py
 modelscope/models/nlp/hf_transformers/__init__.py
 modelscope/models/nlp/hf_transformers/backbone.py
+modelscope/models/nlp/llama/__init__.py
+modelscope/models/nlp/llama/backbone.py
+modelscope/models/nlp/llama/configuration.py
+modelscope/models/nlp/llama/convert_llama_weights_to_hf.py
+modelscope/models/nlp/llama/text_generation.py
+modelscope/models/nlp/llama/tokenization.py
+modelscope/models/nlp/llama/tokenization_fast.py
 modelscope/models/nlp/lstm/__init__.py
 modelscope/models/nlp/lstm/backbone.py
 modelscope/models/nlp/lstm/token_classification.py
 modelscope/models/nlp/megatron_bert/__init__.py
 modelscope/models/nlp/megatron_bert/backbone.py
 modelscope/models/nlp/megatron_bert/configuration.py
 modelscope/models/nlp/megatron_bert/fill_mask.py
@@ -1655,20 +1693,30 @@
 modelscope/msdatasets/utils/oss_utils.py
 modelscope/msdatasets/utils/upload_utils.py
 modelscope/ops/__init__.py
 modelscope/ops/ailut/__init__.py
 modelscope/ops/ailut/pyinterfaces.py
 modelscope/ops/ailut/Ailut/__init__.py
 modelscope/ops/ailut/Ailut/csrc/__init__.py
+modelscope/ops/ailut/Ailut/csrc/ailut_transform.cpp
+modelscope/ops/ailut/Ailut/csrc/ailut_transform_cpu.cpp
+modelscope/ops/ailut/Ailut/csrc/ailut_transform_cuda.cu
 modelscope/ops/quadtree_attention/__init__.py
 modelscope/ops/quadtree_attention/functions/__init__.py
 modelscope/ops/quadtree_attention/functions/quadtree_attention.py
 modelscope/ops/quadtree_attention/modules/__init__.py
 modelscope/ops/quadtree_attention/modules/quadtree_attention.py
 modelscope/ops/quadtree_attention/src/__init__.py
+modelscope/ops/quadtree_attention/src/score_computation.cpp
+modelscope/ops/quadtree_attention/src/score_computation.h
+modelscope/ops/quadtree_attention/src/score_computation_kernal.cu
+modelscope/ops/quadtree_attention/src/utils.h
+modelscope/ops/quadtree_attention/src/value_aggregation.cpp
+modelscope/ops/quadtree_attention/src/value_aggregation.h
+modelscope/ops/quadtree_attention/src/value_aggregation_kernel.cu
 modelscope/outputs/__init__.py
 modelscope/outputs/cv_outputs.py
 modelscope/outputs/nlp_outputs.py
 modelscope/outputs/outputs.py
 modelscope/pipelines/__init__.py
 modelscope/pipelines/base.py
 modelscope/pipelines/builder.py
@@ -1778,14 +1826,15 @@
 modelscope/pipelines/cv/movie_scene_segmentation_pipeline.py
 modelscope/pipelines/cv/mtcnn_face_detection_pipeline.py
 modelscope/pipelines/cv/nerf_recon_acc_pipeline.py
 modelscope/pipelines/cv/object_detection_3d_pipeline.py
 modelscope/pipelines/cv/ocr_detection_pipeline.py
 modelscope/pipelines/cv/ocr_recognition_pipeline.py
 modelscope/pipelines/cv/panorama_depth_estimation_pipeline.py
+modelscope/pipelines/cv/pedestrian_attribute_recognition_pipeline.py
 modelscope/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py
 modelscope/pipelines/cv/product_retrieval_embedding_pipeline.py
 modelscope/pipelines/cv/product_segmentation_pipeline.py
 modelscope/pipelines/cv/realtime_video_object_detection_pipeline.py
 modelscope/pipelines/cv/referring_video_object_segmentation_pipeline.py
 modelscope/pipelines/cv/retina_face_detection_pipeline.py
 modelscope/pipelines/cv/shop_segmentation_pipleline.py
@@ -1839,14 +1888,15 @@
 modelscope/pipelines/cv/ocr_utils/ocr_modules/timm_tinyc.py
 modelscope/pipelines/cv/ocr_utils/ocr_modules/vitstr.py
 modelscope/pipelines/cv/tbs_detection_utils/__init__.py
 modelscope/pipelines/cv/tbs_detection_utils/utils.py
 modelscope/pipelines/multi_modal/__init__.py
 modelscope/pipelines/multi_modal/asr_pipeline.py
 modelscope/pipelines/multi_modal/document_vl_embedding_pipeline.py
+modelscope/pipelines/multi_modal/efficient_diffusion_tuning_pipeline.py
 modelscope/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py
 modelscope/pipelines/multi_modal/gridvlp_pipeline.py
 modelscope/pipelines/multi_modal/image_captioning_pipeline.py
 modelscope/pipelines/multi_modal/image_text_retrieval_pipeline.py
 modelscope/pipelines/multi_modal/mgeo_ranking_pipeline.py
 modelscope/pipelines/multi_modal/multi_modal_embedding_pipeline.py
 modelscope/pipelines/multi_modal/ocr_recognition_pipeline.py
@@ -1868,14 +1918,15 @@
 modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py
 modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py
 modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/__init__.py
 modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py
 modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/utils.py
 modelscope/pipelines/nlp/__init__.py
 modelscope/pipelines/nlp/automatic_post_editing_pipeline.py
+modelscope/pipelines/nlp/canmt_translation_pipeline.py
 modelscope/pipelines/nlp/codegeex_code_generation_pipeline.py
 modelscope/pipelines/nlp/codegeex_code_translation_pipeline.py
 modelscope/pipelines/nlp/conversational_text_to_sql_pipeline.py
 modelscope/pipelines/nlp/dialog_intent_prediction_pipeline.py
 modelscope/pipelines/nlp/dialog_modeling_pipeline.py
 modelscope/pipelines/nlp/dialog_state_tracking_pipeline.py
 modelscope/pipelines/nlp/distributed_gpt3_pipeline.py
@@ -1887,14 +1938,15 @@
 modelscope/pipelines/nlp/document_segmentation_pipeline.py
 modelscope/pipelines/nlp/extractive_summarization_pipeline.py
 modelscope/pipelines/nlp/faq_question_answering_pipeline.py
 modelscope/pipelines/nlp/fasttext_text_classification_pipeline.py
 modelscope/pipelines/nlp/feature_extraction_pipeline.py
 modelscope/pipelines/nlp/fid_dialogue_pipeline.py
 modelscope/pipelines/nlp/fill_mask_pipeline.py
+modelscope/pipelines/nlp/glm130b_text_generation_pipeline.py
 modelscope/pipelines/nlp/information_extraction_pipeline.py
 modelscope/pipelines/nlp/interactive_translation_pipeline.py
 modelscope/pipelines/nlp/language_identification_pipline.py
 modelscope/pipelines/nlp/mglm_text_summarization_pipeline.py
 modelscope/pipelines/nlp/named_entity_recognition_pipeline.py
 modelscope/pipelines/nlp/sentence_embedding_pipeline.py
 modelscope/pipelines/nlp/siamese_uie_pipeline.py
@@ -1939,28 +1991,30 @@
 modelscope/preprocessors/cv/util.py
 modelscope/preprocessors/cv/video_stabilization.py
 modelscope/preprocessors/cv/video_super_resolution.py
 modelscope/preprocessors/movie_scene_segmentation/__init__.py
 modelscope/preprocessors/movie_scene_segmentation/transforms.py
 modelscope/preprocessors/nlp/__init__.py
 modelscope/preprocessors/nlp/bert_seq_cls_tokenizer.py
+modelscope/preprocessors/nlp/canmt_translation.py
 modelscope/preprocessors/nlp/dialog_classification_use_preprocessor.py
 modelscope/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py
 modelscope/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py
 modelscope/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py
 modelscope/preprocessors/nlp/document_segmentation_preprocessor.py
 modelscope/preprocessors/nlp/faq_question_answering_preprocessor.py
 modelscope/preprocessors/nlp/feature_extraction_preprocessor.py
 modelscope/preprocessors/nlp/fill_mask_preprocessor.py
 modelscope/preprocessors/nlp/mgeo_ranking_preprocessor.py
 modelscope/preprocessors/nlp/mglm_summarization_preprocessor.py
 modelscope/preprocessors/nlp/relation_extraction_preprocessor.py
 modelscope/preprocessors/nlp/sentence_embedding_preprocessor.py
 modelscope/preprocessors/nlp/siamese_uie_preprocessor.py
 modelscope/preprocessors/nlp/text_classification_preprocessor.py
+modelscope/preprocessors/nlp/text_clean.py
 modelscope/preprocessors/nlp/text_error_correction.py
 modelscope/preprocessors/nlp/text_generation_preprocessor.py
 modelscope/preprocessors/nlp/text_ranking_preprocessor.py
 modelscope/preprocessors/nlp/token_classification_preprocessor.py
 modelscope/preprocessors/nlp/token_classification_thai_preprocessor.py
 modelscope/preprocessors/nlp/token_classification_viet_preprocessor.py
 modelscope/preprocessors/nlp/transformers_tokenizer.py
@@ -2100,14 +2154,16 @@
 modelscope/trainers/lrscheduler/warmup/base.py
 modelscope/trainers/lrscheduler/warmup/warmup.py
 modelscope/trainers/multi_modal/__init__.py
 modelscope/trainers/multi_modal/mgeo_ranking_trainer.py
 modelscope/trainers/multi_modal/clip/__init__.py
 modelscope/trainers/multi_modal/clip/clip_trainer.py
 modelscope/trainers/multi_modal/clip/clip_trainer_utils.py
+modelscope/trainers/multi_modal/efficient_diffusion_tuning/__init__.py
+modelscope/trainers/multi_modal/efficient_diffusion_tuning/efficient_diffusion_tuning_trainer.py
 modelscope/trainers/multi_modal/mplug/__init__.py
 modelscope/trainers/multi_modal/mplug/mplug_trainer.py
 modelscope/trainers/multi_modal/ofa/__init__.py
 modelscope/trainers/multi_modal/ofa/ofa_trainer.py
 modelscope/trainers/multi_modal/ofa/ofa_trainer_utils.py
 modelscope/trainers/multi_modal/team/__init__.py
 modelscope/trainers/multi_modal/team/team_trainer.py
@@ -2141,14 +2197,18 @@
 modelscope/trainers/optimizer/child_tuning_adamw_optimizer.py
 modelscope/trainers/parallel/__init__.py
 modelscope/trainers/parallel/builder.py
 modelscope/trainers/parallel/utils.py
 modelscope/trainers/utils/__init__.py
 modelscope/trainers/utils/inference.py
 modelscope/trainers/utils/log_buffer.py
+modelscope/tuners/__init__.py
+modelscope/tuners/control_sd_lora.py
+modelscope/tuners/lora.py
+modelscope/tuners/sd_lora.py
 modelscope/utils/__init__.py
 modelscope/utils/ast_index_file.py
 modelscope/utils/ast_utils.py
 modelscope/utils/checkpoint.py
 modelscope/utils/chinese_utils.py
 modelscope/utils/config.py
 modelscope/utils/config_ds.py
```

### Comparing `modelscope-1.4.2/modelscope.egg-info/requires.txt` & `modelscope-1.5.0/modelscope.egg-info/requires.txt`

 * *Files 7% similar despite different names*

```diff
@@ -1,22 +1,25 @@
 addict
 attrs
 datasets<=2.8.0,>=2.7.0
 einops
 filelock>=3.3.0
 gast>=0.2.2
-jsonplus
+mmdet<=2.28.2
 numpy<1.24.0
 oss2
 Pillow>=6.2.0
 pyarrow!=9.0.0,>=6.0.0
+python-dateutil>=2.1
 pyyaml
 requests
 scipy
-setuptools
+setuptools==59.8.0
+simplejson>=3.3.0
+sortedcontainers>=1.5.9
 tqdm>=4.64.0
 yapf
 
 [all]
 accelerate
 albumentations>=1.0.3
 av>=9.2.0
@@ -42,15 +45,15 @@
 lap
 lmdb
 lpips
 ml_collections
 mmcls>=0.21.0
 mmdet>=2.25.0
 mmdet3d==1.0.0a1
-mmsegmentation
+mmsegmentation<=0.30.0
 moviepy>=1.0.3
 nerfacc==0.2.2
 networkx
 numba
 omegaconf
 onnx
 onnxruntime>=1.10
@@ -79,15 +82,15 @@
 torchvision
 transformers>=4.26.0
 trimesh
 ujson
 utils
 videofeatures_clipit>=1.0
 accelerate
-diffusers>=0.11.1
+diffusers<0.15.0,>=0.13.1
 ftfy>=6.0.3
 librosa<=0.9.2
 opencv-python
 pycocoevalcap>=1.2
 pycocotools>=2.0.4
 pytorch_lightning<=1.7.7
 rapidfuzz
@@ -132,15 +135,15 @@
 ml_collections
 scipy
 tensorboardX
 tokenizers
 
 [audio]
 easyasr>=0.0.2
-funasr>=0.3.0
+funasr>=0.4.0
 kaldiio
 kwsbp>=0.0.6
 matplotlib
 numpy
 py_sound_connect>=0.1
 scipy
 SoundFile>0.10
@@ -183,15 +186,15 @@
 traitlets>=5.3.0
 ttsfrd>=0.1.2
 unidecode
 wcwidth>=0.2.5
 
 [audio_asr]
 easyasr>=0.0.2
-funasr>=0.3.0
+funasr>=0.4.0
 
 [audio_kws]
 kaldiio
 kwsbp>=0.0.6
 matplotlib
 numpy
 py_sound_connect>=0.1
@@ -268,15 +271,15 @@
 lap
 lmdb
 lpips
 ml_collections
 mmcls>=0.21.0
 mmdet>=2.25.0
 mmdet3d==1.0.0a1
-mmsegmentation
+mmsegmentation<=0.30.0
 moviepy>=1.0.3
 nerfacc==0.2.2
 networkx
 numba
 omegaconf
 onnx
 onnxruntime>=1.10
@@ -307,15 +310,15 @@
 trimesh
 ujson
 utils
 videofeatures_clipit>=1.0
 
 [multi-modal]
 accelerate
-diffusers>=0.11.1
+diffusers<0.15.0,>=0.13.1
 ftfy>=6.0.3
 librosa<=0.9.2
 opencv-python
 pycocoevalcap>=1.2
 pycocotools>=2.0.4
 pytorch_lightning<=1.7.7
 rapidfuzz
```

